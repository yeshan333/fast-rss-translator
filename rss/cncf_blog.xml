<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>CNCF - Blog</title>
    <link>http://rsshub.rssforever.com/cncf</link>
    <description>CNCF - Blog - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)</description>
    <managingEditor>i@diygod.me (DIYgod)</managingEditor>
    <item>
      <title>„ÄêSlurm: An HPC workload manager„ÄëSlurmÔºöHPC Â∑•‰ΩúË¥üËΩΩÁÆ°ÁêÜÂô®</title>
      <link>https://www.cncf.io/blog/2024/07/08/slurm-an-hpc-workload-manager/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/&#34;&gt;SuperOrbital‚Äôs blog&lt;/a&gt; by Sean Kane&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/cover.jpg&#34; alt=&#34;datacenter servers&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this article, we are going to explore&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm&#34;&gt;Slurm&lt;/a&gt;, a popular open-source high-performance computing (HPC&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:hpc&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;) workload manager, and discover what it is, why people use it, and how it differs from Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For context, when we talk about high-performance computing, we are primarily talking about computational tasks that require a lot of time on very powerful computer systems and are often designed, so that they can be&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Parallel_computing&#34;&gt;highly parallelized&lt;/a&gt;, across resources within a super-computer or large cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These sorts of jobs are very common in scientific research,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_simulation&#34;&gt;computer simulation&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_intelligence&#34;&gt;artificial intelligence&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&gt;machine learning&lt;/a&gt;&amp;nbsp;tasks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;what-is-slurm&#34;&gt;What is Slurm?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To almost directly quote the Slurm repository&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm/blob/master/README.rst#slurm-workload-manager&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;Slurm is an open-source cluster resource management and job scheduling system for Linux (&lt;em&gt;primarily&lt;/em&gt;) that strives to be simple, scalable, portable, fault-tolerant, and interconnect agnostic. As a cluster resource manager, Slurm provides three key functions:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Second, it provides a framework for starting, executing, and monitoring work (&lt;em&gt;normally a parallel job&lt;/em&gt;) on the set of allocated nodes.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;And finally, it arbitrates conflicting requests for resources by managing a queue of pending work.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In short, Slurm is a system that is primarily designed to manage&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Batch_processing&#34;&gt;batch processing&lt;/a&gt;&amp;nbsp;workloads running in a shared&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_cluster&#34;&gt;computing cluster&lt;/a&gt;. Slurm can be configured to run any type of batch job but is often utilized on systems that support multiple users with parallel computing tasks to run.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;According to&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;, the primary support and development company for the Slurm community, Slurm is being used on approximately 65% of the&amp;nbsp;&lt;a href=&#34;https://top500.org/lists/top500/2024/06/&#34;&gt;TOP500 supercomputers in the world&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;a-bit-of-history&#34;&gt;A bit of history&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the mid-1990s, as Linux became more popular, there was an increased interest in making large-scale computer processing more accessible through the use of commodity hardware and projects like&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Beowulf_cluster&#34;&gt;Beowulf&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In 2001, engineers at the&amp;nbsp;&lt;a href=&#34;https://www.llnl.gov/&#34;&gt;Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;(LLNL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:llnl&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;) in the United States, started looking into how they might migrate from proprietary HPC systems to open-source solutions. They determined that there weren‚Äôt any good resource managers available at the time, and this directly led to the creation of Slurm, which was initially released in 2002.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;a href=&#34;https://slurm.schedmd.com/slurm_design.pdf&#34;&gt;The name Slurm is a reference&lt;/a&gt;&amp;nbsp;to the&amp;nbsp;&lt;a href=&#34;https://futurama.fandom.com/wiki/Slurm&#34;&gt;soda&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://www.imdb.com/title/tt0149460/&#34;&gt;Futurama&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the years, Slurm has evolved to support a wide variety of hardware configurations and in 2010, some initial scheduling logic was added to Slurm, enabling it to function as both a stand-alone resource manager and a scheduler&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:history&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-architecture&#34;&gt;The Architecture&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Like many similar systems, a Slurm setup consists of at least one controller node, and at least one compute node, which can be on the same physical systems.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/slurm-layout.gif&#34; alt=&#34;Diagram of the basic Slurm architecture&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;the-controller&#34;&gt;The Controller&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Two daemons are responsible for managing the cluster. These two processes can be run on a single node or separated, depending on the performance and resiliency required by the administrator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmctld&#34;&gt;slurmctld&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmctld&lt;/code&gt;&amp;nbsp;is the primary management daemon for Slurm. It is responsible for all the daemons, resources, and jobs, within the cluster. Because it is a critical component in the cluster, it is often configured with a passive backup that can take over management if the primary daemon ever fails.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmdbd&#34;&gt;slurmdbd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmdbd&lt;/code&gt;&amp;nbsp;provides an interface for the&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/MySQL&#34;&gt;MySQL database&lt;/a&gt;&amp;nbsp;that Slurm uses for stateful data, like job accounting records.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;compute-nodes&#34;&gt;Compute Nodes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These represent the nodes whose resources are available to any jobs that might need to be scheduled in the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmd&#34;&gt;slurmd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmd&lt;/code&gt;&amp;nbsp;is the daemon that runs on each compute node and is responsible for accepting work, launching tasks, and generally managing the workloads on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SchedMD maintains the core documentation for Slurm and the&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/quickstart_admin.html#quick_start&#34;&gt;current installation documentation&lt;/a&gt;&amp;nbsp;can be found on their website.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Getting Slurm working is not particularly difficult, but fine-tuning it for your requirements can be a time-consuming task. Since performance is often a primary focus with these clusters and Slurm is designed to work with a very wide set of hardware and software libraries, it can take a lot of effort to fine-tune the installation, so that it utilizes the hardware to the best of its ability, and all the proper software is available to streamline the types of jobs that will be run on the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;At a minimum clusters will require node name resolution, a MySQL instance, shared block storage, a synchronized time source, synchronized users, the&amp;nbsp;&lt;a href=&#34;https://dun.github.io/munge/&#34;&gt;Munge&lt;/a&gt;&amp;nbsp;daemon for user authentication, a full set of custom-built Slurm executables, and various configuration files.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In addition to this, the nodes will need to contain the various hardware, programming languages, libraries, and other tools that the majority of jobs will require to run. This could include things like GPUs&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:gpu&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;and their related toolchains (CUDA&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cuda&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;/cuDNN&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cudnn&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;), MPI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:mpi&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;libraries, OCI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:oci&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;runtimes for&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/containers.html&#34;&gt;container support&lt;/a&gt;, and much more.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm‚Äôs greatest asset and liability is how flexible it is. This makes it something that can be highly optimized for the requirements of the organization, while also making it susceptible to misconfiguration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;workloads&#34;&gt;Workloads&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To provide some context, let‚Äôs take a look at an example user job and workflow that utilizes some of the features of Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the&amp;nbsp;&lt;code&gt;mpi-ping-pong.py&lt;/code&gt;&amp;nbsp;code below, you will find a&amp;nbsp;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt;&amp;nbsp;script that simulates a pseudo-game of&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Table_tennis&#34;&gt;Ping Pong/Table Tennis&lt;/a&gt;&amp;nbsp;by making use of MPI to pass a message (&lt;em&gt;‚Äúball‚Äù&lt;/em&gt;) between as many processes as we have configured to participate in the parallel job.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;local-testing&#34;&gt;Local Testing&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run this code, you simply need to have&amp;nbsp;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/quickstart.html&#34;&gt;OpenMPI&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://mpi4py.readthedocs.io/en/stable/install.html&#34;&gt;mpi4py&lt;/a&gt;&amp;nbsp;installed on your system.mpi-ping-pong.py&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/mpi-ping-pong.py&#34;&gt;mpi-ping-pong.py&lt;/a&gt;&amp;nbsp;is what many would refer to as an&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Embarrassingly_parallel&#34;&gt;embarrassingly parallel&lt;/a&gt;&amp;nbsp;workload because it can be scaled with no real effort at all.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you run this Python application on your local system, you will see something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ python ./mpi-ping-pong.py&#xA;&#xA;Hello, World! I am üèì Player (Rank)  0 . Process  1  of  1  on  laptop.local&#xA;Player (Rank)  0 &#39;s receiving üèì neighbor is Player (Rank) 0 and my sending üèì neighbor is Player (Rank) 0.&#xA;Player 0 started round 1 by hitting the üèì ball towards the wall.&#xA;Player 0 received the üèì ball that bounced off the wall.&#xA;Player 0 started round 2 by hitting the üèì ball towards the wall.&#xA;Player 0 received the üèì ball that bounced off the wall.&#xA;‚Ä¶&#xA;Player 0 started round 299 by hitting the üèì ball towards the wall.&#xA;Player 0 received the üèì ball that bounced off the wall.&#xA;üéâ Player 0 won the game of üèì ping pong, since they were playing alone! üéâ&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As you can see the application runs perfectly fine, and uses a single rank&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:rank&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;/process (&lt;em&gt;CPU&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cpu&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;core, in this case&lt;/em&gt;). Since we only have a single process participating in this game of Ping Pong, the program simply sends and receives messages to and from from itself.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, assuming that we have at least two CPU cores, then we can run this application utilizing as many of them as we want, by leveraging the MPI command line tool&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run 12 instances of our application, which will all communicate with each other in parallel across 12 cores on our system, we can do the following on our local system:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: These messages will be somewhat out of order. This is the nature of parallel computing, buffers, etc. To deal with this you would likely need to send all the messages back to single process for ordering and printing.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ mpirun -np 12 ./mpi-ping-pong.py&#xA;&#xA;Player (Rank)  0 &#39;s receiving üèì neighbor is Player (Rank) 11 and my sending üèì neighbor is Player (Rank) 1.&#xA;Hello, World! I am üèì Player (Rank)  0 . Process  1  of  12  on  laptop.local&#xA;Player (Rank)  1 &#39;s receiving üèì neighbor is Player (Rank) 0 and my sending üèì neighbor is Player (Rank) 2.&#xA;Hello, World! I am üèì Player (Rank)  1 . Process  2  of  12  on  laptop.local&#xA;Player (Rank)  2 &#39;s receiving üèì neighbor is Player (Rank) 1 and my sending üèì neighbor is Player (Rank) 3.&#xA;Hello, World! I am üèì Player (Rank)  2 . Process  3  of  12  on  laptop.local&#xA;‚Ä¶&#xA;üõë Player 0 is bored of üèì ping pong and is quitting! üõë&#xA;üõë Player 1 is bored of üèì ping pong and is quitting! üõë&#xA;üõë Player 2 is bored of üèì ping pong and is quitting! üõë&#xA;‚Ä¶&#xA;üéâ Player 11 won the game of üèì ping pong, since everyone else quit! üéâ&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;massive-scale-with-slurm&#34;&gt;Massive Scale with Slurm&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Now, imagine that we want to run this across hundreds of CPUs. Slurm makes this trivial as long as you have enough systems in your cluster to support this work.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In Slurm, you can define a batch job with a shell script (&lt;em&gt;e.g. /shared_storage/mpi-ping-pong.slurm&lt;/em&gt;) that looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;#!/bin/bash&#xA;&#xA;#SBATCH -J mpi-ping-pong                         # Name the job&#xA;#SBATCH -o /shared_storage/mpi-ping-pong-%j.out  # The standard output file&#xA;#SBATCH -e /shared_storage/mpi-ping-pong-%j.err  # The standard error file&#xA;#SBATCH -t 0-2:00:00         # Run for a max time of 0d, 2h, 0m, 0s&#xA;#SBATCH --nodes=3            # Request 3 nodes&#xA;#SBATCH --ntasks-per-node=1  # Request 1 cores or task per node&#xA;&#xA;echo &#34;Date start                = $(date)&#34;&#xA;echo &#34;Initiating Host           = $(hostname)&#34;&#xA;echo &#34;Working Directory         = $(pwd)&#34;&#xA;echo &#34;&#34;&#xA;echo &#34;Number of Nodes Allocated = ${SLURM_JOB_NUM_NODES}&#34;&#xA;echo &#34;Number of Tasks Allocated = ${SLURM_NTASKS}&#34;&#xA;echo &#34;&#34;&#xA;&#xA;mpirun python /shared_storage/mpi-ping-pong.py&#xA;RETURN=${?}&#xA;&#xA;echo &#34;&#34;&#xA;echo &#34;Exit code                 = ${RETURN}&#34;&#xA;echo &#34;Date end                  = $(date)&#34;&#xA;echo &#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Each&amp;nbsp;&lt;code&gt;#SBATCH&lt;/code&gt;&amp;nbsp;line defines an&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/sbatch.html&#34;&gt;&lt;code&gt;sbatch&lt;/code&gt;&lt;/a&gt;&amp;nbsp;command line argument to be used for the job. In this case, we are going to request that the workload run on three nodes and that it will only use a single core on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that all the compute nodes in our cluster are configured correctly and our Python script exists on the shared storage at&amp;nbsp;&lt;em&gt;/shared_storage/mpi-ping-pong.py&lt;/em&gt;&amp;nbsp;then we can submit this job to the cluster, by running:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ sbatch /shared_storage/mpi-ping-pong.slurm&#xA;&#xA;Submitted batch job 87&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This job will run very fast, but if we needed to we could use commands like&amp;nbsp;&lt;code&gt;squeue&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;sinfo&lt;/code&gt;&amp;nbsp;to examine the state of the cluster and jobs while it was running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When the job is finished we can look at&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.err&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.out&lt;/code&gt;&amp;nbsp;to see all the output from our application. In this case, the error file is empty, but our output file looks like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;Date start                = Tue May 14 10:15:02 PM UTC 2024&#xA;Initiating Host           = worker-0&#xA;Working Directory         = /shared_storage&#xA;&#xA;Number of Nodes Allocated = 3&#xA;Number of Tasks Allocated = 3&#xA;&#xA;Hello, World! I am üèì Player (Rank)  1 . Process  2  of  3  on  worker-1&#xA;Player (Rank)  1 &#39;s receiving üèì neighbor is Player (Rank) 0 and my sending üèì neighbor is Player (Rank) 2.&#xA;Hello, World! I am üèì Player (Rank)  2 . Process  3  of  3  on  worker-2&#xA;Player (Rank)  2 &#39;s receiving üèì neighbor is Player (Rank) 1 and my sending üèì neighbor is Player (Rank) 0.&#xA;Hello, World! I am üèì Player (Rank)  0 . Process  1  of  3  on  worker-0&#xA;Player (Rank)  0 &#39;s receiving üèì neighbor is Player (Rank) 2 and my sending üèì neighbor is Player (Rank) 1.&#xA;Player 0 sent the üèì ball to Player 1.&#xA;Player 1 received the üèì ball from Player 0.&#xA;Player 1 sent the üèì ball to Player 2.&#xA;Player 2 received the üèì ball from Player 1.&#xA;Player 2 sent the üèì ball to Player 0.&#xA;Player 0 received the üèì ball from Player 2.&#xA;Player 0 started round 2 by sending the üèì ball to Player 1.&#xA;‚Ä¶&#xA;Player 0 started round 900 by sending the üèì ball to Player 1.&#xA;üõë Player 0 is bored of üèì ping pong and is quitting! üõë&#xA;Player 1 received the üèì ball from Player 0.&#xA;Player 1 sent the üèì ball to Player 2.&#xA;üõë Player 1 is bored of üèì ping pong and is quitting! üõë&#xA;Player 2 received the üèì ball from Player 1.&#xA;üéâ Player 2 won the game of üèì ping pong, since everyone else quit! üéâ&#xA;&#xA;Exit code                 = 0&#xA;Date end                  = Tue May 14 10:15:03 PM UTC 2024&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you look closely at the&amp;nbsp;&lt;code&gt;Hello, World!&lt;/code&gt;&amp;nbsp;lines for each rank/process you will notice that they are all running on completely different nodes (&lt;em&gt;worker-[0-2]&lt;/em&gt;), but the application still behaves exactly as we expect it to, and the messages are happily passed between each process running on a different core on different systems. Because of this, we could use Slurm to easily run our application across hundreds or even thousands of cores if that is what we needed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, could we do all of this with Kubernetes today? Well, yes and no.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The real power of Slurm is that you have so much control over how it is built, how you assign resources to your workloads, and how they are partitioned while running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;mpi---message-passing-interface&#34;&gt;MPI ‚Äì Message Passing Interface&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although you could easily run this application inside a single pod on Kubernetes, to truly leverage the MPI capabilities that we are expecting we would need to install and utilize something like&amp;nbsp;&lt;a href=&#34;https://www.kubeflow.org/&#34;&gt;KubeFlow&lt;/a&gt;&amp;nbsp;along with&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/mpi-operator&#34;&gt;KubeFlow‚Äôs beta MPI Operator&lt;/a&gt;&amp;nbsp;to even get started. You might also want to consider something like the&amp;nbsp;&lt;a href=&#34;https://kueue.sigs.k8s.io/&#34;&gt;Kubernetes-native Job Queueing support&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;At the time of writing this&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/manifests/tree/v1.8.0?tab=readme-ov-file#prerequisites&#34;&gt;KubeFlow v.1.8.0 doesn‚Äôt support versions of Kubernetes greater than v1.26&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One obvious side effect of using Kubernetes is that we&amp;nbsp;&lt;strong&gt;MUST&lt;/strong&gt;&amp;nbsp;run our code inside one or more containers. This is possible with Slurm but is not a hard requirement. We will also need to ensure that Python, MPI, and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;are all installed and configured correctly in our container, as we will need them all to run our application and manage each of the MPI worker pods.&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is required because this is how&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;will connect to each of the pods to launch the parallel instances of our application. The manifest that we will need to apply to our Kubernetes cluster for use with&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;is simply named&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/manifests/mpi-ping-pong/mpijob.yaml&#34;&gt;mpijob.yaml&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;You can also find&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/Dockerfile&#34;&gt;the Dockerfile for the container that we are running&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground&#34;&gt;the GitHub repo&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;mpijob.yaml&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;em&gt;&lt;/em&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that Kubeflow and the mpi-operator are properly installed in our cluster, then applying the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;manifest should ‚Äújust work‚Äù.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ git clone https://github.com/superorbital/mpi-playground.git&#xA;$ cd mpi-playground&#xA;$ kubectl apply -f ./manifests/mpi-ping-pong/mpijob.yaml&#xA;mpijob.kubeflow.org/mpi-ping-pong created&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If you want to try this out yourself, you will likely find that it is easiest to delete the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;and then re-apply it each time you want to run it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;code&gt;kubectl delete mpijobs.kubeflow.org mpi-ping-pong&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the animated GIF below you will see the manifest being launched, followed by the launcher and worker pods being spun up, once the workers are done, then the launcher will complete and the launcher logs will be displayed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/demo.gif&#34; alt=&#34;MPI Ping Pong in Kubernetes demo&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The sequence of events looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The required number of workers are started and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is run inside each worker container.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher is started and executes the desired&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;command.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;connects to each worker and starts the individual tasks.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The workers complete their tasks and exit.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher completes and exits.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The pod logs for the launcher will contain all of the output that we would normally expect to see from an&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;session.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, today it is technically possible to run at least some MPI-style jobs inside Kubernetes, but how practical is it?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Well, one of the big advantages of Kubernetes is its ability to scale. Unlike Slurm, which has no cluster scaling functionality, a Kubernetes cluster can be designed to scale up nodes for a very large workflow and then scale back down once that workflow is finished and those nodes are no longer required for any immediately upcoming workflows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But, I think the real answer here is that it depends a lot on what you are trying to do, how much control you have over the Kubernetes cluster setup, and how much effort you want to put into getting it all working.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;artificial-intelligence-and-machine-learning&#34;&gt;Artificial Intelligence and Machine Learning&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although the technical details are different, most of the basic observations in this article also apply to artificial intelligence (AI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ai&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;), machine learning (ML&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ml&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;), and deep learning (DL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:dl&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;) applications. In most instances, these workloads will make heavy use of systems equipped with advanced GPUs to significantly improve the performance of their computation tasks. Ensuring that each process has exclusive access to the required hardware resources is often critical for the success and performance of the task.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;slurm-on-kubernetes&#34;&gt;Slurm on Kubernetes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the last couple of years,&amp;nbsp;&lt;a href=&#34;https://www.linkedin.com/in/timwickberg/&#34;&gt;Tim Wickberg, SchedMD‚Äôs CTO&lt;/a&gt;, has given a talk entitled ‚Äú&lt;a href=&#34;https://slurm.schedmd.com/SC23/Slurm-and-or-vs-Kubernetes.pdf&#34;&gt;Slurm and/or/vs Kubernetes&lt;/a&gt;‚Äù, which looks at Slurm and Kubernetes and explores how each of these tools could be extended to leverage their strengths and generally improve the current state of high-performance workload managers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One project that has come out of the effort to find synergy between these two tools and communities, is&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/blog/sunk-slurm-on-kubernetes-implementations&#34;&gt;SUNK&lt;/a&gt;, which stands for&amp;nbsp;&lt;em&gt;&lt;strong&gt;S&lt;/strong&gt;l&lt;strong&gt;U&lt;/strong&gt;rm o&lt;strong&gt;N&lt;/strong&gt;&amp;nbsp;&lt;strong&gt;K&lt;/strong&gt;ubernetes&lt;/em&gt;. It is a project in early development by&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/&#34;&gt;CoreWeave&lt;/a&gt;&amp;nbsp;to enable Slurm workloads in Kubernetes Clusters.&amp;nbsp;&lt;a href=&#34;https://www.youtube.com/watch?v=48ONt0UKiew&#34;&gt;The CoreWeave talk that officially introduced this project can viewed on YouTube&lt;/a&gt;&amp;nbsp;if you would like to hear more details about the work that is being done with this. You can also grab&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/SLUG23/CoreWeave-SLUG23.pdf&#34;&gt;a PDF of the slides&lt;/a&gt;&amp;nbsp;for the talk. Once the code is released it should be available at&amp;nbsp;&lt;a href=&#34;https://github.com/coreweave/sunk&#34;&gt;github.com/coreweave/sunk&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;kubernetes-device-management&#34;&gt;Kubernetes Device Management&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The&amp;nbsp;&lt;a href=&#34;https://community.arm.com/arm-research/b/articles/posts/a-smarter-device-manager-for-kubernetes-on-the-edge&#34;&gt;Smarter Device Project&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://www.arm.com/resources/research&#34;&gt;ARM research&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:arm&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;is also an interesting&amp;nbsp;&lt;a href=&#34;https://github.com/smarter-project/smarter-device-manager&#34;&gt;Kubernetes application&lt;/a&gt;&amp;nbsp;that might be useful for these sorts of workloads and makes it possible to expose, reserve, and manage any Linux device (&lt;em&gt;e.g. /dev/video*&lt;/em&gt;) within Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For GPUs,&amp;nbsp;&lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/k8s-device-plugin&#34;&gt;NVIDIA has a device plugin&lt;/a&gt;&amp;nbsp;to provide similar functionality, which is often deployed along with their&amp;nbsp;&lt;a href=&#34;https://github.com/NVIDIA/gpu-operator&#34;&gt;gpu-operator&lt;/a&gt;. This also enables features in Kubernetes like GPU time-slicing, something similar to the generic&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/gres.html#Sharding&#34;&gt;GPU sharding&lt;/a&gt;&amp;nbsp;available in Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;in-conclusion&#34;&gt;In Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm is a very useful tool for running parallel applications that require very specific resources inside a compute cluster. It was designed for a set of workloads that are outside the standard stateless microservice focus of Kubernetes, but even today Kubernetes can be made to work with some of these workloads without too much hassle, however, truly leveraging the advantages of Kubernetes for these more traditional high-performance computing workloads will require projects like SUNK and&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;to reach a reasonable level of maturity and then in the future, there may be newer projects that rethink the current approaches and build Kubernetes-first tools from the ground-up that leverage the strengths that are already available in the platform while extending its ability to handle the type of workloads that have been traditionally handled by tools like Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;further-reading&#34;&gt;Further Reading&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained&#34;&gt;Machine learning, explained&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.geeksforgeeks.org/machine-learning/&#34;&gt;Machine Learning Tutorial&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://carleton.ca/rcs/rcdc/introduction-to-mpi/&#34;&gt;Introduction to MPI&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://mpitutorial.com/&#34;&gt;A Comprehensive MPI Tutorial Resource&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Intro to Multi-Node Machine Learning&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-1-setting-up-an-hpc-cluster&#34;&gt;1: Setting up an HPC cluster&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-2-using-slurm&#34;&gt;2: Using Slurm&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;Unreleased as of 05/16/2024&lt;/em&gt;&amp;nbsp;‚Äì&amp;nbsp;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-3-multi-node-training&#34;&gt;3: Multi-Node Training&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://blog.kubeflow.org/integrations/operators/2020/03/16/mpi-operator.html&#34;&gt;Introduction to Kubeflow MPI Operator and Industry Adoption&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Cover image by&amp;nbsp;&lt;a href=&#34;https://pixabay.com/users/dlohner-4631193/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;dlohner&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://pixabay.com//?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;Pixabay&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Slurm architecture overview by SchedMD for their&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/network.html&#34;&gt;network overview&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/High-performance_computing&#34;&gt;HPC ‚Äì high-performance computing&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:hpc&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Lawrence_Livermore_National_Laboratory&#34;&gt;LLNL ‚Äì Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:llnl&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.schedmd.com/about-schedmd/slurm-history&#34;&gt;Slurm history&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:history&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Graphics_processing_unit&#34;&gt;GPU ‚Äì graphics processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:gpu&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA¬Æ ‚Äì Compute Unified Device Architecture&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cuda&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN ‚Äì NVIDIA CUDA¬Æ Deep Neural Network library&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cudnn&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.open-mpi.org/&#34;&gt;MPI ‚Äì Message Passing Interface&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:mpi&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Open_Container_Initiative&#34;&gt;OCI ‚Äì Open Container Initiative&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:oci&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.uio.no/studier/emner/matnat/ifi/INF3380/v11/undervisningsmateriale/inf3380-week06.pdf&#34;&gt;Rank&lt;/a&gt;&amp;nbsp;‚Äì Each process has a unique rank, i.e. an integer identifier, within a communicator. The rank value is between zero and the number of processes minus one. The rank value is used to distinguish one process from another.&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:rank&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Central_processing_unit&#34;&gt;CPU ‚Äì central processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cpu&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/artificial-intelligence&#34;&gt;AI ‚Äì Artificial Intelligence&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ai&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/machine-learning&#34;&gt;ML ‚Äì Machine Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ml&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/deep-learning&#34;&gt;DL ‚Äì Deep Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:dl&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/ARM_architecture_family&#34;&gt;ARM ‚Äì Advanced RISC Machine&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:arm&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/&#34;&gt;SuperOrbital‚Äôs blog&lt;/a&gt; by Sean Kane&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/cover.jpg&#34; alt=&#34;datacenter servers&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this article, we are going to explore&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm&#34;&gt;Slurm&lt;/a&gt;, a popular open-source high-performance computing (HPC&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:hpc&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;) workload manager, and discover what it is, why people use it, and how it differs from Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For context, when we talk about high-performance computing, we are primarily talking about computational tasks that require a lot of time on very powerful computer systems and are often designed, so that they can be&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Parallel_computing&#34;&gt;highly parallelized&lt;/a&gt;, across resources within a super-computer or large cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These sorts of jobs are very common in scientific research,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_simulation&#34;&gt;computer simulation&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_intelligence&#34;&gt;artificial intelligence&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&gt;machine learning&lt;/a&gt;&amp;nbsp;tasks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;what-is-slurm&#34;&gt;What is Slurm?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To almost directly quote the Slurm repository&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm/blob/master/README.rst#slurm-workload-manager&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;Slurm is an open-source cluster resource management and job scheduling system for Linux (&lt;em&gt;primarily&lt;/em&gt;) that strives to be simple, scalable, portable, fault-tolerant, and interconnect agnostic. As a cluster resource manager, Slurm provides three key functions:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Second, it provides a framework for starting, executing, and monitoring work (&lt;em&gt;normally a parallel job&lt;/em&gt;) on the set of allocated nodes.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;And finally, it arbitrates conflicting requests for resources by managing a queue of pending work.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In short, Slurm is a system that is primarily designed to manage&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Batch_processing&#34;&gt;batch processing&lt;/a&gt;&amp;nbsp;workloads running in a shared&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_cluster&#34;&gt;computing cluster&lt;/a&gt;. Slurm can be configured to run any type of batch job but is often utilized on systems that support multiple users with parallel computing tasks to run.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;According to&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;, the primary support and development company for the Slurm community, Slurm is being used on approximately 65% of the&amp;nbsp;&lt;a href=&#34;https://top500.org/lists/top500/2024/06/&#34;&gt;TOP500 supercomputers in the world&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;a-bit-of-history&#34;&gt;A bit of history&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the mid-1990s, as Linux became more popular, there was an increased interest in making large-scale computer processing more accessible through the use of commodity hardware and projects like&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Beowulf_cluster&#34;&gt;Beowulf&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In 2001, engineers at the&amp;nbsp;&lt;a href=&#34;https://www.llnl.gov/&#34;&gt;Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;(LLNL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:llnl&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;) in the United States, started looking into how they might migrate from proprietary HPC systems to open-source solutions. They determined that there weren‚Äôt any good resource managers available at the time, and this directly led to the creation of Slurm, which was initially released in 2002.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;a href=&#34;https://slurm.schedmd.com/slurm_design.pdf&#34;&gt;The name Slurm is a reference&lt;/a&gt;&amp;nbsp;to the&amp;nbsp;&lt;a href=&#34;https://futurama.fandom.com/wiki/Slurm&#34;&gt;soda&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://www.imdb.com/title/tt0149460/&#34;&gt;Futurama&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the years, Slurm has evolved to support a wide variety of hardware configurations and in 2010, some initial scheduling logic was added to Slurm, enabling it to function as both a stand-alone resource manager and a scheduler&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:history&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-architecture&#34;&gt;The Architecture&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Like many similar systems, a Slurm setup consists of at least one controller node, and at least one compute node, which can be on the same physical systems.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/slurm-layout.gif&#34; alt=&#34;Diagram of the basic Slurm architecture&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;the-controller&#34;&gt;The Controller&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Two daemons are responsible for managing the cluster. These two processes can be run on a single node or separated, depending on the performance and resiliency required by the administrator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmctld&#34;&gt;slurmctld&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmctld&lt;/code&gt;&amp;nbsp;is the primary management daemon for Slurm. It is responsible for all the daemons, resources, and jobs, within the cluster. Because it is a critical component in the cluster, it is often configured with a passive backup that can take over management if the primary daemon ever fails.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmdbd&#34;&gt;slurmdbd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmdbd&lt;/code&gt;&amp;nbsp;provides an interface for the&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/MySQL&#34;&gt;MySQL database&lt;/a&gt;&amp;nbsp;that Slurm uses for stateful data, like job accounting records.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;compute-nodes&#34;&gt;Compute Nodes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These represent the nodes whose resources are available to any jobs that might need to be scheduled in the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmd&#34;&gt;slurmd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmd&lt;/code&gt;&amp;nbsp;is the daemon that runs on each compute node and is responsible for accepting work, launching tasks, and generally managing the workloads on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SchedMD maintains the core documentation for Slurm and the&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/quickstart_admin.html#quick_start&#34;&gt;current installation documentation&lt;/a&gt;&amp;nbsp;can be found on their website.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Getting Slurm working is not particularly difficult, but fine-tuning it for your requirements can be a time-consuming task. Since performance is often a primary focus with these clusters and Slurm is designed to work with a very wide set of hardware and software libraries, it can take a lot of effort to fine-tune the installation, so that it utilizes the hardware to the best of its ability, and all the proper software is available to streamline the types of jobs that will be run on the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;At a minimum clusters will require node name resolution, a MySQL instance, shared block storage, a synchronized time source, synchronized users, the&amp;nbsp;&lt;a href=&#34;https://dun.github.io/munge/&#34;&gt;Munge&lt;/a&gt;&amp;nbsp;daemon for user authentication, a full set of custom-built Slurm executables, and various configuration files.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In addition to this, the nodes will need to contain the various hardware, programming languages, libraries, and other tools that the majority of jobs will require to run. This could include things like GPUs&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:gpu&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;and their related toolchains (CUDA&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cuda&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;/cuDNN&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cudnn&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;), MPI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:mpi&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;libraries, OCI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:oci&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;runtimes for&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/containers.html&#34;&gt;container support&lt;/a&gt;, and much more.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm‚Äôs greatest asset and liability is how flexible it is. This makes it something that can be highly optimized for the requirements of the organization, while also making it susceptible to misconfiguration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;workloads&#34;&gt;Workloads&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To provide some context, let‚Äôs take a look at an example user job and workflow that utilizes some of the features of Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the&amp;nbsp;&lt;code&gt;mpi-ping-pong.py&lt;/code&gt;&amp;nbsp;code below, you will find a&amp;nbsp;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt;&amp;nbsp;script that simulates a pseudo-game of&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Table_tennis&#34;&gt;Ping Pong/Table Tennis&lt;/a&gt;&amp;nbsp;by making use of MPI to pass a message (&lt;em&gt;‚Äúball‚Äù&lt;/em&gt;) between as many processes as we have configured to participate in the parallel job.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;local-testing&#34;&gt;Local Testing&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run this code, you simply need to have&amp;nbsp;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/quickstart.html&#34;&gt;OpenMPI&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://mpi4py.readthedocs.io/en/stable/install.html&#34;&gt;mpi4py&lt;/a&gt;&amp;nbsp;installed on your system.mpi-ping-pong.py&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/mpi-ping-pong.py&#34;&gt;mpi-ping-pong.py&lt;/a&gt;&amp;nbsp;is what many would refer to as an&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Embarrassingly_parallel&#34;&gt;embarrassingly parallel&lt;/a&gt;&amp;nbsp;workload because it can be scaled with no real effort at all.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you run this Python application on your local system, you will see something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ python ./mpi-ping-pong.py&#xA;&#xA;Hello, World! I am üèì Player (Rank)  0 . Process  1  of  1  on  laptop.local&#xA;Player (Rank)  0 &#39;s receiving üèì neighbor is Player (Rank) 0 and my sending üèì neighbor is Player (Rank) 0.&#xA;Player 0 started round 1 by hitting the üèì ball towards the wall.&#xA;Player 0 received the üèì ball that bounced off the wall.&#xA;Player 0 started round 2 by hitting the üèì ball towards the wall.&#xA;Player 0 received the üèì ball that bounced off the wall.&#xA;‚Ä¶&#xA;Player 0 started round 299 by hitting the üèì ball towards the wall.&#xA;Player 0 received the üèì ball that bounced off the wall.&#xA;üéâ Player 0 won the game of üèì ping pong, since they were playing alone! üéâ&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As you can see the application runs perfectly fine, and uses a single rank&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:rank&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;/process (&lt;em&gt;CPU&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cpu&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;core, in this case&lt;/em&gt;). Since we only have a single process participating in this game of Ping Pong, the program simply sends and receives messages to and from from itself.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, assuming that we have at least two CPU cores, then we can run this application utilizing as many of them as we want, by leveraging the MPI command line tool&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run 12 instances of our application, which will all communicate with each other in parallel across 12 cores on our system, we can do the following on our local system:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: These messages will be somewhat out of order. This is the nature of parallel computing, buffers, etc. To deal with this you would likely need to send all the messages back to single process for ordering and printing.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ mpirun -np 12 ./mpi-ping-pong.py&#xA;&#xA;Player (Rank)  0 &#39;s receiving üèì neighbor is Player (Rank) 11 and my sending üèì neighbor is Player (Rank) 1.&#xA;Hello, World! I am üèì Player (Rank)  0 . Process  1  of  12  on  laptop.local&#xA;Player (Rank)  1 &#39;s receiving üèì neighbor is Player (Rank) 0 and my sending üèì neighbor is Player (Rank) 2.&#xA;Hello, World! I am üèì Player (Rank)  1 . Process  2  of  12  on  laptop.local&#xA;Player (Rank)  2 &#39;s receiving üèì neighbor is Player (Rank) 1 and my sending üèì neighbor is Player (Rank) 3.&#xA;Hello, World! I am üèì Player (Rank)  2 . Process  3  of  12  on  laptop.local&#xA;‚Ä¶&#xA;üõë Player 0 is bored of üèì ping pong and is quitting! üõë&#xA;üõë Player 1 is bored of üèì ping pong and is quitting! üõë&#xA;üõë Player 2 is bored of üèì ping pong and is quitting! üõë&#xA;‚Ä¶&#xA;üéâ Player 11 won the game of üèì ping pong, since everyone else quit! üéâ&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;massive-scale-with-slurm&#34;&gt;Massive Scale with Slurm&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Now, imagine that we want to run this across hundreds of CPUs. Slurm makes this trivial as long as you have enough systems in your cluster to support this work.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In Slurm, you can define a batch job with a shell script (&lt;em&gt;e.g. /shared_storage/mpi-ping-pong.slurm&lt;/em&gt;) that looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;#!/bin/bash&#xA;&#xA;#SBATCH -J mpi-ping-pong                         # Name the job&#xA;#SBATCH -o /shared_storage/mpi-ping-pong-%j.out  # The standard output file&#xA;#SBATCH -e /shared_storage/mpi-ping-pong-%j.err  # The standard error file&#xA;#SBATCH -t 0-2:00:00         # Run for a max time of 0d, 2h, 0m, 0s&#xA;#SBATCH --nodes=3            # Request 3 nodes&#xA;#SBATCH --ntasks-per-node=1  # Request 1 cores or task per node&#xA;&#xA;echo &#34;Date start                = $(date)&#34;&#xA;echo &#34;Initiating Host           = $(hostname)&#34;&#xA;echo &#34;Working Directory         = $(pwd)&#34;&#xA;echo &#34;&#34;&#xA;echo &#34;Number of Nodes Allocated = ${SLURM_JOB_NUM_NODES}&#34;&#xA;echo &#34;Number of Tasks Allocated = ${SLURM_NTASKS}&#34;&#xA;echo &#34;&#34;&#xA;&#xA;mpirun python /shared_storage/mpi-ping-pong.py&#xA;RETURN=${?}&#xA;&#xA;echo &#34;&#34;&#xA;echo &#34;Exit code                 = ${RETURN}&#34;&#xA;echo &#34;Date end                  = $(date)&#34;&#xA;echo &#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Each&amp;nbsp;&lt;code&gt;#SBATCH&lt;/code&gt;&amp;nbsp;line defines an&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/sbatch.html&#34;&gt;&lt;code&gt;sbatch&lt;/code&gt;&lt;/a&gt;&amp;nbsp;command line argument to be used for the job. In this case, we are going to request that the workload run on three nodes and that it will only use a single core on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that all the compute nodes in our cluster are configured correctly and our Python script exists on the shared storage at&amp;nbsp;&lt;em&gt;/shared_storage/mpi-ping-pong.py&lt;/em&gt;&amp;nbsp;then we can submit this job to the cluster, by running:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ sbatch /shared_storage/mpi-ping-pong.slurm&#xA;&#xA;Submitted batch job 87&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This job will run very fast, but if we needed to we could use commands like&amp;nbsp;&lt;code&gt;squeue&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;sinfo&lt;/code&gt;&amp;nbsp;to examine the state of the cluster and jobs while it was running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When the job is finished we can look at&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.err&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.out&lt;/code&gt;&amp;nbsp;to see all the output from our application. In this case, the error file is empty, but our output file looks like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;Date start                = Tue May 14 10:15:02 PM UTC 2024&#xA;Initiating Host           = worker-0&#xA;Working Directory         = /shared_storage&#xA;&#xA;Number of Nodes Allocated = 3&#xA;Number of Tasks Allocated = 3&#xA;&#xA;Hello, World! I am üèì Player (Rank)  1 . Process  2  of  3  on  worker-1&#xA;Player (Rank)  1 &#39;s receiving üèì neighbor is Player (Rank) 0 and my sending üèì neighbor is Player (Rank) 2.&#xA;Hello, World! I am üèì Player (Rank)  2 . Process  3  of  3  on  worker-2&#xA;Player (Rank)  2 &#39;s receiving üèì neighbor is Player (Rank) 1 and my sending üèì neighbor is Player (Rank) 0.&#xA;Hello, World! I am üèì Player (Rank)  0 . Process  1  of  3  on  worker-0&#xA;Player (Rank)  0 &#39;s receiving üèì neighbor is Player (Rank) 2 and my sending üèì neighbor is Player (Rank) 1.&#xA;Player 0 sent the üèì ball to Player 1.&#xA;Player 1 received the üèì ball from Player 0.&#xA;Player 1 sent the üèì ball to Player 2.&#xA;Player 2 received the üèì ball from Player 1.&#xA;Player 2 sent the üèì ball to Player 0.&#xA;Player 0 received the üèì ball from Player 2.&#xA;Player 0 started round 2 by sending the üèì ball to Player 1.&#xA;‚Ä¶&#xA;Player 0 started round 900 by sending the üèì ball to Player 1.&#xA;üõë Player 0 is bored of üèì ping pong and is quitting! üõë&#xA;Player 1 received the üèì ball from Player 0.&#xA;Player 1 sent the üèì ball to Player 2.&#xA;üõë Player 1 is bored of üèì ping pong and is quitting! üõë&#xA;Player 2 received the üèì ball from Player 1.&#xA;üéâ Player 2 won the game of üèì ping pong, since everyone else quit! üéâ&#xA;&#xA;Exit code                 = 0&#xA;Date end                  = Tue May 14 10:15:03 PM UTC 2024&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you look closely at the&amp;nbsp;&lt;code&gt;Hello, World!&lt;/code&gt;&amp;nbsp;lines for each rank/process you will notice that they are all running on completely different nodes (&lt;em&gt;worker-[0-2]&lt;/em&gt;), but the application still behaves exactly as we expect it to, and the messages are happily passed between each process running on a different core on different systems. Because of this, we could use Slurm to easily run our application across hundreds or even thousands of cores if that is what we needed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, could we do all of this with Kubernetes today? Well, yes and no.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The real power of Slurm is that you have so much control over how it is built, how you assign resources to your workloads, and how they are partitioned while running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;mpi---message-passing-interface&#34;&gt;MPI ‚Äì Message Passing Interface&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although you could easily run this application inside a single pod on Kubernetes, to truly leverage the MPI capabilities that we are expecting we would need to install and utilize something like&amp;nbsp;&lt;a href=&#34;https://www.kubeflow.org/&#34;&gt;KubeFlow&lt;/a&gt;&amp;nbsp;along with&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/mpi-operator&#34;&gt;KubeFlow‚Äôs beta MPI Operator&lt;/a&gt;&amp;nbsp;to even get started. You might also want to consider something like the&amp;nbsp;&lt;a href=&#34;https://kueue.sigs.k8s.io/&#34;&gt;Kubernetes-native Job Queueing support&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;At the time of writing this&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/manifests/tree/v1.8.0?tab=readme-ov-file#prerequisites&#34;&gt;KubeFlow v.1.8.0 doesn‚Äôt support versions of Kubernetes greater than v1.26&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One obvious side effect of using Kubernetes is that we&amp;nbsp;&lt;strong&gt;MUST&lt;/strong&gt;&amp;nbsp;run our code inside one or more containers. This is possible with Slurm but is not a hard requirement. We will also need to ensure that Python, MPI, and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;are all installed and configured correctly in our container, as we will need them all to run our application and manage each of the MPI worker pods.&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is required because this is how&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;will connect to each of the pods to launch the parallel instances of our application. The manifest that we will need to apply to our Kubernetes cluster for use with&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;is simply named&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/manifests/mpi-ping-pong/mpijob.yaml&#34;&gt;mpijob.yaml&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;You can also find&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/Dockerfile&#34;&gt;the Dockerfile for the container that we are running&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground&#34;&gt;the GitHub repo&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;mpijob.yaml&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;em&gt;&lt;/em&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that Kubeflow and the mpi-operator are properly installed in our cluster, then applying the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;manifest should ‚Äújust work‚Äù.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ git clone https://github.com/superorbital/mpi-playground.git&#xA;$ cd mpi-playground&#xA;$ kubectl apply -f ./manifests/mpi-ping-pong/mpijob.yaml&#xA;mpijob.kubeflow.org/mpi-ping-pong created&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If you want to try this out yourself, you will likely find that it is easiest to delete the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;and then re-apply it each time you want to run it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;code&gt;kubectl delete mpijobs.kubeflow.org mpi-ping-pong&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the animated GIF below you will see the manifest being launched, followed by the launcher and worker pods being spun up, once the workers are done, then the launcher will complete and the launcher logs will be displayed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/demo.gif&#34; alt=&#34;MPI Ping Pong in Kubernetes demo&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The sequence of events looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The required number of workers are started and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is run inside each worker container.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher is started and executes the desired&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;command.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;connects to each worker and starts the individual tasks.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The workers complete their tasks and exit.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher completes and exits.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The pod logs for the launcher will contain all of the output that we would normally expect to see from an&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;session.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, today it is technically possible to run at least some MPI-style jobs inside Kubernetes, but how practical is it?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Well, one of the big advantages of Kubernetes is its ability to scale. Unlike Slurm, which has no cluster scaling functionality, a Kubernetes cluster can be designed to scale up nodes for a very large workflow and then scale back down once that workflow is finished and those nodes are no longer required for any immediately upcoming workflows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But, I think the real answer here is that it depends a lot on what you are trying to do, how much control you have over the Kubernetes cluster setup, and how much effort you want to put into getting it all working.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;artificial-intelligence-and-machine-learning&#34;&gt;Artificial Intelligence and Machine Learning&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although the technical details are different, most of the basic observations in this article also apply to artificial intelligence (AI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ai&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;), machine learning (ML&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ml&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;), and deep learning (DL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:dl&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;) applications. In most instances, these workloads will make heavy use of systems equipped with advanced GPUs to significantly improve the performance of their computation tasks. Ensuring that each process has exclusive access to the required hardware resources is often critical for the success and performance of the task.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;slurm-on-kubernetes&#34;&gt;Slurm on Kubernetes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the last couple of years,&amp;nbsp;&lt;a href=&#34;https://www.linkedin.com/in/timwickberg/&#34;&gt;Tim Wickberg, SchedMD‚Äôs CTO&lt;/a&gt;, has given a talk entitled ‚Äú&lt;a href=&#34;https://slurm.schedmd.com/SC23/Slurm-and-or-vs-Kubernetes.pdf&#34;&gt;Slurm and/or/vs Kubernetes&lt;/a&gt;‚Äù, which looks at Slurm and Kubernetes and explores how each of these tools could be extended to leverage their strengths and generally improve the current state of high-performance workload managers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One project that has come out of the effort to find synergy between these two tools and communities, is&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/blog/sunk-slurm-on-kubernetes-implementations&#34;&gt;SUNK&lt;/a&gt;, which stands for&amp;nbsp;&lt;em&gt;&lt;strong&gt;S&lt;/strong&gt;l&lt;strong&gt;U&lt;/strong&gt;rm o&lt;strong&gt;N&lt;/strong&gt;&amp;nbsp;&lt;strong&gt;K&lt;/strong&gt;ubernetes&lt;/em&gt;. It is a project in early development by&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/&#34;&gt;CoreWeave&lt;/a&gt;&amp;nbsp;to enable Slurm workloads in Kubernetes Clusters.&amp;nbsp;&lt;a href=&#34;https://www.youtube.com/watch?v=48ONt0UKiew&#34;&gt;The CoreWeave talk that officially introduced this project can viewed on YouTube&lt;/a&gt;&amp;nbsp;if you would like to hear more details about the work that is being done with this. You can also grab&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/SLUG23/CoreWeave-SLUG23.pdf&#34;&gt;a PDF of the slides&lt;/a&gt;&amp;nbsp;for the talk. Once the code is released it should be available at&amp;nbsp;&lt;a href=&#34;https://github.com/coreweave/sunk&#34;&gt;github.com/coreweave/sunk&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;kubernetes-device-management&#34;&gt;Kubernetes Device Management&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The&amp;nbsp;&lt;a href=&#34;https://community.arm.com/arm-research/b/articles/posts/a-smarter-device-manager-for-kubernetes-on-the-edge&#34;&gt;Smarter Device Project&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://www.arm.com/resources/research&#34;&gt;ARM research&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:arm&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;is also an interesting&amp;nbsp;&lt;a href=&#34;https://github.com/smarter-project/smarter-device-manager&#34;&gt;Kubernetes application&lt;/a&gt;&amp;nbsp;that might be useful for these sorts of workloads and makes it possible to expose, reserve, and manage any Linux device (&lt;em&gt;e.g. /dev/video*&lt;/em&gt;) within Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For GPUs,&amp;nbsp;&lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/k8s-device-plugin&#34;&gt;NVIDIA has a device plugin&lt;/a&gt;&amp;nbsp;to provide similar functionality, which is often deployed along with their&amp;nbsp;&lt;a href=&#34;https://github.com/NVIDIA/gpu-operator&#34;&gt;gpu-operator&lt;/a&gt;. This also enables features in Kubernetes like GPU time-slicing, something similar to the generic&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/gres.html#Sharding&#34;&gt;GPU sharding&lt;/a&gt;&amp;nbsp;available in Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;in-conclusion&#34;&gt;In Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm is a very useful tool for running parallel applications that require very specific resources inside a compute cluster. It was designed for a set of workloads that are outside the standard stateless microservice focus of Kubernetes, but even today Kubernetes can be made to work with some of these workloads without too much hassle, however, truly leveraging the advantages of Kubernetes for these more traditional high-performance computing workloads will require projects like SUNK and&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;to reach a reasonable level of maturity and then in the future, there may be newer projects that rethink the current approaches and build Kubernetes-first tools from the ground-up that leverage the strengths that are already available in the platform while extending its ability to handle the type of workloads that have been traditionally handled by tools like Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;further-reading&#34;&gt;Further Reading&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained&#34;&gt;Machine learning, explained&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.geeksforgeeks.org/machine-learning/&#34;&gt;Machine Learning Tutorial&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://carleton.ca/rcs/rcdc/introduction-to-mpi/&#34;&gt;Introduction to MPI&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://mpitutorial.com/&#34;&gt;A Comprehensive MPI Tutorial Resource&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Intro to Multi-Node Machine Learning&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-1-setting-up-an-hpc-cluster&#34;&gt;1: Setting up an HPC cluster&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-2-using-slurm&#34;&gt;2: Using Slurm&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;Unreleased as of 05/16/2024&lt;/em&gt;&amp;nbsp;‚Äì&amp;nbsp;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-3-multi-node-training&#34;&gt;3: Multi-Node Training&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://blog.kubeflow.org/integrations/operators/2020/03/16/mpi-operator.html&#34;&gt;Introduction to Kubeflow MPI Operator and Industry Adoption&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Cover image by&amp;nbsp;&lt;a href=&#34;https://pixabay.com/users/dlohner-4631193/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;dlohner&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://pixabay.com//?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;Pixabay&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Slurm architecture overview by SchedMD for their&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/network.html&#34;&gt;network overview&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/High-performance_computing&#34;&gt;HPC ‚Äì high-performance computing&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:hpc&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Lawrence_Livermore_National_Laboratory&#34;&gt;LLNL ‚Äì Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:llnl&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.schedmd.com/about-schedmd/slurm-history&#34;&gt;Slurm history&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:history&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Graphics_processing_unit&#34;&gt;GPU ‚Äì graphics processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:gpu&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA¬Æ ‚Äì Compute Unified Device Architecture&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cuda&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN ‚Äì NVIDIA CUDA¬Æ Deep Neural Network library&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cudnn&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.open-mpi.org/&#34;&gt;MPI ‚Äì Message Passing Interface&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:mpi&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Open_Container_Initiative&#34;&gt;OCI ‚Äì Open Container Initiative&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:oci&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.uio.no/studier/emner/matnat/ifi/INF3380/v11/undervisningsmateriale/inf3380-week06.pdf&#34;&gt;Rank&lt;/a&gt;&amp;nbsp;‚Äì Each process has a unique rank, i.e. an integer identifier, within a communicator. The rank value is between zero and the number of processes minus one. The rank value is used to distinguish one process from another.&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:rank&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Central_processing_unit&#34;&gt;CPU ‚Äì central processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cpu&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/artificial-intelligence&#34;&gt;AI ‚Äì Artificial Intelligence&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ai&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/machine-learning&#34;&gt;ML ‚Äì Machine Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ml&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/deep-learning&#34;&gt;DL ‚Äì Deep Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:dl&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/ARM_architecture_family&#34;&gt;ARM ‚Äì Advanced RISC Machine&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:arm&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Sun, 07 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêUnlocking the power of ephemeral environments with Devtron„Äë‰ΩøÁî® Devtron ÈáäÊîæÁü≠ÊöÇÁéØÂ¢ÉÁöÑÂäõÈáè</title>
      <link>https://www.cncf.io/blog/2024/07/12/unlocking-the-power-of-ephemeral-environments-with-devtron/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://devtron.ai/blog/unlocking-the-power-of-ephemeral-environments-with-devtron/&#34;&gt;Devtron‚Äôs blog&lt;/a&gt; by Abhinav Dubey&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;TL;DR: The blog talks about how ephemeral environments with Devtron become much easier, reducing the complexities, automating the process, and optimizing infra cost.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the world of software development,&amp;nbsp;&lt;strong&gt;ephemeral environments&lt;/strong&gt;&amp;nbsp;are temporary setups that serve specific purposes, such as testing or staging new features. These environments are short-lived, designed to exist only for the duration of their use case‚Äîlike testing a feature branch‚Äîbefore being dismantled.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments contrast with traditional static environments, which are permanent and can lead to inefficiencies, especially when underutilized. They offer a dynamic approach, allowing developers to create an isolated environment on demand without affecting the main codebase or other ongoing development activities.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-technical-and-business-value-of-ephemeral-environments&#34;&gt;The Technical and Business Value of Ephemeral Environments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments provide significant advantages in different sectors as mentioned below:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cost Efficiency&lt;/strong&gt;: By creating environments only when needed and tearing them down afterward, organizations avoid the cost of maintaining idle resources. This is particularly beneficial for companies where lower-end environments such as&amp;nbsp;&lt;code&gt;dev-env&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;non-prod&lt;/code&gt;&amp;nbsp;can cost up to five times more than production environments.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Agility and Speed&lt;/strong&gt;: Developers can quickly spin up environments to test new features or bug fixes without waiting for access to a shared environment. This agility accelerates development cycles and time-to-market.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Risk Reduction&lt;/strong&gt;: Testing in isolated environments ensures that unstable code does not affect the rest of the system, reducing the risk of bugs in production.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;is-an-ephemeral-environment-right-for-you&#34;&gt;Is an Ephemeral Environment Right for You?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Deciding whether ephemeral environments are suitable for your organization involves considering your development needs and organizational goals. Key questions include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Do your development teams frequently need isolated environments for testing?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Are you looking to optimize costs associated with non-production environments?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Is there a need to increase deployment speed and reduce risk in production?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you answered ‚Äúyes‚Äù to any of these, ephemeral environments could be highly beneficial for you.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;traditional-approach-to-ephemeral-environment&#34;&gt;Traditional Approach to Ephemeral Environment&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environment as mentioned above are the short lived environments, created and destroyed once the task is completed. We can create our scripts maybe in Terraform or Ansible or in python/shell to spin up a complete new environment that can be your VM Machines or Kubernetes clusters. Even though the automation can be achieved, there are few disadvantages associated with this approach, that include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Delay in Releases:&lt;/strong&gt;&amp;nbsp;The time taken to bring up the entire infrastructure can lead to delays in testing features or conducting sanity checks for bug fixes, resulting in a longer time to market.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;High Complexity:&amp;nbsp;&lt;/strong&gt;Creating and maintaining scripts to standardize environments across different stages can be complex and error-prone&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Manual Interventions:&amp;nbsp;&lt;/strong&gt;Even with automation scripts, manual interventions are often required to configure and install dependencies based on the application‚Äôs specific requirements, adding to the setup time.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;DevOps Dependencies:&lt;/strong&gt;&amp;nbsp;Developers typically lack expertise in tools like Terraform or Ansible, making them dependent on DevOps or SRE teams to make changes and install dependencies for their applications, which can slow down the development process.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Resource Management:&amp;nbsp;&lt;/strong&gt;Managing the lifecycle of ephemeral environments can be challenging. These environments need to be deleted once tasks are completed; otherwise, they lead to resource wastage and increased costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;High Infra Cost:&amp;nbsp;&lt;/strong&gt;The costs associated with spinning up and maintaining ephemeral environments, particularly in cloud-based setups, can add significantly to the overall infrastructure expenses.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;rethinking-ephemeral-environments&#34;&gt;Rethinking Ephemeral Environments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When we talk in terms of Kubernetes, setting up Ephemeral Environments becomes a lot easier than the traditional approach. Kubernetes has a beautiful thing called namespaces, a logical separation of group of resources, providing isolation of workloads within the same cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;By leveraging namespaces and some advanced autoscaling methods, it becomes much more easier to create a ephemeral environment that is cost-effective, less complex and helps you dynamically bring up the resources and hibernate when not in use. &amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;how-to-set-up-an-ephemeral-environment-in-k8s-manually&#34;&gt;How to Set Up an Ephemeral Environment in K8s Manually?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Setting up an ephemeral environment, especially within a Kubernetes ecosystem, involves several key steps that ensure agility, efficiency, and cost-effectiveness. Below, we detail a straightforward approach to creating and managing these temporary environments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 1: Define Your Infrastructure Requirements&lt;/strong&gt;&lt;br&gt;Before you create an ephemeral environment, it‚Äôs essential to understand the specific requirements of the application or feature being tested. This includes the necessary computing resources, the required services, and any dependencies that need to be replicated from the production environment.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 2: Automate the Environment Setup&lt;/strong&gt;&lt;br&gt;Automation is crucial in managing ephemeral environments to ensure they can be spun up and torn down efficiently. Tools like Terraform or Ansible can be used to script the creation of your infrastructure. In Kubernetes, you might automate setting up namespaces, deploying container images, and configuring network policies through CI/CD pipelines.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Using Kubernetes Namespaces&lt;/strong&gt;&lt;br&gt;In Kubernetes, namespaces provide a way to divide cluster resources between multiple users. Each ephemeral environment can be created in its namespace, isolating its running processes and resources from other environments&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl create namespace your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 3: Deploy Your Application&lt;/strong&gt;&lt;br&gt;Once the namespace is ready, deploy your application using Kubernetes manifests or Helm charts. This step often involves setting up the necessary config maps and secrets to configure the application according to the environment&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl apply -f your-application-deployment.yaml -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Or using Helm&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;helm install your-application-release your-helm-chart/ -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 4: Configure Autoscaling and Monitoring&lt;/strong&gt;&lt;br&gt;To optimize costs and resource usage, configure autoscaling for your application workloads. Kubernetes Horizontal Pod Autoscaler (HPA) or a more advanced tool like KEDA can be used to automatically adjust the number of pods based on traffic or other metrics&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;apiVersion: autoscaling/v2beta2&#xA;kind: HorizontalPodAutoscaler&#xA;metadata:&#xA;  name: your-application-hpa&#xA;  namespace: your-environment-name&#xA;spec:&#xA;  scaleTargetRef:&#xA;    apiVersion: apps/v1&#xA;    kind: Deployment&#xA;    name: your-application-deployment&#xA;  minReplicas: 1&#xA;  maxReplicas: 10&#xA;  metrics:&#xA;  - type: Resource&#xA;    resource:&#xA;      name: cpu&#xA;      target:&#xA;        type: Utilization&#xA;        averageUtilization: 50&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Monitoring is also essential to track the performance and health of your temporary environment. Tools like Prometheus for monitoring and Grafana for visualization can be integrated to monitor the environment‚Äôs metrics.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 5: Implement Cleanup Procedures&lt;/strong&gt;&lt;br&gt;To ensure that resources are not wasted, set up automatic cleanup procedures to tear down the environment after use. This can be scheduled using cron jobs or integrated into your CI/CD pipeline to destroy the environment once the testing is complete:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl delete namespace your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Or a more controlled cleanup with Helm&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;helm uninstall your-application-release -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 6: Documentation and Training&lt;/strong&gt;&lt;br&gt;Finally, document the entire process and provide training for your teams. This ensures that everyone understands how to efficiently use ephemeral environments, which helps in maximizing the benefits while minimizing potential disruptions or misuse.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Manually creating and deleting the namespaces, and integrating it within pipelines can be something a big pain when it comes to developer productivity. Integrating different tools such as Grafana, Prometheus, Jenkins, ArgoCD, KEDA, etc can be a tedius task for DevOps / SRE engineers as well. With the the involvement of custom scripting, again the complexities increases, high rish to human errors. With Devtron‚Äôs simplified workflow, it becomes a lot more easier to automate the process, and improve the developer productivity while reducing its high dependencies from DevOps/ SRE teams.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;how-devtron-simplifies-ephemeral-environments&#34;&gt;How Devtron Simplifies Ephemeral Environments?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Devtron enhances the management of ephemeral environments through its modern dashboard, simplified workflows, automation and effective cost-management strategies. Here are key features:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Namespace Utilization&lt;/strong&gt;: In Kubernetes, namespaces provide logical separation, allowing multiple ephemeral environments within the same cluster without additional cost. Devtron leverages this to minimize the overhead associated with setting up and tearing down environments.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Cost Management&lt;/strong&gt;: Devtron implements strategies such as leveraging spot instances and right-sizing resources, ensuring that the infrastructure costs are kept to a minimum. For example, by using spot instances, organizations can save up to 70-90% compared to standard costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Automated Scaling&lt;/strong&gt;: Devtron employs tools like KEDA for event-driven autoscaling, ensuring resources are used efficiently. Environments can scale down automatically during inactivity and scale up when needed, further optimizing costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Simplified Workflow:&amp;nbsp;&lt;/strong&gt;Devtron provides an intuitive dashboard for all operating on Kubernetes, providing Kubernetes-native CI/CD pipelines, simplifying the heavy scripting and stitching up of different tools to complete an end-to-end workflow.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Along with that, there are many other factors which makes the entire process much more seamless, such as visibility of workloads, application metrics, configurations management, etc.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;setting-up-ephemeral-environments-with-devtron&#34;&gt;Setting-up Ephemeral Environments With Devtron&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Devtron is a Software Distribution Platform designed for Kubernetes. On its mission to democratize Kubernetes, ephemeral environments are one among the many other features, that make life easier. With Devtron‚Äôs intuitive dashboard, operations on Kubernetes become flawless, and it goes with ephemeral environments as well. To get started with ephemeral environment, follow the below steps.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 1: Install the keda-add-on-http from the chart‚Äôs marketplace. Navigate to the&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/deploy-chart/overview-of-charts?ref=devtron.ai&#34;&gt;charts store&lt;/a&gt;, search for Keda, and as you can see in the below image, you can see all charts related to Keda. Select the appropriate helm chart and deploy it. To add any helm chart which is not listed on the charts store,&amp;nbsp;&lt;a href=&#34;https://devtron.ai/blog/helm-chart-deployment/&#34;&gt;free feel to check out this blog&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-11-at-3.35.11-PM.png&#34; alt=&#34;keda-http-add-on&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 1] KEDA HTTP Add-on Controller&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 2: Once the controller has been successfully installed, you can see a consolidated view of the deployed helm chart, along with its resources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-11-at-3.40.27-PM.png&#34; alt=&#34;resources-grouped-view&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 2] Controller Successfully Deployed&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 3: Now, let‚Äôs move and configure the ephemeral environment for my microservice called,&amp;nbsp;&lt;code&gt;payment-svc&lt;/code&gt;. To configure the ephemeral environment for any application, the process remains the same and you should be able to configure/ clone the workflows for different applications. Navigate to&amp;nbsp;&lt;code&gt;Workflow Editor&lt;/code&gt;, add a workflow for the respective environment where you want to deploy your applications, in our case, its&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment as you can see in the below image. To understand more about workflows in Devtron, feel free to refer the&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/applications/creating-application/workflow/ci-pipeline?ref=devtron.ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-4.45.56-PM.png&#34; alt=&#34;workflow-editor&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 3] Adding Deployment Pipeline&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 4: Once the workflow has been created, Devtron automatically creates&amp;nbsp;&lt;code&gt;environment-overrides&lt;/code&gt;&amp;nbsp;for the deployment environment.&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/applications/creating-application/environment-overrides?ref=devtron.ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Environment overrides&lt;/a&gt;&amp;nbsp;help you manage your Kubernetes configuration for the specific environment in a more efficient way. Under the&amp;nbsp;&lt;code&gt;environment override&lt;/code&gt;&amp;nbsp;&amp;gt;&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment, we can add the relevant configurations required in the deployment template which would create the HttpScaledObject, responsible for bringing the environment up and running dynamically as it receives any HTTP request.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-4.46.31-PM.png&#34; alt=&#34;deployment-template&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 4] Configuring HTTPScaledObject&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 5: After providing the relevant configuration, navigate to&amp;nbsp;&lt;code&gt;Build &amp;amp; Deploy&lt;/code&gt;&amp;nbsp;section, select the relevant image, and deploy it in the&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment. Upon successful deployment, you can see the application status as Healthy, and all details about the deployment are as you can see in the below image.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.14.09-PM.png&#34; alt=&#34;application-details&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 5] Application Details&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;You can also see all the resources deployed along with the deployment in a resourced grouped view, and perform operations such as checking the logs, events, manifests, or exec into the terminals. You can notice in the below image that, we have a Deployment object but there isn‚Äôt any pod running as of now. This is because it automatically scaled down the workload since there is no HTTP request hitting the given hostname/ service.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.28.14-PM.png&#34; alt=&#34;deployment-replicaset&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 6] Deployment &amp;amp; ReplicaSet&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 6: In the Devtron dashboard, it automatically picks up the ingress host and shows it in&amp;nbsp;&lt;code&gt;URLs&lt;/code&gt;&amp;nbsp;section at the top right of the dashboard as you can see in Fig. 5, and if any request has been made into the hostname, it will automatically scale up the pod and it can serve the traffic as you can see in the below image.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.51.33-PM.png&#34; alt=&#34;scaled-up-pod&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 7] Dynamically Scaled-up Pod&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments offer a flexible, cost-effective solution for managing development stages, particularly in a dynamic and fast-paced software development landscape. Devtron‚Äôs approach not only simplifies the management of these environments but also enhances cost efficiency and deployment agility.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Organizations looking to streamline their development processes and reduce costs should consider implementing ephemeral environments, especially those already using Kubernetes. With Devtron, the transition is smoother, allowing teams to focus more on innovation and less on infrastructure management.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Feel free to join our&amp;nbsp;&lt;a href=&#34;https://discord.devtron.ai/?ref=devtron.ai&#34;&gt;Discord Community&lt;/a&gt;&amp;nbsp;if you have any questions. Would love to address any queries or questions. If you liked Devtron, do give it a&amp;nbsp;&lt;a href=&#34;https://github.com/devtron-labs/devtron?ref=devtron.ai&#34;&gt;Star ‚≠êÔ∏è on GitHub&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://devtron.ai/blog/unlocking-the-power-of-ephemeral-environments-with-devtron/&#34;&gt;Devtron‚Äôs blog&lt;/a&gt; by Abhinav Dubey&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;TL;DR: The blog talks about how ephemeral environments with Devtron become much easier, reducing the complexities, automating the process, and optimizing infra cost.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the world of software development,&amp;nbsp;&lt;strong&gt;ephemeral environments&lt;/strong&gt;&amp;nbsp;are temporary setups that serve specific purposes, such as testing or staging new features. These environments are short-lived, designed to exist only for the duration of their use case‚Äîlike testing a feature branch‚Äîbefore being dismantled.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments contrast with traditional static environments, which are permanent and can lead to inefficiencies, especially when underutilized. They offer a dynamic approach, allowing developers to create an isolated environment on demand without affecting the main codebase or other ongoing development activities.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-technical-and-business-value-of-ephemeral-environments&#34;&gt;The Technical and Business Value of Ephemeral Environments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments provide significant advantages in different sectors as mentioned below:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cost Efficiency&lt;/strong&gt;: By creating environments only when needed and tearing them down afterward, organizations avoid the cost of maintaining idle resources. This is particularly beneficial for companies where lower-end environments such as&amp;nbsp;&lt;code&gt;dev-env&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;non-prod&lt;/code&gt;&amp;nbsp;can cost up to five times more than production environments.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Agility and Speed&lt;/strong&gt;: Developers can quickly spin up environments to test new features or bug fixes without waiting for access to a shared environment. This agility accelerates development cycles and time-to-market.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Risk Reduction&lt;/strong&gt;: Testing in isolated environments ensures that unstable code does not affect the rest of the system, reducing the risk of bugs in production.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;is-an-ephemeral-environment-right-for-you&#34;&gt;Is an Ephemeral Environment Right for You?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Deciding whether ephemeral environments are suitable for your organization involves considering your development needs and organizational goals. Key questions include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Do your development teams frequently need isolated environments for testing?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Are you looking to optimize costs associated with non-production environments?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Is there a need to increase deployment speed and reduce risk in production?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you answered ‚Äúyes‚Äù to any of these, ephemeral environments could be highly beneficial for you.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;traditional-approach-to-ephemeral-environment&#34;&gt;Traditional Approach to Ephemeral Environment&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environment as mentioned above are the short lived environments, created and destroyed once the task is completed. We can create our scripts maybe in Terraform or Ansible or in python/shell to spin up a complete new environment that can be your VM Machines or Kubernetes clusters. Even though the automation can be achieved, there are few disadvantages associated with this approach, that include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Delay in Releases:&lt;/strong&gt;&amp;nbsp;The time taken to bring up the entire infrastructure can lead to delays in testing features or conducting sanity checks for bug fixes, resulting in a longer time to market.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;High Complexity:&amp;nbsp;&lt;/strong&gt;Creating and maintaining scripts to standardize environments across different stages can be complex and error-prone&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Manual Interventions:&amp;nbsp;&lt;/strong&gt;Even with automation scripts, manual interventions are often required to configure and install dependencies based on the application‚Äôs specific requirements, adding to the setup time.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;DevOps Dependencies:&lt;/strong&gt;&amp;nbsp;Developers typically lack expertise in tools like Terraform or Ansible, making them dependent on DevOps or SRE teams to make changes and install dependencies for their applications, which can slow down the development process.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Resource Management:&amp;nbsp;&lt;/strong&gt;Managing the lifecycle of ephemeral environments can be challenging. These environments need to be deleted once tasks are completed; otherwise, they lead to resource wastage and increased costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;High Infra Cost:&amp;nbsp;&lt;/strong&gt;The costs associated with spinning up and maintaining ephemeral environments, particularly in cloud-based setups, can add significantly to the overall infrastructure expenses.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;rethinking-ephemeral-environments&#34;&gt;Rethinking Ephemeral Environments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When we talk in terms of Kubernetes, setting up Ephemeral Environments becomes a lot easier than the traditional approach. Kubernetes has a beautiful thing called namespaces, a logical separation of group of resources, providing isolation of workloads within the same cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;By leveraging namespaces and some advanced autoscaling methods, it becomes much more easier to create a ephemeral environment that is cost-effective, less complex and helps you dynamically bring up the resources and hibernate when not in use. &amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;how-to-set-up-an-ephemeral-environment-in-k8s-manually&#34;&gt;How to Set Up an Ephemeral Environment in K8s Manually?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Setting up an ephemeral environment, especially within a Kubernetes ecosystem, involves several key steps that ensure agility, efficiency, and cost-effectiveness. Below, we detail a straightforward approach to creating and managing these temporary environments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 1: Define Your Infrastructure Requirements&lt;/strong&gt;&lt;br&gt;Before you create an ephemeral environment, it‚Äôs essential to understand the specific requirements of the application or feature being tested. This includes the necessary computing resources, the required services, and any dependencies that need to be replicated from the production environment.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 2: Automate the Environment Setup&lt;/strong&gt;&lt;br&gt;Automation is crucial in managing ephemeral environments to ensure they can be spun up and torn down efficiently. Tools like Terraform or Ansible can be used to script the creation of your infrastructure. In Kubernetes, you might automate setting up namespaces, deploying container images, and configuring network policies through CI/CD pipelines.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Using Kubernetes Namespaces&lt;/strong&gt;&lt;br&gt;In Kubernetes, namespaces provide a way to divide cluster resources between multiple users. Each ephemeral environment can be created in its namespace, isolating its running processes and resources from other environments&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl create namespace your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 3: Deploy Your Application&lt;/strong&gt;&lt;br&gt;Once the namespace is ready, deploy your application using Kubernetes manifests or Helm charts. This step often involves setting up the necessary config maps and secrets to configure the application according to the environment&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl apply -f your-application-deployment.yaml -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Or using Helm&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;helm install your-application-release your-helm-chart/ -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 4: Configure Autoscaling and Monitoring&lt;/strong&gt;&lt;br&gt;To optimize costs and resource usage, configure autoscaling for your application workloads. Kubernetes Horizontal Pod Autoscaler (HPA) or a more advanced tool like KEDA can be used to automatically adjust the number of pods based on traffic or other metrics&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;apiVersion: autoscaling/v2beta2&#xA;kind: HorizontalPodAutoscaler&#xA;metadata:&#xA;  name: your-application-hpa&#xA;  namespace: your-environment-name&#xA;spec:&#xA;  scaleTargetRef:&#xA;    apiVersion: apps/v1&#xA;    kind: Deployment&#xA;    name: your-application-deployment&#xA;  minReplicas: 1&#xA;  maxReplicas: 10&#xA;  metrics:&#xA;  - type: Resource&#xA;    resource:&#xA;      name: cpu&#xA;      target:&#xA;        type: Utilization&#xA;        averageUtilization: 50&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Monitoring is also essential to track the performance and health of your temporary environment. Tools like Prometheus for monitoring and Grafana for visualization can be integrated to monitor the environment‚Äôs metrics.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 5: Implement Cleanup Procedures&lt;/strong&gt;&lt;br&gt;To ensure that resources are not wasted, set up automatic cleanup procedures to tear down the environment after use. This can be scheduled using cron jobs or integrated into your CI/CD pipeline to destroy the environment once the testing is complete:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl delete namespace your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Or a more controlled cleanup with Helm&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;helm uninstall your-application-release -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 6: Documentation and Training&lt;/strong&gt;&lt;br&gt;Finally, document the entire process and provide training for your teams. This ensures that everyone understands how to efficiently use ephemeral environments, which helps in maximizing the benefits while minimizing potential disruptions or misuse.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Manually creating and deleting the namespaces, and integrating it within pipelines can be something a big pain when it comes to developer productivity. Integrating different tools such as Grafana, Prometheus, Jenkins, ArgoCD, KEDA, etc can be a tedius task for DevOps / SRE engineers as well. With the the involvement of custom scripting, again the complexities increases, high rish to human errors. With Devtron‚Äôs simplified workflow, it becomes a lot more easier to automate the process, and improve the developer productivity while reducing its high dependencies from DevOps/ SRE teams.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;how-devtron-simplifies-ephemeral-environments&#34;&gt;How Devtron Simplifies Ephemeral Environments?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Devtron enhances the management of ephemeral environments through its modern dashboard, simplified workflows, automation and effective cost-management strategies. Here are key features:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Namespace Utilization&lt;/strong&gt;: In Kubernetes, namespaces provide logical separation, allowing multiple ephemeral environments within the same cluster without additional cost. Devtron leverages this to minimize the overhead associated with setting up and tearing down environments.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Cost Management&lt;/strong&gt;: Devtron implements strategies such as leveraging spot instances and right-sizing resources, ensuring that the infrastructure costs are kept to a minimum. For example, by using spot instances, organizations can save up to 70-90% compared to standard costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Automated Scaling&lt;/strong&gt;: Devtron employs tools like KEDA for event-driven autoscaling, ensuring resources are used efficiently. Environments can scale down automatically during inactivity and scale up when needed, further optimizing costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Simplified Workflow:&amp;nbsp;&lt;/strong&gt;Devtron provides an intuitive dashboard for all operating on Kubernetes, providing Kubernetes-native CI/CD pipelines, simplifying the heavy scripting and stitching up of different tools to complete an end-to-end workflow.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Along with that, there are many other factors which makes the entire process much more seamless, such as visibility of workloads, application metrics, configurations management, etc.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;setting-up-ephemeral-environments-with-devtron&#34;&gt;Setting-up Ephemeral Environments With Devtron&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Devtron is a Software Distribution Platform designed for Kubernetes. On its mission to democratize Kubernetes, ephemeral environments are one among the many other features, that make life easier. With Devtron‚Äôs intuitive dashboard, operations on Kubernetes become flawless, and it goes with ephemeral environments as well. To get started with ephemeral environment, follow the below steps.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 1: Install the keda-add-on-http from the chart‚Äôs marketplace. Navigate to the&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/deploy-chart/overview-of-charts?ref=devtron.ai&#34;&gt;charts store&lt;/a&gt;, search for Keda, and as you can see in the below image, you can see all charts related to Keda. Select the appropriate helm chart and deploy it. To add any helm chart which is not listed on the charts store,&amp;nbsp;&lt;a href=&#34;https://devtron.ai/blog/helm-chart-deployment/&#34;&gt;free feel to check out this blog&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-11-at-3.35.11-PM.png&#34; alt=&#34;keda-http-add-on&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 1] KEDA HTTP Add-on Controller&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 2: Once the controller has been successfully installed, you can see a consolidated view of the deployed helm chart, along with its resources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-11-at-3.40.27-PM.png&#34; alt=&#34;resources-grouped-view&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 2] Controller Successfully Deployed&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 3: Now, let‚Äôs move and configure the ephemeral environment for my microservice called,&amp;nbsp;&lt;code&gt;payment-svc&lt;/code&gt;. To configure the ephemeral environment for any application, the process remains the same and you should be able to configure/ clone the workflows for different applications. Navigate to&amp;nbsp;&lt;code&gt;Workflow Editor&lt;/code&gt;, add a workflow for the respective environment where you want to deploy your applications, in our case, its&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment as you can see in the below image. To understand more about workflows in Devtron, feel free to refer the&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/applications/creating-application/workflow/ci-pipeline?ref=devtron.ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-4.45.56-PM.png&#34; alt=&#34;workflow-editor&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 3] Adding Deployment Pipeline&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 4: Once the workflow has been created, Devtron automatically creates&amp;nbsp;&lt;code&gt;environment-overrides&lt;/code&gt;&amp;nbsp;for the deployment environment.&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/applications/creating-application/environment-overrides?ref=devtron.ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Environment overrides&lt;/a&gt;&amp;nbsp;help you manage your Kubernetes configuration for the specific environment in a more efficient way. Under the&amp;nbsp;&lt;code&gt;environment override&lt;/code&gt;&amp;nbsp;&amp;gt;&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment, we can add the relevant configurations required in the deployment template which would create the HttpScaledObject, responsible for bringing the environment up and running dynamically as it receives any HTTP request.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-4.46.31-PM.png&#34; alt=&#34;deployment-template&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 4] Configuring HTTPScaledObject&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 5: After providing the relevant configuration, navigate to&amp;nbsp;&lt;code&gt;Build &amp;amp; Deploy&lt;/code&gt;&amp;nbsp;section, select the relevant image, and deploy it in the&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment. Upon successful deployment, you can see the application status as Healthy, and all details about the deployment are as you can see in the below image.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.14.09-PM.png&#34; alt=&#34;application-details&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 5] Application Details&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;You can also see all the resources deployed along with the deployment in a resourced grouped view, and perform operations such as checking the logs, events, manifests, or exec into the terminals. You can notice in the below image that, we have a Deployment object but there isn‚Äôt any pod running as of now. This is because it automatically scaled down the workload since there is no HTTP request hitting the given hostname/ service.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.28.14-PM.png&#34; alt=&#34;deployment-replicaset&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 6] Deployment &amp;amp; ReplicaSet&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 6: In the Devtron dashboard, it automatically picks up the ingress host and shows it in&amp;nbsp;&lt;code&gt;URLs&lt;/code&gt;&amp;nbsp;section at the top right of the dashboard as you can see in Fig. 5, and if any request has been made into the hostname, it will automatically scale up the pod and it can serve the traffic as you can see in the below image.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.51.33-PM.png&#34; alt=&#34;scaled-up-pod&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 7] Dynamically Scaled-up Pod&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments offer a flexible, cost-effective solution for managing development stages, particularly in a dynamic and fast-paced software development landscape. Devtron‚Äôs approach not only simplifies the management of these environments but also enhances cost efficiency and deployment agility.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Organizations looking to streamline their development processes and reduce costs should consider implementing ephemeral environments, especially those already using Kubernetes. With Devtron, the transition is smoother, allowing teams to focus more on innovation and less on infrastructure management.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Feel free to join our&amp;nbsp;&lt;a href=&#34;https://discord.devtron.ai/?ref=devtron.ai&#34;&gt;Discord Community&lt;/a&gt;&amp;nbsp;if you have any questions. Would love to address any queries or questions. If you liked Devtron, do give it a&amp;nbsp;&lt;a href=&#34;https://github.com/devtron-labs/devtron?ref=devtron.ai&#34;&gt;Star ‚≠êÔ∏è on GitHub&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Thu, 11 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêKubestronaut in Orbit: Peter Barczi„ÄëÂú®ËΩ®ÈÅì‰∏äÁöÑ KubetronautÔºöPeter Barczi</title>
      <link>https://www.cncf.io/blog/2024/07/05/kubestronaut-in-orbit-peter-barczi/</link>
      <description>„Äê&lt;h2 class=&#34;wp-block-heading&#34;&gt;Get to know Peter Barczi&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1650&#34; height=&#34;866&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4.jpg&#34; alt=&#34;Kubestronaut in Orbit - Peter Barczi&#34; class=&#34;wp-image-113542&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4.jpg 1650w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-300x157.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1024x537.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-768x403.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-194x102.jpg 194w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-388x204.jpg 388w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-776x408.jpg 776w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1552x816.jpg 1552w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-900x472.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-381x200.jpg 381w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-762x400.jpg 762w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-590x310.jpg 590w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1180x620.jpg 1180w&#34; sizes=&#34;(max-width: 1650px) 100vw, 1650px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Peter, one of our first Kubestronauts, has been working with Kubernetes only since 2021 but has still managed to pass all of CNCF‚Äôs Kubernetes certifications. He‚Äôs currently the Sr. DevOps Engineer / TechLead at a company building a cloud offering with focus on confidential computing. In his role, he also manages physical servers, clouds, operating Linux OSs and Kubernetes clusters at scale and is an Internal Linux Trainer.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you‚Äôd like to be a Kubestronaut, get more details on the&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt; CNCF Kubestronaut &lt;/a&gt;page.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;When did you get started with kubernetes‚Äìwhat was your first project?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I started with K8s as self-learner in 2021.&amp;nbsp;My first project (not surprisingly) was the NGINX web server running on Kubernetes.&amp;nbsp; Later on, I used K8s during the migration of our infra services into Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Nowadays, within my current project, almost everything is a Kubernetes object, even the network switches, so I use it daily.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What are the primary CNCF projects you work on or use today?&amp;nbsp; What projects have you enjoyed the most in your career?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The primary CNCF projects I work with the most are:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes&#34;&gt;Kubernetes&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/fluxcd/&#34;&gt;FluxCD&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/prometheus&#34;&gt;Prometheus&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://github.com/grafana&#34;&gt;Grafana&lt;/a&gt; stack&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cilium&#34;&gt;Cilium&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/rook&#34;&gt;Rook&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Discovering a GitOps approach was a real game-changer in my work life.&amp;nbsp; And thanks to tools like &lt;a href=&#34;https://github.com/argoproj/argo-cd&#34;&gt;ArgoCD &lt;/a&gt;and &lt;a href=&#34;https://github.com/fluxcd/&#34;&gt;FluxCD,&lt;/a&gt; I realised how ‚Äúeasy‚Äù it can be to deploy and track apps using git.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Furthermore, thanks to &lt;a href=&#34;https://rook.io/&#34;&gt;Rook-ceph storage provide&lt;/a&gt;r I‚Äôm now able to spin up and configure storage clusters in a matter of a few minutes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What motivated you to get all the kubernetes certs?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Each certification has its own story.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;KCNA was my first certification. I took it right before a job interview for extra confidence in my Kubernetes skills.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;CKA &amp;amp; CKAD&amp;nbsp; were two certs I needed to help with a project I was working on.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;CKS was a cert I got as a personal goal.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KCSA was my last one. I got this one right after Kubecon when the Kubestronaut program was announced and realized I only needed one more.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How have the certs helped you in your career?&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The certifications allow me to practice and learn about technology and tools I do not use on a daily basis. Preparation for the certification is a way to get the education needed to be ready for future projects.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What are some other books/sites/courses you recommend for people who want to work with k8s?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I found these online courses were a good starting point for my Kubernetes learning journey:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‚Äì CKA and CKAD courses from kodekloud.com&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‚Äì CKS online course from Kim Wuestkamp on YouTube&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‚Äì CKA/CKAD/CKS practise exams from aclodguru/pluralsight&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What do you do in your free time?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I spend all my free time with my family, my 15-month-old daughter is my single point of interest currently. I also spend time in my cottage house, so I balance my work time there.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;As you know Kubernetes is turning 10 this year, what are you most excited about for Kubernetes in the next 10 years?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;My magic ball doesn‚Äôt have answers so far into the future:)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What would you tell someone who is just starting their K8s certification journey? Any tips or tricks?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;There are no tips and tricks better than to practise, practise, practise.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Today the Cloud native ecosystem is way more than Kubernetes. Do you plan to get other cloud native certifications from the CNCF?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Absolutely, the next challenge for me is to dive into the Prometheus stack and become familiar enough with it to become a &lt;a href=&#34;https://www.cncf.io/training/certification/pca/&#34;&gt;Prometheus Certified Associate (PCA)&lt;/a&gt;&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How did you get involved with cloud native and Kubernetes in general?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I have a background in Linux administration, so from there it was a logical and natural to progress to Kubernetes and cloud native.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you‚Äôd like to be a Kubestronaut,&amp;nbsp;get more details on the&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt; CNCF Kubestronaut &lt;/a&gt;page.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;h2 class=&#34;wp-block-heading&#34;&gt;‰∫ÜËß£ Peter Barczi&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =‚Äúwp-block-image size-full‚Äù&gt;&lt;imgÂä†ËΩΩ=‚Äúlazy‚ÄùËß£Á†Å=‚ÄúÂºÇÊ≠•‚ÄùÂÆΩÂ∫¶=‚Äú1650‚ÄùÈ´òÂ∫¶=‚Äú866‚Äùsrc=‚Äúhttps://www.cncf.io/ wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4.jpg&#34; alt=&#34;ËΩ®ÈÅì‰∏äÁöÑ Kubestronaut - Peter Barczi&#34; class=&#34;wp-image-113542&#34; srcset=&#34;https://www.cncf. io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4.jpg 1650wÔºåhttps://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4 -300x157.jpg 300wÔºåhttps://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1024x537.jpg 1024wÔºåhttps://www.cncf.io/wp -content/uploads/2024/06/Kubestronaut-in-Orbit-4-768x403.jpg 768wÔºåhttps://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4- 194x102.jpg 194wÔºåhttps://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-388x204.jpg 388wÔºåhttps://www.cncf.io/wp-ÂÜÖÂÆπ/uploads/2024/06/Kubestronaut-in-Orbit-4-776x408.jpg 776wÔºåhttps://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1552x816 .jpg 1552wÔºåhttps://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-900x472.jpg 900wÔºåhttps://www.cncf.io/wp-content /uploads/2024/06/Kubestronaut-in-Orbit-4-381x200.jpg 381wÔºåhttps://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-762x400„ÄÇ jpg 762wÔºåhttps://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-590x310.jpg 590wÔºåhttps://www.cncf.io/wp-content/ uploads/2024/06/Kubestronaut-in-Orbit-4-1180x620.jpg 1180w&#34;sizes=&#34;(max-width: 1650px) 100vw, 1650px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Peter ÊòØÊàë‰ª¨È¶ñÊâπ Kubetronaut ‰πã‰∏ÄÔºå‰ªñ‰ªé 2021 Âπ¥ÊâçÂºÄÂßã‰ΩøÁî® KubernetesÔºå‰ΩÜ‰ªçÁÑ∂ËÆæÊ≥ïÈÄöËøá‰∫Ü CNCF ÁöÑÊâÄÊúâ Kubernetes ËÆ§ËØÅ„ÄÇ‰ªñÁõÆÂâçÊòØ‰∏ÄÂÆ∂ÂÖ¨Âè∏ÁöÑÈ´òÁ∫ß DevOps Â∑•Á®ãÂ∏à/ÊäÄÊúØ‰∏ªÁÆ°ÔºåËØ•ÂÖ¨Âè∏Ëá¥Âäõ‰∫éÊûÑÂª∫‰∏ìÊ≥®‰∫éÊú∫ÂØÜËÆ°ÁÆóÁöÑ‰∫ë‰∫ßÂìÅ„ÄÇÂú®Ê≠§ËÅå‰Ωç‰∏äÔºå‰ªñËøòÁÆ°ÁêÜÁâ©ÁêÜÊúçÂä°Âô®„ÄÅ‰∫ë„ÄÅÂ§ßËßÑÊ®°Êìç‰Ωú Linux Êìç‰ΩúÁ≥ªÁªüÂíå Kubernetes ÈõÜÁæ§ÔºåÂπ∂‰∏îÊòØ‰∏ÄÂêçÂÜÖÈÉ® Linux ÂüπËÆ≠Â∏à„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Â¶ÇÊûúÊÇ®ÊÉ≥Êàê‰∏∫ KubestronautÔºåËØ∑Âú®&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt;CNCF Kubestronaut&lt;/a&gt;È°µÈù¢Ëé∑ÂèñÊõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ¬†&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ÊÇ®‰ªÄ‰πàÊó∂ÂÄôÂºÄÂßã‰ΩøÁî® kubernetes ‚Äì ÊÇ®ÁöÑÁ¨¨‰∏Ä‰∏™È°πÁõÆÊòØ‰ªÄ‰πàÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Êàë‰∫é 2021 Âπ¥ÂºÄÂßãËá™Â≠¶ K8s„ÄÇÊàëÁöÑÁ¨¨‰∏Ä‰∏™È°πÁõÆÔºàÊØ´‰∏çÂ•áÊÄ™ÔºâÊòØÂú® Kubernetes ‰∏äËøêË°åÁöÑ NGINX Web ÊúçÂä°Âô®„ÄÇ¬† ÂêéÊù•ÔºåÊàëÂú®Â∞ÜÂü∫Á°ÄËÆæÊñΩÊúçÂä°ËøÅÁßªÂà∞ Kubernetes ÁöÑËøáÁ®ã‰∏≠‰ΩøÁî®‰∫Ü K8s„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Áé∞Âú®ÔºåÂú®ÊàëÂΩìÂâçÁöÑÈ°πÁõÆ‰∏≠ÔºåÂá†‰πéÊâÄÊúâ‰∏úË•øÈÉΩÊòØ Kubernetes ÂØπË±°ÔºåÁîöËá≥ÊòØÁΩëÁªú‰∫§Êç¢Êú∫ÔºåÊâÄ‰ª•ÊàëÊØèÂ§©ÈÉΩ‰ΩøÁî®ÂÆÉ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ÊÇ®Áé∞Âú®‰ªé‰∫ãÊàñ‰ΩøÁî®ÁöÑ‰∏ªË¶Å CNCF È°πÁõÆÊòØ‰ªÄ‰πàÔºü¬† Âú®ÊÇ®ÁöÑËÅå‰∏öÁîüÊ∂Ø‰∏≠ÔºåÊÇ®ÊúÄÂñúÊ¨¢Âì™‰∫õÈ°πÁõÆÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊàëÂèÇ‰∏éÊúÄÂ§öÁöÑ‰∏ªË¶Å CNCF È°πÁõÆÊòØÔºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes&#34;&gt;Kubernetes&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/fluxcd/&#34;&gt;FluxCD&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/prometheus&#34;&gt;Prometheus&lt;/a&gt; Âíå &lt;a href=&#34;https://github.com/grafana&#34;&gt;Grafana&lt;/a&gt; Â†ÜÊ†à&lt;/li &gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cilium&#34;&gt;Cilium&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/rook&#34;&gt;Rook&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÂèëÁé∞ GitOps ÊñπÊ≥ïÁúüÊ≠£ÊîπÂèò‰∫ÜÊàëÁöÑÂ∑•‰ΩúÁîüÊ¥ª„ÄÇ¬† ÊÑüË∞¢ &lt;a href=&#34;https://github.com/argoproj/argo-cd&#34;&gt;ArgoCD &lt;/a&gt; Âíå &lt;a href=&#34;https://github.com/fluxcd/&#34;&gt;FluxCD Á≠âÂ∑•ÂÖ∑Ôºå &lt;/a&gt; ÊàëÊÑèËØÜÂà∞‰ΩøÁî® git ÈÉ®ÁΩ≤ÂíåË∑üË∏™Â∫îÁî®Á®ãÂ∫èÊòØÂ§ö‰πà‚ÄúÂÆπÊòì‚Äù„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ê≠§Â§ñÔºåÊÑüË∞¢ &lt;a href=&#34;https://rook.io/&#34;&gt;Rook-ceph Â≠òÂÇ®Êèê‰æõÂïÜ&lt;/a&gt;ÔºåÊàëÁé∞Âú®Âè™ÈúÄÂá†ÂàÜÈíüÂç≥ÂèØÂêØÂä®ÂíåÈÖçÁΩÆÂ≠òÂÇ®ÈõÜÁæ§ÂàÜÈíü„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ÊòØ‰ªÄ‰πà‰øÉ‰ΩøÊÇ®Ëé∑ÂæóÊâÄÊúâ Kubernetes ËØÅ‰π¶Ôºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊØè‰∏™ËÆ§ËØÅÈÉΩÊúâËá™Â∑±ÁöÑÊïÖ‰∫ã„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;KCNA ÊòØÊàëÁöÑÁ¨¨‰∏Ä‰∏™ËÆ§ËØÅ„ÄÇÊàëÂú®Â∑•‰ΩúÈù¢ËØïÂâçÊãçÊëÑ‰∫ÜÂÆÉÔºå‰ª•‰æøÂØπÊàëÁöÑ Kubernetes ÊäÄËÉΩÊõ¥Êúâ‰ø°ÂøÉ„ÄÇ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;CKA Âíå CKAD ÊòØÊàëÈúÄË¶ÅÂ∏ÆÂä©ÂÆåÊàêÊàëÊ≠£Âú®ÂºÄÂ±ïÁöÑÈ°πÁõÆÁöÑ‰∏§‰∏™ËØÅ‰π¶„ÄÇ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;CKS ÊòØÊàë‰Ωú‰∏∫‰∏™‰∫∫ÁõÆÊ†áËÄåËé∑ÂæóÁöÑËØÅ‰π¶„ÄÇ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KCSA ÊòØÊàëÁöÑÊúÄÂêé‰∏Ä‰∏™„ÄÇ Kubecon ÁªìÊùüÂêéÔºåÂΩì Kubetronaut ËÆ°ÂàíÂÆ£Â∏ÉÊó∂ÔºåÊàëÂæóÂà∞‰∫ÜËøô‰∏™ÔºåÂπ∂ÊÑèËØÜÂà∞ÊàëÂè™ÈúÄË¶ÅÂÜç‰∏Ä‰∏™„ÄÇ&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ëøô‰∫õËØÅ‰π¶ÂØπÊÇ®ÁöÑËÅå‰∏öÁîüÊ∂ØÊúâ‰ΩïÂ∏ÆÂä©Ôºü¬†&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ëøô‰∫õËÆ§ËØÅ‰ΩøÊàëËÉΩÂ§üÁªÉ‰π†ÂíåÂ≠¶‰π†ÊàëÊó•Â∏∏‰∏ç‰ΩøÁî®ÁöÑÊäÄÊúØÂíåÂ∑•ÂÖ∑„ÄÇÂáÜÂ§áËÆ§ËØÅÊòØËé∑Âæó‰∏∫Êú™Êù•È°πÁõÆÂÅöÂ•ΩÂáÜÂ§áÊâÄÈúÄÊïôËÇ≤ÁöÑ‰∏ÄÁßçÊñπÂºè„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ÊÇ®‰∏∫ÊÉ≥Ë¶Å‰ΩøÁî® k8s ÁöÑ‰∫∫Êé®Ëçê‰∫ÜÂì™‰∫õÂÖ∂‰ªñ‰π¶Á±ç/ÁΩëÁ´ô/ËØæÁ®ãÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊàëÂèëÁé∞Ëøô‰∫õÂú®Á∫øËØæÁ®ãÊòØÊàëÁöÑ Kubernetes Â≠¶‰π†‰πãÊóÖÁöÑ‰∏Ä‰∏™ÂæàÂ•ΩÁöÑËµ∑ÁÇπÔºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‚Äì Êù•Ëá™ kodekloud.com ÁöÑ CKA Âíå CKAD ËØæÁ®ã&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‚Äì Kim Wuestkamp Âú® YouTube ‰∏äÊèê‰æõÁöÑ CKS Âú®Á∫øËØæÁ®ã&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‚Äì Êù•Ëá™ aclodguru/pluralsight ÁöÑ CKA/CKAD/CKS Ê®°ÊãüËÄÉËØï&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;‰Ω†Á©∫Èó≤Êó∂Èó¥ÂÅö‰ªÄ‰πàÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊàëÊâÄÊúâÁöÑÁ©∫Èó≤Êó∂Èó¥ÈÉΩÂíåÂÆ∂‰∫∫‰∏ÄËµ∑Â∫¶ËøáÔºåÊàë 15 ‰∏™ÊúàÂ§ßÁöÑÂ•≥ÂÑøÁõÆÂâçÊòØÊàëÂîØ‰∏ÄÁöÑÂÖ¥Ë∂£ÁÇπ„ÄÇÊàë‰πüËä±Êó∂Èó¥Âú®ÊàëÁöÑÂ∞èÂ±ãÈáåÔºåÊâÄ‰ª•ÊàëÂú®ÈÇ£ÈáåÂπ≥Ë°°ÊàëÁöÑÂ∑•‰ΩúÊó∂Èó¥„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ê≠£Â¶ÇÊÇ®ÊâÄÁü•ÔºåKubernetes ‰ªäÂπ¥Â∑≤Êª° 10 Â≤Å‰∫ÜÔºåÊÇ®ÂØπ Kubernetes Êú™Êù• 10 Âπ¥ÊúÄÂÖ¥Â•ãÁöÑÊòØ‰ªÄ‰πàÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊàëÁöÑÈ≠îÊ≥ïÁêÉÂà∞ÁõÆÂâç‰∏∫Ê≠¢ËøòÊ≤°ÊúâÁ≠îÊ°à:)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ÊÇ®‰ºöÂØπÂàöÂàöÂºÄÂßã K8s ËÆ§ËØÅ‰πãÊóÖÁöÑ‰∫∫ËØ¥‰∫õ‰ªÄ‰πàÔºüÊúâ‰ªÄ‰πàÊèêÁ§∫ÊàñÊäÄÂ∑ßÂêóÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ê≤°ÊúâÊØîÁªÉ‰π†„ÄÅÁªÉ‰π†„ÄÅÂÜçÁªÉ‰π†Êõ¥Â•ΩÁöÑÊèêÁ§∫ÂíåÊäÄÂ∑ß‰∫Ü„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Â¶Ç‰ªäÔºå‰∫ëÂéüÁîüÁîüÊÄÅÁ≥ªÁªüÁöÑÊÑè‰πâËøú‰∏çÊ≠¢ Kubernetes„ÄÇÊÇ®ÊòØÂê¶ËÆ°ÂàíËé∑Âæó CNCF ÁöÑÂÖ∂‰ªñ‰∫ëÂéüÁîüËÆ§ËØÅÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÂΩìÁÑ∂ÔºåÊàëÁöÑ‰∏ã‰∏Ä‰∏™ÊåëÊàòÊòØÊ∑±ÂÖ•Á†îÁ©∂ Prometheus Â†ÜÊ†àÂπ∂Ë∂≥Â§üÁÜüÊÇâÂÆÉÔºåÊàê‰∏∫‰∏ÄÂêç &lt;a href=&#34;https://www.cncf.io/training/certification/pca/&#34;&gt;PrometheusËÆ§ËØÅÂä©ÁêÜ (PCA)&lt;/a&gt;¬†&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ÊÇ®ÊòØÂ¶Ç‰ΩïÊ∂âË∂≥‰∫ëÂéüÁîüÂíå Kubernetes ÁöÑÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊàëÊúâ Linux ÁÆ°ÁêÜËÉåÊôØÔºåÂõ†Ê≠§‰ªéÈÇ£Êó∂Ëµ∑ÔºåÂèëÂ±ïÂà∞ Kubernetes Âíå‰∫ëÂéüÁîüÊòØÂêà‰πéÈÄªËæë‰∏îËá™ÁÑ∂ÁöÑ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Â¶ÇÊûúÊÇ®ÊÉ≥Êàê‰∏∫ KubestronautÔºåËØ∑Âú®&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt;CNCF Kubestronaut&lt;/a&gt;È°µÈù¢Ëé∑ÂèñÊõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ¬†&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Thu, 04 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêA new App Development WG has now been launched!„ÄëÊñ∞ÁöÑÂ∫îÁî®Á®ãÂ∫èÂºÄÂèëÂ∑•‰ΩúÁªÑÁé∞Â∑≤ÂêØÂä®ÔºÅ</title>
      <link>https://www.cncf.io/blog/2024/07/05/a-new-app-development-wg-has-now-been-launched/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;TAG post from TAG App Delivery &lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Calling all developers!&lt;/strong&gt; We‚Äôre excited to announce the launch of the new &lt;a href=&#34;https://tag-app-delivery.cncf.io/wgs/app-development/&#34;&gt;App Development Working Group&lt;/a&gt; within the &lt;a href=&#34;https://tag-app-delivery.cncf.io/&#34;&gt;TAG App Delivery&lt;/a&gt;. This group is dedicated to bridging the gap between developers and CNCF projects directly affecting your daily workflow.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Get involved and help shape the future of cloud-native application development! The &lt;a href=&#34;https://tag-app-delivery.cncf.io/wgs/app-development/charter/charter.md/&#34;&gt;charter&lt;/a&gt; provides more information about the working group‚Äôs goals and direction.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The working group is co-chaired by Daniel Oh from Red Hat, Thomas Vitale from Systematic, and Mauricio Salatino from Diagrid. Leveraging the deep expertise of the co-chairs and group members (e.g., Ryan Nowak ‚Äì Microsoft, Eli Aleyner ‚Äì Docker, Marcos Lilljedahl ‚Äì Dagger, Sonali Srivastava ‚Äì InfraCloud Technologies, and Yacine Kheddache ‚Äì Microcks) in development practices, platform engineering, and the CNCF ecosystem, the group is initially focused on highlighting Graduated and Incubating projects that directly benefit developers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Beyond raising awareness, the working group actively shapes the cloud-native development landscape by classifying tools and fostering collaboration between CNCF projects and app developers. This makes it easier for the developers to find the right tools and share best practices. The working group also actively seeks out and integrates new projects prioritizing best practices for cloud-native application development.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Join the movement!&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Are you a cloud-native developer passionate about improving development? We‚Äôd love for you to be part of it!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://tag-app-delivery.cncf.io/wgs/app-development/&#34;&gt;Bi-weekly meetings&lt;/a&gt;: Participate in our discussions on each month‚Äôs first and third Wednesdays.&amp;nbsp;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/1L7e2szHX_gpYnC0cs_BTQH6hTNzh8acWzqkzhrPOJds/edit#wg-app-developmen&#34;&gt;CNCF Slack Channel&lt;/a&gt;: Connect with us on the #wg-app-development channel on the CNCF Slack workspace:&amp;nbsp;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Are you heading to &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&#34;&gt;KubeCon + CLoudNativeCon North America 2024&lt;/a&gt;? Don‚Äôt miss out on connecting with fellow developers at the co-located &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&#34;&gt;AppDevelopmentCon&lt;/a&gt; event! We‚Äôre also going to &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-india/&#34;&gt;KubeCon + CloudNativeCon India 2024&lt;/a&gt;. Let‚Äôs meet up and chat about cloud-native development.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Êù•Ëá™ TAG App Delivery ÁöÑ TAG Â∏ñÂ≠ê&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ÂëºÂè´ÊâÄÊúâÂºÄÂèëËÄÖÔºÅ&lt;/strong&gt;Êàë‰ª¨ÂæàÈ´òÂÖ¥Âú∞ÂÆ£Â∏ÉÊé®Âá∫Êñ∞ÁöÑ&lt;a href=&#34;https://tag-app-delivery.cncf.io/wgs/app-development/ &lt;a href=&#34;https://tag-app-delivery.cncf.io/&#34;&gt;TAG Â∫îÁî®‰∫§‰ªò&lt;/a&gt;ÂÜÖÁöÑ&#34;&gt;Â∫îÁî®ÂºÄÂèëÂ∑•‰ΩúÁªÑ&lt;/a&gt;„ÄÇËØ•Â∞èÁªÑËá¥Âäõ‰∫éÂº•ÂêàÂºÄÂèë‰∫∫Âëò‰∏éÁõ¥Êé•ÂΩ±ÂìçÊÇ®Êó•Â∏∏Â∑•‰ΩúÊµÅÁ®ãÁöÑ CNCF È°πÁõÆ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÂèÇ‰∏éÂπ∂Â∏ÆÂä©Â°ëÈÄ†‰∫ëÂéüÁîüÂ∫îÁî®Á®ãÂ∫èÂºÄÂèëÁöÑÊú™Êù•ÔºÅ &lt;a href=&#34;https://tag-app-delivery.cncf.io/wgs/app-development/charter/charter.md/&#34;&gt;Á´†Á®ã&lt;/a&gt;Êèê‰æõ‰∫ÜÊúâÂÖ≥Â∑•‰ΩúÁªÑÁõÆÊ†áÂíåÊñπÂêëÁöÑÊõ¥Â§ö‰ø°ÊÅØ„ÄÇ &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ËØ•Â∑•‰ΩúÁªÑÁî±Êù•Ëá™ Red Hat ÁöÑ Daniel Oh„ÄÅÊù•Ëá™ Systematic ÁöÑ Thomas Vitale ÂíåÊù•Ëá™ Diagrid ÁöÑ Mauricio Salatino ÊãÖ‰ªªËÅîÂêà‰∏ªÂ∏≠„ÄÇÂà©Áî®ËÅîÂêà‰∏ªÂ∏≠ÂíåÂ∞èÁªÑÊàêÂëòÔºà‰æãÂ¶ÇÔºåRyan Nowak ‚Äì Microsoft„ÄÅEli Aleyner ‚Äì Docker„ÄÅMarcos Lilljedahl ‚Äì Dagger„ÄÅSonali Srivastava ‚Äì InfraCloud Technologies Âíå Yacine Kheddache ‚Äì MicrocksÔºâÂú®ÂºÄÂèëÂÆûË∑µ„ÄÅÂπ≥Âè∞Â∑•Á®ãÂíåÂú® CNCF ÁîüÊÄÅÁ≥ªÁªü‰∏≠ÔºåËØ•Â∞èÁªÑÊúÄÂàù‰∏ìÊ≥®‰∫éÁ™ÅÂá∫Áõ¥Êé•‰ΩøÂºÄÂèë‰∫∫ÂëòÂèóÁõäÁöÑÊØï‰∏öÂíåÂ≠µÂåñÈ°πÁõÆ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Èô§‰∫ÜÊèêÈ´òËÆ§ËØÜ‰πãÂ§ñÔºåÂ∑•‰ΩúÁªÑËøòÈÄöËøáÂØπÂ∑•ÂÖ∑ËøõË°åÂàÜÁ±ªÂπ∂‰øÉËøõ CNCF È°πÁõÆÂíåÂ∫îÁî®ÂºÄÂèë‰∫∫Âëò‰πãÈó¥ÁöÑÂçè‰ΩúÔºåÁßØÊûÅÂ°ëÈÄ†‰∫ëÂéüÁîüÂºÄÂèëÊ†ºÂ±Ä„ÄÇËøô‰ΩøÂæóÂºÄÂèë‰∫∫ÂëòÊõ¥ÂÆπÊòìÊâæÂà∞ÂêàÈÄÇÁöÑÂ∑•ÂÖ∑Âπ∂ÂàÜ‰∫´ÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇËØ•Â∑•‰ΩúÁªÑËøòÁßØÊûÅÂØªÊâæÂπ∂Êï¥ÂêàÊñ∞È°πÁõÆÔºå‰ºòÂÖàËÄÉËôë‰∫ëÂéüÁîüÂ∫îÁî®Á®ãÂ∫èÂºÄÂèëÁöÑÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Âä†ÂÖ•ËøêÂä®ÔºÅ&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊÇ®ÊòØ‰∏Ä‰ΩçÁÉ≠Ë°∑‰∫éÊîπËøõÂºÄÂèëÁöÑ‰∫ëÂéüÁîüÂºÄÂèë‰∫∫ÂëòÂêóÔºüÊàë‰ª¨Â∏åÊúõÊÇ®ËÉΩÂèÇ‰∏éÂÖ∂‰∏≠ÔºÅ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://tag-app-delivery.cncf.io/wgs/app-development/&#34;&gt;ÊØè‰∏§Âë®‰∏ÄÊ¨°ÁöÑ‰ºöËÆÆ&lt;/a&gt;ÔºöÂèÇ‰∏éÊàë‰ª¨ÊØèÊúàÁ¨¨‰∏Ä‰∏™ÂíåÁ¨¨‰∏â‰∏™Âë®‰∏âÁöÑËÆ®ËÆ∫„ÄÇ¬†&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/1L7e2szHX_gpYnC0cs_BTQH6hTNzh8acWzqkzhrPOJds/edit#wg-app-developmen&#34;&gt;CNCF Slack È¢ëÈÅì&lt;/a&gt;ÔºöÈÄöËøá #wg-app ‰∏éÊàë‰ª¨ËÅîÁ≥ª-CNCF Slack Â∑•‰ΩúÂå∫‰∏äÁöÑÂºÄÂèëÈ¢ëÈÅìÔºö&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊÇ®Ë¶ÅÂéªÂèÇÂä†&lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&#34;&gt;KubeCon + CLoudNativeCon North America 2024&lt;/a&gt;ÂêóÔºü‰∏çË¶ÅÈîôËøáÂú®ÂêåÊúü‰∏æË°åÁöÑ &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&#34;&gt;AppDevelopmentCon&lt;/a&gt; Ê¥ªÂä®‰∏≠‰∏éÂÖ∂‰ªñÂºÄÂèë‰∫∫Âëò‰∫§ÊµÅÔºÅÊàë‰ª¨ËøòÂ∞ÜÂèÇÂä† &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-india/&#34;&gt;KubeCon + CloudNativeCon India 2024&lt;/a&gt;„ÄÇËÆ©Êàë‰ª¨ËÅöÂú®‰∏ÄËµ∑ËÅäËÅä‰∫ëÂéüÁîüÂºÄÂèëÂêß„ÄÇ&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Thu, 04 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêNow what? Kubernetes troubleshooting with AI?„ÄëÊÄé‰πàÂäûÔºü Kubernetes ‰ΩøÁî® AI ËøõË°åÊïÖÈöúÊéíÈô§Ôºü</title>
      <link>https://www.cncf.io/blog/2024/07/11/now-what-kubernetes-troubleshooting-with-ai/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;Ambassador post originally published on &lt;a href=&#34;https://eminalemdar.medium.com/&#34;&gt;Medium &lt;/a&gt;by Emin Alemdar&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;cb51&#34;&gt;We all know that Kubernetes troubleshooting is difficult and it can get pretty complex from time to time. We can easily get lost in the logs, jumping between pods, searching through events and what have you. Most importantly, finding a meaningful explanation can be an extremely huge problem because not all of the logs are easily understandable. We also have to accept that it is time consuming. Of course we can use an external observability tool or even an observability stack with all of the components but still not all of the outputs are easy to understand and even read. At the end of the day, diagnose times and of course the troubleshooting times are extending drastically as a result.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;358a&#34;&gt;But let‚Äôs talk about the elephant in the room here. Can AI help us here? Especially for Kubernetes troubleshooting. Because we started using&amp;nbsp;&lt;strong&gt;AI&lt;/strong&gt;&amp;nbsp;and mostly&amp;nbsp;&lt;strong&gt;LLMs&lt;/strong&gt;&amp;nbsp;everyday not just in our jobs but in our daily activities as well. There are of course some&amp;nbsp;&lt;strong&gt;AIOps&lt;/strong&gt;&amp;nbsp;tools out there that help us implement the observability solution with the help of AI but almost all of those tools consume huge amounts of GPU resources and that increases the underlying cost and of course the maintainability issue here.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;140d&#34;&gt;So, in this blog post, I want to introduce a&amp;nbsp;&lt;strong&gt;CNCF Sandbox&lt;/strong&gt;&amp;nbsp;project to you which is a really powerful tool designed to simplify Kubernetes management using AI and natural language processing. The project is called&amp;nbsp;&lt;strong&gt;K8sGPT&lt;/strong&gt;&amp;nbsp;and K8sGPT is a tool for scanning your Kubernetes clusters, diagnosing and triaging issues in simple English. So basically, K8sGPT integrates with various AI backends, including OpenAI, Azure OpenAI, and Amazon Bedrock, to provide clear and actionable insights into your Kubernetes environment and it provides these insights in a user friendly format.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*rWs5LOWvxt5df7gW&#34; alt=&#34;Image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 class=&#34;wp-block-heading&#34; id=&#34;439b&#34;&gt;Key Features of K8sGPT&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;6ba7&#34;&gt;Before sharing some examples of K8sGPT usage, I want to share some specific key features of it. Let‚Äôs break them down into bullet points.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Workload Health Analysis&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;969d&#34;&gt;Of course we should start with this because that is one of the most important reasons this tool is here. K8sGPT scans your Kubernetes clusters to identify critical issues affecting your workloads. It transforms complex diagnostic data and logs into simple, understandable suggestions, making it easier to maintain cluster health. Yes, K8sGPT also provides suggestions for the problems it detects in the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;AI-Powered Diagnostics&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a06c&#34;&gt;This one is obvious. K8sGPT leverages AI and LLMs. K8sGPT provides detailed explanations of detected issues in plain English. This feature is powered by integrations with leading AI platforms like OpenAI. Basically, by using these AI platforms, K8sGPT transforms the diagnostic data into a very human friendly format.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Built-in and Custom Analysers&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;5583&#34;&gt;K8sGPT includes several built-in analyzers for various Kubernetes resources, such as Pods, Services, and Nodes. But of course with additional integrations, you can also create custom analysers to meet specific needs. K8sGPT has integrations with AWS, Prometheus, KEDA and Trivy. With these native integrations, you can have more detailed analysis. Of course I believe this list is going to be extended and more integrations will be added here in the future.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Continuous Monitoring and Integration&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;d67b&#34;&gt;You can also deploy K8sGPT as a Kubernetes Operator within a Kubernetes cluster. K8sGPT can continuously monitor the environment and integrate seamlessly with existing tools like Prometheus and Alertmanager in the cluster. You can see the components that the K8sGPT Operator instals and manages from the diagram below.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*loluayfTjzR3OyVv&#34; alt=&#34;image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Security CVE Review&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;e023&#34;&gt;With the Trivy integration, K8sGPT helps in identifying and addressing security vulnerabilities within your Kubernetes clusters. Once you activate the Trivy integration, Trivy Kubernetes Operator will be installed into the Kubernetes cluster and make it possible for K8sGPT to interact with the results of the Operator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;fbd6&#34;&gt;I also want to specifically talk about the&amp;nbsp;&lt;strong&gt;AWS&lt;/strong&gt;&amp;nbsp;integration here. I‚Äôm pretty sure you‚Äôre already familiar with the&amp;nbsp;&lt;strong&gt;AWS Controllers for Kubernetes (ACK)&lt;/strong&gt;&amp;nbsp;project but if not, I also have a blog post about that and you can check that out from&amp;nbsp;&lt;a href=&#34;https://medium.com/@eminalemdar/manage-your-aws-resources-from-kubernetes-with-ack-3cf06a4b0770&#34;&gt;here&lt;/a&gt;. In short, ACK allows you to manage AWS services from your Kubernetes clusters with CRDs. This integration also helps K8sGPT to interact with the AWS resources managed by ACK. As a result, you can use K8sGPT to analyse and manage not only your Kubernetes resources but also your AWS resources that are under the management of ACK.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;01bf&#34;&gt;Let‚Äôs play with K8sGPT&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;4756&#34;&gt;Installation of K8sGPT CLI is extremely easy and you can use popular packet managers like Brew to install it on your machine. After the installation, you will need to authenticate with your chosen AI provider. I‚Äôm using OpenAI here with GPT4 but you can of course choose the appropriate one for you:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;k8sgpt auth list&lt;br&gt;Default:&lt;br&gt;&amp;gt; openai&lt;br&gt;Active:&lt;br&gt;&amp;gt; openai&lt;br&gt;Unused:&lt;br&gt;&amp;gt; localai&lt;br&gt;&amp;gt; azureopenai&lt;br&gt;&amp;gt; cohere&lt;br&gt;&amp;gt; amazonbedrock&lt;br&gt;&amp;gt; amazonsagemaker&lt;br&gt;&amp;gt; google&lt;br&gt;&amp;gt; noopai&lt;br&gt;&amp;gt; huggingface&lt;br&gt;&amp;gt; googlevertexai&lt;br&gt;&amp;gt; oci&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;ddc5&#34;&gt;You can use&amp;nbsp;&lt;code&gt;k8sgpt auth add&lt;/code&gt;&amp;nbsp;command to add the provider backend authentication.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;0e6f&#34;&gt;After this step, it‚Äôs pretty straightforward actually. Let‚Äôs start the first analysis with the&amp;nbsp;&lt;code&gt;k8sgpt analyse&lt;/code&gt;&amp;nbsp;command. But before running the command let me just deploy a broken Pod to see the actual diagnosis part.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl apply -f - &amp;lt;&amp;lt;EOF&lt;br&gt;apiVersion: v1&lt;br&gt;kind: Pod&lt;br&gt;metadata:&lt;br&gt;name: broken-pod&lt;br&gt;namespace: default&lt;br&gt;spec:&lt;br&gt;containers:&lt;br&gt;- name: broken-pod&lt;br&gt;image: nginx:1.a.b.c&lt;br&gt;livenessProbe:&lt;br&gt;httpGet:&lt;br&gt;path: /&lt;br&gt;port: 81&lt;br&gt;initialDelaySeconds: 3&lt;br&gt;periodSeconds: 3&lt;br&gt;EOF&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;8645&#34;&gt;As you can see the image part of this Pod definition is wrong. So let me run the&amp;nbsp;&lt;code&gt;k8sgpt analyse&lt;/code&gt;&amp;nbsp;command and see the output.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*mSTa1ykjgzbeBas2&#34; alt=&#34;Code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;616a&#34;&gt;We can see the error but not in detail but there is a flag for us to use here. If we add the&amp;nbsp;&lt;code&gt;k8sgpt analyse --explain&lt;/code&gt;&amp;nbsp;flag here, K8sGPT will connect to the AI provider and LLM here.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*xAdRU8htHZ3VxrtY&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;637f&#34;&gt;So now we have some more details about the issue and also some recommendations as well. I know this is a very basic example but let‚Äôs extend this. I will now enable two integrations, AWS and Trivy. If we look at the filters we can use now, we see the added filters coming from the integrations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:390/0*s-zY2Cb0irELZNms&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a48f&#34;&gt;Let‚Äôs start with the EKS filter and see if there‚Äôs anything wrong with that.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:466/0*uwSRiMmwhveF5-k5&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;2054&#34;&gt;Brilliant! No problems here but let‚Äôs see the results of the Vulnerability Report from Trivy.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*5tX1K5Wi2SOZNAv2&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;3f28&#34;&gt;Wow! That‚Äôs a lot! Let‚Äôs dive in to understand some of these issues.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*smPQrxM4w4VOtjP0&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*YtfG2_J9R1f4oWMk&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a139&#34;&gt;As you can see from the screenshots, We have very detailed information about each issue and again some really good recommendations on how to solve the problems.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;ae22&#34;&gt;Let me also deploy some other resources to the cluster and see the information about those.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*5rbT8-9aQ-bXGYcz&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;2953&#34;&gt;As you can see from the broken resources, I now have a very good understanding of what is really happening in my cluster here.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;bdbf&#34;&gt;Some final words here. Of course we all have some questions around security of these AI backends but there is also another option here called anonymise which basically anonymise the data before sending it to the AI backend.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;2ade&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;25df&#34;&gt;In the future K8sGPT might turn into one of the tools that‚Äôs going to transform Kubernetes management, problem diagnosis and of course troubleshooting by making it more accessible and efficient. Whether you‚Äôre an SRE, DevOps Engineer or Platform Engineer, K8sGPT can help you reduce troubleshooting times. It is extremely easy to get started with K8sGPT, you can just install it on your Kubernetes environment or on your local machine, authenticate with your chosen AI backend, and connect it to your cluster. In my opinion, you should definitely give K8sGPT a try.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://eminalemdar.medium.com/?source=post_page-----9c68d26e00ac--------------------------------&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Â§ß‰ΩøÂ∏ñÂ≠êÊúÄÂàùÁî± Emin Alemdar Âú® &lt;a href=&#34;https://eminalemdar.medium.com/&#34;&gt;Medium&lt;/a&gt; ‰∏äÂèëÂ∏É&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;cb51&#34;&gt;Êàë‰ª¨ÈÉΩÁü•ÈÅì Kubernetes ÊïÖÈöúÊéíÈô§ÂæàÂõ∞ÈöæÔºåËÄå‰∏îÊúâÊó∂‰ºöÂèòÂæóÈùûÂ∏∏Â§çÊùÇ„ÄÇÊàë‰ª¨ÂæàÂÆπÊòìËø∑Â§±Âú®Êó•Âøó‰∏≠ÔºåÂú® Pod ‰πãÈó¥Ë∑≥ËΩ¨ÔºåÊêúÁ¥¢‰∫ã‰ª∂‰ª•Âèä‰Ω†Êã•ÊúâÁöÑ‰∏úË•ø„ÄÇÊúÄÈáçË¶ÅÁöÑÊòØÔºåÊâæÂà∞ÊúâÊÑè‰πâÁöÑËß£ÈáäÂèØËÉΩÊòØ‰∏Ä‰∏™ÊûÅÂÖ∂Â∑®Â§ßÁöÑÈóÆÈ¢òÔºåÂõ†‰∏∫Âπ∂ÈùûÊâÄÊúâÊó•ÂøóÈÉΩÊòì‰∫éÁêÜËß£„ÄÇÊàë‰ª¨ËøòÂøÖÈ°ªÊâøËÆ§ËøôÂæàËÄóÊó∂„ÄÇÂΩìÁÑ∂ÔºåÊàë‰ª¨ÂèØ‰ª•‰ΩøÁî®Â§ñÈÉ®ÂèØËßÇÂØüÊÄßÂ∑•ÂÖ∑ÔºåÁîöËá≥ÂèØ‰ª•‰ΩøÁî®ÂåÖÂê´ÊâÄÊúâÁªÑ‰ª∂ÁöÑÂèØËßÇÂØüÊÄßÂ†ÜÊ†àÔºå‰ΩÜ‰ªçÁÑ∂Âπ∂ÈùûÊâÄÊúâËæìÂá∫ÈÉΩÊòì‰∫éÁêÜËß£ÁîöËá≥ÈòÖËØª„ÄÇÂΩíÊ†πÁªìÂ∫ïÔºåËØäÊñ≠Êó∂Èó¥‰ª•ÂèäÊïÖÈöúÊéíÈô§Êó∂Èó¥ÈÉΩ‰ºöÂõ†Ê≠§ËÄåÂ§ßÂπÖÂª∂Èïø„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;358a&#34;&gt;‰ΩÜÊòØÊàë‰ª¨Êù•Ë∞àË∞àÊàøÈó¥ÈáåÁöÑÂ§ßË±°„ÄÇ‰∫∫Â∑•Êô∫ËÉΩÂèØ‰ª•Âú®ËøôÊñπÈù¢Â∏ÆÂä©Êàë‰ª¨ÂêóÔºüÁâπÂà´ÊòØÂØπ‰∫é Kubernetes ÊïÖÈöúÊéíÈô§„ÄÇÂõ†‰∏∫Êàë‰ª¨ÊØèÂ§©ÈÉΩÂºÄÂßã‰ΩøÁî®&lt;strong&gt;‰∫∫Â∑•Êô∫ËÉΩ&lt;/strong&gt;ÔºåÂπ∂‰∏î‰∏ªË¶ÅÊòØ&lt;strong&gt;Ê≥ïÂ≠¶Á°ïÂ£´&lt;/strong&gt;Ôºå‰∏ç‰ªÖÂú®Êàë‰ª¨ÁöÑÂ∑•‰Ωú‰∏≠ÔºåËÄå‰∏îÂú®Êàë‰ª¨ÁöÑÊó•Â∏∏Ê¥ªÂä®‰∏≠„ÄÇÂΩìÁÑ∂ÔºåÊúâ‰∏Ä‰∫õ&lt;strong&gt;AIOps&lt;/strong&gt;Â∑•ÂÖ∑ÂèØ‰ª•Â∏ÆÂä©Êàë‰ª¨Âú®‰∫∫Â∑•Êô∫ËÉΩÁöÑÂ∏ÆÂä©‰∏ãÂÆûÁé∞ÂèØËßÇÊµãÊÄßËß£ÂÜ≥ÊñπÊ°àÔºå‰ΩÜÂá†‰πéÊâÄÊúâËøô‰∫õÂ∑•ÂÖ∑ÈÉΩ‰ºöÊ∂àËÄóÂ§ßÈáèÁöÑ GPU ËµÑÊ∫êÔºåËøô‰ºöÂ¢ûÂä†ÊΩúÂú®ÊàêÊú¨ÔºåÂΩìÁÑ∂ËøôÈáåÁöÑÂèØÁª¥Êä§ÊÄßÈóÆÈ¢ò„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;140d&#34;&gt;Âõ†Ê≠§ÔºåÂú®ËøôÁØáÂçöÊñá‰∏≠ÔºåÊàëÊÉ≥ÂêëÊÇ®‰ªãÁªç‰∏Ä‰∏™&lt;strong&gt;CNCF Sandbox&lt;/strong&gt;È°πÁõÆÔºåËøôÊòØ‰∏Ä‰∏™ÈùûÂ∏∏Âº∫Â§ßÁöÑÂ∑•ÂÖ∑ÔºåÊó®Âú®‰ΩøÁî®‰∫∫Â∑•Êô∫ËÉΩÂíåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊù•ÁÆÄÂåñ Kubernetes ÁÆ°ÁêÜ„ÄÇËØ•È°πÁõÆÂêç‰∏∫&lt;strong&gt;K8sGPT&lt;/strong&gt;ÔºåK8sGPT ÊòØ‰∏Ä‰∏™Áî®ÁÆÄÂçïÁöÑËã±ËØ≠Êâ´Êèè Kubernetes ÈõÜÁæ§„ÄÅËØäÊñ≠ÂíåÂàÜÁ±ªÈóÆÈ¢òÁöÑÂ∑•ÂÖ∑„ÄÇÂõ†Ê≠§ÔºåÂü∫Êú¨‰∏äÔºåK8sGPT ‰∏éÂêÑÁßç AI ÂêéÁ´ØÈõÜÊàêÔºåÂåÖÊã¨ OpenAI„ÄÅAzure OpenAI Âíå Amazon BedrockÔºå‰∏∫ÊÇ®ÁöÑ Kubernetes ÁéØÂ¢ÉÊèê‰æõÊ∏ÖÊô∞‰∏îÂèØÊìç‰ΩúÁöÑËßÅËß£ÔºåÂπ∂‰ª•Áî®Êà∑ÂèãÂ•ΩÁöÑÊ†ºÂºèÊèê‰æõËøô‰∫õËßÅËß£„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*rWs5LOWvxt5df7gW&#34; alt=&#34;Image&#34;referrerpolicy =&#34;Êó†ÂºïËçê&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 class=&#34;wp-block-heading&#34; id=&#34;439b&#34;&gt;K8sGPT ÁöÑ‰∏ªË¶ÅÁâπÊÄß&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;6ba7&#34;&gt;Âú®ÂàÜ‰∫´‰∏Ä‰∫õ K8sGPT ‰ΩøÁî®Á§∫‰æã‰πãÂâçÔºåÊàëÊÉ≥ÂÖàÂàÜ‰∫´‰∏Ä‰∏ãÂÆÉÁöÑ‰∏Ä‰∫õÂÖ∑‰ΩìÂÖ≥ÈîÆÂäüËÉΩ„ÄÇËÆ©Êàë‰ª¨Â∞ÜÂÆÉ‰ª¨ÂàÜËß£‰∏∫Ë¶ÅÁÇπ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Â∑•‰ΩúË¥üËΩΩÂÅ•Â∫∑ÂàÜÊûê&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;969d&#34;&gt;ÂΩìÁÑ∂Êàë‰ª¨Â∫îËØ•‰ªéËøô‰∏™ÂºÄÂßãÔºåÂõ†‰∏∫ËøôÊòØËøô‰∏™Â∑•ÂÖ∑Âá∫Áé∞ÁöÑÊúÄÈáçË¶ÅÂéüÂõ†‰πã‰∏Ä„ÄÇ K8sGPT Êâ´ÊèèÊÇ®ÁöÑ Kubernetes ÈõÜÁæ§‰ª•ËØÜÂà´ÂΩ±ÂìçÊÇ®Â∑•‰ΩúË¥üËΩΩÁöÑÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇÂÆÉÂ∞ÜÂ§çÊùÇÁöÑËØäÊñ≠Êï∞ÊçÆÂíåÊó•ÂøóËΩ¨Êç¢‰∏∫ÁÆÄÂçï„ÄÅÊòì‰∫éÁêÜËß£ÁöÑÂª∫ËÆÆÔºå‰ªéËÄåÊõ¥ËΩªÊùæÂú∞Áª¥Êä§ÈõÜÁæ§ÂÅ•Â∫∑Áä∂ÂÜµ„ÄÇÊòØÁöÑÔºåK8sGPT ËøòÈíàÂØπÂÆÉÂú®ÈõÜÁæ§‰∏≠Ê£ÄÊµãÂà∞ÁöÑÈóÆÈ¢òÊèê‰æõÂª∫ËÆÆ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;‰∫∫Â∑•Êô∫ËÉΩÈ©±Âä®ÁöÑËØäÊñ≠&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a06c&#34;&gt;Ëøô‰∏ÄÁÇπÊòØÊòæËÄåÊòìËßÅÁöÑ„ÄÇ K8sGPT Âà©Áî®‰∫∫Â∑•Êô∫ËÉΩÂíåÊ≥ïÂ≠¶Á°ïÂ£´„ÄÇ K8sGPT Áî®ÁÆÄÂçïÁöÑËã±ËØ≠Êèê‰æõ‰∫ÜÊ£ÄÊµãÂà∞ÁöÑÈóÆÈ¢òÁöÑËØ¶ÁªÜËß£Èáä„ÄÇÊ≠§ÂäüËÉΩÁî±‰∏é OpenAI Á≠âÈ¢ÜÂÖà AI Âπ≥Âè∞ÁöÑÈõÜÊàêÊèê‰æõÊîØÊåÅ„ÄÇÂü∫Êú¨‰∏äÔºåÈÄöËøá‰ΩøÁî®Ëøô‰∫õ‰∫∫Â∑•Êô∫ËÉΩÂπ≥Âè∞ÔºåK8sGPT Â∞ÜËØäÊñ≠Êï∞ÊçÆËΩ¨Êç¢‰∏∫ÈùûÂ∏∏‰∫∫ÊÄßÂåñÁöÑÊ†ºÂºè„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;ÂÜÖÁΩÆÂíåËá™ÂÆö‰πâÂàÜÊûêÂô®&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;5583&#34;&gt;K8sGPT ÂåÖÂê´Â§ö‰∏™ÈíàÂØπÂêÑÁßç Kubernetes ËµÑÊ∫êÔºà‰æãÂ¶Ç Pod„ÄÅÊúçÂä°ÂíåËäÇÁÇπÔºâÁöÑÂÜÖÁΩÆÂàÜÊûêÂô®„ÄÇÂΩìÁÑ∂ÔºåÈÄöËøáÈ¢ùÂ§ñÁöÑÈõÜÊàêÔºåÊÇ®ËøòÂèØ‰ª•ÂàõÂª∫Ëá™ÂÆö‰πâÂàÜÊûêÂô®Êù•Êª°Ë∂≥ÁâπÂÆöÈúÄÊ±Ç„ÄÇ K8sGPT ‰∏é AWS„ÄÅPrometheus„ÄÅKEDA Âíå Trivy ÈõÜÊàê„ÄÇÈÄöËøáËøô‰∫õÂéüÁîüÈõÜÊàêÔºåÊÇ®ÂèØ‰ª•ËøõË°åÊõ¥ËØ¶ÁªÜÁöÑÂàÜÊûê„ÄÇÂΩìÁÑ∂ÔºåÊàëÁõ∏‰ø°Ëøô‰∏™ÂàóË°®Â∞Ü‰ºöÊâ©Â±ïÔºåÂπ∂‰∏îÂ∞ÜÊù•‰ºöÊ∑ªÂä†Êõ¥Â§öÈõÜÊàê„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;ÊåÅÁª≠ÁõëÊéßÂíåÈõÜÊàê&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;d67b&#34;&gt;ÊÇ®ËøòÂèØ‰ª•Â∞Ü K8sGPT ‰Ωú‰∏∫ Kubernetes Operator Âú® Kubernetes ÈõÜÁæ§‰∏≠ÈÉ®ÁΩ≤„ÄÇ K8sGPT ÂèØ‰ª•ÊåÅÁª≠ÁõëÊéßÁéØÂ¢ÉÔºåÂπ∂‰∏éÈõÜÁæ§‰∏≠Áé∞ÊúâÁöÑ Prometheus„ÄÅAlertmanager Á≠âÂ∑•ÂÖ∑Êó†ÁºùÈõÜÊàê„ÄÇÊÇ®ÂèØ‰ª•‰ªé‰∏ãÂõæ‰∏≠ÁúãÂà∞ K8sGPT Operator ÂÆâË£ÖÂíåÁÆ°ÁêÜÁöÑÁªÑ‰ª∂„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*loluayfTjzR3OyVv&#34; alt=&#34;image&#34;referrerpolicy =&#34;Êó†ÂºïËçê&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;ÂÆâÂÖ® CVE ÂÆ°Ê†∏&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;e023&#34;&gt;ÈÄöËøá Trivy ÈõÜÊàêÔºåK8sGPT ÊúâÂä©‰∫éËØÜÂà´ÂíåËß£ÂÜ≥ Kubernetes ÈõÜÁæ§ÂÜÖÁöÑÂÆâÂÖ®ÊºèÊ¥û„ÄÇ‰∏ÄÊó¶ÊøÄÊ¥ª Trivy ÈõÜÊàêÔºåTrivy Kubernetes Operator Â∞ÜË¢´ÂÆâË£ÖÂà∞ Kubernetes ÈõÜÁæ§‰∏≠ÔºåÂπ∂‰Ωø K8sGPT ËÉΩÂ§ü‰∏é Operator ÁöÑÁªìÊûúËøõË°å‰∫§‰∫í„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;fbd6&#34;&gt;ÊàëËøòÊÉ≥Âú®Ê≠§‰∏ìÈó®ËÆ®ËÆ∫ &lt;strong&gt;AWS&lt;/strong&gt; ÈõÜÊàê„ÄÇÊàëÂæàÁ°ÆÂÆöÊÇ®Â∑≤ÁªèÁÜüÊÇâ&lt;strong&gt;AWS Controllers for Kubernetes (ACK)&lt;/strong&gt;È°πÁõÆÔºå‰ΩÜÂ¶ÇÊûú‰∏çÁÜüÊÇâÔºåÊàëËøòÊúâ‰∏ÄÁØáÂÖ≥‰∫éËØ•È°πÁõÆÁöÑÂçöÊñáÔºåÊÇ®ÂèØ‰ª•‰ªé&lt;a href= ‚Äúhttps://medium.com/@eminalemdar/manage-your-aws-resources-from-kubernetes-with-ack-3cf06a4b0770&#34;&gt;Ê≠§Â§Ñ&lt;/a&gt;„ÄÇÁÆÄËÄåË®Ä‰πãÔºåACK ÂÖÅËÆ∏ÊÇ®‰ΩøÁî® CRD ‰ªé Kubernetes ÈõÜÁæ§ÁÆ°ÁêÜ AWS ÊúçÂä°„ÄÇËøôÁßçÈõÜÊàêËøòÊúâÂä©‰∫é K8sGPT ‰∏é ACK ÁÆ°ÁêÜÁöÑ AWS ËµÑÊ∫êËøõË°å‰∫§‰∫í„ÄÇÂõ†Ê≠§ÔºåÊÇ®‰∏ç‰ªÖÂèØ‰ª•‰ΩøÁî® K8sGPT Êù•ÂàÜÊûêÂíåÁÆ°ÁêÜ Kubernetes ËµÑÊ∫êÔºåËøòÂèØ‰ª•ÂàÜÊûêÂíåÁÆ°ÁêÜ ACK ÁÆ°ÁêÜ‰∏ãÁöÑ AWS ËµÑÊ∫ê„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;01bf&#34;&gt;ËÆ©Êàë‰ª¨Êù•Áé©‰∏Ä‰∏ã K8sGPT&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;4756&#34;&gt;K8sGPT CLI ÁöÑÂÆâË£ÖÈùûÂ∏∏ÁÆÄÂçïÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî® Brew Á≠âÊµÅË°åÁöÑÊï∞ÊçÆÂåÖÁÆ°ÁêÜÂô®Â∞ÜÂÖ∂ÂÆâË£ÖÂú®ÊÇ®ÁöÑËÆ°ÁÆóÊú∫‰∏ä„ÄÇÂÆâË£ÖÂêéÔºåÊÇ®ÈúÄË¶ÅÂêëÊÇ®ÈÄâÊã©ÁöÑ AI Êèê‰æõÂïÜËøõË°åË∫´‰ªΩÈ™åËØÅ„ÄÇÊàëÂú®ËøôÈáå‰ΩøÁî® OpenAI Âíå GPT4Ôºå‰ΩÜÊÇ®ÂΩìÁÑ∂ÂèØ‰ª•ÈÄâÊã©ÈÄÇÂêàÊÇ®ÁöÑÔºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;k8sgpt Ë∫´‰ªΩÈ™åËØÅÂàóË°®&lt;br&gt;ÈªòËÆ§Ôºö&lt;br&gt;&gt; openai&lt;br&gt;Ê¥ªÂä®Ôºö&lt;br&gt;&gt; openai&lt;br&gt;Êú™‰ΩøÁî®Ôºö&lt;br&gt; &gt; localai&lt;br&gt;&gt; azureopenai&lt;br&gt;&gt; cohere&lt;br&gt;&gt; amazonbedrock&lt;br&gt;&gt; amazonsagemaker&lt;br&gt;&gt; google&lt;br&gt;&gt; noopai&lt;br&gt;&gt; Huggingface&lt;br&gt;&gt; googlevertexai&lt;br&gt;&gt; oci&lt;/code &gt;&lt;/Ââç&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;ddc5&#34;&gt;ÊÇ®ÂèØ‰ª•‰ΩøÁî®&lt;code&gt;k8sgpt auth add&lt;/code&gt;ÂëΩ‰ª§Ê∑ªÂä†Êèê‰æõÂïÜÂêéÁ´ØË∫´‰ªΩÈ™åËØÅ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;0e6f&#34;&gt;ÂÆåÊàêËøô‰∏ÄÊ≠•ÂêéÔºåÂÆûÈôÖ‰∏äÈùûÂ∏∏ÁÆÄÂçï„ÄÇËÆ©Êàë‰ª¨‰ΩøÁî® &lt;code&gt;k8sgpt analysis&lt;/code&gt; ÂëΩ‰ª§ÂºÄÂßãÁ¨¨‰∏ÄÊ¨°ÂàÜÊûê„ÄÇ‰ΩÜÂú®ËøêË°åÂëΩ‰ª§‰πãÂâçÔºåËÆ©ÊàëÈÉ®ÁΩ≤‰∏Ä‰∏™ÊçüÂùèÁöÑ Pod Êù•Êü•ÁúãÂÆûÈôÖÁöÑËØäÊñ≠ÈÉ®ÂàÜ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl apply -f - &lt;&lt;EOF&lt;br&gt;apiÁâàÊú¨Ôºöv1&lt;br&gt;ÁßçÁ±ªÔºöPod&lt;br&gt;ÂÖÉÊï∞ÊçÆÔºö&lt;br&gt;ÂêçÁß∞ÔºöÊçüÂùè-pod&lt;br&gt;ÂëΩÂêçÁ©∫Èó¥ÔºöÈªòËÆ§&lt;br&gt;ËßÑÊ†ºÔºö&lt;br&gt;ÂÆπÂô®Ôºö&lt;br&gt;- ÂêçÁß∞Ôºöbroken-pod&lt;br&gt;ÂõæÂÉèÔºönginx:1.a.b.c&lt;br&gt;livenessProbe:&lt;br&gt;httpGet:&lt;br&gt;Ë∑ØÂæÑÔºö/&lt;br&gt;Á´ØÂè£Ôºö81&lt;br&gt;initialDelaySecondsÔºö3&lt;br&gt;periodSecondsÔºö3&lt;br&gt;EOF&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;8645&#34;&gt;Â¶ÇÊÇ®ÊâÄËßÅÔºåÊ≠§ Pod ÂÆö‰πâÁöÑÂõæÂÉèÈÉ®ÂàÜÊòØÈîôËØØÁöÑ„ÄÇÈÇ£‰πàËÆ©ÊàëËøêË°å &lt;code&gt;k8sgpt analysis&lt;/code&gt; ÂëΩ‰ª§Âπ∂Êü•ÁúãËæìÂá∫„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*mSTa1ykjgzbeBas2&#34; alt=&#34;‰ª£Á†Å&#34;referrerpolicy =‚ÄúÊó†ÂºïÁî®ËÄÖ‚Äù&gt;&lt;/Âõæ&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;616a&#34;&gt;Êàë‰ª¨ÂèØ‰ª•ÁúãÂà∞ÈîôËØØÔºå‰ΩÜÁúã‰∏çÂà∞ËØ¶ÁªÜ‰ø°ÊÅØÔºå‰ΩÜËøôÈáåÊúâ‰∏Ä‰∏™Ê†áÂøó‰æõÊàë‰ª¨‰ΩøÁî®„ÄÇÂ¶ÇÊûúÊàë‰ª¨Âú®Ê≠§Â§ÑÊ∑ªÂä† &lt;code&gt;k8sgptanalyze--explain&lt;/code&gt; Ê†áÂøóÔºåK8sGPT Â∞ÜÂú®Ê≠§Â§ÑËøûÊé•Âà∞ AI Êèê‰æõÂïÜÂíå LLM„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*xAdRU8htHZ3VxrtY&#34; alt=&#34;code&#34;referrerpolicy =‚ÄúÊó†ÂºïÁî®ËÄÖ‚Äù&gt;&lt;/Âõæ&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;637f&#34;&gt;Áé∞Âú®Êàë‰ª¨Êúâ‰∫ÜÊúâÂÖ≥ËØ•ÈóÆÈ¢òÁöÑÊõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØ‰ª•Âèä‰∏Ä‰∫õÂª∫ËÆÆ„ÄÇÊàëÁü•ÈÅìËøôÊòØ‰∏Ä‰∏™ÈùûÂ∏∏Âü∫Êú¨ÁöÑÁ§∫‰æãÔºå‰ΩÜËÆ©Êàë‰ª¨Êâ©Â±ï‰∏Ä‰∏ã„ÄÇÊàëÁé∞Âú®Â∞ÜÂêØÁî®‰∏§‰∏™ÈõÜÊàêÔºöAWS Âíå Trivy„ÄÇÂ¶ÇÊûúÊàë‰ª¨Êü•ÁúãÁé∞Âú®ÂèØ‰ª•‰ΩøÁî®ÁöÑËøáÊª§Âô®ÔºåÊàë‰ª¨‰ºöÁúãÂà∞Ê∑ªÂä†ÁöÑËøáÊª§Âô®Êù•Ëá™ÈõÜÊàê„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:390/0*s-zY2Cb0irELZNms&#34; alt=&#34;code ‚Äúreferrerpolicy =‚Äúno-referrer‚Äù&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a48f&#34;&gt;ËÆ©Êàë‰ª¨‰ªé EKS ËøáÊª§Âô®ÂºÄÂßãÔºåÁúãÁúãÊòØÂê¶Êúâ‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:466/0*uwSRiMmwhveF5-k5&#34; alt=&#34;code ‚Äúreferrerpolicy =‚Äúno-referrer‚Äù&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;2054&#34;&gt;Â§™Ê£í‰∫ÜÔºÅËøôÈáåÊ≤°ÊúâÈóÆÈ¢òÔºå‰ΩÜËÆ©Êàë‰ª¨ÁúãÁúã Trivy ÁöÑÊºèÊ¥ûÊä•ÂëäÁöÑÁªìÊûú„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*5tX1K5Wi2SOZNAv2&#34; alt=&#34;code&#34;referrerpolicy =‚ÄúÊó†ÂºïÁî®ËÄÖ‚Äù&gt;&lt;/Âõæ&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;3f28&#34;&gt;ÂìáÔºÅÂ•ΩÂ§öÂïäÔºÅËÆ©Êàë‰ª¨Ê∑±ÂÖ•‰∫ÜËß£ÂÖ∂‰∏≠‰∏Ä‰∫õÈóÆÈ¢ò„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*smPQrxM4w4VOtjP0&#34; alt=&#34;code&#34;referrerpolicy =‚ÄúÊó†ÂºïÁî®ËÄÖ‚Äù&gt;&lt;/Âõæ&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*YtfG2_J9R1f4oWMk&#34; alt=&#34;code&#34;referrerpolicy =‚ÄúÊó†ÂºïÁî®ËÄÖ‚Äù&gt;&lt;/Âõæ&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a139&#34;&gt;Ê≠£Â¶ÇÊÇ®‰ªéÂ±èÂπïÊà™Âõæ‰∏≠ÁúãÂà∞ÁöÑÔºåÊàë‰ª¨Êèê‰æõ‰∫ÜÊúâÂÖ≥ÊØè‰∏™ÈóÆÈ¢òÁöÑÈùûÂ∏∏ËØ¶ÁªÜÁöÑ‰ø°ÊÅØÔºåÂπ∂‰∏îËøòÊèê‰æõ‰∫Ü‰∏Ä‰∫õÂÖ≥‰∫éÂ¶Ç‰ΩïËß£ÂÜ≥ÈóÆÈ¢òÁöÑÈùûÂ∏∏Â•ΩÁöÑÂª∫ËÆÆ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;ae22&#34;&gt;ËÆ©Êàë‰πüÂ∞Ü‰∏Ä‰∫õÂÖ∂‰ªñËµÑÊ∫êÈÉ®ÁΩ≤Âà∞ÈõÜÁæ§Âπ∂Êü•ÁúãÊúâÂÖ≥Ëøô‰∫õËµÑÊ∫êÁöÑ‰ø°ÊÅØ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*5rbT8-9aQ-bXGYcz&#34; alt= ‚Äú‰ª£Á†Å‚Äùreferrerpolicy =‚Äúno-referrer‚Äù&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;2953&#34;&gt;Ê≠£Â¶ÇÊÇ®‰ªéÊçüÂùèÁöÑËµÑÊ∫ê‰∏≠ÁúãÂà∞ÁöÑÈÇ£Ê†∑ÔºåÊàëÁé∞Âú®ÂØπÈõÜÁæ§‰∏≠ÂÆûÈôÖÂèëÁîüÁöÑÊÉÖÂÜµÊúâ‰∫ÜÂæàÂ•ΩÁöÑ‰∫ÜËß£„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;bdbf&#34;&gt;ÊúÄÂêé‰∏Ä‰∫õËØù„ÄÇÂΩìÁÑ∂ÔºåÊàë‰ª¨ÈÉΩÂØπËøô‰∫õ‰∫∫Â∑•Êô∫ËÉΩÂêéÁ´ØÁöÑÂÆâÂÖ®ÊÄßÊúâ‰∏Ä‰∫õÁñëÈóÆÔºå‰ΩÜËøôÈáåËøòÊúâÂè¶‰∏Ä‰∏™ÈÄâÈ°πÔºåÁß∞‰∏∫ÂåøÂêçÔºåÂÆÉÂü∫Êú¨‰∏äÂú®Â∞ÜÊï∞ÊçÆÂèëÈÄÅÂà∞‰∫∫Â∑•Êô∫ËÉΩÂêéÁ´Ø‰πãÂâçÂØπÊï∞ÊçÆËøõË°åÂåøÂêçÂåñ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;2ade&#34;&gt;ÁªìËÆ∫&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;25df&#34;&gt;Â∞ÜÊù•ÔºåK8sGPT ÂèØËÉΩ‰ºöÊàê‰∏∫‰∏ÄÁßçÂ∑•ÂÖ∑ÔºåÈÄöËøá‰ΩøÂÖ∂Êõ¥Êòì‰∫éËÆøÈóÆÂíåÊõ¥È´òÊïàÔºåÊù•ÊîπÂèò Kubernetes ÁÆ°ÁêÜ„ÄÅÈóÆÈ¢òËØäÊñ≠ÂíåÊïÖÈöúÊéíÈô§„ÄÇÊó†ËÆ∫ÊÇ®ÊòØ SRE„ÄÅDevOps Â∑•Á®ãÂ∏àËøòÊòØÂπ≥Âè∞Â∑•Á®ãÂ∏àÔºåK8sGPT ÈÉΩÂèØ‰ª•Â∏ÆÂä©ÊÇ®ÂáèÂ∞ëÊïÖÈöúÊéíÈô§Êó∂Èó¥„ÄÇ K8sGPT ÂÖ•Èó®ÈùûÂ∏∏ÁÆÄÂçïÔºåÊÇ®Âè™ÈúÄÂ∞ÜÂÖ∂ÂÆâË£ÖÂú® Kubernetes ÁéØÂ¢ÉÊàñÊú¨Âú∞ËÆ°ÁÆóÊú∫‰∏äÔºå‰ΩøÁî®ÊÇ®ÈÄâÊã©ÁöÑ AI ÂêéÁ´ØËøõË°åË∫´‰ªΩÈ™åËØÅÔºåÁÑ∂ÂêéÂ∞ÜÂÖ∂ËøûÊé•Âà∞ÊÇ®ÁöÑÈõÜÁæ§„ÄÇÂú®ÊàëÁúãÊù•Ôºå‰Ω†ÁªùÂØπÂ∫îËØ•Â∞ùËØï‰∏Ä‰∏ã K8sGPT„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://eminalemdar.medium.com/?source=post_page-----9c68d26e00ac---------------------- ----------&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Wed, 10 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêWebAssembly components: the next wave of cloud native computing„ÄëWebAssembly ÁªÑ‰ª∂Ôºö‰∫ëÂéüÁîüËÆ°ÁÆóÁöÑ‰∏ã‰∏ÄÊ≥¢Êµ™ÊΩÆ</title>
      <link>https://www.cncf.io/blog/2024/07/09/webassembly-components-the-next-wave-of-cloud-native-computing/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;Member post by Liam Randall, Cosmonic CEO and CNCF Ambassador and Bailey Hayes, Cosmonic CTO, Bytecode Alliance TSC director, and WASI SG co-chair&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The advent of containers marked an inflection point for computing in the 21st century‚Äîa paradigm shift (&lt;a href=&#34;https://en.wikipedia.org/wiki/Paradigm_shift&#34;&gt;per Thomas Kuhn&lt;/a&gt;) that gave rise to the entire cloud native landscape. In 2024, the arrival of WebAssembly components represents a new inflection point, and the next paradigm shift is already underway.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Why components are made for the cloud&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly (Wasm) has been around for about a decade now‚Äîmuch as Linux kernel namespaces were in use for over a decade before the debut of Docker (and 15 years before Kubernetes reached the mainstream). Like the Linux namespace, the core WebAssembly standard has provided a firm foundation to build on.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the case of Wasm, that means we have a bytecode format and virtual instruction set architecture (ISA) that enable us to compile code from any language to a common standard, without needing to build for a particular kernel or architecture. Over the last decade, Wasm‚Äôs flexibility has proven itself not only in the browser, but‚Ä¶pretty much everywhere else as well. Today, Wasm is used every where from &lt;a href=&#34;https://www.amazon.science/blog/how-prime-video-updates-its-app-for-more-than-8-000-device-types&#34;&gt;Amazon Prime Video&lt;/a&gt; to &lt;a href=&#34;https://www.youtube.com/watch?v=ms5_0wOl79I&#34;&gt;Google Earth&lt;/a&gt; to &lt;a href=&#34;https://www.cncf.io/blog/2022/11/17/better-together-a-kubernetes-and-wasm-case-study/&#34;&gt;Adobe&lt;/a&gt; to &lt;a href=&#34;https://www.cncf.io/blog/2024/01/05/bringing-webassembly-to-telecoms-with-cncf-wasmcloud/&#34;&gt;telecoms like Orange&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Core Wasm is particularly well-suited to the cloud. In addition to their flexibility, Wasm binaries are tiny, sandboxed, and efficient, allowing for much greater density and speed of download or startup in cloud environments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly &lt;strong&gt;components&lt;/strong&gt; take all of this one step further. (Or many big leaps further, as we‚Äôll see.) Components are WebAssembly binaries conforming to an additional specification called the Component Model. Components bring all the same benefits of ordinary Wasm modules (as the Component Model is built on top of the Core WebAssembly specification), but they are also &lt;strong&gt;interoperable&lt;/strong&gt; and &lt;strong&gt;composable&lt;/strong&gt; with other components:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Interoperability&lt;/strong&gt; means that components can communicate over strictly-defined interfaces. There is a common set of standardized interfaces called the &lt;a href=&#34;https://github.com/WebAssembly/WASI/&#34;&gt;&lt;strong&gt;WASI&lt;/strong&gt;&lt;/a&gt;, consisting of high-level APIs (at various stages of proposal and standardization) for functionality like HTTP, CLI, blob storage, key-value storage, and much more. Developers can use their favorite libraries in their favorite languages, and once they compile the code to a Wasm component, other components can make use of the functions they expose‚Äîregardless of the language and libraries used to write &lt;em&gt;those&lt;/em&gt; components.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Composability&lt;/strong&gt; means that multiple components can be combined into a single component. Functions exposed (or ‚Äúexported‚Äù) by one component on a given interface can be used (or ‚Äúimported‚Äù) by another component, and those two components can be compiled together into a single binary. For a microservice‚Äîor any two pieces talking, really‚Äîcomposition is much more efficient than sending data over a network boundary where calls within a composed component happen with the same process in nanoseconds vs a network request in milliseconds. If you need to link components dynamically, you can achieve a compositional effect with distributed components using an open source transport protocol like &lt;a href=&#34;https://github.com/wrpc/wrpc&#34;&gt;wRPC (WIT over RPC)&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In short, if core WebAssembly binaries are flat-surfaced, fundamental building blocks, components are studded construction bricks‚Äîdesigned for building sophisticated, interconnected applications in new ways.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1086&#34; height=&#34;506&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/components.jpg&#34; alt=&#34;components&#34; class=&#34;wp-image-113994&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/components.jpg 1086w, https://www.cncf.io/wp-content/uploads/2024/07/components-300x140.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/components-1024x477.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/components-768x358.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/components-900x419.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/components-429x200.jpg 429w, https://www.cncf.io/wp-content/uploads/2024/07/components-858x400.jpg 858w&#34; sizes=&#34;(max-width: 1086px) 100vw, 1086px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If a WASI API doesn‚Äôt exist for your use case, no problem‚Äîyou can write your own interface in the open &lt;strong&gt;WebAssembly Interface Type (WIT)&lt;/strong&gt; interface description language. Open standards make the ecosystem infinitely extensible.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly enthusiasts (like us) often share this quote from Docker founder Solomon Hykes:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;If WASM+WASI existed in 2008, we wouldn&#39;t have needed to create Docker.&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We love that quote! But if we use it without explaining how components work, we run the risk of obscuring what is so transformative about them. It‚Äôs a mistake to think of components as a more efficient alternative to containers. Yes, they are efficient, portable delivery mechanisms for cloud native workloads‚Äîbut components are also an entirely new paradigm that unlocks entirely new models of computing.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;The next wave is already here&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The last couple of years have built to a convergence moment in which key pieces have fallen into place (or rather, been wrestled into place by many, many people working across the ecosystem). The most important of these is the release of WASI 0.2 and the Component Model in January 2024. With a common model and common APIs in place, the community has wasted no time building and updating a wide array of open source tools and native support in standard language libraries. For just a handful of examples:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasmtime&#34;&gt;&lt;strong&gt;Wasmtime&lt;/strong&gt;&lt;/a&gt;: Standalone runtime for WebAssembly components&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasm-tools&#34;&gt;&lt;strong&gt;&lt;code&gt;wasm-tools&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;: A multi-functional tool for interacting with components (read WIT interfaces, compose, and more)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://wasmcloud.com/&#34;&gt;&lt;strong&gt;wasmCloud&lt;/strong&gt;&lt;/a&gt;: Build and run components anywhere, including edge and distributed environments&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wit-deps&#34;&gt;&lt;strong&gt;&lt;code&gt;wit-deps&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;: Manage WIT dependencies for a component project&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasi-virt&#34;&gt;&lt;strong&gt;WASI Virt&lt;/strong&gt;&lt;/a&gt;: Use composition to virtualize a component within another encapsulating component&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Better yet, components increasingly integrate with common cloud native tools and standards:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;OCI has emerged as &lt;a href=&#34;https://tag-runtime.cncf.io/wgs/wasm/deliverables/wasm-oci-artifact/&#34;&gt;the standard for packaging components in registries&lt;/a&gt;.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Platforms like wasmCloud &lt;a href=&#34;https://wasmcloud.com/docs/kubernetes&#34;&gt;integrate with Kubernetes&lt;/a&gt;, OpenTelemetry, and Open Policy Agent.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Teams don‚Äôt have to start from square one to run components in production, but can use their existing cloud native tooling. For those brand new to components, the community is developing more and more resources like &lt;a href=&#34;https://component-model.bytecodealliance.org/&#34;&gt;The Component Model Book&lt;/a&gt; and &lt;a href=&#34;https://wasi.dev/&#34;&gt;WASI.dev&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;All the while, the component development experience is growing more and more polished across more and more languages. In the JavaScript ecosystem, &lt;a href=&#34;https://github.com/bytecodealliance/jco&#34;&gt;&lt;code&gt;jco&lt;/code&gt;&lt;/a&gt; enables JavaScript developers to write idiomatic code and compile to WebAssembly, while Rustaceans can compile directly to the &lt;a href=&#34;https://doc.rust-lang.org/nightly/rustc/platform-support/wasm32-wasip2.html&#34;&gt;&lt;code&gt;wasm32-wasip2&lt;/code&gt; target&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Around the world, teams are already unlocking new possibilities with components. In resource-constrained environments like edge devices on factory floors, manufacturing analytics company &lt;a href=&#34;https://www.youtube.com/watch?v=fQdkNGZqYZA&amp;amp;list=PLj6h78yzYM2MQteKoXxICTWiUdZYEw6RI&amp;amp;index=6&#34;&gt;MachineMetrics&lt;/a&gt; is using components to process high-frequency data directly on factory-floor devices and stream it to the cloud in real-time‚Äîrunning in places that containers typically don‚Äôt reach, all the while integrating with Kubernetes. Only a handful of months after the release of WASI 0.2, components are already changing what is possible.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Accelerating innovation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Developers‚Äîand ecosystems as a whole‚Äîdon‚Äôt necessarily adopt new technologies because they are theoretically more efficient or secure. New ways of working emerge when a technology enables us to be more effective, more productive, more innovative. WebAssembly components are doing just that‚Äîbreaking down language silos and revealing new opportunities.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Today, we‚Äôre about a decade out from the first days of Wasm. In the timeline of containers, this is where we reached an inflection point; the Component Model and WASI 0.2 are ushering in the same sort of paradigm shift. The tooling is there for developers to not just build with WebAssembly, but to be more productive, more effective, and more innovative. The common interfaces of WASI make component development cycles incredibly rapid, and components themselves incredibly flexible. Teams will take components to places that we can‚Äôt predict, and from here, the next wave of cloud native computing will only become more transformative.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Cosmonic È¶ñÂ∏≠ÊâßË°åÂÆòÂÖº CNCF Â§ß‰Ωø Liam Randall Âíå Cosmonic È¶ñÂ∏≠ÊäÄÊúØÂÆò„ÄÅÂ≠óËäÇÁ†ÅËÅîÁõü TSC ÊÄªÁõëÂÖº WASI SG ËÅîÂêà‰∏ªÂ∏≠ Bailey Hayes ÂèëË°®ÁöÑ‰ºöÂëòÂ∏ñÂ≠ê&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÂÆπÂô®ÁöÑÂá∫Áé∞Ê†áÂøóÁùÄ 21 ‰∏ñÁ∫™ËÆ°ÁÆóÁöÑÊãêÁÇπ‚Äî‚ÄîËåÉÂºèËΩ¨ÂèòÔºà&lt;a href=&#34;https://en.wikipedia.org/wiki/Paradigm_shift&#34;&gt;ÊØè Thomas Kuhn&lt;/a&gt;ÔºâËøôÂÇ¨Áîü‰∫ÜÊï¥‰∏™‰∫ëÂéüÁîüÊôØËßÇ„ÄÇ 2024 Âπ¥ÔºåWebAssembly ÁªÑ‰ª∂ÁöÑÂà∞Êù•‰ª£Ë°®ÁùÄ‰∏Ä‰∏™Êñ∞ÁöÑÊãêÁÇπÔºå‰∏ã‰∏Ä‰∏™ËåÉÂºèËΩ¨ÂèòÂ∑≤ÁªèÂºÄÂßã„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;‰∏∫‰ªÄ‰πàÁªÑ‰ª∂ÊòØ‰∏∫‰∫ëËÄåËÆæËÆ°ÁöÑ&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly (Wasm) Â∑≤ÁªèÂ≠òÂú®‰∫ÜÂ§ßÁ∫¶ÂçÅÂπ¥ÔºåÂ∞±ÂÉè Linux ÂÜÖÊ†∏ÂëΩÂêçÁ©∫Èó¥Âú® Docker Âá∫Áé∞‰πãÂâçÂ∑≤Áªè‰ΩøÁî®‰∫ÜÂçÅÂ§öÂπ¥‰∏ÄÊ†∑Ôºà‰ª•Âèä Kubernetes Êàê‰∏∫‰∏ªÊµÅ‰πãÂâçÁöÑ 15 Âπ¥Ôºâ„ÄÇ‰∏é Linux ÂëΩÂêçÁ©∫Èó¥‰∏ÄÊ†∑ÔºåÊ†∏ÂøÉ WebAssembly Ê†áÂáÜÊèê‰æõ‰∫ÜÂùöÂÆûÁöÑÂü∫Á°Ä„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Â∞± Wasm ËÄåË®ÄÔºåËøôÊÑèÂë≥ÁùÄÊàë‰ª¨Êã•ÊúâÂ≠óËäÇÁ†ÅÊ†ºÂºèÂíåËôöÊãüÊåá‰ª§ÈõÜÊû∂ÊûÑ (ISA)Ôºå‰ΩøÊàë‰ª¨ËÉΩÂ§üÂ∞Ü‰ªª‰ΩïËØ≠Ë®ÄÁöÑ‰ª£Á†ÅÁºñËØë‰∏∫ÈÄöÁî®Ê†áÂáÜÔºåËÄåÊó†ÈúÄÈíàÂØπÁâπÂÆöÂÜÖÊ†∏ÊàñÊû∂ÊûÑËøõË°åÊûÑÂª∫„ÄÇÂú®ËøáÂéªÁöÑÂçÅÂπ¥‰∏≠ÔºåWasm ÁöÑÁÅµÊ¥ªÊÄß‰∏ç‰ªÖÂú®ÊµèËßàÂô®‰∏≠ÂæóÂà∞‰∫ÜËØÅÊòéÔºåËÄå‰∏î......Âá†‰πéÂú®ÂÖ∂‰ªñ‰ªª‰ΩïÂú∞ÊñπÈÉΩÂæóÂà∞‰∫ÜËØÅÊòé„ÄÇÂ¶Ç‰ªäÔºåWasm Â∑≤Ë¢´ÂπøÊ≥õ‰ΩøÁî® &lt;a href=&#34;https://www.amazon.science/blog/how-prime-video-updates-its-app-for-more-than-8-000-device-types &#34;&gt;Amazon Prime Video&lt;/a&gt; Ëá≥ &lt;a href=&#34;https://www.youtube.com/watch?v=ms5_0wOl79I&#34;&gt;Google Âú∞ÁêÉ&lt;/a&gt; Ëá≥ &lt;a href=&#34;https://www. cncf.io/blog/2022/11/17/better-together-a-kubernetes-and-wasm-case-study/&#34;&gt;Adobe&lt;/a&gt; Ëá≥ &lt;a href=&#34;https://www.cncf.io /blog/2024/01/05/bringing-web assembly-to-telecoms-with-cncf-wasmcloud/&#34;&gt;ÂÉè Orange ËøôÊ†∑ÁöÑÁîµ‰ø°ÂÖ¨Âè∏&lt;/a&gt;„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Core Wasm ÁâπÂà´ÈÄÇÂêà‰∫ë„ÄÇÈô§‰∫ÜÁÅµÊ¥ªÊÄß‰πãÂ§ñÔºåWasm ‰∫åËøõÂà∂Êñá‰ª∂‰ΩìÁßØÂ∞è„ÄÅÊ≤ôÁÆ±Âåñ‰∏îÈ´òÊïàÔºåÂèØÂú®‰∫ëÁéØÂ¢É‰∏≠ÂÆûÁé∞Êõ¥È´òÁöÑ‰∏ãËΩΩÊàñÂêØÂä®ÂØÜÂ∫¶ÂíåÈÄüÂ∫¶„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly &lt;strong&gt;ÁªÑ‰ª∂&lt;/strong&gt;‰ΩøËøô‰∏ÄÂàáÊõ¥Ëøõ‰∏ÄÊ≠•„ÄÇ ÔºàÊàñËÄÖÊàë‰ª¨Â∞ÜÁúãÂà∞Êõ¥Â§öÁöÑÂ∑®Â§ßÈ£ûË∑É„ÄÇÔºâÁªÑ‰ª∂ÊòØÁ¨¶ÂêàÁß∞‰∏∫ÁªÑ‰ª∂Ê®°ÂûãÁöÑÈôÑÂä†ËßÑËåÉÁöÑ WebAssembly ‰∫åËøõÂà∂Êñá‰ª∂„ÄÇÁªÑ‰ª∂Â∏¶Êù•‰∫ÜÊôÆÈÄö Wasm Ê®°ÂùóÁöÑÊâÄÊúâÁõ∏Âêå‰ºòÂäøÔºàÂõ†‰∏∫ÁªÑ‰ª∂Ê®°ÂûãÊûÑÂª∫Âú® Core WebAssembly ËßÑËåÉ‰πã‰∏äÔºâÔºå‰ΩÜÂÆÉ‰ª¨‰πüÂèØ‰ª•‰∏éÂÖ∂‰ªñÁªÑ‰ª∂&lt;strong&gt;‰∫íÊìç‰Ωú&lt;/strong&gt;Âíå&lt;strong&gt;ÂèØÁªÑÂêà&lt;/strong&gt; Ôºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;‰∫íÊìç‰ΩúÊÄß&lt;/strong&gt;ÊÑèÂë≥ÁùÄÁªÑ‰ª∂ÂèØ‰ª•ÈÄöËøá‰∏•Ê†ºÂÆö‰πâÁöÑÊé•Âè£ËøõË°åÈÄö‰ø°„ÄÇÊúâ‰∏ÄÁªÑÈÄöÁî®ÁöÑÊ†áÂáÜÂåñÊé•Âè£ÔºåÁß∞‰∏∫ &lt;a href=&#34;https://github.com/WebAssembly/WASI/&#34;&gt;&lt;strong&gt;WASI&lt;/strong&gt;&lt;/a&gt;ÔºåÁî±È´òÁ∫ß APIÔºà‰Ωç‰∫éÊèêÊ°àÂíåÊ†áÂáÜÂåñÁöÑÂêÑ‰∏™Èò∂ÊÆµÔºâÔºåÁî®‰∫é HTTP„ÄÅCLI„ÄÅblob Â≠òÂÇ®„ÄÅÈîÆÂÄºÂ≠òÂÇ®Á≠âÂäüËÉΩ„ÄÇÂºÄÂèë‰∫∫ÂëòÂèØ‰ª•Áî®‰ªñ‰ª¨ÊúÄÂñúÊ¨¢ÁöÑËØ≠Ë®Ä‰ΩøÁî®‰ªñ‰ª¨ÊúÄÂñúÊ¨¢ÁöÑÂ∫ìÔºå‰∏ÄÊó¶‰ªñ‰ª¨Â∞Ü‰ª£Á†ÅÁºñËØë‰∏∫ Wasm ÁªÑ‰ª∂ÔºåÂÖ∂‰ªñÁªÑ‰ª∂Â∞±ÂèØ‰ª•‰ΩøÁî®ÂÆÉ‰ª¨ÂÖ¨ÂºÄÁöÑÂäüËÉΩ - Êó†ËÆ∫Áî®‰∫éÁºñÂÜôËøô‰∫õÂäüËÉΩÁöÑËØ≠Ë®ÄÂíåÂ∫ìÊòØ‰ªÄ‰πà&lt;em&gt;&lt;/em&gt;ÁªÑ‰ª∂„ÄÇ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;ÂèØÁªÑÂêàÊÄß&lt;/strong&gt;ÊÑèÂë≥ÁùÄÂ§ö‰∏™ÁªÑ‰ª∂ÂèØ‰ª•ÁªÑÂêàÊàê‰∏Ä‰∏™ÁªÑ‰ª∂„ÄÇÁªôÂÆöÊé•Âè£‰∏äÁöÑ‰∏Ä‰∏™ÁªÑ‰ª∂ÂÖ¨ÂºÄÔºàÊàñ‚ÄúÂØºÂá∫‚ÄùÔºâÁöÑÂáΩÊï∞ÂèØ‰ª•Áî±Âè¶‰∏ÄÁªÑ‰ª∂‰ΩøÁî®ÔºàÊàñ‚ÄúÂØºÂÖ•‚ÄùÔºâÔºåÂπ∂‰∏îËøô‰∏§‰∏™ÁªÑ‰ª∂ÂèØ‰ª•‰∏ÄËµ∑ÁºñËØëÊàêÂçï‰∏™‰∫åËøõÂà∂Êñá‰ª∂„ÄÇÂØπ‰∫éÂæÆÊúçÂä°ÔºàÊàñËÄÖ‰ªª‰Ωï‰∏§‰∏™Ê≠£Âú®ËØ¥ËØùÁöÑÈÉ®ÂàÜÔºâÊù•ËØ¥ÔºåÁªÑÂêàÊØîÈÄöËøáÁΩëÁªúËæπÁïåÂèëÈÄÅÊï∞ÊçÆË¶ÅÈ´òÊïàÂæóÂ§öÔºåÂú®ÁΩëÁªúËæπÁïå‰∏≠ÔºåÁªÑÂêàÁªÑ‰ª∂ÂÜÖÁöÑË∞ÉÁî®Âú®Á∫≥ÁßíÂÜÖÈÄöËøáÁõ∏ÂêåÁöÑËøõÁ®ãËøõË°åÔºåËÄåÁΩëÁªúËØ∑Ê±ÇÂàôÂú®ÊØ´ÁßíÂÜÖÂèëÁîü„ÄÇÂ¶ÇÊûúÊÇ®ÈúÄË¶ÅÂä®ÊÄÅÈìæÊé•ÁªÑ‰ª∂ÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî®ÂºÄÊ∫ê‰º†ËæìÂçèËÆÆÔºàÂ¶Ç &lt;a href=&#34;https://github.com/wrpc/wrpc&#34;&gt;wRPC (WIT over RPC)&lt;/‰∏Ä‰∏™&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÁÆÄËÄåË®Ä‰πãÔºåÂ¶ÇÊûúÊ†∏ÂøÉ WebAssembly ‰∫åËøõÂà∂Êñá‰ª∂ÊòØÂπ≥Èù¢ÁöÑÂü∫Êú¨ÊûÑÂª∫ÂùóÔºåÈÇ£‰πàÁªÑ‰ª∂Â∞±ÊòØÈï∂ÂµåÁöÑÊûÑÈÄ†ÂùóÔºåÊó®Âú®‰ª•Êñ∞ÁöÑÊñπÂºèÊûÑÂª∫Â§çÊùÇÁöÑ‰∫íËøûÂ∫îÁî®Á®ãÂ∫è„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =‚Äúwp-block-image size-full‚Äù&gt;&lt;imgÂä†ËΩΩ=‚Äúlazy‚ÄùËß£Á†Å=‚ÄúÂºÇÊ≠•‚ÄùÂÆΩÂ∫¶=‚Äú1086‚ÄùÈ´òÂ∫¶=‚Äú506‚Äùsrc=‚Äúhttps://www.cncf.io/ wp-content/uploads/2024/07/components.jpg&#34; alt=&#34;ÁªÑ‰ª∂&#34; class=&#34;wp-image-113994&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07 /components.jpg 1086wÔºåhttps://www.cncf.io/wp-content/uploads/2024/07/components-300x140.jpg 300wÔºåhttps://www.cncf.io/wp-content/uploads/2024 /07/components-1024x477.jpg 1024wÔºåhttps://www.cncf.io/wp-content/uploads/2024/07/components-768x358.jpg 768wÔºåhttps://www.cncf.io/wp-content /uploads/2024/07/components-900x419.jpg 900wÔºåhttps://www.cncf.io/wp-content/uploads/2024/07/components-429x200.jpg 429wÔºåhttps://www.cncf.io /wp-content/uploads/2024/07/components-858x400.jpg 858w‚ÄúÂ∞∫ÂØ∏=‚ÄùÔºàÊúÄÂ§ßÂÆΩÂ∫¶Ôºö1086pxÔºâ100vwÔºå1086px‚Äúreferrerpolicy=‚Äúno-referrer‚Äù&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Â¶ÇÊûúÊÇ®ÁöÑÁî®‰æã‰∏çÂ≠òÂú® WASI APIÔºå‰πüÊ≤°ÊúâÈóÆÈ¢ò - ÊÇ®ÂèØ‰ª•‰ΩøÁî®ÂºÄÊîæÁöÑ &lt;strong&gt;WebAssembly Êé•Âè£Á±ªÂûã (WIT)&lt;/strong&gt; Êé•Âè£ÊèèËø∞ËØ≠Ë®ÄÁºñÂÜôËá™Â∑±ÁöÑÊé•Âè£„ÄÇÂºÄÊîæÊ†áÂáÜ‰ΩøÁîüÊÄÅÁ≥ªÁªüÊó†ÈôêÊâ©Â±ï„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly Áà±Â•ΩËÄÖÔºàÂÉèÊàë‰ª¨‰∏ÄÊ†∑ÔºâÁªèÂ∏∏ÂàÜ‰∫´ Docker ÂàõÂßã‰∫∫ Solomon Hykes ÁöÑËøôÂè•ËØùÔºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;Â¶ÇÊûú WASM+WASI Âú® 2008 Âπ¥Â∞±Â≠òÂú®ÔºåÊàë‰ª¨Â∞±‰∏çÈúÄË¶ÅÂàõÂª∫ Docker„ÄÇ&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Êàë‰ª¨ÂñúÊ¨¢ËøôÂè•ËØùÔºÅ‰ΩÜÂ¶ÇÊûúÊàë‰ª¨Âú®‰∏çËß£ÈáäÁªÑ‰ª∂Â¶Ç‰ΩïÂ∑•‰ΩúÁöÑÊÉÖÂÜµ‰∏ã‰ΩøÁî®ÂÆÉÔºåÊàë‰ª¨Â∞±ÊúâÂèØËÉΩÊé©ÁõñÂÆÉ‰ª¨ÁöÑÂèòÈù©ÊÄß„ÄÇÂ∞ÜÁªÑ‰ª∂ËßÜ‰∏∫ÊØîÂÆπÂô®Êõ¥ÊúâÊïàÁöÑÊõø‰ª£ÂìÅÊòØÈîôËØØÁöÑ„ÄÇÊòØÁöÑÔºåÂÆÉ‰ª¨ÊòØ‰∫ëÂéüÁîüÂ∑•‰ΩúË¥üËΩΩÁöÑÈ´òÊïà„ÄÅÂèØÁßªÊ§çÁöÑ‰∫§‰ªòÊú∫Âà∂Ôºå‰ΩÜÁªÑ‰ª∂‰πüÊòØ‰∏ÄÁßçÂÖ®Êñ∞ÁöÑËåÉ‰æãÔºåÂèØ‰ª•Ëß£ÈîÅÂÖ®Êñ∞ÁöÑËÆ°ÁÆóÊ®°Âûã„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;‰∏ã‰∏ÄÊ≥¢Êµ™ÊΩÆÂ∑≤ÁªèÂà∞Êù•&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ËøáÂéªÂá†Âπ¥Â∑≤ÁªèÂΩ¢Êàê‰∫Ü‰∏Ä‰∏™ËûçÂêàÊó∂ÂàªÔºåÂÖ≥ÈîÆÈÉ®ÂàÜÂ∑≤ÁªèÂ∞±‰ΩçÔºàÊàñËÄÖÊõ¥Á°ÆÂàáÂú∞ËØ¥ÔºåÊòØÁî±Êï¥‰∏™ÁîüÊÄÅÁ≥ªÁªü‰∏≠ÁöÑËÆ∏Â§ö‰∫∫Âä™ÂäõÂ∞±‰ΩçÔºâ„ÄÇÂÖ∂‰∏≠ÊúÄÈáçË¶ÅÁöÑÊòØ 2024 Âπ¥ 1 ÊúàÂèëÂ∏ÉÁöÑ WASI 0.2 ÂíåÁªÑ‰ª∂Ê®°Âûã„ÄÇÊúâ‰∫ÜÈÄöÁî®Ê®°ÂûãÂíåÈÄöÁî® APIÔºåÁ§æÂå∫Á´ãÂç≥ÊûÑÂª∫ÂíåÊõ¥Êñ∞‰∫ÜÂêÑÁßçÂºÄÊ∫êÂ∑•ÂÖ∑ÂíåÊ†áÂáÜÁöÑÊú¨Êú∫ÊîØÊåÅËØ≠Ë®ÄÂ∫ì„ÄÇ‰ªÖ‰∏æÂá†‰∏™‰æãÂ≠êÔºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasmtime&#34;&gt;&lt;strong&gt;Wasmtime&lt;/strong&gt;&lt;/a&gt;ÔºöWebAssembly ÁªÑ‰ª∂ÁöÑÁã¨Á´ãËøêË°åÊó∂&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasm-tools&#34;&gt;&lt;strong&gt;&lt;code&gt;wasm-tools&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;ÔºöÂ§öÂäüËÉΩÂ∑•ÂÖ∑Áî®‰∫é‰∏éÁªÑ‰ª∂‰∫§‰∫íÔºàÈòÖËØª WIT Êé•Âè£„ÄÅÊí∞ÂÜôÁ≠âÔºâ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://wasmcloud.com/&#34;&gt;&lt;strong&gt;wasmCloud&lt;/strong&gt;&lt;/a&gt;ÔºöÂú®‰ªª‰ΩïÂú∞ÊñπÊûÑÂª∫ÂíåËøêË°åÁªÑ‰ª∂ÔºåÂåÖÊã¨ËæπÁºòÂíåÂàÜÂ∏ÉÂºèÁéØÂ¢É&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wit-deps&#34;&gt;&lt;strong&gt;&lt;code&gt;wit-deps&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;ÔºöÁÆ°ÁêÜ WIT ‰æùËµñÈ°πÁªÑ‰ª∂È°πÁõÆ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasi-virt&#34;&gt;&lt;strong&gt;WASI Virt&lt;/strong&gt;&lt;/a&gt;Ôºö‰ΩøÁî®ÁªÑÂêàÂú®Âè¶‰∏Ä‰∏™Â∞ÅË£ÖÁªÑ‰ª∂‰∏≠ËôöÊãüÂåñ‰∏Ä‰∏™ÁªÑ‰ª∂&lt;/li &gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Êõ¥Â•ΩÁöÑÊòØÔºåÁªÑ‰ª∂Ë∂äÊù•Ë∂äÂ§öÂú∞‰∏éÂ∏∏ËßÅÁöÑ‰∫ëÂéüÁîüÂ∑•ÂÖ∑ÂíåÊ†áÂáÜÈõÜÊàêÔºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;OCI Â∑≤Êàê‰∏∫&lt;a href=&#34;https://tag-runtime.cncf.io/wgs/wasm/deliverables/wasm-oci-artifact/&#34;&gt;Âú®Ê≥®ÂÜåË°®‰∏≠ÊâìÂåÖÁªÑ‰ª∂ÁöÑÊ†áÂáÜ&lt;/a&gt;„ÄÇ &lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;wasmCloud Á≠âÂπ≥Âè∞&lt;a href=&#34;https://wasmcloud.com/docs/kubernetes&#34;&gt;‰∏é Kubernetes ÈõÜÊàê&lt;/a&gt;„ÄÅOpenTelemetry ÂíåÂºÄÊîæÁ≠ñÁï•‰ª£ÁêÜ„ÄÇ&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Âõ¢Èòü‰∏çÂøÖ‰ªéÂ§¥ÂºÄÂßãÂú®Áîü‰∫ß‰∏≠ËøêË°åÁªÑ‰ª∂ÔºåËÄåÊòØÂèØ‰ª•‰ΩøÁî®Áé∞ÊúâÁöÑ‰∫ëÂéüÁîüÂ∑•ÂÖ∑„ÄÇÂØπ‰∫éÈÇ£‰∫õÂÖ®Êñ∞ÁöÑÁªÑ‰ª∂ÔºåÁ§æÂå∫Ê≠£Âú®ÂºÄÂèëË∂äÊù•Ë∂äÂ§öÁöÑËµÑÊ∫êÔºå‰æãÂ¶Ç &lt;a href=&#34;https://component-model.bytecodealliance.org/&#34;&gt;ÁªÑ‰ª∂Ê®°ÂûãÊâãÂÜå&lt;/a&gt; Âíå &lt;a href=&#34;https ://wasi.dev/&#34;&gt;WASI.dev&lt;/a&gt;„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‰∏éÊ≠§ÂêåÊó∂ÔºåÁªÑ‰ª∂ÂºÄÂèë‰ΩìÈ™åÂú®Ë∂äÊù•Ë∂äÂ§öÁöÑËØ≠Ë®Ä‰∏≠ÂèòÂæóË∂äÊù•Ë∂äÂÆåÂñÑ„ÄÇÂú® JavaScript ÁîüÊÄÅÁ≥ªÁªü‰∏≠Ôºå&lt;a href=&#34;https://github.com/bytecodealliance/jco&#34;&gt;&lt;code&gt;jco&lt;/code&gt;&lt;/a&gt; ‰Ωø JavaScript ÂºÄÂèë‰∫∫ÂëòËÉΩÂ§üÁºñÂÜôÊÉØÁî®‰ª£Á†ÅÂπ∂ÁºñËØë‰∏∫ WebAssemblyÔºåËÄå Rustaceans ÂèØ‰ª•Áõ¥Êé•ÁºñËØëÂà∞ &lt;a href=&#34;https://doc.rust-lang.org/nightly/rustc/platform-support/wasm32-wasip2.html&#34;&gt;&lt;code&gt;wasm32-wasip2&lt;/code&gt; ÁõÆÊ†á&lt;/a &gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‰∏ñÁïåÂêÑÂú∞ÁöÑÂõ¢ÈòüÂ∑≤ÁªèÂú®Âà©Áî®ÁªÑ‰ª∂Ëß£ÈîÅÊñ∞ÁöÑÂèØËÉΩÊÄß„ÄÇÂú®Â∑•ÂéÇËΩ¶Èó¥ËæπÁºòËÆæÂ§áÁ≠âËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠ÔºåÂà∂ÈÄ†ÂàÜÊûêÂÖ¨Âè∏ &lt;a href=&#34;https://www.youtube.com/watch?v=fQdkNGZqYZA&amp;list=PLj6h78yzYM2MQteKoXxICTWiUdZYEw6RI&amp;index=6&#34;&gt;MachineMetrics&lt;/a&gt; Ê≠£Âú®‰ΩøÁî®ÁªÑ‰ª∂Êù•Áõ¥Êé•Âú®Â∑•ÂéÇËΩ¶Èó¥ËÆæÂ§á‰∏äÂ§ÑÁêÜÈ´òÈ¢ëÊï∞ÊçÆÂπ∂Â∞ÜÂÖ∂ÂÆûÊó∂‰º†ËæìÂà∞‰∫ëÁ´Ø‚Äî‚ÄîÂú®ÂÆπÂô®ÈÄöÂ∏∏Êó†Ê≥ïÂà∞ËææÁöÑÂú∞ÊñπËøêË°åÔºåÂêåÊó∂‰∏é Kubernetes ÈõÜÊàê„ÄÇ WASI 0.2 ÂèëÂ∏É‰ªÖÂá†‰∏™ÊúàÂêéÔºåÁªÑ‰ª∂Â∞±Â∑≤ÁªèÊîπÂèò‰∫Ü‰∏ÄÂàáÂèØËÉΩ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Âä†ÈÄüÂàõÊñ∞&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÂºÄÂèëËÄÖ‰ª•ÂèäÊï¥‰∏™ÁîüÊÄÅÁ≥ªÁªü‰∏ç‰∏ÄÂÆö‰ºöÈááÁî®Êñ∞ÊäÄÊúØÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Âú®ÁêÜËÆ∫‰∏äÊõ¥È´òÊïàÊàñÊõ¥ÂÆâÂÖ®„ÄÇÂΩìÊäÄÊúØ‰ΩøÊàë‰ª¨ÂèòÂæóÊõ¥ÊúâÊïà„ÄÅÊõ¥ÊúâÁîü‰∫ßÂäõ„ÄÅÊõ¥ÂÖ∑ÂàõÊñ∞ÊÄßÊó∂ÔºåÊñ∞ÁöÑÂ∑•‰ΩúÊñπÂºèÂ∞±‰ºöÂá∫Áé∞„ÄÇ WebAssembly ÁªÑ‰ª∂Ê≠£Âú®ËøôÊ†∑ÂÅö‚Äî‚ÄîÊâìÁ†¥ËØ≠Ë®ÄÂ≠§Â≤õÂπ∂Êè≠Á§∫Êñ∞ÁöÑÊú∫‰ºö„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‰ªäÂ§©ÔºåË∑ù Wasm ËØûÁîüÂ∑≤ÊúâÂçÅÂπ¥‰∫Ü„ÄÇÂú®ÂÆπÂô®ÁöÑÊó∂Èó¥ËΩ¥‰∏äÔºåËøôÊòØÊàë‰ª¨Âà∞Ëææ‰∏Ä‰∏™ÊãêÁÇπÁöÑÂú∞ÊñπÔºõÁªÑ‰ª∂Ê®°ÂûãÂíå WASI 0.2 Ê≠£Âú®ËøéÊù•ÂêåÊ†∑ÁöÑËåÉÂºèËΩ¨Âèò„ÄÇËØ•Â∑•ÂÖ∑‰∏ç‰ªÖÂèØ‰ª•ËÆ©ÂºÄÂèë‰∫∫Âëò‰ΩøÁî® WebAssembly ËøõË°åÊûÑÂª∫ÔºåËøòÂèØ‰ª•ÊèêÈ´òÁîü‰∫ßÂäõ„ÄÅÊïàÁéáÂíåÂàõÊñ∞ÊÄß„ÄÇ WASI ÁöÑÈÄöÁî®Êé•Âè£‰ΩøÁªÑ‰ª∂ÂºÄÂèëÂë®ÊúüÂèòÂæóÂºÇÂ∏∏Âø´ÈÄüÔºåÂπ∂‰∏îÁªÑ‰ª∂Êú¨Ë∫´ÈùûÂ∏∏ÁÅµÊ¥ª„ÄÇÂõ¢ÈòüÂ∞ÜÊääÁªÑ‰ª∂Â∏¶Âà∞Êàë‰ª¨Êó†Ê≥ïÈ¢ÑÊµãÁöÑÂú∞ÊñπÔºå‰ªéËøôÈáåÂºÄÂßãÔºå‰∏ã‰∏ÄÊ≥¢‰∫ëÂéüÁîüËÆ°ÁÆóÂè™‰ºöÂèòÂæóÊõ¥ÂÖ∑ÂèòÈù©ÊÄß„ÄÇ&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Mon, 08 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêAs we reach mid-year 2024, a look at CNCF, Linux Foundation, and top 30 open source project velocity„ÄëÂà∞‰∫Ü 2024 Âπ¥Âπ¥‰∏≠ÔºåÊàë‰ª¨Êù•ÁúãÁúã CNCF„ÄÅLinux Âü∫Èáë‰ºöÂíåÊéíÂêçÂâç 30 ÁöÑÂºÄÊ∫êÈ°πÁõÆÈÄüÂ∫¶</title>
      <link>https://www.cncf.io/blog/2024/07/11/as-we-reach-mid-year-2024-a-look-at-cncf-linux-foundation-and-top-30-open-source-project-velocity/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;Staff post by Chris Aniszczyk&amp;nbsp;&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Date/Time:&lt;/strong&gt; July 11 at 8am&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For the last several years we have tracked open source project velocity, which has enabled us to monitor the trends and technologies that resonate with developers and end users. For comparison, have a look at past timeframes from our &lt;a href=&#34;https://www.cncf.io/?s=Velocity&#34;&gt;blogs&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Here are the main takeaways I see from these charts:&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Kubernetes continues to mature with its consistent and largest contributor base&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;OpenTelemetry continues to grow its contributor base and remains the second highest velocity project; they recently added &lt;a href=&#34;https://opentelemetry.io/blog/2024/profiling/&#34;&gt;profiling&lt;/a&gt; as a new signal type&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Backstage grows solving an important pain point around developer experience&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;GitOps continues to be important in the cloud native ecosystem, where projects like Argo and Flux continue to cultivate large communities&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Crossplane grew its contributor base by over 20% in the last year reflecting a desire for open source control planes in the era of open source relicensing issues&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KeyCloak joined CNCF last year as an incubating project and has a large community pushing open source identity and access management forward&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;In many cases, CNCF projects &lt;a href=&#34;https://www.cncf.io/case-studies/openai/&#34;&gt;underpin large scale AI infrastructure&lt;/a&gt; and we have Kubeflow appearing on the top 30 CNCF project list for the first time in 2023.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;CNCF projects ‚Äì Last 12 months &lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;interactive map&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXeXEdq4E15JkoUMLwMRdFVFC0jeXT7El7VLVnVChbBOZk-v2I-03MLI5R8HMhgOrnlMS8LC-dQlKrKI4m8ybZHMzQ4Khm4_3dm5bboQhepmg4N9mf2IVWD4dOE4kxEv2T1rmQ0J33ATxZsT4Oi3XsxZh01x?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;cncf projects&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Linux Foundation Projects ‚Äì Last 12 months &lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507&#34;&gt;interactive map&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXcBhcGzavp5kG5-l6g2MW-hNTvn2BIl2HIkb07TFm2dhUOJEymAhE18jAxE2_hgqitU9vJCRSDZ18EzppV9auq0eHq-qyGiYoT-JxQ5znQRMK_rq-mA_naXmtdQrNeuAeI8paEXZtUSxWxXLbsgRSBZi74?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;LF projects&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Top 30 open source projects ‚Äì Last 12 months&lt;/strong&gt; (&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507&#34;&gt;interactive map&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdrzMUoz3xCWNweqwfxhzfI5koLh1Z68AbH4oWMjsQsH6FoBfZDx4bpsY2Cwyaxx7ZT9exDO1qtNy-oP00BuSczhhCZYCreLEDjdv0bQssLvSGEDkuVSaNCy2GQyjB0GUJIo4amk3C_St4m7tqM9zj8wAsv?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;Open source projects&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: We use bubble charts to show three axes of data: commits, authors, and comments/pull requests, and plot on a log-log chart to show the data across large scales.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;The bubble‚Äôs area is proportional to the number of authors&amp;nbsp;&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;The y-axis is the total number of pull requests and issues&amp;nbsp;&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;The x-axis is the number of commits&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;All of the the &lt;a href=&#34;https://github.com/cncf/velocity#current-reports&#34;&gt;current&lt;/a&gt; and &lt;a href=&#34;https://github.com/cncf/velocity#past-reports&#34;&gt;past&lt;/a&gt; reports are available on GitHub, as well as a list and charts on the Google sheets below:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;All CNCF projects for July 2023-July 2024&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507&#34;&gt;All Linux Foundation projects for July 2023-July 2024&lt;/a&gt;&amp;nbsp;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507&#34;&gt;Top 30 open source projects for July 2023-July 2024&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;All of the scripts used to generate this data are at &lt;a href=&#34;https://github.com/cncf/velocity&#34;&gt;https://github.com/cncf/velocity&lt;/a&gt; (under an Apache 2.0 &lt;a href=&#34;https://www.cncf.io/blog/2017/02/01/cncf-recommends-aslv2/&#34;&gt;license&lt;/a&gt;). If you see any errors, please open an issue there.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Past blog posts about project velocity:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2024/01/17/a-look-back-at-cncf-linux-foundation-and-top-30-open-source-project-velocity-in-2023/&#34;&gt;A look back at CNCF, Linux Foundation, and top 30 open source project velocity in 2023&amp;nbsp;&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/10/27/october-2023-where-we-are-with-velocity-of-cncf-lf-and-top-30-open-source-projects/&#34;&gt;October 2023: where we are with velocity of CNCF, LF, and top 30 open source projects&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/01/11/a-look-at-the-2022-velocity-of-cncf-linux-foundation-and-top-30-open-source-projects/&#34;&gt;A look at the 2022 velocity of CNCF, Linux Foundation, and top 30 open source projects&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2022/08/23/mid-year-update-on-2022-cncf-linux-foundation-and-open-source-velocity/&#34;&gt;Mid-year update on 2022 CNCF, Linux Foundation, and open source velocity&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/12/15/end-of-year-update-on-cncf-and-open-source-velocity-in-2021/&#34;&gt;End of year update on CNCF and open source velocity in 2021&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/08/02/update-on-cncf-and-open-source-project-velocity-2020/&#34;&gt;Update on CNCF and Open Source Project Velocity 2020&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2017/06/05/30-highest-velocity-open-source-projects/&#34;&gt;The 30 highest velocity open source projects&lt;/a&gt;&amp;nbsp;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Chris Aniszczyk ÁöÑÂëòÂ∑•Â∏ñÂ≠ê&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Êó•Êúü/Êó∂Èó¥Ôºö&lt;/strong&gt;7 Êúà 11 Êó•‰∏äÂçà 8 ÁÇπ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Âú®ËøáÂéªÁöÑÂá†Âπ¥ÈáåÔºåÊàë‰ª¨‰∏ÄÁõ¥Âú®Ë∑üË∏™ÂºÄÊ∫êÈ°πÁõÆÁöÑÈÄüÂ∫¶ÔºåËøô‰ΩøÊàë‰ª¨ËÉΩÂ§üÁõëÊéß‰∏éÂºÄÂèë‰∫∫ÂëòÂíåÊúÄÁªàÁî®Êà∑‰∫ßÁîüÂÖ±È∏£ÁöÑË∂ãÂäøÂíåÊäÄÊúØ„ÄÇ‰∏∫‰∫ÜËøõË°åÊØîËæÉÔºåËØ∑Êü•ÁúãÊàë‰ª¨ÁöÑ&lt;a href=&#34;https://www.cncf.io/?s=Velocity&#34;&gt;ÂçöÂÆ¢&lt;/a&gt;‰∏≠ËøáÂéªÁöÑÊó∂Èó¥ËåÉÂõ¥„ÄÇ¬†&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‰ª•‰∏ãÊòØÊàë‰ªéËøô‰∫õÂõæË°®‰∏≠ÁúãÂà∞ÁöÑ‰∏ªË¶ÅÁªìËÆ∫Ôºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Kubernetes Âá≠ÂÄüÂÖ∂‰∏ÄËá¥‰∏îÊúÄÂ§ßÁöÑË¥°ÁåÆËÄÖÁæ§‰Ωì‰∏çÊñ≠Ëµ∞ÂêëÊàêÁÜü&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;OpenTelemetry ÁöÑË¥°ÁåÆËÄÖÂü∫Á°Ä‰∏çÊñ≠Êâ©Â§ßÔºåÂπ∂‰∏î‰ªçÁÑ∂ÊòØÁ¨¨‰∫åÈ´òÈÄüÂ∫¶‚Äã‚ÄãÈ°πÁõÆÔºõ‰ªñ‰ª¨ÊúÄËøëÊ∑ªÂä†‰∫Ü&lt;a href=&#34;https://opentelemetry.io/blog/2024/profiling/&#34;&gt;ÂàÜÊûê&lt;/a&gt;‰Ωú‰∏∫Êñ∞ÁöÑ‰ø°Âè∑Á±ªÂûã&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;ÂêéÂè∞‰∏çÊñ≠ÂèëÂ±ïÔºåËß£ÂÜ≥‰∫ÜÂºÄÂèëËÄÖ‰ΩìÈ™åÊñπÈù¢ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÁóõÁÇπ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;GitOps Âú®‰∫ëÂéüÁîüÁîüÊÄÅÁ≥ªÁªü‰∏≠‰ªçÁÑ∂ÂèëÊå•ÁùÄÈáçË¶Å‰ΩúÁî®ÔºåArgo Âíå Flux Á≠âÈ°πÁõÆÁªßÁª≠ÂüπËÇ≤Â§ßÂûãÁ§æÂå∫&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;ÂéªÂπ¥ÔºåCrossplane ÁöÑË¥°ÁåÆËÄÖÊï∞ÈáèÂ¢ûÈïø‰∫Ü 20% ‰ª•‰∏äÔºåÂèçÊò†Âá∫Âú®ÂºÄÊ∫êÈáçÊñ∞ËÆ∏ÂèØÈóÆÈ¢òÊó∂‰ª£ÂØπÂºÄÊ∫êÊéßÂà∂Âπ≥Èù¢ÁöÑÊ∏¥Êúõ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KeyCloak ÂéªÂπ¥‰Ωú‰∏∫Â≠µÂåñÈ°πÁõÆÂä†ÂÖ• CNCFÔºåÊã•ÊúâÊé®Âä®ÂºÄÊ∫êË∫´‰ªΩÂíåËÆøÈóÆÁÆ°ÁêÜÂêëÂâçÂèëÂ±ïÁöÑÂ§ßÂûãÁ§æÂå∫&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Âú®ËÆ∏Â§öÊÉÖÂÜµ‰∏ãÔºåCNCF È°πÁõÆ&lt;a href=&#34;https://www.cncf.io/case-studies/openai/&#34;&gt;ÊîØÊíëÂ§ßËßÑÊ®°‰∫∫Â∑•Êô∫ËÉΩÂü∫Á°ÄËÆæÊñΩ&lt;/a&gt;ÔºåÂπ∂‰∏î Kubeflow Âá∫Áé∞Âú®Ââç 30 Âêç‰∏≠2023Âπ¥È¶ñÊ¨°CNCFÈ°πÁõÆÂêçÂçï„ÄÇ&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;CNCF È°πÁõÆ ‚Äì ËøáÂéª 12 ‰∏™Êúà&lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;‰∫§‰∫íÂºèÂú∞Âõæ&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXeXEdq4E15JkoUMLwMRdFVFC0jeXT7El7VLVnVChbBOZk-v2I-03MLI5R8HMhgOrnlMS8LC-dQlKrKI4m8ybZHMzQ4Khm4_ 3dm5bboQhepmg4N9mf2IVWD4dOE4kxEv2T1rmQ0J33ATxZsT4Oi3XsxZh01x?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;cncf È°πÁõÆ&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Linux Âü∫Èáë‰ºöÈ°πÁõÆ ‚Äì ËøáÂéª 12 ‰∏™Êúà&lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507 &#34;&gt;‰∫§‰∫íÂºèÂú∞Âõæ&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXcBhcGzavp5kG5-l6g2MW-hNTvn2BIl2HIkb07TFm2dhUOJEymAhE18jAxE2_hgqitU9vJCRSDZ18EzppV9auq0eHq- qyGiYoT-JxQ5znQRMK_rq-mA_naXmtdQrNeuAeI8paEXZtUSxWxXLbsgRSBZi74?key= CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;LF È°πÁõÆ&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ââç 30 ‰∏™ÂºÄÊ∫êÈ°πÁõÆ - ËøáÂéª 12 ‰∏™Êúà&lt;/strong&gt; (&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507 &#34;&gt;‰∫§‰∫íÂºèÂú∞Âõæ&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdrzMUoz3xCWNweqwfxhzfI5koLh1Z68AbH4oWMjsQsH6FoBfZDx4bpsY2Cwyaxx7ZT9exDO1qtNy-oP00BuSczhhCZY CreLEDjdv0bQssLvSGEDkuVSaNCy2GQyjB0GUJIo4amk3C_St4m7tqM9zj8wAsv?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;ÊâìÂºÄÊ∫êÈ°πÁõÆ&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Ê≥®ÊÑè&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;ÔºöÊàë‰ª¨‰ΩøÁî®Ê∞îÊ≥°ÂõæÊù•ÊòæÁ§∫Êï∞ÊçÆÁöÑ‰∏â‰∏™ËΩ¥ÔºöÊèê‰∫§„ÄÅ‰ΩúËÄÖÂíåËØÑËÆ∫/ÊãâÂèñËØ∑Ê±ÇÔºåÂπ∂Âú®ÂØπÊï∞Êó•Âøó‰∏äÁªòÂà∂ÂõæË°®ÊòæÁ§∫Â§ßËåÉÂõ¥ÂÜÖÁöÑÊï∞ÊçÆ„ÄÇ¬†&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;Ê∞îÊ≥°Èù¢ÁßØ‰∏é‰ΩúËÄÖÊï∞ÈáèÊàêÊ≠£ÊØî&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;y ËΩ¥ÊòØÊãâÂèñËØ∑Ê±ÇÂíåÈóÆÈ¢òÁöÑÊÄªÊï∞&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;x ËΩ¥ÊòØÊèê‰∫§Ê¨°Êï∞&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊâÄÊúâ&lt;a href=&#34;https://github.com/cncf/velocity#current-reports&#34;&gt;ÂΩìÂâç&lt;/a&gt;Âíå&lt;a href=&#34;https://github.com/cncf/ velocity#past-reports&#34;&gt;ËøáÂéªÁöÑ&lt;/a&gt;Êä•ÂëäÂèØÂú® GitHub ‰∏äÊâæÂà∞Ôºå‰∏ãÈù¢ÁöÑ Google Â∑•‰ΩúË°®‰∏äËøòÊèê‰æõÂàóË°®ÂíåÂõæË°®Ôºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;2023 Âπ¥ 7 ÊúàËá≥ 2024 Âπ¥ 7 ÊúàÁöÑÊâÄÊúâ CNCF È°πÁõÆ&lt;/a&gt;&lt;/Êùé&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507&#34;&gt;2023 Âπ¥ 7 ÊúàËá≥ 2024 Âπ¥ 7 ÊúàÁöÑÊâÄÊúâ Linux Âü∫Èáë‰ºöÈ°πÁõÆ&lt;/a &gt;¬†&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507&#34;&gt;2023 Âπ¥ 7 ÊúàËá≥ 2024 Âπ¥ 7 ÊúàÊéíÂêçÂâç 30 ÁöÑÂºÄÊ∫êÈ°πÁõÆ&lt;/a&gt; &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Áî®‰∫éÁîüÊàêÊ≠§Êï∞ÊçÆÁöÑÊâÄÊúâËÑöÊú¨Âùá‰Ωç‰∫é &lt;a href=&#34;https://github.com/cncf/velocity&#34;&gt;https://github.com/cncf/velocity&lt;/a&gt;ÔºàÂú®Apache 2.0 &lt;a href=&#34;https://www.cncf.io/blog/2017/02/01/cncf-recommends-aslv2/&#34;&gt;ËÆ∏ÂèØËØÅ&lt;/a&gt;Ôºâ„ÄÇÂ¶ÇÊûúÊÇ®ÂèëÁé∞‰ªª‰ΩïÈîôËØØÔºåËØ∑Âú®ÈÇ£ÈáåÊèêÂá∫ÈóÆÈ¢ò„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ËøáÂéªÊúâÂÖ≥È°πÁõÆÈÄüÂ∫¶ÁöÑÂçöÂÆ¢ÊñáÁ´†Ôºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2024/01/17/a-look-back-at-cncf-linux-foundation-and-top-30-open-source- project-velocity-in-2023/&#34;&gt;CNCF„ÄÅLinux Âü∫Èáë‰ºöÂíå 2023 Âπ¥ÊéíÂêçÂâç 30 ÁöÑÂºÄÊ∫êÈ°πÁõÆÈÄüÂ∫¶ÂõûÈ°æ&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/10/27/october-2023-where-we-are-with-velocity-of-cncf-lf-and-top- 30-open-source-projects/&#34;&gt;2023 Âπ¥ 10 ÊúàÔºöCNCF„ÄÅLF ÂíåÂâç 30 ‰∏™ÂºÄÊ∫êÈ°πÁõÆÁöÑÂèëÂ±ïÈÄüÂ∫¶&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/01/11/a-look-at-the-2022-velocity-of-cncf-linux-foundation-and-top- 30-open-source-projects/&#34;&gt;CNCF„ÄÅLinux Âü∫Èáë‰ºöÂíåÂâç 30 ‰∏™ÂºÄÊ∫êÈ°πÁõÆ 2022 Âπ¥ÈÄüÂ∫¶‰∏ÄËßà&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2022/08/23/mid-year-update-on-2022-cncf-linux-foundation-and-open-source-velocity/ &#34;&gt;2022 Âπ¥ CNCF„ÄÅLinux Âü∫Èáë‰ºöÂíåÂºÄÊ∫êÈÄüÂ∫¶ÁöÑÂπ¥‰∏≠Êõ¥Êñ∞&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/12/15/end-of-year-update-on-cncf-and-open-source-velocity-in-2021/ &#34;&gt;CNCF Âπ¥ÁªàÊõ¥Êñ∞Âíå 2021 Âπ¥ÂºÄÊ∫êÈÄüÂ∫¶&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/08/02/update-on-cncf-and-open-source-project-velocity-2020/&#34;&gt;CNCF Âíå2020 Âπ¥ÂºÄÊ∫êÈ°πÁõÆÈÄüÂ∫¶&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2017/06/05/30-highest-velocity-open-source-projects/&#34;&gt;30 ‰∏™ÈÄüÂ∫¶ÊúÄÂø´ÁöÑÂºÄÊ∫êÈ°πÁõÆ&lt;/a &gt;¬†&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Wed, 10 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêBuilding the future of 5G with cloud native tech: insights from Joel and Ashan from Swisscom„ÄëÂà©Áî®‰∫ëÂéüÁîüÊäÄÊúØÊûÑÂª∫ 5G ÁöÑÊú™Êù•ÔºöSwisscom ÁöÑ Joel Âíå Ashan ÁöÑËßÅËß£</title>
      <link>https://www.cncf.io/blog/2024/07/02/building-the-future-of-5g-with-cloud-native-tech-insights-from-joel-and-ashan-from-swisscom/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;Member post by &lt;a href=&#34;https://www.linkedin.com/in/anjali-udasi/&#34;&gt;Anjali Udasi&lt;/a&gt;, Technical Writer, Zenduty and &lt;a href=&#34;https://www.linkedin.com/in/shubham67/&#34;&gt;Shubham Srivastava&lt;/a&gt;, Developer Relations Engineer, Zenduty&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/joelstudler/&#34;&gt;Joel Studler&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/ashan-senevirathne/&#34;&gt;Ashan Senevirathne&lt;/a&gt; took the stage at KubeCon + CloudNativeCon Europe in Paris with their presentation, ‚Äú&lt;a href=&#34;https://www.youtube.com/watch?v=crmTnB6Zwt8&#34;&gt;From GitOps to Kubernetes Resource Model&lt;/a&gt;,‚Äù highlighting Swisscom‚Äôs automation journey in the 5G Core and reflecting the company‚Äôs evolution from telco to TechCo.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Their talk was truly compelling, sparking our interest in learning more about their experiences and journey at &lt;a href=&#34;https://www.linkedin.com/company/swisscom/&#34;&gt;Swisscom&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/shubham67/&#34;&gt;Shubham Srivastava&lt;/a&gt;, who currently leads Developer Relations at &lt;a href=&#34;https://www.zenduty.com/&#34;&gt;Zenduty&lt;/a&gt;, had the pleasure of speaking with this dynamic duo: Joel, a DevOps Engineer and System Architect dedicated to building the next generation of mobile networking using cloud-native technologies, and Ashan, a Product Owner overseeing the design, implementation, and delivery of a cloud-native orchestration framework for the mobile organization.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1600&#34; height=&#34;988&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-2.jpg&#34; alt=&#34;Building the Future of 5G with Cloud-Native Tech presented by Ashan Senevirathne and Joel Studler from swisscom&#34; class=&#34;wp-image-113961&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-2.jpg 1600w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-300x185.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-1024x632.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-768x474.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-900x556.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-324x200.jpg 324w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-648x400.jpg 648w&#34; sizes=&#34;(max-width: 1600px) 100vw, 1600px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this blog featuring Joel and Ashan, we peel the layers of the telco world, the struggles of modernization, and the cutting-edge tools these minds put to use. This is a conversation you don‚Äôt want to miss!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: It‚Äôs great to chat with you both. We‚Äôd love to know what an average day for you both looks like. You‚Äôre leading DevOps and reliability at Telco with very tight error budgets and room for failure. So what does that look like behind the scenes?&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan: &lt;/strong&gt;For us, our main focus is on developing tooling capabilities for the upcoming 5G core technology, which we find applicable to other areas of the business as well. We put a lot of emphasis on community-driven initiatives. While our main focus is on Kubernetes environments, we also address the transition from legacy-based change management to cloud-native approaches, which requires a shift in organizational mindset.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: My role involves handling technical interfaces with the product, and collaborating closely with Ashan on architecture and engineering. Our daily tasks involve building reliable tools and automation, predominantly through &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; operators. We prioritize designing sustainable and efficient solutions while optimizing existing workloads. Testing and deployment typically occur on live or pre-production clusters.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: Ashan, you mentioned that half of the job is not migrating processes, it‚Äôs also migrating the mindsets of the people. So stemming from that, what do you think is the hardest part about maintaining and updating reliability and tooling in a telecom industry that‚Äôs typically viewed as being very archaic and having a lot of legacy processes?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan: &lt;/strong&gt;The biggest hurdle in the telecom industry is adapting to a more open and flexible approach to network management. Traditionally, telecom relies on vendor-provided ‚Äúblack box‚Äù software, making it difficult to maintain and update tools reliably.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But, now we‚Äôre tackling this by:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Greenfield approach:&lt;/strong&gt; Building new 5G core tools from scratch instead of relying on legacy systems.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;CNCF adoption:&lt;/strong&gt; Utilizing tools and concepts from the Cloud Native Computing Foundation (CNCF) for automation and containerization.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Shifting mindset:&lt;/strong&gt; Moving from a ‚Äúblack box‚Äù to a ‚Äúwhite box‚Äù mentality, where software is open and modifiable for better control and reliability.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And also with the fact that we strategically decided to go forward with Kubernetes operators and the Kubernetes concepts for automation, has a big impact on many other topics like change management.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance: How do you implement a change? How do you plan with Kubernetes resources? You don‚Äôt control when the change happens. The operator just rolls it out now and then. You don‚Äôt control it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It‚Äôs a dynamic system, sparking a range of questions that we‚Äôll be discussing a lot in the near future. This shift impacts not just technology but also requires a cultural change within the organization. The company is focusing on education and demonstrations to promote this new way of working.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; As an organization that‚Äôs so large and serves millions of people every day with mission-critical services, how do you handle the transition to Kubernetes? Specifically, how do you support Kubernetes and other open-source tools that enhance its capabilities? How do you vet open-source tools and new technologies from the CNCF ecosystem or elsewhere to ensure they‚Äôre stable and suitable for our organization?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan:&lt;/strong&gt; From a Kubernetes perspective, we use a vendor-provided distribution for our infrastructure. For deployment, we utilize &lt;a href=&#34;https://fluxcd.io/&#34;&gt;Flux&lt;/a&gt;, along with the external secret operator, cert-manager, and several other mature tools within the ecosystem.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For anything telco-specific, we often develop our solutions and have strong support internally to open-source these projects. This allows us to contribute to the community and encourage contributions from other operators, integrating telco-specific use cases into the Kubernetes ecosystem.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When selecting tools, we prioritize maturity and support from the community and other industry players over novelty. This ensures we choose reliable, well-supported tools rather than simply the latest trends.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: Joel, are there any tools that have caught your eye recently, tools that you‚Äôd love to play around with and are watching closely?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: For the development flow, I‚Äôm interested in &lt;a href=&#34;https://microcks.io/&#34;&gt;Microcks&lt;/a&gt;. It‚Äôs a mocking framework and its innovation lies in its usability both within your IDE and on the Kubernetes cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We‚Äôre also exploring testing tools like &lt;a href=&#34;https://litmuschaos.io/&#34;&gt;Litmus&lt;/a&gt; for &lt;a href=&#34;https://www.zenduty.com/blog/chaos-engineering/&#34;&gt;chaos engineering&lt;/a&gt; and &lt;a href=&#34;https://testkube.io/&#34;&gt;Testkube&lt;/a&gt;, a testing wrapper.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance, we‚Äôre adopting cert-manager, but in a Mobile Core on-prem environment with black box applications, it‚Äôs challenging. We‚Äôre pushing vendors to ensure compatibility with cert-manager, despite their tendency to fork and maintain their own versions.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan&lt;/strong&gt;: Additionally, we‚Äôre looking into a project called &lt;a href=&#34;https://nephio.org/&#34;&gt;Nephio&lt;/a&gt;, driven by the Linux Foundation. It‚Äôs designed for deploying and managing the 5G core in a cloud-native way. While we don‚Äôt use Nephio tooling directly, we adapt its framework and thinking. For instance, we‚Äôre contributing to and leveraging the &lt;a href=&#34;https://docs.sdcio.dev/&#34;&gt;SDC (Schema Driven Configuration)&lt;/a&gt; tool within the project.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: Are there any problems that you guys face currently and you‚Äôre waiting for a tool to come up and solve it? Like a hard problem that you guys would not want to build something to solve and you‚Äôre looking for someone else to come up and solve?&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan&lt;/strong&gt;: The problem that we‚Äôre seeing is, or the technical challenge that we have is, we have these telco applications and it‚Äôs treated as an appliance. And during the lifecycle space or the deployment and the configuration phase, we need to do the deployment in a cloud-native way.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And then what comes on top on the appliance level or the configuration, it‚Äôs done in a telco way. So there‚Äôs this proprietary interface defined by the telco standards and you need to do this configuration or apply or define these network services outside of the Kubernetes layer. And to achieve this, you need to do all these workarounds on top of that where we need to implement custom operators or find certain ways to bring this, what‚Äôs done outside of the Kubernetes layer, more into the in-bed Kubernetes layer.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If there‚Äôs an ask, the ask would be to have this configuration done in a Kubernetes native way, which means moving away from these &lt;a href=&#34;https://en.wikipedia.org/wiki/NETCONF&#34;&gt;NETCONF&lt;/a&gt;-based files, into a Kubernetes resource model. This shift would provide significant benefits, especially considering the time and effort we currently invest in making the configuration Kubernetes-native.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: Our biggest pain point right now is that we can‚Äôt handle our applications as true cloud-native citizens. The applications we receive from vendors are still treated like traditional hardware, with manual configurations akin to putting a server into a rack or setting up a bare metal appliance.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The mindset remains tied to the idea of a permanent system, like a &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt; release, where changes are made directly on the running system. This approach prevents us from implementing practices like blue-green deployments, and even a simple redeployment becomes a huge effort due to the manual steps involved.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We believe that introducing a cloud-native configuration interface would simplify lifecycle management, updates, and configurations.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;&lt;strong&gt;: &lt;/strong&gt;Observability must be crucial in your journey, especially as you ramp up. What does your observability framework look like? What metrics are you spending most of your time monitoring? We‚Äôd love to know more about how you handle &lt;a href=&#34;https://www.zenduty.com/blog/observability-vs-monitoring/&#34;&gt;monitoring and observability&lt;/a&gt; at Swisscom.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel: &lt;/strong&gt;Currently, we use a standard observability stack with &lt;a href=&#34;https://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt; in our clusters and &lt;a href=&#34;https://grafana.com/oss/loki/&#34;&gt;Loki&lt;/a&gt; for logging. For centralized deployment, we use &lt;a href=&#34;https://thanos.io/&#34;&gt;Thanos&lt;/a&gt;. Additionally, we consume an internal Observability-as-a-Service stack that any Swisscom team can use, built on the standard Prometheus-Grafana stack, which integrates well with our Incident Management tooling.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We focus on a minimal, relevant subset of metrics to ensure service health. The 5G core and applications are more complex due to their black-box nature, so we work closely with vendors to identify the right metrics.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan:&lt;/strong&gt; We use HTTP-based requests to monitor golden signals and key performance indicators (KPIs) such as user attachments, latency, and DNS metrics. For example in 5G core, we will have how many users are attached, what‚Äôs the latency or some metrics on the DNS.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While some metrics are standard, others are more telco-specific, requiring vendor collaboration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; You mentioned a while ago that you‚Äôre spending a lot of effort bolstering site reliability movements within your organization. SRE can be very demanding, as we‚Äôre all familiar with it. What‚Äôs the story at your org? How are you managing work-life balance, especially in a Telco environment where nothing can go wrong?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan&lt;/strong&gt;: I can speak on behalf of the mobile organization at Swisscom. We have a significant IT side as well, but our focus here is on the mobile sector.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Throughout our SRE journey, we‚Äôve learned that not all Google-defined site reliability engineering practices directly apply to the Telco space. Instead, we‚Äôve shifted our focus to service reliability, defining specific services offered by our mobile organization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance, 5G is infrastructure, but the service is mobile data‚Äîlike users browsing YouTube. We start by defining these services, identifying the underlying resources, and establishing &lt;a href=&#34;https://www.zenduty.com/blog/understanding-sla-slo-and-sli/&#34;&gt;SLAs and SLOs&lt;/a&gt; for each service. From there, we implement best practices in release engineering, observability, reliability, and security.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the 4G era, particularly with the Evolved Packet Core on virtual machines, we‚Äôve heavily invested in these principles. As we transition to the 5G core, we will apply the same principles, but in a cloud-native way, simplifying processes. This convergence of SRE and cloud-native transformations is key to our approach in the 5G domain.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: Another unique aspect of Swisscom‚Äôs approach is encouraging a cultural shift among our engineers. We emphasize that every decision in engineering or operations has a reliability impact. Encouraging individual responsibility and continuous improvement in operations is crucial. Moreover, having management that supports and encourages this mindset is crucial. This cultural shift has had the most significant impact on our organization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We‚Äôve started defining SLOs and maintaining error budgets for our services, but we apply them selectively, not at every resource level. When moving to Kubernetes operators, many SRE concepts, such as reconciliation, are automated by the Kubernetes layer. This automation puts us on the right track, and we‚Äôre excited to see the benefits it will bring to the organization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And that wraps up our conversation with Joel and Ashan! It‚Äôs always insightful to discuss observability, the demanding nature of SRE, and the innovative tools these reliability heroes are using to build products used by millions of people.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you‚Äôre fascinated by reliability and the intricate process of recovering from downtime, check out our &lt;a href=&#34;https://www.zenduty.com/podcast/&#34;&gt;podcast ‚Äì Incidentally Reliable&lt;/a&gt;, where veterans from Docker, Amazon, Walmart, and other industry-leading organizations, share their experiences, challenges, and success stories from the Cloud Native world.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Authors:&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/anjali-udasi/&#34;&gt;Anjali Udasi&lt;/a&gt; (Technical Writer), &lt;a href=&#34;https://www.linkedin.com/in/shubham67/&#34;&gt;Shubham Srivastava&lt;/a&gt; (Developer Relations Engineer)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Author‚Äôs headshot:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image aligncenter size-full is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1104&#34; height=&#34;1600&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image.jpeg&#34; alt=&#34;Anjali Udasi&#34; class=&#34;wp-image-113963&#34; style=&#34;width:220px;height:auto&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image.jpeg 1104w, https://www.cncf.io/wp-content/uploads/2024/07/image-207x300.jpeg 207w, https://www.cncf.io/wp-content/uploads/2024/07/image-707x1024.jpeg 707w, https://www.cncf.io/wp-content/uploads/2024/07/image-768x1113.jpeg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-900x1304.jpeg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-138x200.jpeg 138w, https://www.cncf.io/wp-content/uploads/2024/07/image-276x400.jpeg 276w&#34; sizes=&#34;(max-width: 1104px) 100vw, 1104px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Anjali Udasi &lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image aligncenter size-full is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;746&#34; height=&#34;1004&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19.jpg&#34; alt=&#34;Shubham Srivastava &#34; class=&#34;wp-image-113537&#34; style=&#34;width:218px;height:auto&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19.jpg 746w, https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19-223x300.jpg 223w, https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19-149x200.jpg 149w, https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19-297x400.jpg 297w&#34; sizes=&#34;(max-width: 746px) 100vw, 746px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Shubham Srivastava&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Member post by &lt;a href=&#34;https://www.linkedin.com/in/anjali-udasi/&#34;&gt;Anjali Udasi&lt;/a&gt;, Technical Writer, Zenduty and &lt;a href=&#34;https://www.linkedin.com/in/shubham67/&#34;&gt;Shubham Srivastava&lt;/a&gt;, Developer Relations Engineer, Zenduty&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/joelstudler/&#34;&gt;Joel Studler&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/ashan-senevirathne/&#34;&gt;Ashan Senevirathne&lt;/a&gt; took the stage at KubeCon + CloudNativeCon Europe in Paris with their presentation, ‚Äú&lt;a href=&#34;https://www.youtube.com/watch?v=crmTnB6Zwt8&#34;&gt;From GitOps to Kubernetes Resource Model&lt;/a&gt;,‚Äù highlighting Swisscom‚Äôs automation journey in the 5G Core and reflecting the company‚Äôs evolution from telco to TechCo.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Their talk was truly compelling, sparking our interest in learning more about their experiences and journey at &lt;a href=&#34;https://www.linkedin.com/company/swisscom/&#34;&gt;Swisscom&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/shubham67/&#34;&gt;Shubham Srivastava&lt;/a&gt;, who currently leads Developer Relations at &lt;a href=&#34;https://www.zenduty.com/&#34;&gt;Zenduty&lt;/a&gt;, had the pleasure of speaking with this dynamic duo: Joel, a DevOps Engineer and System Architect dedicated to building the next generation of mobile networking using cloud-native technologies, and Ashan, a Product Owner overseeing the design, implementation, and delivery of a cloud-native orchestration framework for the mobile organization.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1600&#34; height=&#34;988&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-2.jpg&#34; alt=&#34;Building the Future of 5G with Cloud-Native Tech presented by Ashan Senevirathne and Joel Studler from swisscom&#34; class=&#34;wp-image-113961&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-2.jpg 1600w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-300x185.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-1024x632.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-768x474.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-900x556.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-324x200.jpg 324w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-648x400.jpg 648w&#34; sizes=&#34;(max-width: 1600px) 100vw, 1600px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this blog featuring Joel and Ashan, we peel the layers of the telco world, the struggles of modernization, and the cutting-edge tools these minds put to use. This is a conversation you don‚Äôt want to miss!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: It‚Äôs great to chat with you both. We‚Äôd love to know what an average day for you both looks like. You‚Äôre leading DevOps and reliability at Telco with very tight error budgets and room for failure. So what does that look like behind the scenes?&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan: &lt;/strong&gt;For us, our main focus is on developing tooling capabilities for the upcoming 5G core technology, which we find applicable to other areas of the business as well. We put a lot of emphasis on community-driven initiatives. While our main focus is on Kubernetes environments, we also address the transition from legacy-based change management to cloud-native approaches, which requires a shift in organizational mindset.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: My role involves handling technical interfaces with the product, and collaborating closely with Ashan on architecture and engineering. Our daily tasks involve building reliable tools and automation, predominantly through &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; operators. We prioritize designing sustainable and efficient solutions while optimizing existing workloads. Testing and deployment typically occur on live or pre-production clusters.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: Ashan, you mentioned that half of the job is not migrating processes, it‚Äôs also migrating the mindsets of the people. So stemming from that, what do you think is the hardest part about maintaining and updating reliability and tooling in a telecom industry that‚Äôs typically viewed as being very archaic and having a lot of legacy processes?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan: &lt;/strong&gt;The biggest hurdle in the telecom industry is adapting to a more open and flexible approach to network management. Traditionally, telecom relies on vendor-provided ‚Äúblack box‚Äù software, making it difficult to maintain and update tools reliably.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But, now we‚Äôre tackling this by:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Greenfield approach:&lt;/strong&gt; Building new 5G core tools from scratch instead of relying on legacy systems.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;CNCF adoption:&lt;/strong&gt; Utilizing tools and concepts from the Cloud Native Computing Foundation (CNCF) for automation and containerization.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Shifting mindset:&lt;/strong&gt; Moving from a ‚Äúblack box‚Äù to a ‚Äúwhite box‚Äù mentality, where software is open and modifiable for better control and reliability.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And also with the fact that we strategically decided to go forward with Kubernetes operators and the Kubernetes concepts for automation, has a big impact on many other topics like change management.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance: How do you implement a change? How do you plan with Kubernetes resources? You don‚Äôt control when the change happens. The operator just rolls it out now and then. You don‚Äôt control it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It‚Äôs a dynamic system, sparking a range of questions that we‚Äôll be discussing a lot in the near future. This shift impacts not just technology but also requires a cultural change within the organization. The company is focusing on education and demonstrations to promote this new way of working.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; As an organization that‚Äôs so large and serves millions of people every day with mission-critical services, how do you handle the transition to Kubernetes? Specifically, how do you support Kubernetes and other open-source tools that enhance its capabilities? How do you vet open-source tools and new technologies from the CNCF ecosystem or elsewhere to ensure they‚Äôre stable and suitable for our organization?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan:&lt;/strong&gt; From a Kubernetes perspective, we use a vendor-provided distribution for our infrastructure. For deployment, we utilize &lt;a href=&#34;https://fluxcd.io/&#34;&gt;Flux&lt;/a&gt;, along with the external secret operator, cert-manager, and several other mature tools within the ecosystem.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For anything telco-specific, we often develop our solutions and have strong support internally to open-source these projects. This allows us to contribute to the community and encourage contributions from other operators, integrating telco-specific use cases into the Kubernetes ecosystem.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When selecting tools, we prioritize maturity and support from the community and other industry players over novelty. This ensures we choose reliable, well-supported tools rather than simply the latest trends.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: Joel, are there any tools that have caught your eye recently, tools that you‚Äôd love to play around with and are watching closely?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: For the development flow, I‚Äôm interested in &lt;a href=&#34;https://microcks.io/&#34;&gt;Microcks&lt;/a&gt;. It‚Äôs a mocking framework and its innovation lies in its usability both within your IDE and on the Kubernetes cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We‚Äôre also exploring testing tools like &lt;a href=&#34;https://litmuschaos.io/&#34;&gt;Litmus&lt;/a&gt; for &lt;a href=&#34;https://www.zenduty.com/blog/chaos-engineering/&#34;&gt;chaos engineering&lt;/a&gt; and &lt;a href=&#34;https://testkube.io/&#34;&gt;Testkube&lt;/a&gt;, a testing wrapper.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance, we‚Äôre adopting cert-manager, but in a Mobile Core on-prem environment with black box applications, it‚Äôs challenging. We‚Äôre pushing vendors to ensure compatibility with cert-manager, despite their tendency to fork and maintain their own versions.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan&lt;/strong&gt;: Additionally, we‚Äôre looking into a project called &lt;a href=&#34;https://nephio.org/&#34;&gt;Nephio&lt;/a&gt;, driven by the Linux Foundation. It‚Äôs designed for deploying and managing the 5G core in a cloud-native way. While we don‚Äôt use Nephio tooling directly, we adapt its framework and thinking. For instance, we‚Äôre contributing to and leveraging the &lt;a href=&#34;https://docs.sdcio.dev/&#34;&gt;SDC (Schema Driven Configuration)&lt;/a&gt; tool within the project.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: Are there any problems that you guys face currently and you‚Äôre waiting for a tool to come up and solve it? Like a hard problem that you guys would not want to build something to solve and you‚Äôre looking for someone else to come up and solve?&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan&lt;/strong&gt;: The problem that we‚Äôre seeing is, or the technical challenge that we have is, we have these telco applications and it‚Äôs treated as an appliance. And during the lifecycle space or the deployment and the configuration phase, we need to do the deployment in a cloud-native way.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And then what comes on top on the appliance level or the configuration, it‚Äôs done in a telco way. So there‚Äôs this proprietary interface defined by the telco standards and you need to do this configuration or apply or define these network services outside of the Kubernetes layer. And to achieve this, you need to do all these workarounds on top of that where we need to implement custom operators or find certain ways to bring this, what‚Äôs done outside of the Kubernetes layer, more into the in-bed Kubernetes layer.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If there‚Äôs an ask, the ask would be to have this configuration done in a Kubernetes native way, which means moving away from these &lt;a href=&#34;https://en.wikipedia.org/wiki/NETCONF&#34;&gt;NETCONF&lt;/a&gt;-based files, into a Kubernetes resource model. This shift would provide significant benefits, especially considering the time and effort we currently invest in making the configuration Kubernetes-native.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: Our biggest pain point right now is that we can‚Äôt handle our applications as true cloud-native citizens. The applications we receive from vendors are still treated like traditional hardware, with manual configurations akin to putting a server into a rack or setting up a bare metal appliance.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The mindset remains tied to the idea of a permanent system, like a &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt; release, where changes are made directly on the running system. This approach prevents us from implementing practices like blue-green deployments, and even a simple redeployment becomes a huge effort due to the manual steps involved.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We believe that introducing a cloud-native configuration interface would simplify lifecycle management, updates, and configurations.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;&lt;strong&gt;: &lt;/strong&gt;Observability must be crucial in your journey, especially as you ramp up. What does your observability framework look like? What metrics are you spending most of your time monitoring? We‚Äôd love to know more about how you handle &lt;a href=&#34;https://www.zenduty.com/blog/observability-vs-monitoring/&#34;&gt;monitoring and observability&lt;/a&gt; at Swisscom.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel: &lt;/strong&gt;Currently, we use a standard observability stack with &lt;a href=&#34;https://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt; in our clusters and &lt;a href=&#34;https://grafana.com/oss/loki/&#34;&gt;Loki&lt;/a&gt; for logging. For centralized deployment, we use &lt;a href=&#34;https://thanos.io/&#34;&gt;Thanos&lt;/a&gt;. Additionally, we consume an internal Observability-as-a-Service stack that any Swisscom team can use, built on the standard Prometheus-Grafana stack, which integrates well with our Incident Management tooling.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We focus on a minimal, relevant subset of metrics to ensure service health. The 5G core and applications are more complex due to their black-box nature, so we work closely with vendors to identify the right metrics.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan:&lt;/strong&gt; We use HTTP-based requests to monitor golden signals and key performance indicators (KPIs) such as user attachments, latency, and DNS metrics. For example in 5G core, we will have how many users are attached, what‚Äôs the latency or some metrics on the DNS.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While some metrics are standard, others are more telco-specific, requiring vendor collaboration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; You mentioned a while ago that you‚Äôre spending a lot of effort bolstering site reliability movements within your organization. SRE can be very demanding, as we‚Äôre all familiar with it. What‚Äôs the story at your org? How are you managing work-life balance, especially in a Telco environment where nothing can go wrong?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan&lt;/strong&gt;: I can speak on behalf of the mobile organization at Swisscom. We have a significant IT side as well, but our focus here is on the mobile sector.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Throughout our SRE journey, we‚Äôve learned that not all Google-defined site reliability engineering practices directly apply to the Telco space. Instead, we‚Äôve shifted our focus to service reliability, defining specific services offered by our mobile organization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance, 5G is infrastructure, but the service is mobile data‚Äîlike users browsing YouTube. We start by defining these services, identifying the underlying resources, and establishing &lt;a href=&#34;https://www.zenduty.com/blog/understanding-sla-slo-and-sli/&#34;&gt;SLAs and SLOs&lt;/a&gt; for each service. From there, we implement best practices in release engineering, observability, reliability, and security.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the 4G era, particularly with the Evolved Packet Core on virtual machines, we‚Äôve heavily invested in these principles. As we transition to the 5G core, we will apply the same principles, but in a cloud-native way, simplifying processes. This convergence of SRE and cloud-native transformations is key to our approach in the 5G domain.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: Another unique aspect of Swisscom‚Äôs approach is encouraging a cultural shift among our engineers. We emphasize that every decision in engineering or operations has a reliability impact. Encouraging individual responsibility and continuous improvement in operations is crucial. Moreover, having management that supports and encourages this mindset is crucial. This cultural shift has had the most significant impact on our organization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We‚Äôve started defining SLOs and maintaining error budgets for our services, but we apply them selectively, not at every resource level. When moving to Kubernetes operators, many SRE concepts, such as reconciliation, are automated by the Kubernetes layer. This automation puts us on the right track, and we‚Äôre excited to see the benefits it will bring to the organization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And that wraps up our conversation with Joel and Ashan! It‚Äôs always insightful to discuss observability, the demanding nature of SRE, and the innovative tools these reliability heroes are using to build products used by millions of people.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you‚Äôre fascinated by reliability and the intricate process of recovering from downtime, check out our &lt;a href=&#34;https://www.zenduty.com/podcast/&#34;&gt;podcast ‚Äì Incidentally Reliable&lt;/a&gt;, where veterans from Docker, Amazon, Walmart, and other industry-leading organizations, share their experiences, challenges, and success stories from the Cloud Native world.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Authors:&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/anjali-udasi/&#34;&gt;Anjali Udasi&lt;/a&gt; (Technical Writer), &lt;a href=&#34;https://www.linkedin.com/in/shubham67/&#34;&gt;Shubham Srivastava&lt;/a&gt; (Developer Relations Engineer)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Author‚Äôs headshot:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image aligncenter size-full is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1104&#34; height=&#34;1600&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image.jpeg&#34; alt=&#34;Anjali Udasi&#34; class=&#34;wp-image-113963&#34; style=&#34;width:220px;height:auto&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image.jpeg 1104w, https://www.cncf.io/wp-content/uploads/2024/07/image-207x300.jpeg 207w, https://www.cncf.io/wp-content/uploads/2024/07/image-707x1024.jpeg 707w, https://www.cncf.io/wp-content/uploads/2024/07/image-768x1113.jpeg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-900x1304.jpeg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-138x200.jpeg 138w, https://www.cncf.io/wp-content/uploads/2024/07/image-276x400.jpeg 276w&#34; sizes=&#34;(max-width: 1104px) 100vw, 1104px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Anjali Udasi &lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image aligncenter size-full is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;746&#34; height=&#34;1004&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19.jpg&#34; alt=&#34;Shubham Srivastava &#34; class=&#34;wp-image-113537&#34; style=&#34;width:218px;height:auto&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19.jpg 746w, https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19-223x300.jpg 223w, https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19-149x200.jpg 149w, https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19-297x400.jpg 297w&#34; sizes=&#34;(max-width: 746px) 100vw, 746px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Shubham Srivastava&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Mon, 01 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêEmbracing the future: our online store moves to a print-on-demand model„ÄëÊã•Êä±Êú™Êù•ÔºöÊàë‰ª¨ÁöÑÂú®Á∫øÂïÜÂ∫óËΩ¨ÂêëÊåâÈúÄÊâìÂç∞Ê®°Âºè</title>
      <link>https://www.cncf.io/blog/2024/07/08/embracing-the-future-our-online-store-moves-to-a-print-on-demand-model/</link>
      <description>„Äê&lt;p&gt;In today‚Äôs fast-paced digital world, businesses must evolve and adapt to meet their customers‚Äô changing needs. We are excited to announce that our &lt;a href=&#34;https://store.cncf.io/&#34;&gt;online store&lt;/a&gt; is transitioning to a Print On Demand (POD) model. This significant change brings numerous benefits for us and, more importantly, our vibrant community.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;What is Print On Demand?&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Print On Demand is a fulfillment method in which items are printed as soon as an order is placed rather than stored in inventory. This model allows for greater flexibility and customization, ensuring each product is made specifically for the person who orders it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;The Benefits of Moving to Print-On-Demand&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Sustainability&lt;/strong&gt;:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reduced Waste&lt;/strong&gt;: Traditional inventory systems often result in overproduction and excess stock, leading to waste. With POD, we only produce what is needed, minimizing our environmental footprint.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Eco-Friendly Materials&lt;/strong&gt;: Many POD services use sustainable materials and eco-friendly printing processes, reducing environmental impact.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reduced Overhead Costs&lt;/strong&gt;: By eliminating the need for warehousing and managing excess inventory, we can focus on improving other aspects of our service, such as customer support and product quality.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;How This Benefits Our Community&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Our shift to a Print-on-Demand model is a testament to our commitment to our community. By reducing waste and promoting sustainability, we can allow more flexibility in our offerings and ensure we always have inventory.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We are excited about this new chapter and look forward to providing you an even better shopping experience. Your support and feedback have been invaluable in making this transition possible. Together, we can positively impact both the environment and the creative community.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Thank you for being a part of our journey. Explore our new&lt;a href=&#34;https://store.cncf.io/collections/all-projects&#34;&gt; Print-On-Demand offerings today&lt;/a&gt; and discover the endless possibilities!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In the Future&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the coming months, we look forward to adding customization to our products and sourcing print-on-demand options with global partners to reduce shipping costs.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Stay tuned for more updates and exciting new products!&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;Âú®ÂΩì‰ªäÂø´ËäÇÂ•èÁöÑÊï∞Â≠ó‰∏ñÁïå‰∏≠Ôºå‰ºÅ‰∏öÂøÖÈ°ª‰∏çÊñ≠ÂèëÂ±ïÂíåÈÄÇÂ∫îÔºå‰ª•Êª°Ë∂≥ÂÆ¢Êà∑‰∏çÊñ≠ÂèòÂåñÁöÑÈúÄÊ±Ç„ÄÇÊàë‰ª¨ÂæàÈ´òÂÖ¥Âú∞ÂÆ£Â∏ÉÔºåÊàë‰ª¨ÁöÑ&lt;a href=&#34;https://store.cncf.io/&#34;&gt;Âú®Á∫øÂïÜÂ∫ó&lt;/a&gt;Ê≠£Âú®ËΩ¨Âèò‰∏∫ÊåâÈúÄÊâìÂç∞ (POD) Ê®°Âºè„ÄÇËøô‰∏ÄÈáçÂ§ßÂèòÂåñ‰∏∫Êàë‰ª¨Â∏¶Êù•‰∫Ü‰ºóÂ§öÂ•ΩÂ§ÑÔºåÊõ¥ÈáçË¶ÅÁöÑÊòØÔºå‰∏∫Êàë‰ª¨ÂÖÖÊª°Ê¥ªÂäõÁöÑÁ§æÂå∫Â∏¶Êù•‰∫Ü‰ºóÂ§öÂ•ΩÂ§Ñ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;‰ªÄ‰πàÊòØÊåâÈúÄÊâìÂç∞Ôºü&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊåâÈúÄÊâìÂç∞ÊòØ‰∏ÄÁßçÂ±•Ë°åÊñπÊ≥ïÔºåÂú®‰∏ãËÆ¢ÂçïÂêéÁ´ãÂç≥ÊâìÂç∞ÂïÜÂìÅÔºåËÄå‰∏çÊòØÂ∞ÜÂÖ∂Â≠òÂÇ®Âú®Â∫ìÂ≠ò‰∏≠„ÄÇËøôÁßçÊ®°ÂºèÊèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑÁÅµÊ¥ªÊÄßÂíåÂÆöÂà∂ÊÄßÔºåÁ°Æ‰øùÊØè‰ª∂‰∫ßÂìÅÈÉΩÊòØ‰∏ìÈó®‰∏∫ËÆ¢Ë¥≠ËÄÖÂà∂ÈÄ†ÁöÑ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;ËΩ¨ÂêëÊåâÈúÄÊâìÂç∞ÁöÑÂ•ΩÂ§Ñ&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;ÂèØÊåÅÁª≠ÊÄß&lt;/strong&gt;Ôºö&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;ÂáèÂ∞ëÊµ™Ë¥π&lt;/strong&gt;Ôºö‰º†ÁªüÂ∫ìÂ≠òÁ≥ªÁªüÈÄöÂ∏∏‰ºöÂØºËá¥Áîü‰∫ßËøáÂâ©ÂíåÂ∫ìÂ≠òËøáÂâ©Ôºå‰ªéËÄåÂØºËá¥Êµ™Ë¥π„ÄÇÈÄöËøá PODÔºåÊàë‰ª¨Âè™Áîü‰∫ßÊâÄÈúÄÁöÑ‰∫ßÂìÅÔºåÊúÄÂ§ßÈôêÂ∫¶Âú∞ÂáèÂ∞ëÂØπÁéØÂ¢ÉÁöÑÂΩ±Âìç„ÄÇ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;ÁéØ‰øùÊùêÊñô&lt;/strong&gt;ÔºöËÆ∏Â§ö POD ÊúçÂä°ÈÉΩ‰ΩøÁî®ÂèØÊåÅÁª≠ÊùêÊñôÂíåÁéØ‰øùÂç∞Âà∑Â∑•Ëâ∫ÔºåÂáèÂ∞ëÂØπÁéØÂ¢ÉÁöÑÂΩ±Âìç„ÄÇ&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;ÊïàÁéáÔºö&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Èôç‰ΩéÁÆ°ÁêÜÊàêÊú¨&lt;/strong&gt;ÔºöÈÄöËøáÊ∂àÈô§‰ªìÂÇ®ÂíåÁÆ°ÁêÜËøáÂâ©Â∫ìÂ≠òÁöÑÈúÄË¶ÅÔºåÊàë‰ª¨ÂèØ‰ª•‰∏ìÊ≥®‰∫éÊîπÂñÑÊúçÂä°ÁöÑÂÖ∂‰ªñÊñπÈù¢Ôºå‰æãÂ¶ÇÂÆ¢Êà∑ÊîØÊåÅÂíå‰∫ßÂìÅË¥®Èáè„ÄÇ&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;ËøôÂØπÊàë‰ª¨ÁöÑÁ§æÂå∫Êúâ‰ΩïÂ•ΩÂ§Ñ&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Êàë‰ª¨ÂêëÊåâÈúÄÊâìÂç∞Ê®°ÂºèÁöÑËΩ¨ÂèòËØÅÊòé‰∫ÜÊàë‰ª¨ÂØπÁ§æÂå∫ÁöÑÊâøËØ∫„ÄÇÈÄöËøáÂáèÂ∞ëÊµ™Ë¥πÂíå‰øÉËøõÂèØÊåÅÁª≠ÂèëÂ±ïÔºåÊàë‰ª¨ÂèØ‰ª•Âú®Êàë‰ª¨ÁöÑ‰∫ßÂìÅ‰∏≠Êèê‰æõÊõ¥Â§ßÁöÑÁÅµÊ¥ªÊÄßÔºåÂπ∂Á°Æ‰øùÊàë‰ª¨ÂßãÁªàÊúâÂ∫ìÂ≠ò„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Êàë‰ª¨ÂØπËøô‰∏ÄÊñ∞ÁØáÁ´†ÊÑüÂà∞ÂÖ¥Â•ãÔºåÂπ∂ÊúüÂæÖ‰∏∫ÊÇ®Êèê‰æõÊõ¥Â•ΩÁöÑË¥≠Áâ©‰ΩìÈ™å„ÄÇÊÇ®ÁöÑÊîØÊåÅÂíåÂèçÈ¶àÂØπ‰∫éÂÆûÁé∞Ëøô‰∏ÄËΩ¨ÂèòËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÂèØ‰ª•ÈΩêÂøÉÂçèÂäõÔºåÂØπÁéØÂ¢ÉÂíåÂàõÊÑèÁ§æÂå∫‰∫ßÁîüÁßØÊûÅÂΩ±Âìç„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊÑüË∞¢ÊÇ®ÂèÇ‰∏éÊàë‰ª¨ÁöÑÊóÖÁ®ã„ÄÇÁ´ãÂç≥Êé¢Á¥¢Êàë‰ª¨Êñ∞ÁöÑ&lt;a href=&#34;https://store.cncf.io/collections/all-projects&#34;&gt;ÊåâÈúÄÊâìÂç∞‰∫ßÂìÅ&lt;/a&gt;Âπ∂ÂèëÁé∞Êó†ÈôêÁöÑÂèØËÉΩÊÄßÔºÅ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Êú™Êù•&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Âú®Êé•‰∏ãÊù•ÁöÑÂá†‰∏™Êúà‰∏≠ÔºåÊàë‰ª¨ÊúüÂæÖ‰∏∫Êàë‰ª¨ÁöÑ‰∫ßÂìÅÂ¢ûÂä†ÂÆöÂà∂ÂäüËÉΩÔºåÂπ∂‰∏éÂÖ®ÁêÉÂêà‰Ωú‰ºô‰º¥‰∏ÄËµ∑ÈááË¥≠ÊåâÈúÄÊâìÂç∞ÈÄâÈ°πÔºå‰ª•Èôç‰ΩéËøêËæìÊàêÊú¨„ÄÇ¬†&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Êï¨ËØ∑ÂÖ≥Ê≥®Êõ¥Â§öÊõ¥Êñ∞Âíå‰ª§‰∫∫ÂÖ¥Â•ãÁöÑÊñ∞‰∫ßÂìÅÔºÅ&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Sun, 07 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêSQL simplifies TSDB ‚Äì how to migrate from InfluxQL to SQL„ÄëSQL ÁÆÄÂåñ TSDB ‚Äì Â¶Ç‰Ωï‰ªé InfluxQL ËøÅÁßªÂà∞ SQL</title>
      <link>https://www.cncf.io/blog/2024/07/10/sql-simplifies-tsdb-how-to-migrate-from-influxql-to-sql/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql&#34;&gt;Greptime‚Äôs blog&lt;/a&gt; by tison&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. SQL is a more common and general language for querying time series data, making migrating from InfluxQL to SQL a growing trend.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/coverimage1.png&#34; alt=&#34;Image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB uses SQL as its primary query language. Once users ingest data into GreptimeDB via the InfluxDB line protocol or other APIs, a common question arises: how can I analyze the data ingested? Specifically, how can existing InfluxQL queries be migrated to SQL queries?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To address the question above, this article outlines the differences between the query languages of InfluxDB (InfluxQL or Flux) and SQL, as well as a cheat sheet for migrating from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;overview-of-query-languages&#34;&gt;Overview of Query Languages&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#overview-of-query-languages&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/&#34;&gt;InfluxQL&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V1. It‚Äôs a SQL-like query language but not a SQL dialect. Below are some examples of InfluxQL queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT * FROM h2o_feet;&#xA;SELECT * FROM h2o_feet LIMIT 5;&#xA;SELECT COUNT(&#34;water_level&#34;) FROM h2o_feet;&#xA;SELECT &#34;level description&#34;, &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;SELECT *::field FROM &#34;h2o_feet&#34;;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When InfluxDB was designed and developed, there weren‚Äôt as many database developers as today. Consequently, despite InfluxQL‚Äôs efforts to closely resemble SQL syntax, implementing basic SQL capabilities supported by relational algebra and adding time series query extensions was quite challenging.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL instead implemented functions and syntax specifically designed for time series data analysis. For instance, all InfluxQL queries default to returning the timestamp column in ascending order, and all queries must include field columns to return results.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, special query syntax is designed for querying over time series rather than rows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Essentially, InfluxQL was developed from the raw need for time series data analysis focused on numerical metrics. As InfluxDB evolved, InfluxQL also supported continuous queries and retention policies to solve some requirements of real-time data processing.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL can still be used in InfluxDB V2, it faces&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v2/query-data/influxql/&#34;&gt;a series of challenges due to model mismatches&lt;/a&gt;, as InfluxDB V2 mainly promotes the Flux query language.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/&#34;&gt;Flux&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V2. Unlike InfluxQL, which has a SQL-like syntax, Flux uses a DataFrame style syntax. Developers who have written programs in Elixir will find the syntax familiar. Here are some examples of Flux queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;erlang&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;from(bucket: &#34;example-bucket&#34;)&#xA;    |&amp;gt; range(start: -1d)&#xA;    |&amp;gt; filter(fn: (r) =&amp;gt; r._measurement == &#34;example-measurement&#34;)&#xA;    |&amp;gt; mean()&#xA;    |&amp;gt; yield(name: &#34;_result&#34;)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Designed to support joint analysis of time series data across various data sources, Flux allows users to fetch data from time series databases (InfluxDB), relational databases (PostgreSQL or MySQL), and CSV files for analysis. For example,&amp;nbsp;&lt;code&gt;sql.from&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;csv.from&lt;/code&gt;&amp;nbsp;can replace&amp;nbsp;&lt;code&gt;from(bucket)&lt;/code&gt;&amp;nbsp;in the example above, allowing fetching data from other sources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Flux can only be used in InfluxDB V2; it is not implemented in V1 and has been&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/future-of-flux/&#34;&gt;abandoned in V3&lt;/a&gt;. The reason is apparent: the learning curve is too steep. Without professional language developers, expanding syntax while fixing various design and implementation issues is almost impossible, resulting in unsustainable engineering costs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, the Structured Query Language, is familiar to data analysts and is based on relational algebra.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Unlike DSLs tailored for specific business scenarios, SQL has a solid theoretical foundation. Since E. F. Codd published the seminal paper ‚Äú&lt;a href=&#34;https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf&#34;&gt;A Relational Model of Data for Large Shared Data Banks&lt;/a&gt;,‚Äù research on relational databases has flourished for over fifty years.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Despite unique extensions in various SQL databases that sometimes confuse users, the basic query and analysis capabilities are consistently implemented across all SQL databases, supported by relational algebra. One or two decades ago, there might have been debates about SQL‚Äôs relevance. However, SQL has undoubtedly reasserted itself as the default choice for data analysis today. Over the years, SQL has been continuously improved and expanded, and it is widely adopted globally through a series of proven implementations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL is the primary query language for InfluxDB V3 and GreptimeDB. Both now recommend users analyze time series data using SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In GreptimeDB,&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql&#34;&gt;you can use standard SQL to query your data&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT ts, idc, AVG(memory_util)&#xA;FROM system_metrics&#xA;GROUP BY idc&#xA;ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The solid theoretical foundation of SQL helps emerging time series databases reliably implement complex query logic and data management tasks. Also, the broader SQL ecosystem enables emerging time series databases to quickly integrate into the data analysis tech stack. For example, in the previous&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-03-19-keyboard-monitoring&#34;&gt;input behavior analysis demos&lt;/a&gt;, we showcase an integration between GreptimeDB and Streamlit for visualizing time series by leveraging GreptimeDB‚Äôs MySQL protocol support.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;challenges-in-time-series-analysis&#34;&gt;Challenges in Time Series Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#challenges-in-time-series-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql-1&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While SQL has a solid theoretical foundation and a broader analytical ecosystem, traditional SQL databases suffer when handling time series data, primarily due to their large size.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The value provided from a single data point of a time series is often very low. Most metrics uploaded by devices aren‚Äôt explicitly handled, and the healthy status reported doesn‚Äôt require special attention. Thus, the cost-efficiency of storing time series data is crucial. How to leverage modern cloud commodity storage to reduce costs and use cutting-edge compression for time series data are key points for time series databases.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Furthermore, extracting essential information efficiently from vast amounts of time series data often requires specific query extensions for optimization. GreptimeDB‚Äôs support for&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY&lt;/a&gt;&amp;nbsp;to help users analyze data aggregation within specific time windows is one such example.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux-1&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The learning curve itself essentially doomed this dialect. As mentioned above, being a DSL solely supported by a single provider, Flux faced significant challenges in language robustness, performance optimization, and ecosystem development. The sole provider has since abandoned further development of Flux, making it a language of the past.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql-1&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL syntax resembles SQL, subtle differences can be frustrating. Despite efforts to mimic SQL syntax, InfluxQL fundamentally remains a DSL tailored to time series analysis needs focusing on metrics. Its challenges in development and maintenance costs are similar to those faced by Flux.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For example, InfluxQL does not support&amp;nbsp;&lt;code&gt;JOIN&lt;/code&gt;&amp;nbsp;queries. Although one can write queries like&amp;nbsp;&lt;code&gt;SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&lt;/code&gt;, it simply reads data from both measurements separately:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&amp;gt; SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&#xA;&#xA;name: h2o_feet&#xA;--------------&#xA;time                   level description      location       pH   water_level&#xA;2015-08-18T00:00:00Z   below 3 feet           santa_monica        2.064&#xA;2015-08-18T00:00:00Z   between 6 and 9 feet   coyote_creek        8.12&#xA;[...]&#xA;2015-09-18T21:36:00Z   between 3 and 6 feet   santa_monica        5.066&#xA;2015-09-18T21:42:00Z   between 3 and 6 feet   santa_monica        4.938&#xA;&#xA;name: h2o_pH&#xA;------------&#xA;time                   level description   location       pH   water_level&#xA;2015-08-18T00:00:00Z                       santa_monica   6&#xA;2015-08-18T00:00:00Z                       coyote_creek   7&#xA;[...]&#xA;2015-09-18T21:36:00Z                       santa_monica   8&#xA;2015-09-18T21:42:00Z                       santa_monica   7&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Moreover, despite InfluxDB V3 supporting InfluxQL due to strong user demand to facilitate migration, InfluxDB V3 primarily promotes SQL-based queries. Thus, it‚Äôs fair to say that InfluxQL is also fading away.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;migrating-to-sql-analysis&#34;&gt;Migrating to SQL Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#migrating-to-sql-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Today, many existing time series data analysis logics are written in InfluxQL. This section outlines the core differences between InfluxQL and SQL and illustrates how to migrate from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;timestamp-column&#34;&gt;Timestamp Column&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#timestamp-column&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;A key difference in application logic migration is that&amp;nbsp;&lt;strong&gt;SQL does not treat the time column especially, while InfluxQL returns the time column by default and sorts results in ascending order by timestamp&lt;/strong&gt;. SQL queries need to explicitly specify the time column to include timestamps in the result set and manually specify sorting logic.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;SELECT &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;SELECT ts, location, water_level FROM h2o_feet ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When writing data, InfluxQL automatically populates the time column with the current time, whereas SQL requires manual specification of the time column value. If using the current time, it must be explicitly written:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;INSERT INTO &#34;measurement&#34; (tag, value) VALUES (&#39;my_tag&#39;, 42);&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support inserting multiple rows in one INSERT statement, whereas SQL databases typically support this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag_0&#39;, 42), (NOW(), &#39;my_tag_1&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, InfluxQL uses the&amp;nbsp;&lt;code&gt;tz()&lt;/code&gt;&amp;nbsp;function to specify the query timezone, while SQL typically has other ways to set the timezone. GreptimeDB supports&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/mysql#time-zone&#34;&gt;MySQL&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/postgresql#time-zone&#34;&gt;PostgreSQL&lt;/a&gt;&amp;nbsp;syntax for setting the timezone.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;time-series&#34;&gt;Time Series&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#time-series&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL implements time series granularity query syntax, such as SLIMIT and&amp;nbsp;&lt;code&gt;SOFFSET&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;SLIMIT&lt;/code&gt;&amp;nbsp;limits the number of data points returned for each time series in the result set. For example,&amp;nbsp;&lt;code&gt;SLIMIT 1&lt;/code&gt;&amp;nbsp;means, at most, one result per time series that meets the filter condition.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, not specifically designed for time series data analysis, requires some workarounds:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT DISTINCT ON (host) * FROM monitor ORDER BY host, ts DESC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This query returns one result per time series, distinguished by the host tag:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;+-----------+---------------------+------+--------+&#xA;| host      | ts                  | cpu  | memory |&#xA;+-----------+---------------------+------+--------+&#xA;| 127.0.0.1 | 2022-11-03 03:39:58 |  0.5 |    0.2 |&#xA;| 127.0.0.2 | 2022-11-03 03:39:58 |  0.2 |    0.3 |&#xA;+-----------+---------------------+------+--------+&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;interval-literals&#34;&gt;Interval Literals&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#interval-literals&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL‚Äôs interval syntax resembles&amp;nbsp;&lt;code&gt;1d&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;12m&lt;/code&gt;, while SQL has standard syntax for time intervals:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INTERVAL &#39;1 DAY&#39;&#xA;INTERVAL &#39;1 YEAR 3 HOURS 20 MINUTES&#39;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;data-columns-and-tag-columns&#34;&gt;Data Columns and Tag Columns&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#data-columns-and-tag-columns&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL distinguishes between data columns and tag columns at the model level; queries that only SELECT tag columns will not return data. InfluxQL also supports the&amp;nbsp;&lt;code&gt;::field&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;::tag&lt;/code&gt;&amp;nbsp;suffixes to specify data columns or tag columns, allowing for columns with the same name.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL standards do not differentiate between data columns and tag columns, treating them all as regular columns. However, specific implementations may map these concepts differently. For example, GreptimeDB‚Äôs&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/concepts/data-model&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;data model&lt;/a&gt;&amp;nbsp;distinguishes between timestamp columns, tag columns, and data columns and has corresponding mapping rules.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image1.png&#34; alt=&#34;The Data Structure of GreptimeDB&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;The Data Structure of GreptimeDB&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;function-names&#34;&gt;Function Names&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#function-names&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Some function names differ between InfluxQL and SQL. For instance, the&amp;nbsp;&lt;code&gt;MEAN&lt;/code&gt;&amp;nbsp;function in InfluxQL corresponds to the&amp;nbsp;&lt;code&gt;AVG&lt;/code&gt;&amp;nbsp;function in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, many other functions, such as&amp;nbsp;&lt;code&gt;COUNT&lt;/code&gt;,&amp;nbsp;&lt;code&gt;SUM&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;MIN,&lt;/code&gt;&amp;nbsp;remain the same in both languages.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;identifiers&#34;&gt;Identifiers&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#identifiers&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In InfluxQL, identifiers are always double-quoted, while SQL supports unquoted identifiers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It is worth noting that SQL identifiers are case-insensitive by default. If case sensitivity is needed, the identifiers should be enclosed in the appropriate quotes. In GreptimeDB, double quotes are used by default. However, when connecting via MySQL or PostgreSQL clients, the corresponding dialect‚Äôs syntax is respected.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Examples of identifier usage differences between InfluxQL and SQL are as follows:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image2.png&#34; alt=&#34;The Usage Differences between InfluxQL and SQL&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;join&#34;&gt;JOIN&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#join&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support JOIN queries, while one of the fundamental capabilities of SQL databases is support for JOIN queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches&lt;/em&gt;&#xA;SELECT a.* FROM system_metrics a JOIN idc_info b ON a.idc = b.idc_id;&#xA;&#xA;&lt;em&gt;-- Select all rows from the idc_info table and system_metrics table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT a.* FROM idc_info a LEFT JOIN system_metrics b ON a.idc_id = b.idc;&#xA;&#xA;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT b.* FROM system_metrics a RIGHT JOIN idc_info b ON a.idc = b.idc_id;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These are examples of&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/reference/sql/join&#34;&gt;JOIN queries in GreptimeDB&lt;/a&gt;, which supports:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;INNER JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;LEFT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;RIGHT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;FULL OUTER JOIN&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;queries-over-time-windows&#34;&gt;Queries over Time Windows&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#queries-over-time-windows&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL‚Äôs&amp;nbsp;&lt;code&gt;GROUP BY&lt;/code&gt;&amp;nbsp;statement supports passing a time window to aggregate data within a specific length of time windows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL does not have such specific query capabilities; the closest equivalent is the&amp;nbsp;&lt;code&gt;OVER ... PARTITION BY&lt;/code&gt;&amp;nbsp;syntax, which can be quite complex to understand.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB implements its own&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY extension syntax&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT &#xA;    ts, &#xA;    host, &#xA;    avg(cpu) RANGE &#39;10s&#39; FILL LINEAR&#xA;FROM monitor&#xA;ALIGN &#39;5s&#39; TO &#39;2023-12-01T00:00:00&#39; BY (host) ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;continuous-aggregation&#34;&gt;Continuous Aggregation&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#continuous-aggregation&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL supports&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/continuous_queries/&#34;&gt;continuous aggregation&lt;/a&gt;, which corresponds to the standard concept of materialized views in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, the implementation of materialized views in most SQL databases is still fragile and remains an area for further exploration.&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-06-04-flow-engine&#34;&gt;GreptimeDB supports continuous aggregation&lt;/a&gt;&amp;nbsp;to meet these needs based on its flow engine.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#conclusion&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. While InfluxQL and Flux are used by InfluxDB and specifically created for handling time series data, SQL is a widely used query language in relational databases. Its robust theoretical foundation and rich ecosystem allow data analysts to quickly get started and use effective tools for time series data analysis.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB natively supports SQL queries. Visit our&amp;nbsp;&lt;a href=&#34;https://greptime.com/&#34;&gt;homepage&lt;/a&gt;&amp;nbsp;for more information or&amp;nbsp;&lt;a href=&#34;https://console.greptime.cloud/signup&#34;&gt;create a free cloud service&lt;/a&gt;&amp;nbsp;instance to start your trial today.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;about-greptime&#34;&gt;About Greptime&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#about-greptime&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We help industries that generate large amounts of time-series data, such as Connected Vehicles (CV), IoT, and Observability, to efficiently uncover the hidden value of data in real-time.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Visit the&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;latest version&lt;/a&gt;&amp;nbsp;from any device to get started and get the most out of your data.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GreptimeDB&lt;/a&gt;, written in Rust, is a distributed, open-source, time-series database designed for scalability, efficiency, and powerful analytics.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://greptime.com/product/carcloud&#34;&gt;Edge-Cloud Integrated TSDB&lt;/a&gt;&amp;nbsp;is designed for the unique demands of edge storage and compute in IoT. It tackles the exponential growth of edge data by integrating a multimodal edge-side database with cloud-based GreptimeDB Enterprise. This combination reduces traffic, computing, and storage costs while enhancing data timeliness and business insights.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.greptime.com/product/cloud&#34;&gt;GreptimeCloud&lt;/a&gt;&amp;nbsp;is a fully-managed cloud database-as-a-service (DBaaS) solution built on GreptimeDB. It efficiently supports applications in fields such as observability, IoT, and finance.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Star us on&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GitHub&lt;/a&gt;&amp;nbsp;or join GreptimeDB Community on&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/slack&#34;&gt;Slack&lt;/a&gt;&amp;nbsp;to get connected. Also, you can go to our&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb/contribute&#34;&gt;contribution page&lt;/a&gt;&amp;nbsp;to find some interesting issues to start with.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql&#34;&gt;Greptime‚Äôs blog&lt;/a&gt; by tison&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. SQL is a more common and general language for querying time series data, making migrating from InfluxQL to SQL a growing trend.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/coverimage1.png&#34; alt=&#34;Image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB uses SQL as its primary query language. Once users ingest data into GreptimeDB via the InfluxDB line protocol or other APIs, a common question arises: how can I analyze the data ingested? Specifically, how can existing InfluxQL queries be migrated to SQL queries?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To address the question above, this article outlines the differences between the query languages of InfluxDB (InfluxQL or Flux) and SQL, as well as a cheat sheet for migrating from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;overview-of-query-languages&#34;&gt;Overview of Query Languages&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#overview-of-query-languages&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/&#34;&gt;InfluxQL&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V1. It‚Äôs a SQL-like query language but not a SQL dialect. Below are some examples of InfluxQL queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT * FROM h2o_feet;&#xA;SELECT * FROM h2o_feet LIMIT 5;&#xA;SELECT COUNT(&#34;water_level&#34;) FROM h2o_feet;&#xA;SELECT &#34;level description&#34;, &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;SELECT *::field FROM &#34;h2o_feet&#34;;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When InfluxDB was designed and developed, there weren‚Äôt as many database developers as today. Consequently, despite InfluxQL‚Äôs efforts to closely resemble SQL syntax, implementing basic SQL capabilities supported by relational algebra and adding time series query extensions was quite challenging.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL instead implemented functions and syntax specifically designed for time series data analysis. For instance, all InfluxQL queries default to returning the timestamp column in ascending order, and all queries must include field columns to return results.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, special query syntax is designed for querying over time series rather than rows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Essentially, InfluxQL was developed from the raw need for time series data analysis focused on numerical metrics. As InfluxDB evolved, InfluxQL also supported continuous queries and retention policies to solve some requirements of real-time data processing.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL can still be used in InfluxDB V2, it faces&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v2/query-data/influxql/&#34;&gt;a series of challenges due to model mismatches&lt;/a&gt;, as InfluxDB V2 mainly promotes the Flux query language.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/&#34;&gt;Flux&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V2. Unlike InfluxQL, which has a SQL-like syntax, Flux uses a DataFrame style syntax. Developers who have written programs in Elixir will find the syntax familiar. Here are some examples of Flux queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;erlang&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;from(bucket: &#34;example-bucket&#34;)&#xA;    |&amp;gt; range(start: -1d)&#xA;    |&amp;gt; filter(fn: (r) =&amp;gt; r._measurement == &#34;example-measurement&#34;)&#xA;    |&amp;gt; mean()&#xA;    |&amp;gt; yield(name: &#34;_result&#34;)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Designed to support joint analysis of time series data across various data sources, Flux allows users to fetch data from time series databases (InfluxDB), relational databases (PostgreSQL or MySQL), and CSV files for analysis. For example,&amp;nbsp;&lt;code&gt;sql.from&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;csv.from&lt;/code&gt;&amp;nbsp;can replace&amp;nbsp;&lt;code&gt;from(bucket)&lt;/code&gt;&amp;nbsp;in the example above, allowing fetching data from other sources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Flux can only be used in InfluxDB V2; it is not implemented in V1 and has been&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/future-of-flux/&#34;&gt;abandoned in V3&lt;/a&gt;. The reason is apparent: the learning curve is too steep. Without professional language developers, expanding syntax while fixing various design and implementation issues is almost impossible, resulting in unsustainable engineering costs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, the Structured Query Language, is familiar to data analysts and is based on relational algebra.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Unlike DSLs tailored for specific business scenarios, SQL has a solid theoretical foundation. Since E. F. Codd published the seminal paper ‚Äú&lt;a href=&#34;https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf&#34;&gt;A Relational Model of Data for Large Shared Data Banks&lt;/a&gt;,‚Äù research on relational databases has flourished for over fifty years.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Despite unique extensions in various SQL databases that sometimes confuse users, the basic query and analysis capabilities are consistently implemented across all SQL databases, supported by relational algebra. One or two decades ago, there might have been debates about SQL‚Äôs relevance. However, SQL has undoubtedly reasserted itself as the default choice for data analysis today. Over the years, SQL has been continuously improved and expanded, and it is widely adopted globally through a series of proven implementations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL is the primary query language for InfluxDB V3 and GreptimeDB. Both now recommend users analyze time series data using SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In GreptimeDB,&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql&#34;&gt;you can use standard SQL to query your data&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT ts, idc, AVG(memory_util)&#xA;FROM system_metrics&#xA;GROUP BY idc&#xA;ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The solid theoretical foundation of SQL helps emerging time series databases reliably implement complex query logic and data management tasks. Also, the broader SQL ecosystem enables emerging time series databases to quickly integrate into the data analysis tech stack. For example, in the previous&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-03-19-keyboard-monitoring&#34;&gt;input behavior analysis demos&lt;/a&gt;, we showcase an integration between GreptimeDB and Streamlit for visualizing time series by leveraging GreptimeDB‚Äôs MySQL protocol support.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;challenges-in-time-series-analysis&#34;&gt;Challenges in Time Series Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#challenges-in-time-series-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql-1&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While SQL has a solid theoretical foundation and a broader analytical ecosystem, traditional SQL databases suffer when handling time series data, primarily due to their large size.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The value provided from a single data point of a time series is often very low. Most metrics uploaded by devices aren‚Äôt explicitly handled, and the healthy status reported doesn‚Äôt require special attention. Thus, the cost-efficiency of storing time series data is crucial. How to leverage modern cloud commodity storage to reduce costs and use cutting-edge compression for time series data are key points for time series databases.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Furthermore, extracting essential information efficiently from vast amounts of time series data often requires specific query extensions for optimization. GreptimeDB‚Äôs support for&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY&lt;/a&gt;&amp;nbsp;to help users analyze data aggregation within specific time windows is one such example.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux-1&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The learning curve itself essentially doomed this dialect. As mentioned above, being a DSL solely supported by a single provider, Flux faced significant challenges in language robustness, performance optimization, and ecosystem development. The sole provider has since abandoned further development of Flux, making it a language of the past.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql-1&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL syntax resembles SQL, subtle differences can be frustrating. Despite efforts to mimic SQL syntax, InfluxQL fundamentally remains a DSL tailored to time series analysis needs focusing on metrics. Its challenges in development and maintenance costs are similar to those faced by Flux.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For example, InfluxQL does not support&amp;nbsp;&lt;code&gt;JOIN&lt;/code&gt;&amp;nbsp;queries. Although one can write queries like&amp;nbsp;&lt;code&gt;SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&lt;/code&gt;, it simply reads data from both measurements separately:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&amp;gt; SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&#xA;&#xA;name: h2o_feet&#xA;--------------&#xA;time                   level description      location       pH   water_level&#xA;2015-08-18T00:00:00Z   below 3 feet           santa_monica        2.064&#xA;2015-08-18T00:00:00Z   between 6 and 9 feet   coyote_creek        8.12&#xA;[...]&#xA;2015-09-18T21:36:00Z   between 3 and 6 feet   santa_monica        5.066&#xA;2015-09-18T21:42:00Z   between 3 and 6 feet   santa_monica        4.938&#xA;&#xA;name: h2o_pH&#xA;------------&#xA;time                   level description   location       pH   water_level&#xA;2015-08-18T00:00:00Z                       santa_monica   6&#xA;2015-08-18T00:00:00Z                       coyote_creek   7&#xA;[...]&#xA;2015-09-18T21:36:00Z                       santa_monica   8&#xA;2015-09-18T21:42:00Z                       santa_monica   7&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Moreover, despite InfluxDB V3 supporting InfluxQL due to strong user demand to facilitate migration, InfluxDB V3 primarily promotes SQL-based queries. Thus, it‚Äôs fair to say that InfluxQL is also fading away.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;migrating-to-sql-analysis&#34;&gt;Migrating to SQL Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#migrating-to-sql-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Today, many existing time series data analysis logics are written in InfluxQL. This section outlines the core differences between InfluxQL and SQL and illustrates how to migrate from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;timestamp-column&#34;&gt;Timestamp Column&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#timestamp-column&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;A key difference in application logic migration is that&amp;nbsp;&lt;strong&gt;SQL does not treat the time column especially, while InfluxQL returns the time column by default and sorts results in ascending order by timestamp&lt;/strong&gt;. SQL queries need to explicitly specify the time column to include timestamps in the result set and manually specify sorting logic.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;SELECT &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;SELECT ts, location, water_level FROM h2o_feet ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When writing data, InfluxQL automatically populates the time column with the current time, whereas SQL requires manual specification of the time column value. If using the current time, it must be explicitly written:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;INSERT INTO &#34;measurement&#34; (tag, value) VALUES (&#39;my_tag&#39;, 42);&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support inserting multiple rows in one INSERT statement, whereas SQL databases typically support this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag_0&#39;, 42), (NOW(), &#39;my_tag_1&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, InfluxQL uses the&amp;nbsp;&lt;code&gt;tz()&lt;/code&gt;&amp;nbsp;function to specify the query timezone, while SQL typically has other ways to set the timezone. GreptimeDB supports&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/mysql#time-zone&#34;&gt;MySQL&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/postgresql#time-zone&#34;&gt;PostgreSQL&lt;/a&gt;&amp;nbsp;syntax for setting the timezone.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;time-series&#34;&gt;Time Series&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#time-series&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL implements time series granularity query syntax, such as SLIMIT and&amp;nbsp;&lt;code&gt;SOFFSET&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;SLIMIT&lt;/code&gt;&amp;nbsp;limits the number of data points returned for each time series in the result set. For example,&amp;nbsp;&lt;code&gt;SLIMIT 1&lt;/code&gt;&amp;nbsp;means, at most, one result per time series that meets the filter condition.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, not specifically designed for time series data analysis, requires some workarounds:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT DISTINCT ON (host) * FROM monitor ORDER BY host, ts DESC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This query returns one result per time series, distinguished by the host tag:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;+-----------+---------------------+------+--------+&#xA;| host      | ts                  | cpu  | memory |&#xA;+-----------+---------------------+------+--------+&#xA;| 127.0.0.1 | 2022-11-03 03:39:58 |  0.5 |    0.2 |&#xA;| 127.0.0.2 | 2022-11-03 03:39:58 |  0.2 |    0.3 |&#xA;+-----------+---------------------+------+--------+&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;interval-literals&#34;&gt;Interval Literals&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#interval-literals&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL‚Äôs interval syntax resembles&amp;nbsp;&lt;code&gt;1d&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;12m&lt;/code&gt;, while SQL has standard syntax for time intervals:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INTERVAL &#39;1 DAY&#39;&#xA;INTERVAL &#39;1 YEAR 3 HOURS 20 MINUTES&#39;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;data-columns-and-tag-columns&#34;&gt;Data Columns and Tag Columns&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#data-columns-and-tag-columns&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL distinguishes between data columns and tag columns at the model level; queries that only SELECT tag columns will not return data. InfluxQL also supports the&amp;nbsp;&lt;code&gt;::field&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;::tag&lt;/code&gt;&amp;nbsp;suffixes to specify data columns or tag columns, allowing for columns with the same name.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL standards do not differentiate between data columns and tag columns, treating them all as regular columns. However, specific implementations may map these concepts differently. For example, GreptimeDB‚Äôs&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/concepts/data-model&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;data model&lt;/a&gt;&amp;nbsp;distinguishes between timestamp columns, tag columns, and data columns and has corresponding mapping rules.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image1.png&#34; alt=&#34;The Data Structure of GreptimeDB&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;The Data Structure of GreptimeDB&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;function-names&#34;&gt;Function Names&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#function-names&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Some function names differ between InfluxQL and SQL. For instance, the&amp;nbsp;&lt;code&gt;MEAN&lt;/code&gt;&amp;nbsp;function in InfluxQL corresponds to the&amp;nbsp;&lt;code&gt;AVG&lt;/code&gt;&amp;nbsp;function in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, many other functions, such as&amp;nbsp;&lt;code&gt;COUNT&lt;/code&gt;,&amp;nbsp;&lt;code&gt;SUM&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;MIN,&lt;/code&gt;&amp;nbsp;remain the same in both languages.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;identifiers&#34;&gt;Identifiers&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#identifiers&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In InfluxQL, identifiers are always double-quoted, while SQL supports unquoted identifiers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It is worth noting that SQL identifiers are case-insensitive by default. If case sensitivity is needed, the identifiers should be enclosed in the appropriate quotes. In GreptimeDB, double quotes are used by default. However, when connecting via MySQL or PostgreSQL clients, the corresponding dialect‚Äôs syntax is respected.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Examples of identifier usage differences between InfluxQL and SQL are as follows:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image2.png&#34; alt=&#34;The Usage Differences between InfluxQL and SQL&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;join&#34;&gt;JOIN&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#join&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support JOIN queries, while one of the fundamental capabilities of SQL databases is support for JOIN queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches&lt;/em&gt;&#xA;SELECT a.* FROM system_metrics a JOIN idc_info b ON a.idc = b.idc_id;&#xA;&#xA;&lt;em&gt;-- Select all rows from the idc_info table and system_metrics table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT a.* FROM idc_info a LEFT JOIN system_metrics b ON a.idc_id = b.idc;&#xA;&#xA;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT b.* FROM system_metrics a RIGHT JOIN idc_info b ON a.idc = b.idc_id;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These are examples of&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/reference/sql/join&#34;&gt;JOIN queries in GreptimeDB&lt;/a&gt;, which supports:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;INNER JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;LEFT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;RIGHT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;FULL OUTER JOIN&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;queries-over-time-windows&#34;&gt;Queries over Time Windows&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#queries-over-time-windows&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL‚Äôs&amp;nbsp;&lt;code&gt;GROUP BY&lt;/code&gt;&amp;nbsp;statement supports passing a time window to aggregate data within a specific length of time windows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL does not have such specific query capabilities; the closest equivalent is the&amp;nbsp;&lt;code&gt;OVER ... PARTITION BY&lt;/code&gt;&amp;nbsp;syntax, which can be quite complex to understand.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB implements its own&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY extension syntax&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT &#xA;    ts, &#xA;    host, &#xA;    avg(cpu) RANGE &#39;10s&#39; FILL LINEAR&#xA;FROM monitor&#xA;ALIGN &#39;5s&#39; TO &#39;2023-12-01T00:00:00&#39; BY (host) ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;continuous-aggregation&#34;&gt;Continuous Aggregation&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#continuous-aggregation&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL supports&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/continuous_queries/&#34;&gt;continuous aggregation&lt;/a&gt;, which corresponds to the standard concept of materialized views in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, the implementation of materialized views in most SQL databases is still fragile and remains an area for further exploration.&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-06-04-flow-engine&#34;&gt;GreptimeDB supports continuous aggregation&lt;/a&gt;&amp;nbsp;to meet these needs based on its flow engine.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#conclusion&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. While InfluxQL and Flux are used by InfluxDB and specifically created for handling time series data, SQL is a widely used query language in relational databases. Its robust theoretical foundation and rich ecosystem allow data analysts to quickly get started and use effective tools for time series data analysis.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB natively supports SQL queries. Visit our&amp;nbsp;&lt;a href=&#34;https://greptime.com/&#34;&gt;homepage&lt;/a&gt;&amp;nbsp;for more information or&amp;nbsp;&lt;a href=&#34;https://console.greptime.cloud/signup&#34;&gt;create a free cloud service&lt;/a&gt;&amp;nbsp;instance to start your trial today.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;about-greptime&#34;&gt;About Greptime&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#about-greptime&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We help industries that generate large amounts of time-series data, such as Connected Vehicles (CV), IoT, and Observability, to efficiently uncover the hidden value of data in real-time.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Visit the&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;latest version&lt;/a&gt;&amp;nbsp;from any device to get started and get the most out of your data.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GreptimeDB&lt;/a&gt;, written in Rust, is a distributed, open-source, time-series database designed for scalability, efficiency, and powerful analytics.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://greptime.com/product/carcloud&#34;&gt;Edge-Cloud Integrated TSDB&lt;/a&gt;&amp;nbsp;is designed for the unique demands of edge storage and compute in IoT. It tackles the exponential growth of edge data by integrating a multimodal edge-side database with cloud-based GreptimeDB Enterprise. This combination reduces traffic, computing, and storage costs while enhancing data timeliness and business insights.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.greptime.com/product/cloud&#34;&gt;GreptimeCloud&lt;/a&gt;&amp;nbsp;is a fully-managed cloud database-as-a-service (DBaaS) solution built on GreptimeDB. It efficiently supports applications in fields such as observability, IoT, and finance.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Star us on&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GitHub&lt;/a&gt;&amp;nbsp;or join GreptimeDB Community on&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/slack&#34;&gt;Slack&lt;/a&gt;&amp;nbsp;to get connected. Also, you can go to our&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb/contribute&#34;&gt;contribution page&lt;/a&gt;&amp;nbsp;to find some interesting issues to start with.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Tue, 09 Jul 2024 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>