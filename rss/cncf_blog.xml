<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>CNCF - Blog</title>
    <link>http://rsshub.rssforever.com/cncf</link>
    <description>CNCF - Blog - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)</description>
    <managingEditor>i@diygod.me (DIYgod)</managingEditor>
    <item>
      <title>„ÄêSlurm: An HPC workload manager„ÄëSlurmÔºöHPC Â∑•‰ΩúË¥üËΩΩÁÆ°ÁêÜÂô®</title>
      <link>https://www.cncf.io/blog/2024/07/08/slurm-an-hpc-workload-manager/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/&#34;&gt;SuperOrbital‚Äôs blog&lt;/a&gt; by Sean Kane&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/cover.jpg&#34; alt=&#34;datacenter servers&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this article, we are going to explore&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm&#34;&gt;Slurm&lt;/a&gt;, a popular open-source high-performance computing (HPC&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:hpc&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;) workload manager, and discover what it is, why people use it, and how it differs from Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For context, when we talk about high-performance computing, we are primarily talking about computational tasks that require a lot of time on very powerful computer systems and are often designed, so that they can be&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Parallel_computing&#34;&gt;highly parallelized&lt;/a&gt;, across resources within a super-computer or large cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These sorts of jobs are very common in scientific research,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_simulation&#34;&gt;computer simulation&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_intelligence&#34;&gt;artificial intelligence&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&gt;machine learning&lt;/a&gt;&amp;nbsp;tasks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;what-is-slurm&#34;&gt;What is Slurm?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To almost directly quote the Slurm repository&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm/blob/master/README.rst#slurm-workload-manager&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;Slurm is an open-source cluster resource management and job scheduling system for Linux (&lt;em&gt;primarily&lt;/em&gt;) that strives to be simple, scalable, portable, fault-tolerant, and interconnect agnostic. As a cluster resource manager, Slurm provides three key functions:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Second, it provides a framework for starting, executing, and monitoring work (&lt;em&gt;normally a parallel job&lt;/em&gt;) on the set of allocated nodes.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;And finally, it arbitrates conflicting requests for resources by managing a queue of pending work.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In short, Slurm is a system that is primarily designed to manage&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Batch_processing&#34;&gt;batch processing&lt;/a&gt;&amp;nbsp;workloads running in a shared&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_cluster&#34;&gt;computing cluster&lt;/a&gt;. Slurm can be configured to run any type of batch job but is often utilized on systems that support multiple users with parallel computing tasks to run.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;According to&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;, the primary support and development company for the Slurm community, Slurm is being used on approximately 65% of the&amp;nbsp;&lt;a href=&#34;https://top500.org/lists/top500/2024/06/&#34;&gt;TOP500 supercomputers in the world&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;a-bit-of-history&#34;&gt;A bit of history&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the mid-1990s, as Linux became more popular, there was an increased interest in making large-scale computer processing more accessible through the use of commodity hardware and projects like&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Beowulf_cluster&#34;&gt;Beowulf&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In 2001, engineers at the&amp;nbsp;&lt;a href=&#34;https://www.llnl.gov/&#34;&gt;Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;(LLNL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:llnl&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;) in the United States, started looking into how they might migrate from proprietary HPC systems to open-source solutions. They determined that there weren‚Äôt any good resource managers available at the time, and this directly led to the creation of Slurm, which was initially released in 2002.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;a href=&#34;https://slurm.schedmd.com/slurm_design.pdf&#34;&gt;The name Slurm is a reference&lt;/a&gt;&amp;nbsp;to the&amp;nbsp;&lt;a href=&#34;https://futurama.fandom.com/wiki/Slurm&#34;&gt;soda&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://www.imdb.com/title/tt0149460/&#34;&gt;Futurama&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the years, Slurm has evolved to support a wide variety of hardware configurations and in 2010, some initial scheduling logic was added to Slurm, enabling it to function as both a stand-alone resource manager and a scheduler&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:history&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-architecture&#34;&gt;The Architecture&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Like many similar systems, a Slurm setup consists of at least one controller node, and at least one compute node, which can be on the same physical systems.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/slurm-layout.gif&#34; alt=&#34;Diagram of the basic Slurm architecture&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;the-controller&#34;&gt;The Controller&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Two daemons are responsible for managing the cluster. These two processes can be run on a single node or separated, depending on the performance and resiliency required by the administrator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmctld&#34;&gt;slurmctld&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmctld&lt;/code&gt;&amp;nbsp;is the primary management daemon for Slurm. It is responsible for all the daemons, resources, and jobs, within the cluster. Because it is a critical component in the cluster, it is often configured with a passive backup that can take over management if the primary daemon ever fails.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmdbd&#34;&gt;slurmdbd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmdbd&lt;/code&gt;&amp;nbsp;provides an interface for the&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/MySQL&#34;&gt;MySQL database&lt;/a&gt;&amp;nbsp;that Slurm uses for stateful data, like job accounting records.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;compute-nodes&#34;&gt;Compute Nodes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These represent the nodes whose resources are available to any jobs that might need to be scheduled in the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmd&#34;&gt;slurmd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmd&lt;/code&gt;&amp;nbsp;is the daemon that runs on each compute node and is responsible for accepting work, launching tasks, and generally managing the workloads on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SchedMD maintains the core documentation for Slurm and the&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/quickstart_admin.html#quick_start&#34;&gt;current installation documentation&lt;/a&gt;&amp;nbsp;can be found on their website.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Getting Slurm working is not particularly difficult, but fine-tuning it for your requirements can be a time-consuming task. Since performance is often a primary focus with these clusters and Slurm is designed to work with a very wide set of hardware and software libraries, it can take a lot of effort to fine-tune the installation, so that it utilizes the hardware to the best of its ability, and all the proper software is available to streamline the types of jobs that will be run on the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;At a minimum clusters will require node name resolution, a MySQL instance, shared block storage, a synchronized time source, synchronized users, the&amp;nbsp;&lt;a href=&#34;https://dun.github.io/munge/&#34;&gt;Munge&lt;/a&gt;&amp;nbsp;daemon for user authentication, a full set of custom-built Slurm executables, and various configuration files.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In addition to this, the nodes will need to contain the various hardware, programming languages, libraries, and other tools that the majority of jobs will require to run. This could include things like GPUs&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:gpu&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;and their related toolchains (CUDA&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cuda&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;/cuDNN&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cudnn&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;), MPI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:mpi&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;libraries, OCI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:oci&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;runtimes for&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/containers.html&#34;&gt;container support&lt;/a&gt;, and much more.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm‚Äôs greatest asset and liability is how flexible it is. This makes it something that can be highly optimized for the requirements of the organization, while also making it susceptible to misconfiguration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;workloads&#34;&gt;Workloads&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To provide some context, let‚Äôs take a look at an example user job and workflow that utilizes some of the features of Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the&amp;nbsp;&lt;code&gt;mpi-ping-pong.py&lt;/code&gt;&amp;nbsp;code below, you will find a&amp;nbsp;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt;&amp;nbsp;script that simulates a pseudo-game of&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Table_tennis&#34;&gt;Ping Pong/Table Tennis&lt;/a&gt;&amp;nbsp;by making use of MPI to pass a message (&lt;em&gt;‚Äúball‚Äù&lt;/em&gt;) between as many processes as we have configured to participate in the parallel job.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;local-testing&#34;&gt;Local Testing&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run this code, you simply need to have&amp;nbsp;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/quickstart.html&#34;&gt;OpenMPI&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://mpi4py.readthedocs.io/en/stable/install.html&#34;&gt;mpi4py&lt;/a&gt;&amp;nbsp;installed on your system.mpi-ping-pong.py&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/mpi-ping-pong.py&#34;&gt;mpi-ping-pong.py&lt;/a&gt;&amp;nbsp;is what many would refer to as an&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Embarrassingly_parallel&#34;&gt;embarrassingly parallel&lt;/a&gt;&amp;nbsp;workload because it can be scaled with no real effort at all.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you run this Python application on your local system, you will see something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ python ./mpi-ping-pong.py&#xA;&#xA;Hello, World! I am üèì Player (Rank)  0 . Process  1  of  1  on  laptop.local&#xA;Player (Rank)  0 &#39;s receiving üèì neighbor is Player (Rank) 0 and my sending üèì neighbor is Player (Rank) 0.&#xA;Player 0 started round 1 by hitting the üèì ball towards the wall.&#xA;Player 0 received the üèì ball that bounced off the wall.&#xA;Player 0 started round 2 by hitting the üèì ball towards the wall.&#xA;Player 0 received the üèì ball that bounced off the wall.&#xA;‚Ä¶&#xA;Player 0 started round 299 by hitting the üèì ball towards the wall.&#xA;Player 0 received the üèì ball that bounced off the wall.&#xA;üéâ Player 0 won the game of üèì ping pong, since they were playing alone! üéâ&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As you can see the application runs perfectly fine, and uses a single rank&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:rank&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;/process (&lt;em&gt;CPU&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cpu&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;core, in this case&lt;/em&gt;). Since we only have a single process participating in this game of Ping Pong, the program simply sends and receives messages to and from from itself.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, assuming that we have at least two CPU cores, then we can run this application utilizing as many of them as we want, by leveraging the MPI command line tool&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run 12 instances of our application, which will all communicate with each other in parallel across 12 cores on our system, we can do the following on our local system:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: These messages will be somewhat out of order. This is the nature of parallel computing, buffers, etc. To deal with this you would likely need to send all the messages back to single process for ordering and printing.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ mpirun -np 12 ./mpi-ping-pong.py&#xA;&#xA;Player (Rank)  0 &#39;s receiving üèì neighbor is Player (Rank) 11 and my sending üèì neighbor is Player (Rank) 1.&#xA;Hello, World! I am üèì Player (Rank)  0 . Process  1  of  12  on  laptop.local&#xA;Player (Rank)  1 &#39;s receiving üèì neighbor is Player (Rank) 0 and my sending üèì neighbor is Player (Rank) 2.&#xA;Hello, World! I am üèì Player (Rank)  1 . Process  2  of  12  on  laptop.local&#xA;Player (Rank)  2 &#39;s receiving üèì neighbor is Player (Rank) 1 and my sending üèì neighbor is Player (Rank) 3.&#xA;Hello, World! I am üèì Player (Rank)  2 . Process  3  of  12  on  laptop.local&#xA;‚Ä¶&#xA;üõë Player 0 is bored of üèì ping pong and is quitting! üõë&#xA;üõë Player 1 is bored of üèì ping pong and is quitting! üõë&#xA;üõë Player 2 is bored of üèì ping pong and is quitting! üõë&#xA;‚Ä¶&#xA;üéâ Player 11 won the game of üèì ping pong, since everyone else quit! üéâ&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;massive-scale-with-slurm&#34;&gt;Massive Scale with Slurm&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Now, imagine that we want to run this across hundreds of CPUs. Slurm makes this trivial as long as you have enough systems in your cluster to support this work.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In Slurm, you can define a batch job with a shell script (&lt;em&gt;e.g. /shared_storage/mpi-ping-pong.slurm&lt;/em&gt;) that looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;#!/bin/bash&#xA;&#xA;#SBATCH -J mpi-ping-pong                         # Name the job&#xA;#SBATCH -o /shared_storage/mpi-ping-pong-%j.out  # The standard output file&#xA;#SBATCH -e /shared_storage/mpi-ping-pong-%j.err  # The standard error file&#xA;#SBATCH -t 0-2:00:00         # Run for a max time of 0d, 2h, 0m, 0s&#xA;#SBATCH --nodes=3            # Request 3 nodes&#xA;#SBATCH --ntasks-per-node=1  # Request 1 cores or task per node&#xA;&#xA;echo &#34;Date start                = $(date)&#34;&#xA;echo &#34;Initiating Host           = $(hostname)&#34;&#xA;echo &#34;Working Directory         = $(pwd)&#34;&#xA;echo &#34;&#34;&#xA;echo &#34;Number of Nodes Allocated = ${SLURM_JOB_NUM_NODES}&#34;&#xA;echo &#34;Number of Tasks Allocated = ${SLURM_NTASKS}&#34;&#xA;echo &#34;&#34;&#xA;&#xA;mpirun python /shared_storage/mpi-ping-pong.py&#xA;RETURN=${?}&#xA;&#xA;echo &#34;&#34;&#xA;echo &#34;Exit code                 = ${RETURN}&#34;&#xA;echo &#34;Date end                  = $(date)&#34;&#xA;echo &#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Each&amp;nbsp;&lt;code&gt;#SBATCH&lt;/code&gt;&amp;nbsp;line defines an&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/sbatch.html&#34;&gt;&lt;code&gt;sbatch&lt;/code&gt;&lt;/a&gt;&amp;nbsp;command line argument to be used for the job. In this case, we are going to request that the workload run on three nodes and that it will only use a single core on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that all the compute nodes in our cluster are configured correctly and our Python script exists on the shared storage at&amp;nbsp;&lt;em&gt;/shared_storage/mpi-ping-pong.py&lt;/em&gt;&amp;nbsp;then we can submit this job to the cluster, by running:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ sbatch /shared_storage/mpi-ping-pong.slurm&#xA;&#xA;Submitted batch job 87&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This job will run very fast, but if we needed to we could use commands like&amp;nbsp;&lt;code&gt;squeue&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;sinfo&lt;/code&gt;&amp;nbsp;to examine the state of the cluster and jobs while it was running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When the job is finished we can look at&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.err&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.out&lt;/code&gt;&amp;nbsp;to see all the output from our application. In this case, the error file is empty, but our output file looks like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;Date start                = Tue May 14 10:15:02 PM UTC 2024&#xA;Initiating Host           = worker-0&#xA;Working Directory         = /shared_storage&#xA;&#xA;Number of Nodes Allocated = 3&#xA;Number of Tasks Allocated = 3&#xA;&#xA;Hello, World! I am üèì Player (Rank)  1 . Process  2  of  3  on  worker-1&#xA;Player (Rank)  1 &#39;s receiving üèì neighbor is Player (Rank) 0 and my sending üèì neighbor is Player (Rank) 2.&#xA;Hello, World! I am üèì Player (Rank)  2 . Process  3  of  3  on  worker-2&#xA;Player (Rank)  2 &#39;s receiving üèì neighbor is Player (Rank) 1 and my sending üèì neighbor is Player (Rank) 0.&#xA;Hello, World! I am üèì Player (Rank)  0 . Process  1  of  3  on  worker-0&#xA;Player (Rank)  0 &#39;s receiving üèì neighbor is Player (Rank) 2 and my sending üèì neighbor is Player (Rank) 1.&#xA;Player 0 sent the üèì ball to Player 1.&#xA;Player 1 received the üèì ball from Player 0.&#xA;Player 1 sent the üèì ball to Player 2.&#xA;Player 2 received the üèì ball from Player 1.&#xA;Player 2 sent the üèì ball to Player 0.&#xA;Player 0 received the üèì ball from Player 2.&#xA;Player 0 started round 2 by sending the üèì ball to Player 1.&#xA;‚Ä¶&#xA;Player 0 started round 900 by sending the üèì ball to Player 1.&#xA;üõë Player 0 is bored of üèì ping pong and is quitting! üõë&#xA;Player 1 received the üèì ball from Player 0.&#xA;Player 1 sent the üèì ball to Player 2.&#xA;üõë Player 1 is bored of üèì ping pong and is quitting! üõë&#xA;Player 2 received the üèì ball from Player 1.&#xA;üéâ Player 2 won the game of üèì ping pong, since everyone else quit! üéâ&#xA;&#xA;Exit code                 = 0&#xA;Date end                  = Tue May 14 10:15:03 PM UTC 2024&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you look closely at the&amp;nbsp;&lt;code&gt;Hello, World!&lt;/code&gt;&amp;nbsp;lines for each rank/process you will notice that they are all running on completely different nodes (&lt;em&gt;worker-[0-2]&lt;/em&gt;), but the application still behaves exactly as we expect it to, and the messages are happily passed between each process running on a different core on different systems. Because of this, we could use Slurm to easily run our application across hundreds or even thousands of cores if that is what we needed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, could we do all of this with Kubernetes today? Well, yes and no.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The real power of Slurm is that you have so much control over how it is built, how you assign resources to your workloads, and how they are partitioned while running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;mpi---message-passing-interface&#34;&gt;MPI ‚Äì Message Passing Interface&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although you could easily run this application inside a single pod on Kubernetes, to truly leverage the MPI capabilities that we are expecting we would need to install and utilize something like&amp;nbsp;&lt;a href=&#34;https://www.kubeflow.org/&#34;&gt;KubeFlow&lt;/a&gt;&amp;nbsp;along with&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/mpi-operator&#34;&gt;KubeFlow‚Äôs beta MPI Operator&lt;/a&gt;&amp;nbsp;to even get started. You might also want to consider something like the&amp;nbsp;&lt;a href=&#34;https://kueue.sigs.k8s.io/&#34;&gt;Kubernetes-native Job Queueing support&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;At the time of writing this&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/manifests/tree/v1.8.0?tab=readme-ov-file#prerequisites&#34;&gt;KubeFlow v.1.8.0 doesn‚Äôt support versions of Kubernetes greater than v1.26&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One obvious side effect of using Kubernetes is that we&amp;nbsp;&lt;strong&gt;MUST&lt;/strong&gt;&amp;nbsp;run our code inside one or more containers. This is possible with Slurm but is not a hard requirement. We will also need to ensure that Python, MPI, and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;are all installed and configured correctly in our container, as we will need them all to run our application and manage each of the MPI worker pods.&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is required because this is how&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;will connect to each of the pods to launch the parallel instances of our application. The manifest that we will need to apply to our Kubernetes cluster for use with&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;is simply named&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/manifests/mpi-ping-pong/mpijob.yaml&#34;&gt;mpijob.yaml&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;You can also find&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/Dockerfile&#34;&gt;the Dockerfile for the container that we are running&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground&#34;&gt;the GitHub repo&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;mpijob.yaml&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;em&gt;&lt;/em&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that Kubeflow and the mpi-operator are properly installed in our cluster, then applying the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;manifest should ‚Äújust work‚Äù.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ git clone https://github.com/superorbital/mpi-playground.git&#xA;$ cd mpi-playground&#xA;$ kubectl apply -f ./manifests/mpi-ping-pong/mpijob.yaml&#xA;mpijob.kubeflow.org/mpi-ping-pong created&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If you want to try this out yourself, you will likely find that it is easiest to delete the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;and then re-apply it each time you want to run it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;code&gt;kubectl delete mpijobs.kubeflow.org mpi-ping-pong&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the animated GIF below you will see the manifest being launched, followed by the launcher and worker pods being spun up, once the workers are done, then the launcher will complete and the launcher logs will be displayed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/demo.gif&#34; alt=&#34;MPI Ping Pong in Kubernetes demo&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The sequence of events looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The required number of workers are started and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is run inside each worker container.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher is started and executes the desired&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;command.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;connects to each worker and starts the individual tasks.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The workers complete their tasks and exit.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher completes and exits.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The pod logs for the launcher will contain all of the output that we would normally expect to see from an&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;session.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, today it is technically possible to run at least some MPI-style jobs inside Kubernetes, but how practical is it?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Well, one of the big advantages of Kubernetes is its ability to scale. Unlike Slurm, which has no cluster scaling functionality, a Kubernetes cluster can be designed to scale up nodes for a very large workflow and then scale back down once that workflow is finished and those nodes are no longer required for any immediately upcoming workflows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But, I think the real answer here is that it depends a lot on what you are trying to do, how much control you have over the Kubernetes cluster setup, and how much effort you want to put into getting it all working.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;artificial-intelligence-and-machine-learning&#34;&gt;Artificial Intelligence and Machine Learning&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although the technical details are different, most of the basic observations in this article also apply to artificial intelligence (AI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ai&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;), machine learning (ML&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ml&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;), and deep learning (DL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:dl&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;) applications. In most instances, these workloads will make heavy use of systems equipped with advanced GPUs to significantly improve the performance of their computation tasks. Ensuring that each process has exclusive access to the required hardware resources is often critical for the success and performance of the task.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;slurm-on-kubernetes&#34;&gt;Slurm on Kubernetes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the last couple of years,&amp;nbsp;&lt;a href=&#34;https://www.linkedin.com/in/timwickberg/&#34;&gt;Tim Wickberg, SchedMD‚Äôs CTO&lt;/a&gt;, has given a talk entitled ‚Äú&lt;a href=&#34;https://slurm.schedmd.com/SC23/Slurm-and-or-vs-Kubernetes.pdf&#34;&gt;Slurm and/or/vs Kubernetes&lt;/a&gt;‚Äù, which looks at Slurm and Kubernetes and explores how each of these tools could be extended to leverage their strengths and generally improve the current state of high-performance workload managers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One project that has come out of the effort to find synergy between these two tools and communities, is&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/blog/sunk-slurm-on-kubernetes-implementations&#34;&gt;SUNK&lt;/a&gt;, which stands for&amp;nbsp;&lt;em&gt;&lt;strong&gt;S&lt;/strong&gt;l&lt;strong&gt;U&lt;/strong&gt;rm o&lt;strong&gt;N&lt;/strong&gt;&amp;nbsp;&lt;strong&gt;K&lt;/strong&gt;ubernetes&lt;/em&gt;. It is a project in early development by&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/&#34;&gt;CoreWeave&lt;/a&gt;&amp;nbsp;to enable Slurm workloads in Kubernetes Clusters.&amp;nbsp;&lt;a href=&#34;https://www.youtube.com/watch?v=48ONt0UKiew&#34;&gt;The CoreWeave talk that officially introduced this project can viewed on YouTube&lt;/a&gt;&amp;nbsp;if you would like to hear more details about the work that is being done with this. You can also grab&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/SLUG23/CoreWeave-SLUG23.pdf&#34;&gt;a PDF of the slides&lt;/a&gt;&amp;nbsp;for the talk. Once the code is released it should be available at&amp;nbsp;&lt;a href=&#34;https://github.com/coreweave/sunk&#34;&gt;github.com/coreweave/sunk&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;kubernetes-device-management&#34;&gt;Kubernetes Device Management&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The&amp;nbsp;&lt;a href=&#34;https://community.arm.com/arm-research/b/articles/posts/a-smarter-device-manager-for-kubernetes-on-the-edge&#34;&gt;Smarter Device Project&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://www.arm.com/resources/research&#34;&gt;ARM research&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:arm&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;is also an interesting&amp;nbsp;&lt;a href=&#34;https://github.com/smarter-project/smarter-device-manager&#34;&gt;Kubernetes application&lt;/a&gt;&amp;nbsp;that might be useful for these sorts of workloads and makes it possible to expose, reserve, and manage any Linux device (&lt;em&gt;e.g. /dev/video*&lt;/em&gt;) within Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For GPUs,&amp;nbsp;&lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/k8s-device-plugin&#34;&gt;NVIDIA has a device plugin&lt;/a&gt;&amp;nbsp;to provide similar functionality, which is often deployed along with their&amp;nbsp;&lt;a href=&#34;https://github.com/NVIDIA/gpu-operator&#34;&gt;gpu-operator&lt;/a&gt;. This also enables features in Kubernetes like GPU time-slicing, something similar to the generic&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/gres.html#Sharding&#34;&gt;GPU sharding&lt;/a&gt;&amp;nbsp;available in Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;in-conclusion&#34;&gt;In Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm is a very useful tool for running parallel applications that require very specific resources inside a compute cluster. It was designed for a set of workloads that are outside the standard stateless microservice focus of Kubernetes, but even today Kubernetes can be made to work with some of these workloads without too much hassle, however, truly leveraging the advantages of Kubernetes for these more traditional high-performance computing workloads will require projects like SUNK and&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;to reach a reasonable level of maturity and then in the future, there may be newer projects that rethink the current approaches and build Kubernetes-first tools from the ground-up that leverage the strengths that are already available in the platform while extending its ability to handle the type of workloads that have been traditionally handled by tools like Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;further-reading&#34;&gt;Further Reading&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained&#34;&gt;Machine learning, explained&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.geeksforgeeks.org/machine-learning/&#34;&gt;Machine Learning Tutorial&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://carleton.ca/rcs/rcdc/introduction-to-mpi/&#34;&gt;Introduction to MPI&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://mpitutorial.com/&#34;&gt;A Comprehensive MPI Tutorial Resource&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Intro to Multi-Node Machine Learning&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-1-setting-up-an-hpc-cluster&#34;&gt;1: Setting up an HPC cluster&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-2-using-slurm&#34;&gt;2: Using Slurm&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;Unreleased as of 05/16/2024&lt;/em&gt;&amp;nbsp;‚Äì&amp;nbsp;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-3-multi-node-training&#34;&gt;3: Multi-Node Training&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://blog.kubeflow.org/integrations/operators/2020/03/16/mpi-operator.html&#34;&gt;Introduction to Kubeflow MPI Operator and Industry Adoption&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Cover image by&amp;nbsp;&lt;a href=&#34;https://pixabay.com/users/dlohner-4631193/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;dlohner&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://pixabay.com//?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;Pixabay&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Slurm architecture overview by SchedMD for their&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/network.html&#34;&gt;network overview&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/High-performance_computing&#34;&gt;HPC ‚Äì high-performance computing&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:hpc&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Lawrence_Livermore_National_Laboratory&#34;&gt;LLNL ‚Äì Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:llnl&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.schedmd.com/about-schedmd/slurm-history&#34;&gt;Slurm history&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:history&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Graphics_processing_unit&#34;&gt;GPU ‚Äì graphics processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:gpu&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA¬Æ ‚Äì Compute Unified Device Architecture&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cuda&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN ‚Äì NVIDIA CUDA¬Æ Deep Neural Network library&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cudnn&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.open-mpi.org/&#34;&gt;MPI ‚Äì Message Passing Interface&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:mpi&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Open_Container_Initiative&#34;&gt;OCI ‚Äì Open Container Initiative&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:oci&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.uio.no/studier/emner/matnat/ifi/INF3380/v11/undervisningsmateriale/inf3380-week06.pdf&#34;&gt;Rank&lt;/a&gt;&amp;nbsp;‚Äì Each process has a unique rank, i.e. an integer identifier, within a communicator. The rank value is between zero and the number of processes minus one. The rank value is used to distinguish one process from another.&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:rank&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Central_processing_unit&#34;&gt;CPU ‚Äì central processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cpu&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/artificial-intelligence&#34;&gt;AI ‚Äì Artificial Intelligence&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ai&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/machine-learning&#34;&gt;ML ‚Äì Machine Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ml&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/deep-learning&#34;&gt;DL ‚Äì Deep Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:dl&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/ARM_architecture_family&#34;&gt;ARM ‚Äì Advanced RISC Machine&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:arm&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/&#34;&gt;SuperOrbital‚Äôs blog&lt;/a&gt; by Sean Kane&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/cover.jpg&#34; alt=&#34;datacenter servers&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this article, we are going to explore&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm&#34;&gt;Slurm&lt;/a&gt;, a popular open-source high-performance computing (HPC&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:hpc&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;) workload manager, and discover what it is, why people use it, and how it differs from Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For context, when we talk about high-performance computing, we are primarily talking about computational tasks that require a lot of time on very powerful computer systems and are often designed, so that they can be&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Parallel_computing&#34;&gt;highly parallelized&lt;/a&gt;, across resources within a super-computer or large cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These sorts of jobs are very common in scientific research,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_simulation&#34;&gt;computer simulation&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_intelligence&#34;&gt;artificial intelligence&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&gt;machine learning&lt;/a&gt;&amp;nbsp;tasks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;what-is-slurm&#34;&gt;What is Slurm?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To almost directly quote the Slurm repository&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm/blob/master/README.rst#slurm-workload-manager&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;Slurm is an open-source cluster resource management and job scheduling system for Linux (&lt;em&gt;primarily&lt;/em&gt;) that strives to be simple, scalable, portable, fault-tolerant, and interconnect agnostic. As a cluster resource manager, Slurm provides three key functions:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Second, it provides a framework for starting, executing, and monitoring work (&lt;em&gt;normally a parallel job&lt;/em&gt;) on the set of allocated nodes.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;And finally, it arbitrates conflicting requests for resources by managing a queue of pending work.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In short, Slurm is a system that is primarily designed to manage&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Batch_processing&#34;&gt;batch processing&lt;/a&gt;&amp;nbsp;workloads running in a shared&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_cluster&#34;&gt;computing cluster&lt;/a&gt;. Slurm can be configured to run any type of batch job but is often utilized on systems that support multiple users with parallel computing tasks to run.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;According to&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;, the primary support and development company for the Slurm community, Slurm is being used on approximately 65% of the&amp;nbsp;&lt;a href=&#34;https://top500.org/lists/top500/2024/06/&#34;&gt;TOP500 supercomputers in the world&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;a-bit-of-history&#34;&gt;A bit of history&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the mid-1990s, as Linux became more popular, there was an increased interest in making large-scale computer processing more accessible through the use of commodity hardware and projects like&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Beowulf_cluster&#34;&gt;Beowulf&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In 2001, engineers at the&amp;nbsp;&lt;a href=&#34;https://www.llnl.gov/&#34;&gt;Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;(LLNL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:llnl&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;) in the United States, started looking into how they might migrate from proprietary HPC systems to open-source solutions. They determined that there weren‚Äôt any good resource managers available at the time, and this directly led to the creation of Slurm, which was initially released in 2002.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;a href=&#34;https://slurm.schedmd.com/slurm_design.pdf&#34;&gt;The name Slurm is a reference&lt;/a&gt;&amp;nbsp;to the&amp;nbsp;&lt;a href=&#34;https://futurama.fandom.com/wiki/Slurm&#34;&gt;soda&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://www.imdb.com/title/tt0149460/&#34;&gt;Futurama&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the years, Slurm has evolved to support a wide variety of hardware configurations and in 2010, some initial scheduling logic was added to Slurm, enabling it to function as both a stand-alone resource manager and a scheduler&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:history&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-architecture&#34;&gt;The Architecture&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Like many similar systems, a Slurm setup consists of at least one controller node, and at least one compute node, which can be on the same physical systems.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/slurm-layout.gif&#34; alt=&#34;Diagram of the basic Slurm architecture&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;the-controller&#34;&gt;The Controller&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Two daemons are responsible for managing the cluster. These two processes can be run on a single node or separated, depending on the performance and resiliency required by the administrator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmctld&#34;&gt;slurmctld&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmctld&lt;/code&gt;&amp;nbsp;is the primary management daemon for Slurm. It is responsible for all the daemons, resources, and jobs, within the cluster. Because it is a critical component in the cluster, it is often configured with a passive backup that can take over management if the primary daemon ever fails.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmdbd&#34;&gt;slurmdbd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmdbd&lt;/code&gt;&amp;nbsp;provides an interface for the&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/MySQL&#34;&gt;MySQL database&lt;/a&gt;&amp;nbsp;that Slurm uses for stateful data, like job accounting records.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;compute-nodes&#34;&gt;Compute Nodes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These represent the nodes whose resources are available to any jobs that might need to be scheduled in the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmd&#34;&gt;slurmd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmd&lt;/code&gt;&amp;nbsp;is the daemon that runs on each compute node and is responsible for accepting work, launching tasks, and generally managing the workloads on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SchedMD maintains the core documentation for Slurm and the&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/quickstart_admin.html#quick_start&#34;&gt;current installation documentation&lt;/a&gt;&amp;nbsp;can be found on their website.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Getting Slurm working is not particularly difficult, but fine-tuning it for your requirements can be a time-consuming task. Since performance is often a primary focus with these clusters and Slurm is designed to work with a very wide set of hardware and software libraries, it can take a lot of effort to fine-tune the installation, so that it utilizes the hardware to the best of its ability, and all the proper software is available to streamline the types of jobs that will be run on the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;At a minimum clusters will require node name resolution, a MySQL instance, shared block storage, a synchronized time source, synchronized users, the&amp;nbsp;&lt;a href=&#34;https://dun.github.io/munge/&#34;&gt;Munge&lt;/a&gt;&amp;nbsp;daemon for user authentication, a full set of custom-built Slurm executables, and various configuration files.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In addition to this, the nodes will need to contain the various hardware, programming languages, libraries, and other tools that the majority of jobs will require to run. This could include things like GPUs&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:gpu&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;and their related toolchains (CUDA&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cuda&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;/cuDNN&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cudnn&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;), MPI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:mpi&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;libraries, OCI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:oci&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;runtimes for&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/containers.html&#34;&gt;container support&lt;/a&gt;, and much more.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm‚Äôs greatest asset and liability is how flexible it is. This makes it something that can be highly optimized for the requirements of the organization, while also making it susceptible to misconfiguration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;workloads&#34;&gt;Workloads&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To provide some context, let‚Äôs take a look at an example user job and workflow that utilizes some of the features of Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the&amp;nbsp;&lt;code&gt;mpi-ping-pong.py&lt;/code&gt;&amp;nbsp;code below, you will find a&amp;nbsp;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt;&amp;nbsp;script that simulates a pseudo-game of&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Table_tennis&#34;&gt;Ping Pong/Table Tennis&lt;/a&gt;&amp;nbsp;by making use of MPI to pass a message (&lt;em&gt;‚Äúball‚Äù&lt;/em&gt;) between as many processes as we have configured to participate in the parallel job.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;local-testing&#34;&gt;Local Testing&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run this code, you simply need to have&amp;nbsp;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/quickstart.html&#34;&gt;OpenMPI&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://mpi4py.readthedocs.io/en/stable/install.html&#34;&gt;mpi4py&lt;/a&gt;&amp;nbsp;installed on your system.mpi-ping-pong.py&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/mpi-ping-pong.py&#34;&gt;mpi-ping-pong.py&lt;/a&gt;&amp;nbsp;is what many would refer to as an&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Embarrassingly_parallel&#34;&gt;embarrassingly parallel&lt;/a&gt;&amp;nbsp;workload because it can be scaled with no real effort at all.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you run this Python application on your local system, you will see something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ python ./mpi-ping-pong.py&#xA;&#xA;Hello, World! I am üèì Player (Rank)  0 . Process  1  of  1  on  laptop.local&#xA;Player (Rank)  0 &#39;s receiving üèì neighbor is Player (Rank) 0 and my sending üèì neighbor is Player (Rank) 0.&#xA;Player 0 started round 1 by hitting the üèì ball towards the wall.&#xA;Player 0 received the üèì ball that bounced off the wall.&#xA;Player 0 started round 2 by hitting the üèì ball towards the wall.&#xA;Player 0 received the üèì ball that bounced off the wall.&#xA;‚Ä¶&#xA;Player 0 started round 299 by hitting the üèì ball towards the wall.&#xA;Player 0 received the üèì ball that bounced off the wall.&#xA;üéâ Player 0 won the game of üèì ping pong, since they were playing alone! üéâ&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As you can see the application runs perfectly fine, and uses a single rank&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:rank&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;/process (&lt;em&gt;CPU&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cpu&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;core, in this case&lt;/em&gt;). Since we only have a single process participating in this game of Ping Pong, the program simply sends and receives messages to and from from itself.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, assuming that we have at least two CPU cores, then we can run this application utilizing as many of them as we want, by leveraging the MPI command line tool&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run 12 instances of our application, which will all communicate with each other in parallel across 12 cores on our system, we can do the following on our local system:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: These messages will be somewhat out of order. This is the nature of parallel computing, buffers, etc. To deal with this you would likely need to send all the messages back to single process for ordering and printing.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ mpirun -np 12 ./mpi-ping-pong.py&#xA;&#xA;Player (Rank)  0 &#39;s receiving üèì neighbor is Player (Rank) 11 and my sending üèì neighbor is Player (Rank) 1.&#xA;Hello, World! I am üèì Player (Rank)  0 . Process  1  of  12  on  laptop.local&#xA;Player (Rank)  1 &#39;s receiving üèì neighbor is Player (Rank) 0 and my sending üèì neighbor is Player (Rank) 2.&#xA;Hello, World! I am üèì Player (Rank)  1 . Process  2  of  12  on  laptop.local&#xA;Player (Rank)  2 &#39;s receiving üèì neighbor is Player (Rank) 1 and my sending üèì neighbor is Player (Rank) 3.&#xA;Hello, World! I am üèì Player (Rank)  2 . Process  3  of  12  on  laptop.local&#xA;‚Ä¶&#xA;üõë Player 0 is bored of üèì ping pong and is quitting! üõë&#xA;üõë Player 1 is bored of üèì ping pong and is quitting! üõë&#xA;üõë Player 2 is bored of üèì ping pong and is quitting! üõë&#xA;‚Ä¶&#xA;üéâ Player 11 won the game of üèì ping pong, since everyone else quit! üéâ&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;massive-scale-with-slurm&#34;&gt;Massive Scale with Slurm&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Now, imagine that we want to run this across hundreds of CPUs. Slurm makes this trivial as long as you have enough systems in your cluster to support this work.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In Slurm, you can define a batch job with a shell script (&lt;em&gt;e.g. /shared_storage/mpi-ping-pong.slurm&lt;/em&gt;) that looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;#!/bin/bash&#xA;&#xA;#SBATCH -J mpi-ping-pong                         # Name the job&#xA;#SBATCH -o /shared_storage/mpi-ping-pong-%j.out  # The standard output file&#xA;#SBATCH -e /shared_storage/mpi-ping-pong-%j.err  # The standard error file&#xA;#SBATCH -t 0-2:00:00         # Run for a max time of 0d, 2h, 0m, 0s&#xA;#SBATCH --nodes=3            # Request 3 nodes&#xA;#SBATCH --ntasks-per-node=1  # Request 1 cores or task per node&#xA;&#xA;echo &#34;Date start                = $(date)&#34;&#xA;echo &#34;Initiating Host           = $(hostname)&#34;&#xA;echo &#34;Working Directory         = $(pwd)&#34;&#xA;echo &#34;&#34;&#xA;echo &#34;Number of Nodes Allocated = ${SLURM_JOB_NUM_NODES}&#34;&#xA;echo &#34;Number of Tasks Allocated = ${SLURM_NTASKS}&#34;&#xA;echo &#34;&#34;&#xA;&#xA;mpirun python /shared_storage/mpi-ping-pong.py&#xA;RETURN=${?}&#xA;&#xA;echo &#34;&#34;&#xA;echo &#34;Exit code                 = ${RETURN}&#34;&#xA;echo &#34;Date end                  = $(date)&#34;&#xA;echo &#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Each&amp;nbsp;&lt;code&gt;#SBATCH&lt;/code&gt;&amp;nbsp;line defines an&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/sbatch.html&#34;&gt;&lt;code&gt;sbatch&lt;/code&gt;&lt;/a&gt;&amp;nbsp;command line argument to be used for the job. In this case, we are going to request that the workload run on three nodes and that it will only use a single core on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that all the compute nodes in our cluster are configured correctly and our Python script exists on the shared storage at&amp;nbsp;&lt;em&gt;/shared_storage/mpi-ping-pong.py&lt;/em&gt;&amp;nbsp;then we can submit this job to the cluster, by running:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ sbatch /shared_storage/mpi-ping-pong.slurm&#xA;&#xA;Submitted batch job 87&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This job will run very fast, but if we needed to we could use commands like&amp;nbsp;&lt;code&gt;squeue&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;sinfo&lt;/code&gt;&amp;nbsp;to examine the state of the cluster and jobs while it was running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When the job is finished we can look at&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.err&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.out&lt;/code&gt;&amp;nbsp;to see all the output from our application. In this case, the error file is empty, but our output file looks like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;Date start                = Tue May 14 10:15:02 PM UTC 2024&#xA;Initiating Host           = worker-0&#xA;Working Directory         = /shared_storage&#xA;&#xA;Number of Nodes Allocated = 3&#xA;Number of Tasks Allocated = 3&#xA;&#xA;Hello, World! I am üèì Player (Rank)  1 . Process  2  of  3  on  worker-1&#xA;Player (Rank)  1 &#39;s receiving üèì neighbor is Player (Rank) 0 and my sending üèì neighbor is Player (Rank) 2.&#xA;Hello, World! I am üèì Player (Rank)  2 . Process  3  of  3  on  worker-2&#xA;Player (Rank)  2 &#39;s receiving üèì neighbor is Player (Rank) 1 and my sending üèì neighbor is Player (Rank) 0.&#xA;Hello, World! I am üèì Player (Rank)  0 . Process  1  of  3  on  worker-0&#xA;Player (Rank)  0 &#39;s receiving üèì neighbor is Player (Rank) 2 and my sending üèì neighbor is Player (Rank) 1.&#xA;Player 0 sent the üèì ball to Player 1.&#xA;Player 1 received the üèì ball from Player 0.&#xA;Player 1 sent the üèì ball to Player 2.&#xA;Player 2 received the üèì ball from Player 1.&#xA;Player 2 sent the üèì ball to Player 0.&#xA;Player 0 received the üèì ball from Player 2.&#xA;Player 0 started round 2 by sending the üèì ball to Player 1.&#xA;‚Ä¶&#xA;Player 0 started round 900 by sending the üèì ball to Player 1.&#xA;üõë Player 0 is bored of üèì ping pong and is quitting! üõë&#xA;Player 1 received the üèì ball from Player 0.&#xA;Player 1 sent the üèì ball to Player 2.&#xA;üõë Player 1 is bored of üèì ping pong and is quitting! üõë&#xA;Player 2 received the üèì ball from Player 1.&#xA;üéâ Player 2 won the game of üèì ping pong, since everyone else quit! üéâ&#xA;&#xA;Exit code                 = 0&#xA;Date end                  = Tue May 14 10:15:03 PM UTC 2024&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you look closely at the&amp;nbsp;&lt;code&gt;Hello, World!&lt;/code&gt;&amp;nbsp;lines for each rank/process you will notice that they are all running on completely different nodes (&lt;em&gt;worker-[0-2]&lt;/em&gt;), but the application still behaves exactly as we expect it to, and the messages are happily passed between each process running on a different core on different systems. Because of this, we could use Slurm to easily run our application across hundreds or even thousands of cores if that is what we needed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, could we do all of this with Kubernetes today? Well, yes and no.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The real power of Slurm is that you have so much control over how it is built, how you assign resources to your workloads, and how they are partitioned while running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;mpi---message-passing-interface&#34;&gt;MPI ‚Äì Message Passing Interface&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although you could easily run this application inside a single pod on Kubernetes, to truly leverage the MPI capabilities that we are expecting we would need to install and utilize something like&amp;nbsp;&lt;a href=&#34;https://www.kubeflow.org/&#34;&gt;KubeFlow&lt;/a&gt;&amp;nbsp;along with&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/mpi-operator&#34;&gt;KubeFlow‚Äôs beta MPI Operator&lt;/a&gt;&amp;nbsp;to even get started. You might also want to consider something like the&amp;nbsp;&lt;a href=&#34;https://kueue.sigs.k8s.io/&#34;&gt;Kubernetes-native Job Queueing support&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;At the time of writing this&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/manifests/tree/v1.8.0?tab=readme-ov-file#prerequisites&#34;&gt;KubeFlow v.1.8.0 doesn‚Äôt support versions of Kubernetes greater than v1.26&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One obvious side effect of using Kubernetes is that we&amp;nbsp;&lt;strong&gt;MUST&lt;/strong&gt;&amp;nbsp;run our code inside one or more containers. This is possible with Slurm but is not a hard requirement. We will also need to ensure that Python, MPI, and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;are all installed and configured correctly in our container, as we will need them all to run our application and manage each of the MPI worker pods.&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is required because this is how&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;will connect to each of the pods to launch the parallel instances of our application. The manifest that we will need to apply to our Kubernetes cluster for use with&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;is simply named&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/manifests/mpi-ping-pong/mpijob.yaml&#34;&gt;mpijob.yaml&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;You can also find&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/Dockerfile&#34;&gt;the Dockerfile for the container that we are running&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground&#34;&gt;the GitHub repo&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;mpijob.yaml&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;em&gt;&lt;/em&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that Kubeflow and the mpi-operator are properly installed in our cluster, then applying the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;manifest should ‚Äújust work‚Äù.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ git clone https://github.com/superorbital/mpi-playground.git&#xA;$ cd mpi-playground&#xA;$ kubectl apply -f ./manifests/mpi-ping-pong/mpijob.yaml&#xA;mpijob.kubeflow.org/mpi-ping-pong created&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If you want to try this out yourself, you will likely find that it is easiest to delete the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;and then re-apply it each time you want to run it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;code&gt;kubectl delete mpijobs.kubeflow.org mpi-ping-pong&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the animated GIF below you will see the manifest being launched, followed by the launcher and worker pods being spun up, once the workers are done, then the launcher will complete and the launcher logs will be displayed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/demo.gif&#34; alt=&#34;MPI Ping Pong in Kubernetes demo&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The sequence of events looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The required number of workers are started and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is run inside each worker container.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher is started and executes the desired&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;command.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;connects to each worker and starts the individual tasks.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The workers complete their tasks and exit.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher completes and exits.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The pod logs for the launcher will contain all of the output that we would normally expect to see from an&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;session.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, today it is technically possible to run at least some MPI-style jobs inside Kubernetes, but how practical is it?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Well, one of the big advantages of Kubernetes is its ability to scale. Unlike Slurm, which has no cluster scaling functionality, a Kubernetes cluster can be designed to scale up nodes for a very large workflow and then scale back down once that workflow is finished and those nodes are no longer required for any immediately upcoming workflows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But, I think the real answer here is that it depends a lot on what you are trying to do, how much control you have over the Kubernetes cluster setup, and how much effort you want to put into getting it all working.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;artificial-intelligence-and-machine-learning&#34;&gt;Artificial Intelligence and Machine Learning&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although the technical details are different, most of the basic observations in this article also apply to artificial intelligence (AI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ai&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;), machine learning (ML&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ml&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;), and deep learning (DL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:dl&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;) applications. In most instances, these workloads will make heavy use of systems equipped with advanced GPUs to significantly improve the performance of their computation tasks. Ensuring that each process has exclusive access to the required hardware resources is often critical for the success and performance of the task.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;slurm-on-kubernetes&#34;&gt;Slurm on Kubernetes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the last couple of years,&amp;nbsp;&lt;a href=&#34;https://www.linkedin.com/in/timwickberg/&#34;&gt;Tim Wickberg, SchedMD‚Äôs CTO&lt;/a&gt;, has given a talk entitled ‚Äú&lt;a href=&#34;https://slurm.schedmd.com/SC23/Slurm-and-or-vs-Kubernetes.pdf&#34;&gt;Slurm and/or/vs Kubernetes&lt;/a&gt;‚Äù, which looks at Slurm and Kubernetes and explores how each of these tools could be extended to leverage their strengths and generally improve the current state of high-performance workload managers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One project that has come out of the effort to find synergy between these two tools and communities, is&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/blog/sunk-slurm-on-kubernetes-implementations&#34;&gt;SUNK&lt;/a&gt;, which stands for&amp;nbsp;&lt;em&gt;&lt;strong&gt;S&lt;/strong&gt;l&lt;strong&gt;U&lt;/strong&gt;rm o&lt;strong&gt;N&lt;/strong&gt;&amp;nbsp;&lt;strong&gt;K&lt;/strong&gt;ubernetes&lt;/em&gt;. It is a project in early development by&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/&#34;&gt;CoreWeave&lt;/a&gt;&amp;nbsp;to enable Slurm workloads in Kubernetes Clusters.&amp;nbsp;&lt;a href=&#34;https://www.youtube.com/watch?v=48ONt0UKiew&#34;&gt;The CoreWeave talk that officially introduced this project can viewed on YouTube&lt;/a&gt;&amp;nbsp;if you would like to hear more details about the work that is being done with this. You can also grab&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/SLUG23/CoreWeave-SLUG23.pdf&#34;&gt;a PDF of the slides&lt;/a&gt;&amp;nbsp;for the talk. Once the code is released it should be available at&amp;nbsp;&lt;a href=&#34;https://github.com/coreweave/sunk&#34;&gt;github.com/coreweave/sunk&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;kubernetes-device-management&#34;&gt;Kubernetes Device Management&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The&amp;nbsp;&lt;a href=&#34;https://community.arm.com/arm-research/b/articles/posts/a-smarter-device-manager-for-kubernetes-on-the-edge&#34;&gt;Smarter Device Project&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://www.arm.com/resources/research&#34;&gt;ARM research&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:arm&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;is also an interesting&amp;nbsp;&lt;a href=&#34;https://github.com/smarter-project/smarter-device-manager&#34;&gt;Kubernetes application&lt;/a&gt;&amp;nbsp;that might be useful for these sorts of workloads and makes it possible to expose, reserve, and manage any Linux device (&lt;em&gt;e.g. /dev/video*&lt;/em&gt;) within Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For GPUs,&amp;nbsp;&lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/k8s-device-plugin&#34;&gt;NVIDIA has a device plugin&lt;/a&gt;&amp;nbsp;to provide similar functionality, which is often deployed along with their&amp;nbsp;&lt;a href=&#34;https://github.com/NVIDIA/gpu-operator&#34;&gt;gpu-operator&lt;/a&gt;. This also enables features in Kubernetes like GPU time-slicing, something similar to the generic&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/gres.html#Sharding&#34;&gt;GPU sharding&lt;/a&gt;&amp;nbsp;available in Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;in-conclusion&#34;&gt;In Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm is a very useful tool for running parallel applications that require very specific resources inside a compute cluster. It was designed for a set of workloads that are outside the standard stateless microservice focus of Kubernetes, but even today Kubernetes can be made to work with some of these workloads without too much hassle, however, truly leveraging the advantages of Kubernetes for these more traditional high-performance computing workloads will require projects like SUNK and&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;to reach a reasonable level of maturity and then in the future, there may be newer projects that rethink the current approaches and build Kubernetes-first tools from the ground-up that leverage the strengths that are already available in the platform while extending its ability to handle the type of workloads that have been traditionally handled by tools like Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;further-reading&#34;&gt;Further Reading&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained&#34;&gt;Machine learning, explained&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.geeksforgeeks.org/machine-learning/&#34;&gt;Machine Learning Tutorial&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://carleton.ca/rcs/rcdc/introduction-to-mpi/&#34;&gt;Introduction to MPI&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://mpitutorial.com/&#34;&gt;A Comprehensive MPI Tutorial Resource&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Intro to Multi-Node Machine Learning&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-1-setting-up-an-hpc-cluster&#34;&gt;1: Setting up an HPC cluster&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-2-using-slurm&#34;&gt;2: Using Slurm&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;Unreleased as of 05/16/2024&lt;/em&gt;&amp;nbsp;‚Äì&amp;nbsp;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-3-multi-node-training&#34;&gt;3: Multi-Node Training&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://blog.kubeflow.org/integrations/operators/2020/03/16/mpi-operator.html&#34;&gt;Introduction to Kubeflow MPI Operator and Industry Adoption&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Cover image by&amp;nbsp;&lt;a href=&#34;https://pixabay.com/users/dlohner-4631193/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;dlohner&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://pixabay.com//?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;Pixabay&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Slurm architecture overview by SchedMD for their&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/network.html&#34;&gt;network overview&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/High-performance_computing&#34;&gt;HPC ‚Äì high-performance computing&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:hpc&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Lawrence_Livermore_National_Laboratory&#34;&gt;LLNL ‚Äì Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:llnl&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.schedmd.com/about-schedmd/slurm-history&#34;&gt;Slurm history&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:history&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Graphics_processing_unit&#34;&gt;GPU ‚Äì graphics processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:gpu&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA¬Æ ‚Äì Compute Unified Device Architecture&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cuda&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN ‚Äì NVIDIA CUDA¬Æ Deep Neural Network library&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cudnn&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.open-mpi.org/&#34;&gt;MPI ‚Äì Message Passing Interface&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:mpi&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Open_Container_Initiative&#34;&gt;OCI ‚Äì Open Container Initiative&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:oci&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.uio.no/studier/emner/matnat/ifi/INF3380/v11/undervisningsmateriale/inf3380-week06.pdf&#34;&gt;Rank&lt;/a&gt;&amp;nbsp;‚Äì Each process has a unique rank, i.e. an integer identifier, within a communicator. The rank value is between zero and the number of processes minus one. The rank value is used to distinguish one process from another.&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:rank&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Central_processing_unit&#34;&gt;CPU ‚Äì central processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cpu&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/artificial-intelligence&#34;&gt;AI ‚Äì Artificial Intelligence&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ai&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/machine-learning&#34;&gt;ML ‚Äì Machine Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ml&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/deep-learning&#34;&gt;DL ‚Äì Deep Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:dl&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/ARM_architecture_family&#34;&gt;ARM ‚Äì Advanced RISC Machine&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:arm&#34;&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Sun, 07 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêInterval tree implementation in Xline„ÄëXline ‰∏≠ÁöÑÂå∫Èó¥Ê†ëÂÆûÁé∞</title>
      <link>https://www.cncf.io/blog/2024/07/17/interval-tree-implementation-in-xline/</link>
      <description>„Äê&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXc_7KDamNKUncNHOQCVmCMuEvNplvfHvc9h4p9bq3Y_iEzQAhHX5RIRuq_EvMlEQSf3F_cYWq-1cpMmDJCE-SbvMh4_8cqa_PQ7zp3p4CxT-_uFXeVTgx1uzh4Bc7yDMQn9B1XN63zuBa0k2vukkszFjaXWLesVxHwASp2gbCqZ9dbaZ9BR5kM?key=Iv6r86UKlwDAHVdsR8rx5g&#34; alt=&#34;Image with title &#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The Reason for Implementing Interval Trees&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In a recent refactoring of Xline, we identified a performance bottleneck caused by two data structures on the critical path: the Speculative Pool and the Uncommitted Pool. These two data structures are used for conflict detection in CURP. Specifically, the protocol requires for each processed command, it is necessary to find all the commands that conflict with the current command.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance, in the KV operations&amp;nbsp;put/get_range/delete_range, we need to consider the possible conflicts between these operations. Since each KV operation covers a range of keys, the problem becomes checking whether a given range of keys intersects with any key ranges in the pool. A plain traversal of the entire set results in a time complexity of O(n) for each query, which reduces efficiency and leads to performance bottlenecks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To address this problem, we need to introduce an interval tree data structure. Interval trees support efficient insertion, deletion and query operations on overlapping intervals, all of which can be completed in O(log(n)) time. Therefore, we can use the interval tree to maintain a collection of key ranges.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Introduction to Interval Tree Implementation&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The interval tree implementation in Xline is based on&amp;nbsp;Introduction to Algorithms (3rd ed.), which extends the self-balancing binary search tree.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The interval tree is based on a binary balanced tree (e.g., a red-black tree), and the interval itself is used as the key. For an interval [low, high], we first sort the intervals by their low values, and then by their high values if the low values are the same, establishing a total order relationship for the set of intervals. If duplicate intervals are not handled, sorting by high values is unnecessary. Additionally, for each node in the balanced tree, we maintain the maximum value of high in the subtree rooted at this node, denoted as max.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Insertion/Deletion&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Same as insertion/deletion in red-black tree, the worst time complexity is O(log(n)).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Query Overlap&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Given an interval i, we need to query the current tree to see if any intervals in the tree overlap with i.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The pseudo-code given in Introduction to Algorithms is as follows&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXf3FeKuovVTbM21lFBSW09-203bdtPYCg_9szdhZikolKFJwXBeWw_3oBpqxTdZKAqabNHuGJlgBgaTNyAY_0vChoKJqv3x2LItLuGDv2B_5i0THxWKLjLSMtxSwEAjyXh9y6wiWAlL0CQ13ISrt9wKj6fLLhLTrNcK0wFzBmFBfUZ1LBOIn5k?key=Iv6r86UKlwDAHVdsR8rx5g&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;With the definition of max, the solution to this problem becomes straightforward: for a subtree T_x rooted at x, if i does not intersect x_i, then i must lie to either the left or right of x_i.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;If i is on the left side of x_i, then we can rule out the right subtree, because i.high is smaller than x_i.low.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;if i is on the right side of x_i: In this case, we can‚Äôt rule out the left subtree, because the intervals of nodes in the left subtree may still intersect i. This is where max comes in handy.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If the maximum value of high in x‚Äôs left subtree is still less than i.low, then we can ignore x‚Äôs left subtree.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;If the maximum value of high in x‚Äôs left subtree is greater than or equal to i.low, then there must be intersecting intervals in x‚Äôs left subtree because all the lows in x‚Äôs left subtree are less than x_i.low, and i is on the right side of x_i, so all the lows in x‚Äôs left subtree are less than i.low, so there must be an intersection.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The above points can validate the correctness of the pseudo-code, and from the code we can infer that the worst-case time complexity of the query is O(log(n)).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Implementing Interval Trees with Safe Rust&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Difficulty&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To construct an interval tree, we first need to implement a red-black tree. In a red-black tree, each node must reference its parent node, which requires multiple ownership of a single node instance.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Rc&amp;lt;RefCell&amp;lt;T&amp;gt;&amp;gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I initially attempted to use Rust‚Äôs most common multi-ownership implementation, Rc&amp;lt;RefCell&amp;lt;T&amp;gt;&amp;gt;, with a tree node structure similar to the following code.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;struct Node&amp;lt;T, V&amp;gt; {&lt;br&gt;&amp;nbsp; &amp;nbsp; left: Option&amp;lt;NodeRef&amp;lt;T, V&amp;gt;&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; right: Option&amp;lt;NodeRef&amp;lt;T, V&amp;gt;&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; parent: Option&amp;lt;NodeRef&amp;lt;T, V&amp;gt;&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; ...&lt;br&gt;}&lt;br&gt;&lt;br&gt;struct NodeRef&amp;lt;T, V&amp;gt;(Rc&amp;lt;RefCell&amp;lt;Node&amp;lt;T, V&amp;gt;&amp;gt;&amp;gt;);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The data structure definition appears straightforward, but in practice it‚Äôs quite cumbersome to use, because RefCell requires the user to explicitly call borrow, or borrow_mut, requiring me to create numerous helper functions to simplify the implementation. Here are some examples.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;impl&amp;lt;T, V&amp;gt; NodeRef&amp;lt;T, V&amp;gt; {&lt;br&gt;&amp;nbsp; &amp;nbsp; fn left&amp;lt;F, R&amp;gt;(&amp;amp;self, op: F) -&amp;gt; R&lt;br&gt;&amp;nbsp; &amp;nbsp; where&lt;br&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; F: FnOnce(&amp;amp;NodeRef&amp;lt;T, V&amp;gt;) -&amp;gt; R,&lt;br&gt;&amp;nbsp; &amp;nbsp; {&lt;br&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; op(self.borrow().left())&lt;br&gt;&amp;nbsp; &amp;nbsp; }&lt;br&gt;&lt;br&gt;&amp;nbsp; &amp;nbsp; fn parent&amp;lt;F, R&amp;gt;(&amp;amp;self, op: F) -&amp;gt; R&lt;br&gt;&amp;nbsp; &amp;nbsp; where&lt;br&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; F: FnOnce(&amp;amp;NodeRef&amp;lt;T, V&amp;gt;) -&amp;gt; R,&lt;br&gt;&amp;nbsp; &amp;nbsp; {&lt;br&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; op(self.borrow().parent())&lt;br&gt;&amp;nbsp; &amp;nbsp; }&lt;br&gt;&lt;br&gt;&amp;nbsp; &amp;nbsp; fn set_right(&amp;amp;self, node: NodeRef&amp;lt;T, V&amp;gt;) {&lt;br&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; let _ignore = self.borrow_mut().right.replace(node);&lt;br&gt;&amp;nbsp; &amp;nbsp; }&lt;br&gt;&lt;br&gt;&amp;nbsp; &amp;nbsp; fn set_max(&amp;amp;self, max: T) {&lt;br&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; let _ignore = self.borrow_mut().max.replace(max);&lt;br&gt;&amp;nbsp; &amp;nbsp; }&lt;br&gt;&amp;nbsp; &amp;nbsp; ...&lt;br&gt;}&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;RefCell is not ergonomic to use, but even worse, we need to use a lot of Rc::clone in our code, because when traversing a tree node from the top down, we need to hold the owned type of a node, not a reference. For example, in the previously mentioned INTERVAL-SEARCH operation, every time x = x.left or x = x.right, we first need to borrow x itself, and then assign it a value. So we need to get the owned type of the left (or right) node first, and then update x to the new value. This leads to a lot of reference counting overhead.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;How substantial is this overhead? I benchmarked our implementation using random data insertion and deletion on my local setup, which includes an Intel 13600KF processor with DDR4 memory.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;test bench_interval_tree_insert_100 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; &amp;nbsp; 9,821 ns/iter (+/- 263)&lt;br&gt;test bench_interval_tree_insert_1000&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; 215,362 ns/iter (+/- 6,536)&lt;br&gt;test bench_interval_tree_insert_10000 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; 2,999,694 ns/iter (+/- 134,979)&lt;br&gt;test bench_interval_tree_insert_remove_100&amp;nbsp; &amp;nbsp; ... bench:&amp;nbsp; &amp;nbsp; &amp;nbsp; 18,395 ns/iter (+/- 750)&lt;br&gt;test bench_interval_tree_insert_remove_1000 &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; 385,858 ns/iter (+/- 7,659)&lt;br&gt;test bench_interval_tree_insert_remove_10000&amp;nbsp; ... bench: &amp;nbsp; 5,465,355 ns/iter (+/- 114,735)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Using the same data and environment, compare it to the golang interval tree implementation of etcd.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;BenchmarkIntervalTreeInsert100-20 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 123747 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 12250 ns/op&lt;br&gt;BenchmarkIntervalTreeInsert1000-20&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 7119&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 189613 ns/op&lt;br&gt;BenchmarkIntervalTreeInsert10_000-20 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 340 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 3237907 ns/op&lt;br&gt;BenchmarkIntervalTreeInsertRemove100-20&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 24584 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 45579 ns/op&lt;br&gt;BenchmarkIntervalTreeInsertRemove1000-20 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 344 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 3462977 ns/op&lt;br&gt;BenchmarkIntervalTreeInsertRemove10_000-20 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 3 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 358284695 ns/op&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As you can see, our Rust implementation has no advantage, and even slows down insertion operations in some cases. (Note: there appears to be an issue with etcd‚Äôs node deletion implementation. Observe the increase in the number of nodes from 1000-&amp;gt;10000, the complexity may not align with O(log(n))).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Thread Safety Issues&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Even if we grudgingly accept the performance, a more serious problem surfaces: Rc&amp;lt;RefCell&amp;lt;T&amp;gt;&amp;gt; cannot be used in a multi-threaded environment! Since Xline is built on top of Rust‚Äôs Tokio runtime, it needs to share a single instance of the interval tree across multiple threads. Unfortunately, Rc itself is !Send, because reference counting inside Rc is incremented/decremented in a non-atomic way. This then results in the entire interval tree data structure not being sent to other threads. Unless we spawn a dedicated thread and communicate through a channel, we can‚Äôt use it in a multi-threaded environment.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Other Smart Pointers&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So we need to consider other smart pointers to resolve this issue. A natural idea is to use Arc&amp;lt;RefCell&amp;lt;T&amp;gt;&amp;gt;. However, RefCell itself is !Sync, because its borrow checking can only be used within a single thread and cannot be shared across multiple threads at the same time, and Arc&amp;lt;T&amp;gt;&amp;gt; is Send if and only if T is Sync, because Arc itself allows cloning.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Arc&amp;lt;Mutex&amp;lt;T&amp;gt;&amp;gt; ?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In a multi-threaded environment, multiple ownership can only be achieved using Arc&amp;lt;Mutex&amp;lt;T&amp;gt;&amp;gt;. However, this is clearly an anti-pattern for our use case, which requires a mutex on each node, and with hundreds of thousands or even millions of nodes in the tree, this is impractical.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;QCell&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;After using conventional methods to no avail, we tried to use a crate called qcell, a multi-threaded alternative to RefCell. The author ingeniously addressed the issue of borrowing checks under multiple ownership.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;QCell design&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Since the design of qcell is formally demonstrated in the GhostCell paper, I will introduce the design in the GhostCell paper here.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In Rust, permissions to manipulate data are tied to the data itself, i.e., you must first own the data in order to modify its state. Specifically, to modify the data T, you either need to have a T itself, or you need to have an &amp;amp;mut T.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The concept of GhostCell‚Äôs design is to separate permissions to manipulate data from the data itself, so that for a piece of data, the data T itself is a type, and its permissions are also of a specific type, denoted P_t. This design is more flexible than Rust‚Äôs existing design because it is possible for an instance of a permission type to have permission over a collection of data, i.e., a single P_t can have multiple T‚Äôs. Under this design, as long as the permission instance itself is thread-safe, the data collection it manages is also thread-safe.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To use this in a qcell, first create a QCellOwner to represent the permissions, and a QCell&amp;lt;T&amp;gt; to represent the stored data.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;let mut owner = QCellOwner::new();&lt;br&gt;let item = Arc::new(QCell::new(&amp;amp;owner, Vec::&amp;lt;u8&amp;gt;::new()));&lt;br&gt;owner.rw(&amp;amp;item).push(0);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;QCellOwner&amp;nbsp;has read/write access to the QCells registered to it (via&amp;nbsp;QCellOwner::rwor&amp;nbsp;QCellOwner::ro&amp;nbsp;), so as long as QCellOwner is thread-safe, the data in the QCells are thread-safe too. Here QCellOwner itself is Send + Sync, and QCell can be Send + Sync as long as T satisfies.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;impl&amp;lt;T: ?Sized + Send&amp;gt; Send for QCell&amp;lt;T&amp;gt;&lt;br&gt;impl&amp;lt;T: ?Sized + Send + Sync&amp;gt; Sync for QCell&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Using QCell&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Thanks to its design, QCell has a very low overhead (I won‚Äôt go into the details here) because it leverages the Rust type system so that borrow checking is done at compile time, whereas RefCell checks at runtime, so using QCell not only allows you to use it in multi-threaded environments, but also gives you a performance boost.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The next step is to apply QCell to our tree implementation. Since QCell only provides internal mutability, in order to be able to use multiple ownerships, we also need to have Arc, which looks like this.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;pub struct IntervalTree {&lt;br&gt;&amp;nbsp; &amp;nbsp; node_owner: QCellOwner,&lt;br&gt;&amp;nbsp; &amp;nbsp; ...&lt;br&gt;}&lt;br&gt;&lt;br&gt;struct NodeRef&amp;lt;T, V&amp;gt;(Arc&amp;lt;QCell&amp;lt;Node&amp;lt;T, V&amp;gt;&amp;gt;&amp;gt;);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Looking good, but how‚Äôs the performance?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;test bench_interval_tree_insert_100 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench:&amp;nbsp; &amp;nbsp; &amp;nbsp; 41,486 ns/iter (+/- 71)&lt;br&gt;test bench_interval_tree_insert_1000&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; 586,854 ns/iter (+/- 13,947)&lt;br&gt;test bench_interval_tree_insert_10000 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; 7,726,849 ns/iter (+/- 102,820)&lt;br&gt;test bench_interval_tree_insert_remove_100&amp;nbsp; &amp;nbsp; ... bench:&amp;nbsp; &amp;nbsp; &amp;nbsp; 75,569 ns/iter (+/- 325)&lt;br&gt;test bench_interval_tree_insert_remove_1000 &amp;nbsp; ... bench: &amp;nbsp; 1,135,232 ns/iter (+/- 7,539)&lt;br&gt;test bench_interval_tree_insert_remove_10000&amp;nbsp; ... bench:&amp;nbsp; 15,686,474 ns/iter (+/- 194,385)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Comparing the results of the previous tests, the performance drops by a factor of 1‚Äì3. This indicates that the biggest overhead is not the cell itself, but the reference counting, and in our interval tree case, using Arc is much slower than Rc.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;An alternative to using Arc would be arena allocation, which allocates memory for all objects at once and deallocates them all at once. However, this approach is unsuitable for a tree data structure because we need to allocate and deallocate node memory dynamically.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Array Analog Pointers&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Performance tests show that our attempts at smart pointers fail. Using smart pointers to implement tree structures within the Rust ownership model results in poor performance.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So can we implement it without using pointers? A natural idea is to use arrays to emulate pointers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Our tree structure is redesigned as follows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;pub struct IntervalTree {&lt;br&gt;&amp;nbsp; &amp;nbsp; nodes: Vec&amp;lt;Node&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; ...&lt;br&gt;}&lt;br&gt;&lt;br&gt;pub struct Node {&lt;br&gt;&amp;nbsp; &amp;nbsp; left: Option&amp;lt;u32&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; right: Option&amp;lt;u32&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; parent: Option&amp;lt;u32&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; ...&lt;br&gt;}&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As you can see, the advantage of arrays in Rust is that you don‚Äôt need to own a node, you just need to keep track of the index. Each time a new node is inserted, it is pushed one node after the nodes, and its index is nodes.len() ‚Äî 1.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Insertion is straightforward, but what about node deletion? A naive approach would be nulling the corresponding node in the Vec, then we are left with a ‚Äúhole‚Äù in our Vec. This requires maintaining extra states to keep track of this hole, so that we can reuse it for the next insertion. Moreover, this approach makes it difficult to reclaim the space of the nodes in the Vec, even if most of the nodes have already been deleted.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, how do we solve this issue? Inspired by the method used in petgraph, we can swap the node to be removed with the last node in the Vec before deleting it. This way, we can efficiently reclaim memory. Note that we also need to update the pointer to the node associated with the last node, since its position has changed. In the petgraph implementation, this operation could be time-consuming because a node may be connected to any number of other nodes. However, in our tree structure, we only need to update the parent, left child, and right child pointers of this node, making this an O(1) operation. This efficiently solves the node deletion problem.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Let‚Äôs benchmark our new implementation:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;test bench_interval_tree_insert_100 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; &amp;nbsp; 3,333 ns/iter (+/- 87)&lt;br&gt;test bench_interval_tree_insert_1000&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench:&amp;nbsp; &amp;nbsp; &amp;nbsp; 85,477 ns/iter (+/- 3,552)&lt;br&gt;test bench_interval_tree_insert_10000 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; 1,406,707 ns/iter (+/- 20,796)&lt;br&gt;test bench_interval_tree_insert_remove_100&amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; &amp;nbsp; 7,157 ns/iter (+/- 69)&lt;br&gt;test bench_interval_tree_insert_remove_1000 &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; 189,277 ns/iter (+/- 3,014)&lt;br&gt;test bench_interval_tree_insert_remove_10000&amp;nbsp; ... bench: &amp;nbsp; 3,060,029 ns/iter (+/- 50,829)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We can observe a huge performance gain with this implementation, about 1‚Äì2x faster than both the previous Rc&amp;lt;RefCell&amp;lt;Node&amp;gt;&amp;gt; approach and the golang implementation in etcd.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Using arrays to emulate pointers not only solves the ownership problem easily, but also makes it more cache friendly due to the contiguous memory layout of arrays, making it even more performant than using actual pointers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Summarizing&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;At this point, we have successfully implemented a interval tree using safe Rust. Through the various attempts described above, we found that using reference-counting smart pointers to implement tree or graph data structures in Rust is ineffective due to their unsuitability for memory-intensive operations. In the future, if I need to use safe Rust to implement pointer-like data structures, I would prefer to use arrays rather than smart pointers.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXc_7KDamNKUncNHOQCVmCMuEvNplvfHvc9h4p9bq3Y_iEzQAhHX5RIRuq_EvMlEQSf3F_cYWq-1cpMmDJCE-SbvMh4_8cqa_PQ7zp3p4CxT-_uFXeVTgx1uzh4Bc7yDMQn9B1XN63zuBa0k2vukkszFjaXWLesVxHwASp2gbCqZ9dbaZ9BR5kM?key=Iv6r86UKlwDAHVdsR8rx5g&#34; alt=&#34;Image with title &#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The Reason for Implementing Interval Trees&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In a recent refactoring of Xline, we identified a performance bottleneck caused by two data structures on the critical path: the Speculative Pool and the Uncommitted Pool. These two data structures are used for conflict detection in CURP. Specifically, the protocol requires for each processed command, it is necessary to find all the commands that conflict with the current command.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance, in the KV operations&amp;nbsp;put/get_range/delete_range, we need to consider the possible conflicts between these operations. Since each KV operation covers a range of keys, the problem becomes checking whether a given range of keys intersects with any key ranges in the pool. A plain traversal of the entire set results in a time complexity of O(n) for each query, which reduces efficiency and leads to performance bottlenecks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To address this problem, we need to introduce an interval tree data structure. Interval trees support efficient insertion, deletion and query operations on overlapping intervals, all of which can be completed in O(log(n)) time. Therefore, we can use the interval tree to maintain a collection of key ranges.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Introduction to Interval Tree Implementation&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The interval tree implementation in Xline is based on&amp;nbsp;Introduction to Algorithms (3rd ed.), which extends the self-balancing binary search tree.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The interval tree is based on a binary balanced tree (e.g., a red-black tree), and the interval itself is used as the key. For an interval [low, high], we first sort the intervals by their low values, and then by their high values if the low values are the same, establishing a total order relationship for the set of intervals. If duplicate intervals are not handled, sorting by high values is unnecessary. Additionally, for each node in the balanced tree, we maintain the maximum value of high in the subtree rooted at this node, denoted as max.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Insertion/Deletion&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Same as insertion/deletion in red-black tree, the worst time complexity is O(log(n)).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Query Overlap&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Given an interval i, we need to query the current tree to see if any intervals in the tree overlap with i.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The pseudo-code given in Introduction to Algorithms is as follows&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXf3FeKuovVTbM21lFBSW09-203bdtPYCg_9szdhZikolKFJwXBeWw_3oBpqxTdZKAqabNHuGJlgBgaTNyAY_0vChoKJqv3x2LItLuGDv2B_5i0THxWKLjLSMtxSwEAjyXh9y6wiWAlL0CQ13ISrt9wKj6fLLhLTrNcK0wFzBmFBfUZ1LBOIn5k?key=Iv6r86UKlwDAHVdsR8rx5g&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;With the definition of max, the solution to this problem becomes straightforward: for a subtree T_x rooted at x, if i does not intersect x_i, then i must lie to either the left or right of x_i.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;If i is on the left side of x_i, then we can rule out the right subtree, because i.high is smaller than x_i.low.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;if i is on the right side of x_i: In this case, we can‚Äôt rule out the left subtree, because the intervals of nodes in the left subtree may still intersect i. This is where max comes in handy.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If the maximum value of high in x‚Äôs left subtree is still less than i.low, then we can ignore x‚Äôs left subtree.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;If the maximum value of high in x‚Äôs left subtree is greater than or equal to i.low, then there must be intersecting intervals in x‚Äôs left subtree because all the lows in x‚Äôs left subtree are less than x_i.low, and i is on the right side of x_i, so all the lows in x‚Äôs left subtree are less than i.low, so there must be an intersection.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The above points can validate the correctness of the pseudo-code, and from the code we can infer that the worst-case time complexity of the query is O(log(n)).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Implementing Interval Trees with Safe Rust&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Difficulty&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To construct an interval tree, we first need to implement a red-black tree. In a red-black tree, each node must reference its parent node, which requires multiple ownership of a single node instance.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Rc&amp;lt;RefCell&amp;lt;T&amp;gt;&amp;gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I initially attempted to use Rust‚Äôs most common multi-ownership implementation, Rc&amp;lt;RefCell&amp;lt;T&amp;gt;&amp;gt;, with a tree node structure similar to the following code.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;struct Node&amp;lt;T, V&amp;gt; {&lt;br&gt;&amp;nbsp; &amp;nbsp; left: Option&amp;lt;NodeRef&amp;lt;T, V&amp;gt;&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; right: Option&amp;lt;NodeRef&amp;lt;T, V&amp;gt;&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; parent: Option&amp;lt;NodeRef&amp;lt;T, V&amp;gt;&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; ...&lt;br&gt;}&lt;br&gt;&lt;br&gt;struct NodeRef&amp;lt;T, V&amp;gt;(Rc&amp;lt;RefCell&amp;lt;Node&amp;lt;T, V&amp;gt;&amp;gt;&amp;gt;);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The data structure definition appears straightforward, but in practice it‚Äôs quite cumbersome to use, because RefCell requires the user to explicitly call borrow, or borrow_mut, requiring me to create numerous helper functions to simplify the implementation. Here are some examples.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;impl&amp;lt;T, V&amp;gt; NodeRef&amp;lt;T, V&amp;gt; {&lt;br&gt;&amp;nbsp; &amp;nbsp; fn left&amp;lt;F, R&amp;gt;(&amp;amp;self, op: F) -&amp;gt; R&lt;br&gt;&amp;nbsp; &amp;nbsp; where&lt;br&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; F: FnOnce(&amp;amp;NodeRef&amp;lt;T, V&amp;gt;) -&amp;gt; R,&lt;br&gt;&amp;nbsp; &amp;nbsp; {&lt;br&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; op(self.borrow().left())&lt;br&gt;&amp;nbsp; &amp;nbsp; }&lt;br&gt;&lt;br&gt;&amp;nbsp; &amp;nbsp; fn parent&amp;lt;F, R&amp;gt;(&amp;amp;self, op: F) -&amp;gt; R&lt;br&gt;&amp;nbsp; &amp;nbsp; where&lt;br&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; F: FnOnce(&amp;amp;NodeRef&amp;lt;T, V&amp;gt;) -&amp;gt; R,&lt;br&gt;&amp;nbsp; &amp;nbsp; {&lt;br&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; op(self.borrow().parent())&lt;br&gt;&amp;nbsp; &amp;nbsp; }&lt;br&gt;&lt;br&gt;&amp;nbsp; &amp;nbsp; fn set_right(&amp;amp;self, node: NodeRef&amp;lt;T, V&amp;gt;) {&lt;br&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; let _ignore = self.borrow_mut().right.replace(node);&lt;br&gt;&amp;nbsp; &amp;nbsp; }&lt;br&gt;&lt;br&gt;&amp;nbsp; &amp;nbsp; fn set_max(&amp;amp;self, max: T) {&lt;br&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; let _ignore = self.borrow_mut().max.replace(max);&lt;br&gt;&amp;nbsp; &amp;nbsp; }&lt;br&gt;&amp;nbsp; &amp;nbsp; ...&lt;br&gt;}&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;RefCell is not ergonomic to use, but even worse, we need to use a lot of Rc::clone in our code, because when traversing a tree node from the top down, we need to hold the owned type of a node, not a reference. For example, in the previously mentioned INTERVAL-SEARCH operation, every time x = x.left or x = x.right, we first need to borrow x itself, and then assign it a value. So we need to get the owned type of the left (or right) node first, and then update x to the new value. This leads to a lot of reference counting overhead.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;How substantial is this overhead? I benchmarked our implementation using random data insertion and deletion on my local setup, which includes an Intel 13600KF processor with DDR4 memory.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;test bench_interval_tree_insert_100 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; &amp;nbsp; 9,821 ns/iter (+/- 263)&lt;br&gt;test bench_interval_tree_insert_1000&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; 215,362 ns/iter (+/- 6,536)&lt;br&gt;test bench_interval_tree_insert_10000 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; 2,999,694 ns/iter (+/- 134,979)&lt;br&gt;test bench_interval_tree_insert_remove_100&amp;nbsp; &amp;nbsp; ... bench:&amp;nbsp; &amp;nbsp; &amp;nbsp; 18,395 ns/iter (+/- 750)&lt;br&gt;test bench_interval_tree_insert_remove_1000 &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; 385,858 ns/iter (+/- 7,659)&lt;br&gt;test bench_interval_tree_insert_remove_10000&amp;nbsp; ... bench: &amp;nbsp; 5,465,355 ns/iter (+/- 114,735)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Using the same data and environment, compare it to the golang interval tree implementation of etcd.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;BenchmarkIntervalTreeInsert100-20 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 123747 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 12250 ns/op&lt;br&gt;BenchmarkIntervalTreeInsert1000-20&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 7119&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 189613 ns/op&lt;br&gt;BenchmarkIntervalTreeInsert10_000-20 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 340 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 3237907 ns/op&lt;br&gt;BenchmarkIntervalTreeInsertRemove100-20&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 24584 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 45579 ns/op&lt;br&gt;BenchmarkIntervalTreeInsertRemove1000-20 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 344 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 3462977 ns/op&lt;br&gt;BenchmarkIntervalTreeInsertRemove10_000-20 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 3 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 358284695 ns/op&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As you can see, our Rust implementation has no advantage, and even slows down insertion operations in some cases. (Note: there appears to be an issue with etcd‚Äôs node deletion implementation. Observe the increase in the number of nodes from 1000-&amp;gt;10000, the complexity may not align with O(log(n))).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Thread Safety Issues&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Even if we grudgingly accept the performance, a more serious problem surfaces: Rc&amp;lt;RefCell&amp;lt;T&amp;gt;&amp;gt; cannot be used in a multi-threaded environment! Since Xline is built on top of Rust‚Äôs Tokio runtime, it needs to share a single instance of the interval tree across multiple threads. Unfortunately, Rc itself is !Send, because reference counting inside Rc is incremented/decremented in a non-atomic way. This then results in the entire interval tree data structure not being sent to other threads. Unless we spawn a dedicated thread and communicate through a channel, we can‚Äôt use it in a multi-threaded environment.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Other Smart Pointers&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So we need to consider other smart pointers to resolve this issue. A natural idea is to use Arc&amp;lt;RefCell&amp;lt;T&amp;gt;&amp;gt;. However, RefCell itself is !Sync, because its borrow checking can only be used within a single thread and cannot be shared across multiple threads at the same time, and Arc&amp;lt;T&amp;gt;&amp;gt; is Send if and only if T is Sync, because Arc itself allows cloning.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Arc&amp;lt;Mutex&amp;lt;T&amp;gt;&amp;gt; ?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In a multi-threaded environment, multiple ownership can only be achieved using Arc&amp;lt;Mutex&amp;lt;T&amp;gt;&amp;gt;. However, this is clearly an anti-pattern for our use case, which requires a mutex on each node, and with hundreds of thousands or even millions of nodes in the tree, this is impractical.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;QCell&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;After using conventional methods to no avail, we tried to use a crate called qcell, a multi-threaded alternative to RefCell. The author ingeniously addressed the issue of borrowing checks under multiple ownership.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;QCell design&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Since the design of qcell is formally demonstrated in the GhostCell paper, I will introduce the design in the GhostCell paper here.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In Rust, permissions to manipulate data are tied to the data itself, i.e., you must first own the data in order to modify its state. Specifically, to modify the data T, you either need to have a T itself, or you need to have an &amp;amp;mut T.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The concept of GhostCell‚Äôs design is to separate permissions to manipulate data from the data itself, so that for a piece of data, the data T itself is a type, and its permissions are also of a specific type, denoted P_t. This design is more flexible than Rust‚Äôs existing design because it is possible for an instance of a permission type to have permission over a collection of data, i.e., a single P_t can have multiple T‚Äôs. Under this design, as long as the permission instance itself is thread-safe, the data collection it manages is also thread-safe.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To use this in a qcell, first create a QCellOwner to represent the permissions, and a QCell&amp;lt;T&amp;gt; to represent the stored data.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;let mut owner = QCellOwner::new();&lt;br&gt;let item = Arc::new(QCell::new(&amp;amp;owner, Vec::&amp;lt;u8&amp;gt;::new()));&lt;br&gt;owner.rw(&amp;amp;item).push(0);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;QCellOwner&amp;nbsp;has read/write access to the QCells registered to it (via&amp;nbsp;QCellOwner::rwor&amp;nbsp;QCellOwner::ro&amp;nbsp;), so as long as QCellOwner is thread-safe, the data in the QCells are thread-safe too. Here QCellOwner itself is Send + Sync, and QCell can be Send + Sync as long as T satisfies.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;impl&amp;lt;T: ?Sized + Send&amp;gt; Send for QCell&amp;lt;T&amp;gt;&lt;br&gt;impl&amp;lt;T: ?Sized + Send + Sync&amp;gt; Sync for QCell&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Using QCell&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Thanks to its design, QCell has a very low overhead (I won‚Äôt go into the details here) because it leverages the Rust type system so that borrow checking is done at compile time, whereas RefCell checks at runtime, so using QCell not only allows you to use it in multi-threaded environments, but also gives you a performance boost.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The next step is to apply QCell to our tree implementation. Since QCell only provides internal mutability, in order to be able to use multiple ownerships, we also need to have Arc, which looks like this.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;pub struct IntervalTree {&lt;br&gt;&amp;nbsp; &amp;nbsp; node_owner: QCellOwner,&lt;br&gt;&amp;nbsp; &amp;nbsp; ...&lt;br&gt;}&lt;br&gt;&lt;br&gt;struct NodeRef&amp;lt;T, V&amp;gt;(Arc&amp;lt;QCell&amp;lt;Node&amp;lt;T, V&amp;gt;&amp;gt;&amp;gt;);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Looking good, but how‚Äôs the performance?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;test bench_interval_tree_insert_100 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench:&amp;nbsp; &amp;nbsp; &amp;nbsp; 41,486 ns/iter (+/- 71)&lt;br&gt;test bench_interval_tree_insert_1000&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; 586,854 ns/iter (+/- 13,947)&lt;br&gt;test bench_interval_tree_insert_10000 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; 7,726,849 ns/iter (+/- 102,820)&lt;br&gt;test bench_interval_tree_insert_remove_100&amp;nbsp; &amp;nbsp; ... bench:&amp;nbsp; &amp;nbsp; &amp;nbsp; 75,569 ns/iter (+/- 325)&lt;br&gt;test bench_interval_tree_insert_remove_1000 &amp;nbsp; ... bench: &amp;nbsp; 1,135,232 ns/iter (+/- 7,539)&lt;br&gt;test bench_interval_tree_insert_remove_10000&amp;nbsp; ... bench:&amp;nbsp; 15,686,474 ns/iter (+/- 194,385)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Comparing the results of the previous tests, the performance drops by a factor of 1‚Äì3. This indicates that the biggest overhead is not the cell itself, but the reference counting, and in our interval tree case, using Arc is much slower than Rc.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;An alternative to using Arc would be arena allocation, which allocates memory for all objects at once and deallocates them all at once. However, this approach is unsuitable for a tree data structure because we need to allocate and deallocate node memory dynamically.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Array Analog Pointers&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Performance tests show that our attempts at smart pointers fail. Using smart pointers to implement tree structures within the Rust ownership model results in poor performance.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So can we implement it without using pointers? A natural idea is to use arrays to emulate pointers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Our tree structure is redesigned as follows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;pub struct IntervalTree {&lt;br&gt;&amp;nbsp; &amp;nbsp; nodes: Vec&amp;lt;Node&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; ...&lt;br&gt;}&lt;br&gt;&lt;br&gt;pub struct Node {&lt;br&gt;&amp;nbsp; &amp;nbsp; left: Option&amp;lt;u32&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; right: Option&amp;lt;u32&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; parent: Option&amp;lt;u32&amp;gt;,&lt;br&gt;&amp;nbsp; &amp;nbsp; ...&lt;br&gt;}&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As you can see, the advantage of arrays in Rust is that you don‚Äôt need to own a node, you just need to keep track of the index. Each time a new node is inserted, it is pushed one node after the nodes, and its index is nodes.len() ‚Äî 1.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Insertion is straightforward, but what about node deletion? A naive approach would be nulling the corresponding node in the Vec, then we are left with a ‚Äúhole‚Äù in our Vec. This requires maintaining extra states to keep track of this hole, so that we can reuse it for the next insertion. Moreover, this approach makes it difficult to reclaim the space of the nodes in the Vec, even if most of the nodes have already been deleted.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, how do we solve this issue? Inspired by the method used in petgraph, we can swap the node to be removed with the last node in the Vec before deleting it. This way, we can efficiently reclaim memory. Note that we also need to update the pointer to the node associated with the last node, since its position has changed. In the petgraph implementation, this operation could be time-consuming because a node may be connected to any number of other nodes. However, in our tree structure, we only need to update the parent, left child, and right child pointers of this node, making this an O(1) operation. This efficiently solves the node deletion problem.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Let‚Äôs benchmark our new implementation:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;test bench_interval_tree_insert_100 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; &amp;nbsp; 3,333 ns/iter (+/- 87)&lt;br&gt;test bench_interval_tree_insert_1000&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench:&amp;nbsp; &amp;nbsp; &amp;nbsp; 85,477 ns/iter (+/- 3,552)&lt;br&gt;test bench_interval_tree_insert_10000 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; 1,406,707 ns/iter (+/- 20,796)&lt;br&gt;test bench_interval_tree_insert_remove_100&amp;nbsp; &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; &amp;nbsp; 7,157 ns/iter (+/- 69)&lt;br&gt;test bench_interval_tree_insert_remove_1000 &amp;nbsp; ... bench: &amp;nbsp; &amp;nbsp; 189,277 ns/iter (+/- 3,014)&lt;br&gt;test bench_interval_tree_insert_remove_10000&amp;nbsp; ... bench: &amp;nbsp; 3,060,029 ns/iter (+/- 50,829)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We can observe a huge performance gain with this implementation, about 1‚Äì2x faster than both the previous Rc&amp;lt;RefCell&amp;lt;Node&amp;gt;&amp;gt; approach and the golang implementation in etcd.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Using arrays to emulate pointers not only solves the ownership problem easily, but also makes it more cache friendly due to the contiguous memory layout of arrays, making it even more performant than using actual pointers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Summarizing&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;At this point, we have successfully implemented a interval tree using safe Rust. Through the various attempts described above, we found that using reference-counting smart pointers to implement tree or graph data structures in Rust is ineffective due to their unsuitability for memory-intensive operations. In the future, if I need to use safe Rust to implement pointer-like data structures, I would prefer to use arrays rather than smart pointers.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Tue, 16 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêOrganising the first KCD Hyderabad ‚Äì my amazing experience„ÄëÁªÑÁªáÁ¨¨‰∏ÄÂ±ä KCD Êµ∑ÂæóÊãâÂ∑¥‚Äî‚ÄîÊàëÁöÑÂ•áÂ¶ôÁªèÂéÜ</title>
      <link>https://www.cncf.io/blog/2024/07/15/organising-the-first-kcd-hyderabad-my-amazing-experience/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;KCD post originally published on &lt;a href=&#34;https://socialmaharaj.com/2024/06/29/first-kcd-hyderabad-experience/&#34;&gt;Social Maharaj &lt;/a&gt;by Atulpriya Sharma&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Being a food and travel blogger, I often attend a lot of food meet-ups where I get to experience different dishes and meet new people as well.&amp;nbsp;&lt;strong&gt;But did you know there are a lot of tech events too?&lt;/strong&gt;&amp;nbsp;Yes, if you asked me a couple of years ago, I would have stared at you like a wall. But today, things are different.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Looking at the cloud native landscape, there are a lot of local, regional, national and international conferences and meet-ups that are organised. Locally, we have&amp;nbsp;&lt;a href=&#34;https://community.cncf.io/hyderabad/&#34;&gt;CNCF Hyderabad&lt;/a&gt;&amp;nbsp;meet-ups that I organise monthly, regionally there are bigger conferences like&amp;nbsp;&lt;strong&gt;Kubernetes Community Days (KCDs)&amp;nbsp;&lt;/strong&gt;that happen at the city level KCD Bengaluru, KCD Chennai, KCD Hyderabad etc., nationally there are&amp;nbsp;&lt;strong&gt;KubeDay&lt;/strong&gt;&amp;nbsp;events&amp;nbsp;&lt;em&gt;(the last one in India was in Bengaluru last year)&amp;nbsp;&lt;/em&gt;and international ones are KubeCon. Fortunately, I got to attend&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2023/11/21/my-kubecon-chicago-2023-experience/&#34;&gt;KubeCon Chicago&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2024/04/06/kubecon-paris-2024-experience/&#34;&gt;KubeCon Paris&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the last couple of years that I‚Äôve been in the cloud native space, I‚Äôve attended and spoken at a lot of meet-ups and conferences be it KubeCon or KCDs. But I was never on the organising side of things. However, over the last 6 months or so I have been busy with a couple of other folks to organise Hyderabad‚Äôs First KCD event.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1024&#34; height=&#34;576&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-3.jpg&#34; alt=&#34;Atulpriya giving a talk&#34; class=&#34;wp-image-114131&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-3.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-300x169.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-768x432.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-900x506.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-356x200.jpg 356w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-711x400.jpg 711w&#34; sizes=&#34;(max-width: 1024px) 100vw, 1024px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We concluded that on June 22nd, so you can get a glimpse of what happened by browsing through the&amp;nbsp;&lt;a href=&#34;https://twitter.com/hashtag/KCDHyderabad?src=hashtag_click&#34;&gt;#KCDHyderabad&lt;/a&gt;&amp;nbsp;hashtag.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this blog post, I‚Äôll talk about my experience of organising the first KCD Hyderabad, things that I learnt, processes and mistakes that I made.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Preparing for KCD Hyderabad&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Every year,&amp;nbsp;&lt;strong&gt;we can host only 3 KCDs in India&lt;/strong&gt;, so the first one was in Kochi followed by Pune and finally ending the year with KCD Hyderabad. I was a speaker at KCD Kerala and KCD Pune and one of the organisers for KCD Hyderabad.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It all started way back in May 2023 when some of our community members reached out asking if we would ever have a KCD Hyderabad and Hyderabad becoming one of the tech hubs in the country, we thought we must plan and have one. So a GitHub issue was raised in May 2023 and what followed was weekly calls with other interested organisers to prepare, plan and execute.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Finalising the Date&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;June 22, was the date that was reserved for KCD Hyderabad. Internally we spent a week or so to figure out the perfect date considering the holidays, schools, festivals etc. to ensure we reach the maximum people.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Further, the next question was whether we wanted to do it as a multiple-day conference with multiple rooms or a single-day event. Since this was the first time we were doing it, we tried to keep it to one day only.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Venue&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Another big ticket item is the venue. Most other KCDs in India took place at star hotels and convention centres. Being part of the F&amp;amp;B industry, I had connections at&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2021/08/04/staycation-in-hyderabad-novotel-hicc/&#34;&gt;Novotel HICC&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2021/10/16/turquoise-le-meridien-hyderabad-review/&#34;&gt;Le Meridien&lt;/a&gt;‚Äôs and other similar hotels. However, even with the connections and discounts, the amount that was quoted was way too much. So we decided to not go for such properties and instead look for smaller venues that weren‚Äôt too costly but also could accommodate a decent crowd.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://pbs.twimg.com/media/GQpM2j9XYAE-3z8?format=jpg&amp;amp;name=large&#34; alt=&#34;T-Hub - venue for KCD Hyderabad&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Eventually after scouting for places, we settled for&amp;nbsp;&lt;strong&gt;T-Hub&lt;/strong&gt;&amp;nbsp;opposite Ikea. It‚Äôs among the largest incubators in the world and is home to a lot of start-ups. They have multiple spaces that they rent out for events. I had attended meet-ups at T-Hub in the past, so was aware of the space. We eventually decided to go with the two rooms in T-Hub that would accommodate 400-430 people.&amp;nbsp; With this, we were now clear on the number of people we could allow.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Sponsors&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Further, we had to look for sponsors who would sponsor the conference. This is a totally new experience for me. We were reaching out to folks on LinkedIn, Twitter, Email and all other mediums to reach out to folks who might be interested in sponsoring the event.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Luckily we found quite a few folks who were interested in sponsoring the event. By the time we were close to the event day, we had to stop taking sponsorships ‚Äì not because we didn‚Äôt need the money but because we couldn‚Äôt fulfil the requirements due to logistical constraints.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Speakers and Agenda&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The most important parts of a tech conference are the speakers and the agenda. People attend conferences to learn and network and that meant we had to carefully choose the speakers and experts that were going to attend KCD Hyderabad.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We opened the&amp;nbsp;&lt;strong&gt;Call For Proposals (CFPs)&lt;/strong&gt;&amp;nbsp;a few months before the event and to our surprise, we had 180 topics that were submitted by over 100 unique speakers. To give you a context, for a single-day event, considering 20 mins for each talk and multiple breaks, we can at max accommodate 10-12 talks.&amp;nbsp;Further, we also had&amp;nbsp;&lt;strong&gt;Mr. Jayesh Ranjan, IAS&lt;/strong&gt;&amp;nbsp;who was the chief guest for the event.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://pbs.twimg.com/media/GQqsxcEXAAALSG3?format=jpg&amp;amp;name=medium&#34; alt=&#34;Jayesh Ranjan, IAS - Chief Guest at KCD Hyderabad&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;With 180 talks, it was a herculean task to review all of them. So I reached out to a few experts in the community to help us review the CPFs and they also took a week to finalise the top talks. With the talks finalised, we had to publish the agenda and inform all the speakers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But guess what? Not all selected speakers respond or can join the event, so we need to have backup speakers as well. Quite a task this was.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Swags&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Another crucial part of any conference is swags and food. Swags are things that the attendees get when they attend a conference. Since KCD Hyderabad like others was a paid event, we had to give them something. So we spent a few weeks figuring out what we could give them.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://media.licdn.com/dms/image/D5622AQGpCw4llMs3Eg/feedshare-shrink_2048_1536/0/1719062626637?e=1722470400&amp;amp;v=beta&amp;amp;t=Jmv_egcrrhfuzMcKNtMDZ171IwE-0xwGrrtKlFHHBf4&#34; alt=&#34;KCD Hyderabad Swags&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Eventually, we settled for a&amp;nbsp;&lt;strong&gt;customised T-shirt, an umbrella, a tote bag&amp;nbsp;&lt;/strong&gt;and&lt;strong&gt;&amp;nbsp;laptop stickers&lt;/strong&gt;. To add to this, we additionally gave a&amp;nbsp;&lt;strong&gt;&lt;a href=&#34;https://amzn.to/3XGbBbV&#34;&gt;Logitech K480 keyboard&lt;/a&gt;&lt;/strong&gt;&amp;nbsp;to all our speakers and a&amp;nbsp;&lt;strong&gt;Portronics Freedom Wireless Charger&lt;/strong&gt;&amp;nbsp;to all the volunteers. Finalising the items was easy, but customising them was a task. I remember visiting local vendors to check the quality of T-shirts, bags and everything else.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Food&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Food is another critical thing. Since we were expecting around 400 people with a lot of people coming from outside of Hyderabad, I was adamant about having a&amp;nbsp;&lt;strong&gt;good Biryani for lunch&lt;/strong&gt;. Further, being the Maharaj, I had to ensure we gave the attendees a taste of Hyderabad. Again, I hunted for eateries and caterers who could give me that experience without burning a hole in my pocket.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I spent a couple of weekends, just visiting restaurants and caterers to finalise them. Eventually, we stuck with&amp;nbsp;&lt;strong&gt;Deccan Kitchen&lt;/strong&gt;, a popular restaurant and caterer in Hyderabad and I guess we did a good job when it came to food. People loved the food and Biryani especially, so I was relieved.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So these were some of the major elements that we had to look at while organising the first KCD in Hyderabad. Over the last 6 months or so, we split our responsibilities to focus on things that we‚Äôre good at and started working towards it. Hundreds of Google meets and calls, in-person meets and eventually, KCD Hyderabad was a success! With over&amp;nbsp;&lt;strong&gt;400 people in attendance from across the globe&lt;/strong&gt;, it was quite an event.&amp;nbsp; You can get a glimpse of what happened by browsing through the&amp;nbsp;&lt;a href=&#34;https://twitter.com/hashtag/KCDHyderabad?src=hashtag_click&#34;&gt;#KCDHyderabad&amp;nbsp;&lt;/a&gt;hashtag.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But as they say, there‚Äôs learning in everything. Organising an event of a scale of KCD isn‚Äôt easy. There are so many things and people involved that you need to ensure that everything is well-planned and executed smoothly. Here are my 5 key learnings after organizing KCD Hyderabad.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;5 Key Learnings&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Planning and preparation are crucial&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I learned that thorough planning is essential. We started preparing months in advance, from choosing the date to finding the right venue. We had to consider holidays, school schedules, and festivals to maximize attendance. Every detail, from deciding on a single-day event to selecting the venue, required careful thought and discussion among organizers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Further,&amp;nbsp;&lt;strong&gt;things don‚Äôt work as planned&lt;/strong&gt;. From spending the entire night at the venue to ensure the event setup was done, to only realising that even on the day of the event we were still not complete. So, always be prepared for eventualities, they will come from nowhere.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Budget management is a balancing act&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Managing the budget was challenging. We had to make smart choices to keep costs down without compromising on quality. For example, instead of expensive hotels, we chose T-Hub, which was more affordable but still accommodated our needs. We also had to be creative with sponsorships, reaching out through various channels to secure funding while&amp;nbsp;&lt;strong&gt;being careful not to over-commit&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Content curation is both exciting and challenging&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Selecting speakers and creating the agenda was one of the most crucial and difficult tasks. We received 180 talk proposals for only 10-12 available slots. It took a team of experts to review and select the best ones.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I learned that it‚Äôs important to have backup speakers too, as not all selected speakers can always make it. Further, with most sponsors wanting a slot to speak, it‚Äôs difficult to maintain the balance of community vs sponsored talks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Attention to attendee experience pays off&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I realized how important it is to focus on the attendee experience. This included carefully choosing swag items like customized T-shirts, umbrellas, and laptop stickers. We also put a lot of effort into selecting good food, especially ensuring we had great biryani to showcase Hyderabad‚Äôs cuisine.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The positive feedback on these aspects was really rewarding. Sure there were a few niggles where some people told the queues were too long and things like that, but overall people were quite satisfied.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Teamwork and delegation are key&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Organizing an event of this scale taught me the importance of teamwork and delegation. Over six months, we divided responsibilities based on our strengths. It involved countless meetings, calls, and in-person discussions. I learned that success depends on everyone working together and focusing on their specific tasks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We also looked out for volunteers who could help. A handful of them did turn up eventually and took care of a lot of things on D-day. I felt, we could have leveraged their support a little earlier in the preparation cycle as well.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;When Is The Next KCD Hyderabad?&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Reflecting on our successful first KCD Hyderabad, we‚Äôve learned valuable lessons about thorough planning, budget management, content curation, attendee experience, and teamwork. These insights will be crucial for future organizers.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As for the next KCD Hyderabad, while I‚Äôd love to announce a date, it‚Äôs not that simple. With only 3 KCDs allowed annually in India, we rotate between cities to reach different communities. Having said that, I‚Äôm optimistic about its return given the fantastic response.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the meantime, stay connected with the&amp;nbsp;&lt;a href=&#34;https://community.cncf.io/hyderabad/&#34;&gt;CNCF Hyderabad community&lt;/a&gt;&amp;nbsp;and watch for KCDs in other Indian cities. Thank you to everyone who made our inaugural event special. When KCD Hyderabad returns, we‚Äôll apply these learnings to make it even better.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;That‚Äôs it for this blog post. I hope you found this helpful to plan an event like this one in your city. Feel free to reach out to me for any assistance. Drop your thoughts in the comments below, tweet to me at&amp;nbsp;&lt;a href=&#34;https://twitter.com/Atulmaharaj&#34;&gt;@Atulmaharaj&lt;/a&gt;, DM on&amp;nbsp;&lt;a href=&#34;https://instagram.com/Atulmaharaj&#34;&gt;Atulmaharaj on Instagram&lt;/a&gt;, or&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/get-in-touch/&#34;&gt;Get In Touch&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;KCD post originally published on &lt;a href=&#34;https://socialmaharaj.com/2024/06/29/first-kcd-hyderabad-experience/&#34;&gt;Social Maharaj &lt;/a&gt;by Atulpriya Sharma&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Being a food and travel blogger, I often attend a lot of food meet-ups where I get to experience different dishes and meet new people as well.&amp;nbsp;&lt;strong&gt;But did you know there are a lot of tech events too?&lt;/strong&gt;&amp;nbsp;Yes, if you asked me a couple of years ago, I would have stared at you like a wall. But today, things are different.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Looking at the cloud native landscape, there are a lot of local, regional, national and international conferences and meet-ups that are organised. Locally, we have&amp;nbsp;&lt;a href=&#34;https://community.cncf.io/hyderabad/&#34;&gt;CNCF Hyderabad&lt;/a&gt;&amp;nbsp;meet-ups that I organise monthly, regionally there are bigger conferences like&amp;nbsp;&lt;strong&gt;Kubernetes Community Days (KCDs)&amp;nbsp;&lt;/strong&gt;that happen at the city level KCD Bengaluru, KCD Chennai, KCD Hyderabad etc., nationally there are&amp;nbsp;&lt;strong&gt;KubeDay&lt;/strong&gt;&amp;nbsp;events&amp;nbsp;&lt;em&gt;(the last one in India was in Bengaluru last year)&amp;nbsp;&lt;/em&gt;and international ones are KubeCon. Fortunately, I got to attend&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2023/11/21/my-kubecon-chicago-2023-experience/&#34;&gt;KubeCon Chicago&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2024/04/06/kubecon-paris-2024-experience/&#34;&gt;KubeCon Paris&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the last couple of years that I‚Äôve been in the cloud native space, I‚Äôve attended and spoken at a lot of meet-ups and conferences be it KubeCon or KCDs. But I was never on the organising side of things. However, over the last 6 months or so I have been busy with a couple of other folks to organise Hyderabad‚Äôs First KCD event.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1024&#34; height=&#34;576&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-3.jpg&#34; alt=&#34;Atulpriya giving a talk&#34; class=&#34;wp-image-114131&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-3.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-300x169.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-768x432.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-900x506.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-356x200.jpg 356w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-711x400.jpg 711w&#34; sizes=&#34;(max-width: 1024px) 100vw, 1024px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We concluded that on June 22nd, so you can get a glimpse of what happened by browsing through the&amp;nbsp;&lt;a href=&#34;https://twitter.com/hashtag/KCDHyderabad?src=hashtag_click&#34;&gt;#KCDHyderabad&lt;/a&gt;&amp;nbsp;hashtag.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this blog post, I‚Äôll talk about my experience of organising the first KCD Hyderabad, things that I learnt, processes and mistakes that I made.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Preparing for KCD Hyderabad&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Every year,&amp;nbsp;&lt;strong&gt;we can host only 3 KCDs in India&lt;/strong&gt;, so the first one was in Kochi followed by Pune and finally ending the year with KCD Hyderabad. I was a speaker at KCD Kerala and KCD Pune and one of the organisers for KCD Hyderabad.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It all started way back in May 2023 when some of our community members reached out asking if we would ever have a KCD Hyderabad and Hyderabad becoming one of the tech hubs in the country, we thought we must plan and have one. So a GitHub issue was raised in May 2023 and what followed was weekly calls with other interested organisers to prepare, plan and execute.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Finalising the Date&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;June 22, was the date that was reserved for KCD Hyderabad. Internally we spent a week or so to figure out the perfect date considering the holidays, schools, festivals etc. to ensure we reach the maximum people.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Further, the next question was whether we wanted to do it as a multiple-day conference with multiple rooms or a single-day event. Since this was the first time we were doing it, we tried to keep it to one day only.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Venue&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Another big ticket item is the venue. Most other KCDs in India took place at star hotels and convention centres. Being part of the F&amp;amp;B industry, I had connections at&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2021/08/04/staycation-in-hyderabad-novotel-hicc/&#34;&gt;Novotel HICC&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2021/10/16/turquoise-le-meridien-hyderabad-review/&#34;&gt;Le Meridien&lt;/a&gt;‚Äôs and other similar hotels. However, even with the connections and discounts, the amount that was quoted was way too much. So we decided to not go for such properties and instead look for smaller venues that weren‚Äôt too costly but also could accommodate a decent crowd.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://pbs.twimg.com/media/GQpM2j9XYAE-3z8?format=jpg&amp;amp;name=large&#34; alt=&#34;T-Hub - venue for KCD Hyderabad&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Eventually after scouting for places, we settled for&amp;nbsp;&lt;strong&gt;T-Hub&lt;/strong&gt;&amp;nbsp;opposite Ikea. It‚Äôs among the largest incubators in the world and is home to a lot of start-ups. They have multiple spaces that they rent out for events. I had attended meet-ups at T-Hub in the past, so was aware of the space. We eventually decided to go with the two rooms in T-Hub that would accommodate 400-430 people.&amp;nbsp; With this, we were now clear on the number of people we could allow.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Sponsors&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Further, we had to look for sponsors who would sponsor the conference. This is a totally new experience for me. We were reaching out to folks on LinkedIn, Twitter, Email and all other mediums to reach out to folks who might be interested in sponsoring the event.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Luckily we found quite a few folks who were interested in sponsoring the event. By the time we were close to the event day, we had to stop taking sponsorships ‚Äì not because we didn‚Äôt need the money but because we couldn‚Äôt fulfil the requirements due to logistical constraints.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Speakers and Agenda&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The most important parts of a tech conference are the speakers and the agenda. People attend conferences to learn and network and that meant we had to carefully choose the speakers and experts that were going to attend KCD Hyderabad.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We opened the&amp;nbsp;&lt;strong&gt;Call For Proposals (CFPs)&lt;/strong&gt;&amp;nbsp;a few months before the event and to our surprise, we had 180 topics that were submitted by over 100 unique speakers. To give you a context, for a single-day event, considering 20 mins for each talk and multiple breaks, we can at max accommodate 10-12 talks.&amp;nbsp;Further, we also had&amp;nbsp;&lt;strong&gt;Mr. Jayesh Ranjan, IAS&lt;/strong&gt;&amp;nbsp;who was the chief guest for the event.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://pbs.twimg.com/media/GQqsxcEXAAALSG3?format=jpg&amp;amp;name=medium&#34; alt=&#34;Jayesh Ranjan, IAS - Chief Guest at KCD Hyderabad&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;With 180 talks, it was a herculean task to review all of them. So I reached out to a few experts in the community to help us review the CPFs and they also took a week to finalise the top talks. With the talks finalised, we had to publish the agenda and inform all the speakers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But guess what? Not all selected speakers respond or can join the event, so we need to have backup speakers as well. Quite a task this was.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Swags&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Another crucial part of any conference is swags and food. Swags are things that the attendees get when they attend a conference. Since KCD Hyderabad like others was a paid event, we had to give them something. So we spent a few weeks figuring out what we could give them.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://media.licdn.com/dms/image/D5622AQGpCw4llMs3Eg/feedshare-shrink_2048_1536/0/1719062626637?e=1722470400&amp;amp;v=beta&amp;amp;t=Jmv_egcrrhfuzMcKNtMDZ171IwE-0xwGrrtKlFHHBf4&#34; alt=&#34;KCD Hyderabad Swags&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Eventually, we settled for a&amp;nbsp;&lt;strong&gt;customised T-shirt, an umbrella, a tote bag&amp;nbsp;&lt;/strong&gt;and&lt;strong&gt;&amp;nbsp;laptop stickers&lt;/strong&gt;. To add to this, we additionally gave a&amp;nbsp;&lt;strong&gt;&lt;a href=&#34;https://amzn.to/3XGbBbV&#34;&gt;Logitech K480 keyboard&lt;/a&gt;&lt;/strong&gt;&amp;nbsp;to all our speakers and a&amp;nbsp;&lt;strong&gt;Portronics Freedom Wireless Charger&lt;/strong&gt;&amp;nbsp;to all the volunteers. Finalising the items was easy, but customising them was a task. I remember visiting local vendors to check the quality of T-shirts, bags and everything else.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Food&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Food is another critical thing. Since we were expecting around 400 people with a lot of people coming from outside of Hyderabad, I was adamant about having a&amp;nbsp;&lt;strong&gt;good Biryani for lunch&lt;/strong&gt;. Further, being the Maharaj, I had to ensure we gave the attendees a taste of Hyderabad. Again, I hunted for eateries and caterers who could give me that experience without burning a hole in my pocket.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I spent a couple of weekends, just visiting restaurants and caterers to finalise them. Eventually, we stuck with&amp;nbsp;&lt;strong&gt;Deccan Kitchen&lt;/strong&gt;, a popular restaurant and caterer in Hyderabad and I guess we did a good job when it came to food. People loved the food and Biryani especially, so I was relieved.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So these were some of the major elements that we had to look at while organising the first KCD in Hyderabad. Over the last 6 months or so, we split our responsibilities to focus on things that we‚Äôre good at and started working towards it. Hundreds of Google meets and calls, in-person meets and eventually, KCD Hyderabad was a success! With over&amp;nbsp;&lt;strong&gt;400 people in attendance from across the globe&lt;/strong&gt;, it was quite an event.&amp;nbsp; You can get a glimpse of what happened by browsing through the&amp;nbsp;&lt;a href=&#34;https://twitter.com/hashtag/KCDHyderabad?src=hashtag_click&#34;&gt;#KCDHyderabad&amp;nbsp;&lt;/a&gt;hashtag.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But as they say, there‚Äôs learning in everything. Organising an event of a scale of KCD isn‚Äôt easy. There are so many things and people involved that you need to ensure that everything is well-planned and executed smoothly. Here are my 5 key learnings after organizing KCD Hyderabad.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;5 Key Learnings&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Planning and preparation are crucial&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I learned that thorough planning is essential. We started preparing months in advance, from choosing the date to finding the right venue. We had to consider holidays, school schedules, and festivals to maximize attendance. Every detail, from deciding on a single-day event to selecting the venue, required careful thought and discussion among organizers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Further,&amp;nbsp;&lt;strong&gt;things don‚Äôt work as planned&lt;/strong&gt;. From spending the entire night at the venue to ensure the event setup was done, to only realising that even on the day of the event we were still not complete. So, always be prepared for eventualities, they will come from nowhere.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Budget management is a balancing act&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Managing the budget was challenging. We had to make smart choices to keep costs down without compromising on quality. For example, instead of expensive hotels, we chose T-Hub, which was more affordable but still accommodated our needs. We also had to be creative with sponsorships, reaching out through various channels to secure funding while&amp;nbsp;&lt;strong&gt;being careful not to over-commit&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Content curation is both exciting and challenging&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Selecting speakers and creating the agenda was one of the most crucial and difficult tasks. We received 180 talk proposals for only 10-12 available slots. It took a team of experts to review and select the best ones.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I learned that it‚Äôs important to have backup speakers too, as not all selected speakers can always make it. Further, with most sponsors wanting a slot to speak, it‚Äôs difficult to maintain the balance of community vs sponsored talks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Attention to attendee experience pays off&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I realized how important it is to focus on the attendee experience. This included carefully choosing swag items like customized T-shirts, umbrellas, and laptop stickers. We also put a lot of effort into selecting good food, especially ensuring we had great biryani to showcase Hyderabad‚Äôs cuisine.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The positive feedback on these aspects was really rewarding. Sure there were a few niggles where some people told the queues were too long and things like that, but overall people were quite satisfied.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Teamwork and delegation are key&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Organizing an event of this scale taught me the importance of teamwork and delegation. Over six months, we divided responsibilities based on our strengths. It involved countless meetings, calls, and in-person discussions. I learned that success depends on everyone working together and focusing on their specific tasks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We also looked out for volunteers who could help. A handful of them did turn up eventually and took care of a lot of things on D-day. I felt, we could have leveraged their support a little earlier in the preparation cycle as well.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;When Is The Next KCD Hyderabad?&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Reflecting on our successful first KCD Hyderabad, we‚Äôve learned valuable lessons about thorough planning, budget management, content curation, attendee experience, and teamwork. These insights will be crucial for future organizers.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As for the next KCD Hyderabad, while I‚Äôd love to announce a date, it‚Äôs not that simple. With only 3 KCDs allowed annually in India, we rotate between cities to reach different communities. Having said that, I‚Äôm optimistic about its return given the fantastic response.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the meantime, stay connected with the&amp;nbsp;&lt;a href=&#34;https://community.cncf.io/hyderabad/&#34;&gt;CNCF Hyderabad community&lt;/a&gt;&amp;nbsp;and watch for KCDs in other Indian cities. Thank you to everyone who made our inaugural event special. When KCD Hyderabad returns, we‚Äôll apply these learnings to make it even better.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;That‚Äôs it for this blog post. I hope you found this helpful to plan an event like this one in your city. Feel free to reach out to me for any assistance. Drop your thoughts in the comments below, tweet to me at&amp;nbsp;&lt;a href=&#34;https://twitter.com/Atulmaharaj&#34;&gt;@Atulmaharaj&lt;/a&gt;, DM on&amp;nbsp;&lt;a href=&#34;https://instagram.com/Atulmaharaj&#34;&gt;Atulmaharaj on Instagram&lt;/a&gt;, or&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/get-in-touch/&#34;&gt;Get In Touch&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Sun, 14 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêSQL simplifies TSDB ‚Äì how to migrate from InfluxQL to SQL„ÄëSQL ÁÆÄÂåñ TSDB ‚Äì Â¶Ç‰Ωï‰ªé InfluxQL ËøÅÁßªÂà∞ SQL</title>
      <link>https://www.cncf.io/blog/2024/07/10/sql-simplifies-tsdb-how-to-migrate-from-influxql-to-sql/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql&#34;&gt;Greptime‚Äôs blog&lt;/a&gt; by tison&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. SQL is a more common and general language for querying time series data, making migrating from InfluxQL to SQL a growing trend.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/coverimage1.png&#34; alt=&#34;Image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB uses SQL as its primary query language. Once users ingest data into GreptimeDB via the InfluxDB line protocol or other APIs, a common question arises: how can I analyze the data ingested? Specifically, how can existing InfluxQL queries be migrated to SQL queries?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To address the question above, this article outlines the differences between the query languages of InfluxDB (InfluxQL or Flux) and SQL, as well as a cheat sheet for migrating from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;overview-of-query-languages&#34;&gt;Overview of Query Languages&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#overview-of-query-languages&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/&#34;&gt;InfluxQL&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V1. It‚Äôs a SQL-like query language but not a SQL dialect. Below are some examples of InfluxQL queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT * FROM h2o_feet;&#xA;SELECT * FROM h2o_feet LIMIT 5;&#xA;SELECT COUNT(&#34;water_level&#34;) FROM h2o_feet;&#xA;SELECT &#34;level description&#34;, &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;SELECT *::field FROM &#34;h2o_feet&#34;;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When InfluxDB was designed and developed, there weren‚Äôt as many database developers as today. Consequently, despite InfluxQL‚Äôs efforts to closely resemble SQL syntax, implementing basic SQL capabilities supported by relational algebra and adding time series query extensions was quite challenging.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL instead implemented functions and syntax specifically designed for time series data analysis. For instance, all InfluxQL queries default to returning the timestamp column in ascending order, and all queries must include field columns to return results.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, special query syntax is designed for querying over time series rather than rows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Essentially, InfluxQL was developed from the raw need for time series data analysis focused on numerical metrics. As InfluxDB evolved, InfluxQL also supported continuous queries and retention policies to solve some requirements of real-time data processing.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL can still be used in InfluxDB V2, it faces&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v2/query-data/influxql/&#34;&gt;a series of challenges due to model mismatches&lt;/a&gt;, as InfluxDB V2 mainly promotes the Flux query language.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/&#34;&gt;Flux&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V2. Unlike InfluxQL, which has a SQL-like syntax, Flux uses a DataFrame style syntax. Developers who have written programs in Elixir will find the syntax familiar. Here are some examples of Flux queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;erlang&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;from(bucket: &#34;example-bucket&#34;)&#xA;    |&amp;gt; range(start: -1d)&#xA;    |&amp;gt; filter(fn: (r) =&amp;gt; r._measurement == &#34;example-measurement&#34;)&#xA;    |&amp;gt; mean()&#xA;    |&amp;gt; yield(name: &#34;_result&#34;)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Designed to support joint analysis of time series data across various data sources, Flux allows users to fetch data from time series databases (InfluxDB), relational databases (PostgreSQL or MySQL), and CSV files for analysis. For example,&amp;nbsp;&lt;code&gt;sql.from&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;csv.from&lt;/code&gt;&amp;nbsp;can replace&amp;nbsp;&lt;code&gt;from(bucket)&lt;/code&gt;&amp;nbsp;in the example above, allowing fetching data from other sources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Flux can only be used in InfluxDB V2; it is not implemented in V1 and has been&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/future-of-flux/&#34;&gt;abandoned in V3&lt;/a&gt;. The reason is apparent: the learning curve is too steep. Without professional language developers, expanding syntax while fixing various design and implementation issues is almost impossible, resulting in unsustainable engineering costs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, the Structured Query Language, is familiar to data analysts and is based on relational algebra.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Unlike DSLs tailored for specific business scenarios, SQL has a solid theoretical foundation. Since E. F. Codd published the seminal paper ‚Äú&lt;a href=&#34;https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf&#34;&gt;A Relational Model of Data for Large Shared Data Banks&lt;/a&gt;,‚Äù research on relational databases has flourished for over fifty years.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Despite unique extensions in various SQL databases that sometimes confuse users, the basic query and analysis capabilities are consistently implemented across all SQL databases, supported by relational algebra. One or two decades ago, there might have been debates about SQL‚Äôs relevance. However, SQL has undoubtedly reasserted itself as the default choice for data analysis today. Over the years, SQL has been continuously improved and expanded, and it is widely adopted globally through a series of proven implementations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL is the primary query language for InfluxDB V3 and GreptimeDB. Both now recommend users analyze time series data using SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In GreptimeDB,&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql&#34;&gt;you can use standard SQL to query your data&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT ts, idc, AVG(memory_util)&#xA;FROM system_metrics&#xA;GROUP BY idc&#xA;ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The solid theoretical foundation of SQL helps emerging time series databases reliably implement complex query logic and data management tasks. Also, the broader SQL ecosystem enables emerging time series databases to quickly integrate into the data analysis tech stack. For example, in the previous&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-03-19-keyboard-monitoring&#34;&gt;input behavior analysis demos&lt;/a&gt;, we showcase an integration between GreptimeDB and Streamlit for visualizing time series by leveraging GreptimeDB‚Äôs MySQL protocol support.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;challenges-in-time-series-analysis&#34;&gt;Challenges in Time Series Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#challenges-in-time-series-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql-1&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While SQL has a solid theoretical foundation and a broader analytical ecosystem, traditional SQL databases suffer when handling time series data, primarily due to their large size.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The value provided from a single data point of a time series is often very low. Most metrics uploaded by devices aren‚Äôt explicitly handled, and the healthy status reported doesn‚Äôt require special attention. Thus, the cost-efficiency of storing time series data is crucial. How to leverage modern cloud commodity storage to reduce costs and use cutting-edge compression for time series data are key points for time series databases.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Furthermore, extracting essential information efficiently from vast amounts of time series data often requires specific query extensions for optimization. GreptimeDB‚Äôs support for&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY&lt;/a&gt;&amp;nbsp;to help users analyze data aggregation within specific time windows is one such example.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux-1&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The learning curve itself essentially doomed this dialect. As mentioned above, being a DSL solely supported by a single provider, Flux faced significant challenges in language robustness, performance optimization, and ecosystem development. The sole provider has since abandoned further development of Flux, making it a language of the past.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql-1&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL syntax resembles SQL, subtle differences can be frustrating. Despite efforts to mimic SQL syntax, InfluxQL fundamentally remains a DSL tailored to time series analysis needs focusing on metrics. Its challenges in development and maintenance costs are similar to those faced by Flux.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For example, InfluxQL does not support&amp;nbsp;&lt;code&gt;JOIN&lt;/code&gt;&amp;nbsp;queries. Although one can write queries like&amp;nbsp;&lt;code&gt;SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&lt;/code&gt;, it simply reads data from both measurements separately:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&amp;gt; SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&#xA;&#xA;name: h2o_feet&#xA;--------------&#xA;time                   level description      location       pH   water_level&#xA;2015-08-18T00:00:00Z   below 3 feet           santa_monica        2.064&#xA;2015-08-18T00:00:00Z   between 6 and 9 feet   coyote_creek        8.12&#xA;[...]&#xA;2015-09-18T21:36:00Z   between 3 and 6 feet   santa_monica        5.066&#xA;2015-09-18T21:42:00Z   between 3 and 6 feet   santa_monica        4.938&#xA;&#xA;name: h2o_pH&#xA;------------&#xA;time                   level description   location       pH   water_level&#xA;2015-08-18T00:00:00Z                       santa_monica   6&#xA;2015-08-18T00:00:00Z                       coyote_creek   7&#xA;[...]&#xA;2015-09-18T21:36:00Z                       santa_monica   8&#xA;2015-09-18T21:42:00Z                       santa_monica   7&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Moreover, despite InfluxDB V3 supporting InfluxQL due to strong user demand to facilitate migration, InfluxDB V3 primarily promotes SQL-based queries. Thus, it‚Äôs fair to say that InfluxQL is also fading away.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;migrating-to-sql-analysis&#34;&gt;Migrating to SQL Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#migrating-to-sql-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Today, many existing time series data analysis logics are written in InfluxQL. This section outlines the core differences between InfluxQL and SQL and illustrates how to migrate from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;timestamp-column&#34;&gt;Timestamp Column&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#timestamp-column&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;A key difference in application logic migration is that&amp;nbsp;&lt;strong&gt;SQL does not treat the time column especially, while InfluxQL returns the time column by default and sorts results in ascending order by timestamp&lt;/strong&gt;. SQL queries need to explicitly specify the time column to include timestamps in the result set and manually specify sorting logic.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;SELECT &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;SELECT ts, location, water_level FROM h2o_feet ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When writing data, InfluxQL automatically populates the time column with the current time, whereas SQL requires manual specification of the time column value. If using the current time, it must be explicitly written:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;INSERT INTO &#34;measurement&#34; (tag, value) VALUES (&#39;my_tag&#39;, 42);&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support inserting multiple rows in one INSERT statement, whereas SQL databases typically support this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag_0&#39;, 42), (NOW(), &#39;my_tag_1&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, InfluxQL uses the&amp;nbsp;&lt;code&gt;tz()&lt;/code&gt;&amp;nbsp;function to specify the query timezone, while SQL typically has other ways to set the timezone. GreptimeDB supports&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/mysql#time-zone&#34;&gt;MySQL&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/postgresql#time-zone&#34;&gt;PostgreSQL&lt;/a&gt;&amp;nbsp;syntax for setting the timezone.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;time-series&#34;&gt;Time Series&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#time-series&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL implements time series granularity query syntax, such as SLIMIT and&amp;nbsp;&lt;code&gt;SOFFSET&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;SLIMIT&lt;/code&gt;&amp;nbsp;limits the number of data points returned for each time series in the result set. For example,&amp;nbsp;&lt;code&gt;SLIMIT 1&lt;/code&gt;&amp;nbsp;means, at most, one result per time series that meets the filter condition.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, not specifically designed for time series data analysis, requires some workarounds:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT DISTINCT ON (host) * FROM monitor ORDER BY host, ts DESC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This query returns one result per time series, distinguished by the host tag:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;+-----------+---------------------+------+--------+&#xA;| host      | ts                  | cpu  | memory |&#xA;+-----------+---------------------+------+--------+&#xA;| 127.0.0.1 | 2022-11-03 03:39:58 |  0.5 |    0.2 |&#xA;| 127.0.0.2 | 2022-11-03 03:39:58 |  0.2 |    0.3 |&#xA;+-----------+---------------------+------+--------+&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;interval-literals&#34;&gt;Interval Literals&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#interval-literals&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL‚Äôs interval syntax resembles&amp;nbsp;&lt;code&gt;1d&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;12m&lt;/code&gt;, while SQL has standard syntax for time intervals:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INTERVAL &#39;1 DAY&#39;&#xA;INTERVAL &#39;1 YEAR 3 HOURS 20 MINUTES&#39;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;data-columns-and-tag-columns&#34;&gt;Data Columns and Tag Columns&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#data-columns-and-tag-columns&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL distinguishes between data columns and tag columns at the model level; queries that only SELECT tag columns will not return data. InfluxQL also supports the&amp;nbsp;&lt;code&gt;::field&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;::tag&lt;/code&gt;&amp;nbsp;suffixes to specify data columns or tag columns, allowing for columns with the same name.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL standards do not differentiate between data columns and tag columns, treating them all as regular columns. However, specific implementations may map these concepts differently. For example, GreptimeDB‚Äôs&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/concepts/data-model&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;data model&lt;/a&gt;&amp;nbsp;distinguishes between timestamp columns, tag columns, and data columns and has corresponding mapping rules.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image1.png&#34; alt=&#34;The Data Structure of GreptimeDB&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;The Data Structure of GreptimeDB&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;function-names&#34;&gt;Function Names&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#function-names&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Some function names differ between InfluxQL and SQL. For instance, the&amp;nbsp;&lt;code&gt;MEAN&lt;/code&gt;&amp;nbsp;function in InfluxQL corresponds to the&amp;nbsp;&lt;code&gt;AVG&lt;/code&gt;&amp;nbsp;function in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, many other functions, such as&amp;nbsp;&lt;code&gt;COUNT&lt;/code&gt;,&amp;nbsp;&lt;code&gt;SUM&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;MIN,&lt;/code&gt;&amp;nbsp;remain the same in both languages.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;identifiers&#34;&gt;Identifiers&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#identifiers&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In InfluxQL, identifiers are always double-quoted, while SQL supports unquoted identifiers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It is worth noting that SQL identifiers are case-insensitive by default. If case sensitivity is needed, the identifiers should be enclosed in the appropriate quotes. In GreptimeDB, double quotes are used by default. However, when connecting via MySQL or PostgreSQL clients, the corresponding dialect‚Äôs syntax is respected.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Examples of identifier usage differences between InfluxQL and SQL are as follows:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image2.png&#34; alt=&#34;The Usage Differences between InfluxQL and SQL&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;join&#34;&gt;JOIN&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#join&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support JOIN queries, while one of the fundamental capabilities of SQL databases is support for JOIN queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches&lt;/em&gt;&#xA;SELECT a.* FROM system_metrics a JOIN idc_info b ON a.idc = b.idc_id;&#xA;&#xA;&lt;em&gt;-- Select all rows from the idc_info table and system_metrics table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT a.* FROM idc_info a LEFT JOIN system_metrics b ON a.idc_id = b.idc;&#xA;&#xA;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT b.* FROM system_metrics a RIGHT JOIN idc_info b ON a.idc = b.idc_id;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These are examples of&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/reference/sql/join&#34;&gt;JOIN queries in GreptimeDB&lt;/a&gt;, which supports:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;INNER JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;LEFT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;RIGHT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;FULL OUTER JOIN&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;queries-over-time-windows&#34;&gt;Queries over Time Windows&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#queries-over-time-windows&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL‚Äôs&amp;nbsp;&lt;code&gt;GROUP BY&lt;/code&gt;&amp;nbsp;statement supports passing a time window to aggregate data within a specific length of time windows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL does not have such specific query capabilities; the closest equivalent is the&amp;nbsp;&lt;code&gt;OVER ... PARTITION BY&lt;/code&gt;&amp;nbsp;syntax, which can be quite complex to understand.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB implements its own&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY extension syntax&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT &#xA;    ts, &#xA;    host, &#xA;    avg(cpu) RANGE &#39;10s&#39; FILL LINEAR&#xA;FROM monitor&#xA;ALIGN &#39;5s&#39; TO &#39;2023-12-01T00:00:00&#39; BY (host) ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;continuous-aggregation&#34;&gt;Continuous Aggregation&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#continuous-aggregation&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL supports&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/continuous_queries/&#34;&gt;continuous aggregation&lt;/a&gt;, which corresponds to the standard concept of materialized views in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, the implementation of materialized views in most SQL databases is still fragile and remains an area for further exploration.&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-06-04-flow-engine&#34;&gt;GreptimeDB supports continuous aggregation&lt;/a&gt;&amp;nbsp;to meet these needs based on its flow engine.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#conclusion&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. While InfluxQL and Flux are used by InfluxDB and specifically created for handling time series data, SQL is a widely used query language in relational databases. Its robust theoretical foundation and rich ecosystem allow data analysts to quickly get started and use effective tools for time series data analysis.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB natively supports SQL queries. Visit our&amp;nbsp;&lt;a href=&#34;https://greptime.com/&#34;&gt;homepage&lt;/a&gt;&amp;nbsp;for more information or&amp;nbsp;&lt;a href=&#34;https://console.greptime.cloud/signup&#34;&gt;create a free cloud service&lt;/a&gt;&amp;nbsp;instance to start your trial today.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;about-greptime&#34;&gt;About Greptime&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#about-greptime&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We help industries that generate large amounts of time-series data, such as Connected Vehicles (CV), IoT, and Observability, to efficiently uncover the hidden value of data in real-time.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Visit the&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;latest version&lt;/a&gt;&amp;nbsp;from any device to get started and get the most out of your data.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GreptimeDB&lt;/a&gt;, written in Rust, is a distributed, open-source, time-series database designed for scalability, efficiency, and powerful analytics.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://greptime.com/product/carcloud&#34;&gt;Edge-Cloud Integrated TSDB&lt;/a&gt;&amp;nbsp;is designed for the unique demands of edge storage and compute in IoT. It tackles the exponential growth of edge data by integrating a multimodal edge-side database with cloud-based GreptimeDB Enterprise. This combination reduces traffic, computing, and storage costs while enhancing data timeliness and business insights.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.greptime.com/product/cloud&#34;&gt;GreptimeCloud&lt;/a&gt;&amp;nbsp;is a fully-managed cloud database-as-a-service (DBaaS) solution built on GreptimeDB. It efficiently supports applications in fields such as observability, IoT, and finance.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Star us on&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GitHub&lt;/a&gt;&amp;nbsp;or join GreptimeDB Community on&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/slack&#34;&gt;Slack&lt;/a&gt;&amp;nbsp;to get connected. Also, you can go to our&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb/contribute&#34;&gt;contribution page&lt;/a&gt;&amp;nbsp;to find some interesting issues to start with.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql&#34;&gt;Greptime‚Äôs blog&lt;/a&gt; by tison&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. SQL is a more common and general language for querying time series data, making migrating from InfluxQL to SQL a growing trend.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/coverimage1.png&#34; alt=&#34;Image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB uses SQL as its primary query language. Once users ingest data into GreptimeDB via the InfluxDB line protocol or other APIs, a common question arises: how can I analyze the data ingested? Specifically, how can existing InfluxQL queries be migrated to SQL queries?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To address the question above, this article outlines the differences between the query languages of InfluxDB (InfluxQL or Flux) and SQL, as well as a cheat sheet for migrating from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;overview-of-query-languages&#34;&gt;Overview of Query Languages&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#overview-of-query-languages&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/&#34;&gt;InfluxQL&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V1. It‚Äôs a SQL-like query language but not a SQL dialect. Below are some examples of InfluxQL queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT * FROM h2o_feet;&#xA;SELECT * FROM h2o_feet LIMIT 5;&#xA;SELECT COUNT(&#34;water_level&#34;) FROM h2o_feet;&#xA;SELECT &#34;level description&#34;, &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;SELECT *::field FROM &#34;h2o_feet&#34;;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When InfluxDB was designed and developed, there weren‚Äôt as many database developers as today. Consequently, despite InfluxQL‚Äôs efforts to closely resemble SQL syntax, implementing basic SQL capabilities supported by relational algebra and adding time series query extensions was quite challenging.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL instead implemented functions and syntax specifically designed for time series data analysis. For instance, all InfluxQL queries default to returning the timestamp column in ascending order, and all queries must include field columns to return results.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, special query syntax is designed for querying over time series rather than rows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Essentially, InfluxQL was developed from the raw need for time series data analysis focused on numerical metrics. As InfluxDB evolved, InfluxQL also supported continuous queries and retention policies to solve some requirements of real-time data processing.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL can still be used in InfluxDB V2, it faces&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v2/query-data/influxql/&#34;&gt;a series of challenges due to model mismatches&lt;/a&gt;, as InfluxDB V2 mainly promotes the Flux query language.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/&#34;&gt;Flux&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V2. Unlike InfluxQL, which has a SQL-like syntax, Flux uses a DataFrame style syntax. Developers who have written programs in Elixir will find the syntax familiar. Here are some examples of Flux queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;erlang&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;from(bucket: &#34;example-bucket&#34;)&#xA;    |&amp;gt; range(start: -1d)&#xA;    |&amp;gt; filter(fn: (r) =&amp;gt; r._measurement == &#34;example-measurement&#34;)&#xA;    |&amp;gt; mean()&#xA;    |&amp;gt; yield(name: &#34;_result&#34;)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Designed to support joint analysis of time series data across various data sources, Flux allows users to fetch data from time series databases (InfluxDB), relational databases (PostgreSQL or MySQL), and CSV files for analysis. For example,&amp;nbsp;&lt;code&gt;sql.from&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;csv.from&lt;/code&gt;&amp;nbsp;can replace&amp;nbsp;&lt;code&gt;from(bucket)&lt;/code&gt;&amp;nbsp;in the example above, allowing fetching data from other sources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Flux can only be used in InfluxDB V2; it is not implemented in V1 and has been&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/future-of-flux/&#34;&gt;abandoned in V3&lt;/a&gt;. The reason is apparent: the learning curve is too steep. Without professional language developers, expanding syntax while fixing various design and implementation issues is almost impossible, resulting in unsustainable engineering costs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, the Structured Query Language, is familiar to data analysts and is based on relational algebra.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Unlike DSLs tailored for specific business scenarios, SQL has a solid theoretical foundation. Since E. F. Codd published the seminal paper ‚Äú&lt;a href=&#34;https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf&#34;&gt;A Relational Model of Data for Large Shared Data Banks&lt;/a&gt;,‚Äù research on relational databases has flourished for over fifty years.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Despite unique extensions in various SQL databases that sometimes confuse users, the basic query and analysis capabilities are consistently implemented across all SQL databases, supported by relational algebra. One or two decades ago, there might have been debates about SQL‚Äôs relevance. However, SQL has undoubtedly reasserted itself as the default choice for data analysis today. Over the years, SQL has been continuously improved and expanded, and it is widely adopted globally through a series of proven implementations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL is the primary query language for InfluxDB V3 and GreptimeDB. Both now recommend users analyze time series data using SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In GreptimeDB,&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql&#34;&gt;you can use standard SQL to query your data&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT ts, idc, AVG(memory_util)&#xA;FROM system_metrics&#xA;GROUP BY idc&#xA;ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The solid theoretical foundation of SQL helps emerging time series databases reliably implement complex query logic and data management tasks. Also, the broader SQL ecosystem enables emerging time series databases to quickly integrate into the data analysis tech stack. For example, in the previous&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-03-19-keyboard-monitoring&#34;&gt;input behavior analysis demos&lt;/a&gt;, we showcase an integration between GreptimeDB and Streamlit for visualizing time series by leveraging GreptimeDB‚Äôs MySQL protocol support.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;challenges-in-time-series-analysis&#34;&gt;Challenges in Time Series Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#challenges-in-time-series-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql-1&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While SQL has a solid theoretical foundation and a broader analytical ecosystem, traditional SQL databases suffer when handling time series data, primarily due to their large size.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The value provided from a single data point of a time series is often very low. Most metrics uploaded by devices aren‚Äôt explicitly handled, and the healthy status reported doesn‚Äôt require special attention. Thus, the cost-efficiency of storing time series data is crucial. How to leverage modern cloud commodity storage to reduce costs and use cutting-edge compression for time series data are key points for time series databases.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Furthermore, extracting essential information efficiently from vast amounts of time series data often requires specific query extensions for optimization. GreptimeDB‚Äôs support for&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY&lt;/a&gt;&amp;nbsp;to help users analyze data aggregation within specific time windows is one such example.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux-1&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The learning curve itself essentially doomed this dialect. As mentioned above, being a DSL solely supported by a single provider, Flux faced significant challenges in language robustness, performance optimization, and ecosystem development. The sole provider has since abandoned further development of Flux, making it a language of the past.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql-1&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL syntax resembles SQL, subtle differences can be frustrating. Despite efforts to mimic SQL syntax, InfluxQL fundamentally remains a DSL tailored to time series analysis needs focusing on metrics. Its challenges in development and maintenance costs are similar to those faced by Flux.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For example, InfluxQL does not support&amp;nbsp;&lt;code&gt;JOIN&lt;/code&gt;&amp;nbsp;queries. Although one can write queries like&amp;nbsp;&lt;code&gt;SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&lt;/code&gt;, it simply reads data from both measurements separately:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&amp;gt; SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&#xA;&#xA;name: h2o_feet&#xA;--------------&#xA;time                   level description      location       pH   water_level&#xA;2015-08-18T00:00:00Z   below 3 feet           santa_monica        2.064&#xA;2015-08-18T00:00:00Z   between 6 and 9 feet   coyote_creek        8.12&#xA;[...]&#xA;2015-09-18T21:36:00Z   between 3 and 6 feet   santa_monica        5.066&#xA;2015-09-18T21:42:00Z   between 3 and 6 feet   santa_monica        4.938&#xA;&#xA;name: h2o_pH&#xA;------------&#xA;time                   level description   location       pH   water_level&#xA;2015-08-18T00:00:00Z                       santa_monica   6&#xA;2015-08-18T00:00:00Z                       coyote_creek   7&#xA;[...]&#xA;2015-09-18T21:36:00Z                       santa_monica   8&#xA;2015-09-18T21:42:00Z                       santa_monica   7&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Moreover, despite InfluxDB V3 supporting InfluxQL due to strong user demand to facilitate migration, InfluxDB V3 primarily promotes SQL-based queries. Thus, it‚Äôs fair to say that InfluxQL is also fading away.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;migrating-to-sql-analysis&#34;&gt;Migrating to SQL Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#migrating-to-sql-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Today, many existing time series data analysis logics are written in InfluxQL. This section outlines the core differences between InfluxQL and SQL and illustrates how to migrate from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;timestamp-column&#34;&gt;Timestamp Column&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#timestamp-column&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;A key difference in application logic migration is that&amp;nbsp;&lt;strong&gt;SQL does not treat the time column especially, while InfluxQL returns the time column by default and sorts results in ascending order by timestamp&lt;/strong&gt;. SQL queries need to explicitly specify the time column to include timestamps in the result set and manually specify sorting logic.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;SELECT &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;SELECT ts, location, water_level FROM h2o_feet ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When writing data, InfluxQL automatically populates the time column with the current time, whereas SQL requires manual specification of the time column value. If using the current time, it must be explicitly written:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;INSERT INTO &#34;measurement&#34; (tag, value) VALUES (&#39;my_tag&#39;, 42);&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support inserting multiple rows in one INSERT statement, whereas SQL databases typically support this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag_0&#39;, 42), (NOW(), &#39;my_tag_1&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, InfluxQL uses the&amp;nbsp;&lt;code&gt;tz()&lt;/code&gt;&amp;nbsp;function to specify the query timezone, while SQL typically has other ways to set the timezone. GreptimeDB supports&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/mysql#time-zone&#34;&gt;MySQL&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/postgresql#time-zone&#34;&gt;PostgreSQL&lt;/a&gt;&amp;nbsp;syntax for setting the timezone.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;time-series&#34;&gt;Time Series&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#time-series&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL implements time series granularity query syntax, such as SLIMIT and&amp;nbsp;&lt;code&gt;SOFFSET&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;SLIMIT&lt;/code&gt;&amp;nbsp;limits the number of data points returned for each time series in the result set. For example,&amp;nbsp;&lt;code&gt;SLIMIT 1&lt;/code&gt;&amp;nbsp;means, at most, one result per time series that meets the filter condition.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, not specifically designed for time series data analysis, requires some workarounds:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT DISTINCT ON (host) * FROM monitor ORDER BY host, ts DESC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This query returns one result per time series, distinguished by the host tag:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;+-----------+---------------------+------+--------+&#xA;| host      | ts                  | cpu  | memory |&#xA;+-----------+---------------------+------+--------+&#xA;| 127.0.0.1 | 2022-11-03 03:39:58 |  0.5 |    0.2 |&#xA;| 127.0.0.2 | 2022-11-03 03:39:58 |  0.2 |    0.3 |&#xA;+-----------+---------------------+------+--------+&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;interval-literals&#34;&gt;Interval Literals&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#interval-literals&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL‚Äôs interval syntax resembles&amp;nbsp;&lt;code&gt;1d&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;12m&lt;/code&gt;, while SQL has standard syntax for time intervals:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INTERVAL &#39;1 DAY&#39;&#xA;INTERVAL &#39;1 YEAR 3 HOURS 20 MINUTES&#39;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;data-columns-and-tag-columns&#34;&gt;Data Columns and Tag Columns&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#data-columns-and-tag-columns&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL distinguishes between data columns and tag columns at the model level; queries that only SELECT tag columns will not return data. InfluxQL also supports the&amp;nbsp;&lt;code&gt;::field&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;::tag&lt;/code&gt;&amp;nbsp;suffixes to specify data columns or tag columns, allowing for columns with the same name.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL standards do not differentiate between data columns and tag columns, treating them all as regular columns. However, specific implementations may map these concepts differently. For example, GreptimeDB‚Äôs&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/concepts/data-model&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;data model&lt;/a&gt;&amp;nbsp;distinguishes between timestamp columns, tag columns, and data columns and has corresponding mapping rules.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image1.png&#34; alt=&#34;The Data Structure of GreptimeDB&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;The Data Structure of GreptimeDB&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;function-names&#34;&gt;Function Names&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#function-names&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Some function names differ between InfluxQL and SQL. For instance, the&amp;nbsp;&lt;code&gt;MEAN&lt;/code&gt;&amp;nbsp;function in InfluxQL corresponds to the&amp;nbsp;&lt;code&gt;AVG&lt;/code&gt;&amp;nbsp;function in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, many other functions, such as&amp;nbsp;&lt;code&gt;COUNT&lt;/code&gt;,&amp;nbsp;&lt;code&gt;SUM&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;MIN,&lt;/code&gt;&amp;nbsp;remain the same in both languages.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;identifiers&#34;&gt;Identifiers&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#identifiers&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In InfluxQL, identifiers are always double-quoted, while SQL supports unquoted identifiers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It is worth noting that SQL identifiers are case-insensitive by default. If case sensitivity is needed, the identifiers should be enclosed in the appropriate quotes. In GreptimeDB, double quotes are used by default. However, when connecting via MySQL or PostgreSQL clients, the corresponding dialect‚Äôs syntax is respected.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Examples of identifier usage differences between InfluxQL and SQL are as follows:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image2.png&#34; alt=&#34;The Usage Differences between InfluxQL and SQL&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;join&#34;&gt;JOIN&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#join&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support JOIN queries, while one of the fundamental capabilities of SQL databases is support for JOIN queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches&lt;/em&gt;&#xA;SELECT a.* FROM system_metrics a JOIN idc_info b ON a.idc = b.idc_id;&#xA;&#xA;&lt;em&gt;-- Select all rows from the idc_info table and system_metrics table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT a.* FROM idc_info a LEFT JOIN system_metrics b ON a.idc_id = b.idc;&#xA;&#xA;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT b.* FROM system_metrics a RIGHT JOIN idc_info b ON a.idc = b.idc_id;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These are examples of&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/reference/sql/join&#34;&gt;JOIN queries in GreptimeDB&lt;/a&gt;, which supports:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;INNER JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;LEFT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;RIGHT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;FULL OUTER JOIN&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;queries-over-time-windows&#34;&gt;Queries over Time Windows&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#queries-over-time-windows&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL‚Äôs&amp;nbsp;&lt;code&gt;GROUP BY&lt;/code&gt;&amp;nbsp;statement supports passing a time window to aggregate data within a specific length of time windows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL does not have such specific query capabilities; the closest equivalent is the&amp;nbsp;&lt;code&gt;OVER ... PARTITION BY&lt;/code&gt;&amp;nbsp;syntax, which can be quite complex to understand.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB implements its own&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY extension syntax&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT &#xA;    ts, &#xA;    host, &#xA;    avg(cpu) RANGE &#39;10s&#39; FILL LINEAR&#xA;FROM monitor&#xA;ALIGN &#39;5s&#39; TO &#39;2023-12-01T00:00:00&#39; BY (host) ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;continuous-aggregation&#34;&gt;Continuous Aggregation&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#continuous-aggregation&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL supports&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/continuous_queries/&#34;&gt;continuous aggregation&lt;/a&gt;, which corresponds to the standard concept of materialized views in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, the implementation of materialized views in most SQL databases is still fragile and remains an area for further exploration.&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-06-04-flow-engine&#34;&gt;GreptimeDB supports continuous aggregation&lt;/a&gt;&amp;nbsp;to meet these needs based on its flow engine.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#conclusion&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. While InfluxQL and Flux are used by InfluxDB and specifically created for handling time series data, SQL is a widely used query language in relational databases. Its robust theoretical foundation and rich ecosystem allow data analysts to quickly get started and use effective tools for time series data analysis.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB natively supports SQL queries. Visit our&amp;nbsp;&lt;a href=&#34;https://greptime.com/&#34;&gt;homepage&lt;/a&gt;&amp;nbsp;for more information or&amp;nbsp;&lt;a href=&#34;https://console.greptime.cloud/signup&#34;&gt;create a free cloud service&lt;/a&gt;&amp;nbsp;instance to start your trial today.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;about-greptime&#34;&gt;About Greptime&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#about-greptime&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We help industries that generate large amounts of time-series data, such as Connected Vehicles (CV), IoT, and Observability, to efficiently uncover the hidden value of data in real-time.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Visit the&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;latest version&lt;/a&gt;&amp;nbsp;from any device to get started and get the most out of your data.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GreptimeDB&lt;/a&gt;, written in Rust, is a distributed, open-source, time-series database designed for scalability, efficiency, and powerful analytics.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://greptime.com/product/carcloud&#34;&gt;Edge-Cloud Integrated TSDB&lt;/a&gt;&amp;nbsp;is designed for the unique demands of edge storage and compute in IoT. It tackles the exponential growth of edge data by integrating a multimodal edge-side database with cloud-based GreptimeDB Enterprise. This combination reduces traffic, computing, and storage costs while enhancing data timeliness and business insights.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.greptime.com/product/cloud&#34;&gt;GreptimeCloud&lt;/a&gt;&amp;nbsp;is a fully-managed cloud database-as-a-service (DBaaS) solution built on GreptimeDB. It efficiently supports applications in fields such as observability, IoT, and finance.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Star us on&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GitHub&lt;/a&gt;&amp;nbsp;or join GreptimeDB Community on&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/slack&#34;&gt;Slack&lt;/a&gt;&amp;nbsp;to get connected. Also, you can go to our&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb/contribute&#34;&gt;contribution page&lt;/a&gt;&amp;nbsp;to find some interesting issues to start with.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Tue, 09 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêUnlocking the power of ephemeral environments with Devtron„Äë‰ΩøÁî® Devtron ÈáäÊîæÁü≠ÊöÇÁéØÂ¢ÉÁöÑÂäõÈáè</title>
      <link>https://www.cncf.io/blog/2024/07/12/unlocking-the-power-of-ephemeral-environments-with-devtron/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://devtron.ai/blog/unlocking-the-power-of-ephemeral-environments-with-devtron/&#34;&gt;Devtron‚Äôs blog&lt;/a&gt; by Abhinav Dubey&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;TL;DR: The blog talks about how ephemeral environments with Devtron become much easier, reducing the complexities, automating the process, and optimizing infra cost.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the world of software development,&amp;nbsp;&lt;strong&gt;ephemeral environments&lt;/strong&gt;&amp;nbsp;are temporary setups that serve specific purposes, such as testing or staging new features. These environments are short-lived, designed to exist only for the duration of their use case‚Äîlike testing a feature branch‚Äîbefore being dismantled.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments contrast with traditional static environments, which are permanent and can lead to inefficiencies, especially when underutilized. They offer a dynamic approach, allowing developers to create an isolated environment on demand without affecting the main codebase or other ongoing development activities.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-technical-and-business-value-of-ephemeral-environments&#34;&gt;The Technical and Business Value of Ephemeral Environments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments provide significant advantages in different sectors as mentioned below:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cost Efficiency&lt;/strong&gt;: By creating environments only when needed and tearing them down afterward, organizations avoid the cost of maintaining idle resources. This is particularly beneficial for companies where lower-end environments such as&amp;nbsp;&lt;code&gt;dev-env&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;non-prod&lt;/code&gt;&amp;nbsp;can cost up to five times more than production environments.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Agility and Speed&lt;/strong&gt;: Developers can quickly spin up environments to test new features or bug fixes without waiting for access to a shared environment. This agility accelerates development cycles and time-to-market.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Risk Reduction&lt;/strong&gt;: Testing in isolated environments ensures that unstable code does not affect the rest of the system, reducing the risk of bugs in production.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;is-an-ephemeral-environment-right-for-you&#34;&gt;Is an Ephemeral Environment Right for You?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Deciding whether ephemeral environments are suitable for your organization involves considering your development needs and organizational goals. Key questions include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Do your development teams frequently need isolated environments for testing?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Are you looking to optimize costs associated with non-production environments?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Is there a need to increase deployment speed and reduce risk in production?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you answered ‚Äúyes‚Äù to any of these, ephemeral environments could be highly beneficial for you.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;traditional-approach-to-ephemeral-environment&#34;&gt;Traditional Approach to Ephemeral Environment&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environment as mentioned above are the short lived environments, created and destroyed once the task is completed. We can create our scripts maybe in Terraform or Ansible or in python/shell to spin up a complete new environment that can be your VM Machines or Kubernetes clusters. Even though the automation can be achieved, there are few disadvantages associated with this approach, that include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Delay in Releases:&lt;/strong&gt;&amp;nbsp;The time taken to bring up the entire infrastructure can lead to delays in testing features or conducting sanity checks for bug fixes, resulting in a longer time to market.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;High Complexity:&amp;nbsp;&lt;/strong&gt;Creating and maintaining scripts to standardize environments across different stages can be complex and error-prone&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Manual Interventions:&amp;nbsp;&lt;/strong&gt;Even with automation scripts, manual interventions are often required to configure and install dependencies based on the application‚Äôs specific requirements, adding to the setup time.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;DevOps Dependencies:&lt;/strong&gt;&amp;nbsp;Developers typically lack expertise in tools like Terraform or Ansible, making them dependent on DevOps or SRE teams to make changes and install dependencies for their applications, which can slow down the development process.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Resource Management:&amp;nbsp;&lt;/strong&gt;Managing the lifecycle of ephemeral environments can be challenging. These environments need to be deleted once tasks are completed; otherwise, they lead to resource wastage and increased costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;High Infra Cost:&amp;nbsp;&lt;/strong&gt;The costs associated with spinning up and maintaining ephemeral environments, particularly in cloud-based setups, can add significantly to the overall infrastructure expenses.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;rethinking-ephemeral-environments&#34;&gt;Rethinking Ephemeral Environments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When we talk in terms of Kubernetes, setting up Ephemeral Environments becomes a lot easier than the traditional approach. Kubernetes has a beautiful thing called namespaces, a logical separation of group of resources, providing isolation of workloads within the same cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;By leveraging namespaces and some advanced autoscaling methods, it becomes much more easier to create a ephemeral environment that is cost-effective, less complex and helps you dynamically bring up the resources and hibernate when not in use. &amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;how-to-set-up-an-ephemeral-environment-in-k8s-manually&#34;&gt;How to Set Up an Ephemeral Environment in K8s Manually?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Setting up an ephemeral environment, especially within a Kubernetes ecosystem, involves several key steps that ensure agility, efficiency, and cost-effectiveness. Below, we detail a straightforward approach to creating and managing these temporary environments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 1: Define Your Infrastructure Requirements&lt;/strong&gt;&lt;br&gt;Before you create an ephemeral environment, it‚Äôs essential to understand the specific requirements of the application or feature being tested. This includes the necessary computing resources, the required services, and any dependencies that need to be replicated from the production environment.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 2: Automate the Environment Setup&lt;/strong&gt;&lt;br&gt;Automation is crucial in managing ephemeral environments to ensure they can be spun up and torn down efficiently. Tools like Terraform or Ansible can be used to script the creation of your infrastructure. In Kubernetes, you might automate setting up namespaces, deploying container images, and configuring network policies through CI/CD pipelines.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Using Kubernetes Namespaces&lt;/strong&gt;&lt;br&gt;In Kubernetes, namespaces provide a way to divide cluster resources between multiple users. Each ephemeral environment can be created in its namespace, isolating its running processes and resources from other environments&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl create namespace your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 3: Deploy Your Application&lt;/strong&gt;&lt;br&gt;Once the namespace is ready, deploy your application using Kubernetes manifests or Helm charts. This step often involves setting up the necessary config maps and secrets to configure the application according to the environment&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl apply -f your-application-deployment.yaml -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Or using Helm&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;helm install your-application-release your-helm-chart/ -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 4: Configure Autoscaling and Monitoring&lt;/strong&gt;&lt;br&gt;To optimize costs and resource usage, configure autoscaling for your application workloads. Kubernetes Horizontal Pod Autoscaler (HPA) or a more advanced tool like KEDA can be used to automatically adjust the number of pods based on traffic or other metrics&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;apiVersion: autoscaling/v2beta2&#xA;kind: HorizontalPodAutoscaler&#xA;metadata:&#xA;  name: your-application-hpa&#xA;  namespace: your-environment-name&#xA;spec:&#xA;  scaleTargetRef:&#xA;    apiVersion: apps/v1&#xA;    kind: Deployment&#xA;    name: your-application-deployment&#xA;  minReplicas: 1&#xA;  maxReplicas: 10&#xA;  metrics:&#xA;  - type: Resource&#xA;    resource:&#xA;      name: cpu&#xA;      target:&#xA;        type: Utilization&#xA;        averageUtilization: 50&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Monitoring is also essential to track the performance and health of your temporary environment. Tools like Prometheus for monitoring and Grafana for visualization can be integrated to monitor the environment‚Äôs metrics.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 5: Implement Cleanup Procedures&lt;/strong&gt;&lt;br&gt;To ensure that resources are not wasted, set up automatic cleanup procedures to tear down the environment after use. This can be scheduled using cron jobs or integrated into your CI/CD pipeline to destroy the environment once the testing is complete:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl delete namespace your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Or a more controlled cleanup with Helm&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;helm uninstall your-application-release -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 6: Documentation and Training&lt;/strong&gt;&lt;br&gt;Finally, document the entire process and provide training for your teams. This ensures that everyone understands how to efficiently use ephemeral environments, which helps in maximizing the benefits while minimizing potential disruptions or misuse.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Manually creating and deleting the namespaces, and integrating it within pipelines can be something a big pain when it comes to developer productivity. Integrating different tools such as Grafana, Prometheus, Jenkins, ArgoCD, KEDA, etc can be a tedius task for DevOps / SRE engineers as well. With the the involvement of custom scripting, again the complexities increases, high rish to human errors. With Devtron‚Äôs simplified workflow, it becomes a lot more easier to automate the process, and improve the developer productivity while reducing its high dependencies from DevOps/ SRE teams.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;how-devtron-simplifies-ephemeral-environments&#34;&gt;How Devtron Simplifies Ephemeral Environments?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Devtron enhances the management of ephemeral environments through its modern dashboard, simplified workflows, automation and effective cost-management strategies. Here are key features:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Namespace Utilization&lt;/strong&gt;: In Kubernetes, namespaces provide logical separation, allowing multiple ephemeral environments within the same cluster without additional cost. Devtron leverages this to minimize the overhead associated with setting up and tearing down environments.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Cost Management&lt;/strong&gt;: Devtron implements strategies such as leveraging spot instances and right-sizing resources, ensuring that the infrastructure costs are kept to a minimum. For example, by using spot instances, organizations can save up to 70-90% compared to standard costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Automated Scaling&lt;/strong&gt;: Devtron employs tools like KEDA for event-driven autoscaling, ensuring resources are used efficiently. Environments can scale down automatically during inactivity and scale up when needed, further optimizing costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Simplified Workflow:&amp;nbsp;&lt;/strong&gt;Devtron provides an intuitive dashboard for all operating on Kubernetes, providing Kubernetes-native CI/CD pipelines, simplifying the heavy scripting and stitching up of different tools to complete an end-to-end workflow.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Along with that, there are many other factors which makes the entire process much more seamless, such as visibility of workloads, application metrics, configurations management, etc.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;setting-up-ephemeral-environments-with-devtron&#34;&gt;Setting-up Ephemeral Environments With Devtron&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Devtron is a Software Distribution Platform designed for Kubernetes. On its mission to democratize Kubernetes, ephemeral environments are one among the many other features, that make life easier. With Devtron‚Äôs intuitive dashboard, operations on Kubernetes become flawless, and it goes with ephemeral environments as well. To get started with ephemeral environment, follow the below steps.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 1: Install the keda-add-on-http from the chart‚Äôs marketplace. Navigate to the&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/deploy-chart/overview-of-charts?ref=devtron.ai&#34;&gt;charts store&lt;/a&gt;, search for Keda, and as you can see in the below image, you can see all charts related to Keda. Select the appropriate helm chart and deploy it. To add any helm chart which is not listed on the charts store,&amp;nbsp;&lt;a href=&#34;https://devtron.ai/blog/helm-chart-deployment/&#34;&gt;free feel to check out this blog&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-11-at-3.35.11-PM.png&#34; alt=&#34;keda-http-add-on&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 1] KEDA HTTP Add-on Controller&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 2: Once the controller has been successfully installed, you can see a consolidated view of the deployed helm chart, along with its resources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-11-at-3.40.27-PM.png&#34; alt=&#34;resources-grouped-view&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 2] Controller Successfully Deployed&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 3: Now, let‚Äôs move and configure the ephemeral environment for my microservice called,&amp;nbsp;&lt;code&gt;payment-svc&lt;/code&gt;. To configure the ephemeral environment for any application, the process remains the same and you should be able to configure/ clone the workflows for different applications. Navigate to&amp;nbsp;&lt;code&gt;Workflow Editor&lt;/code&gt;, add a workflow for the respective environment where you want to deploy your applications, in our case, its&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment as you can see in the below image. To understand more about workflows in Devtron, feel free to refer the&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/applications/creating-application/workflow/ci-pipeline?ref=devtron.ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-4.45.56-PM.png&#34; alt=&#34;workflow-editor&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 3] Adding Deployment Pipeline&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 4: Once the workflow has been created, Devtron automatically creates&amp;nbsp;&lt;code&gt;environment-overrides&lt;/code&gt;&amp;nbsp;for the deployment environment.&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/applications/creating-application/environment-overrides?ref=devtron.ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Environment overrides&lt;/a&gt;&amp;nbsp;help you manage your Kubernetes configuration for the specific environment in a more efficient way. Under the&amp;nbsp;&lt;code&gt;environment override&lt;/code&gt;&amp;nbsp;&amp;gt;&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment, we can add the relevant configurations required in the deployment template which would create the HttpScaledObject, responsible for bringing the environment up and running dynamically as it receives any HTTP request.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-4.46.31-PM.png&#34; alt=&#34;deployment-template&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 4] Configuring HTTPScaledObject&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 5: After providing the relevant configuration, navigate to&amp;nbsp;&lt;code&gt;Build &amp;amp; Deploy&lt;/code&gt;&amp;nbsp;section, select the relevant image, and deploy it in the&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment. Upon successful deployment, you can see the application status as Healthy, and all details about the deployment are as you can see in the below image.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.14.09-PM.png&#34; alt=&#34;application-details&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 5] Application Details&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;You can also see all the resources deployed along with the deployment in a resourced grouped view, and perform operations such as checking the logs, events, manifests, or exec into the terminals. You can notice in the below image that, we have a Deployment object but there isn‚Äôt any pod running as of now. This is because it automatically scaled down the workload since there is no HTTP request hitting the given hostname/ service.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.28.14-PM.png&#34; alt=&#34;deployment-replicaset&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 6] Deployment &amp;amp; ReplicaSet&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 6: In the Devtron dashboard, it automatically picks up the ingress host and shows it in&amp;nbsp;&lt;code&gt;URLs&lt;/code&gt;&amp;nbsp;section at the top right of the dashboard as you can see in Fig. 5, and if any request has been made into the hostname, it will automatically scale up the pod and it can serve the traffic as you can see in the below image.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.51.33-PM.png&#34; alt=&#34;scaled-up-pod&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 7] Dynamically Scaled-up Pod&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments offer a flexible, cost-effective solution for managing development stages, particularly in a dynamic and fast-paced software development landscape. Devtron‚Äôs approach not only simplifies the management of these environments but also enhances cost efficiency and deployment agility.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Organizations looking to streamline their development processes and reduce costs should consider implementing ephemeral environments, especially those already using Kubernetes. With Devtron, the transition is smoother, allowing teams to focus more on innovation and less on infrastructure management.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Feel free to join our&amp;nbsp;&lt;a href=&#34;https://discord.devtron.ai/?ref=devtron.ai&#34;&gt;Discord Community&lt;/a&gt;&amp;nbsp;if you have any questions. Would love to address any queries or questions. If you liked Devtron, do give it a&amp;nbsp;&lt;a href=&#34;https://github.com/devtron-labs/devtron?ref=devtron.ai&#34;&gt;Star ‚≠êÔ∏è on GitHub&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://devtron.ai/blog/unlocking-the-power-of-ephemeral-environments-with-devtron/&#34;&gt;Devtron‚Äôs blog&lt;/a&gt; by Abhinav Dubey&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;TL;DR: The blog talks about how ephemeral environments with Devtron become much easier, reducing the complexities, automating the process, and optimizing infra cost.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the world of software development,&amp;nbsp;&lt;strong&gt;ephemeral environments&lt;/strong&gt;&amp;nbsp;are temporary setups that serve specific purposes, such as testing or staging new features. These environments are short-lived, designed to exist only for the duration of their use case‚Äîlike testing a feature branch‚Äîbefore being dismantled.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments contrast with traditional static environments, which are permanent and can lead to inefficiencies, especially when underutilized. They offer a dynamic approach, allowing developers to create an isolated environment on demand without affecting the main codebase or other ongoing development activities.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-technical-and-business-value-of-ephemeral-environments&#34;&gt;The Technical and Business Value of Ephemeral Environments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments provide significant advantages in different sectors as mentioned below:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cost Efficiency&lt;/strong&gt;: By creating environments only when needed and tearing them down afterward, organizations avoid the cost of maintaining idle resources. This is particularly beneficial for companies where lower-end environments such as&amp;nbsp;&lt;code&gt;dev-env&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;non-prod&lt;/code&gt;&amp;nbsp;can cost up to five times more than production environments.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Agility and Speed&lt;/strong&gt;: Developers can quickly spin up environments to test new features or bug fixes without waiting for access to a shared environment. This agility accelerates development cycles and time-to-market.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Risk Reduction&lt;/strong&gt;: Testing in isolated environments ensures that unstable code does not affect the rest of the system, reducing the risk of bugs in production.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;is-an-ephemeral-environment-right-for-you&#34;&gt;Is an Ephemeral Environment Right for You?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Deciding whether ephemeral environments are suitable for your organization involves considering your development needs and organizational goals. Key questions include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Do your development teams frequently need isolated environments for testing?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Are you looking to optimize costs associated with non-production environments?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Is there a need to increase deployment speed and reduce risk in production?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you answered ‚Äúyes‚Äù to any of these, ephemeral environments could be highly beneficial for you.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;traditional-approach-to-ephemeral-environment&#34;&gt;Traditional Approach to Ephemeral Environment&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environment as mentioned above are the short lived environments, created and destroyed once the task is completed. We can create our scripts maybe in Terraform or Ansible or in python/shell to spin up a complete new environment that can be your VM Machines or Kubernetes clusters. Even though the automation can be achieved, there are few disadvantages associated with this approach, that include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Delay in Releases:&lt;/strong&gt;&amp;nbsp;The time taken to bring up the entire infrastructure can lead to delays in testing features or conducting sanity checks for bug fixes, resulting in a longer time to market.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;High Complexity:&amp;nbsp;&lt;/strong&gt;Creating and maintaining scripts to standardize environments across different stages can be complex and error-prone&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Manual Interventions:&amp;nbsp;&lt;/strong&gt;Even with automation scripts, manual interventions are often required to configure and install dependencies based on the application‚Äôs specific requirements, adding to the setup time.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;DevOps Dependencies:&lt;/strong&gt;&amp;nbsp;Developers typically lack expertise in tools like Terraform or Ansible, making them dependent on DevOps or SRE teams to make changes and install dependencies for their applications, which can slow down the development process.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Resource Management:&amp;nbsp;&lt;/strong&gt;Managing the lifecycle of ephemeral environments can be challenging. These environments need to be deleted once tasks are completed; otherwise, they lead to resource wastage and increased costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;High Infra Cost:&amp;nbsp;&lt;/strong&gt;The costs associated with spinning up and maintaining ephemeral environments, particularly in cloud-based setups, can add significantly to the overall infrastructure expenses.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;rethinking-ephemeral-environments&#34;&gt;Rethinking Ephemeral Environments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When we talk in terms of Kubernetes, setting up Ephemeral Environments becomes a lot easier than the traditional approach. Kubernetes has a beautiful thing called namespaces, a logical separation of group of resources, providing isolation of workloads within the same cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;By leveraging namespaces and some advanced autoscaling methods, it becomes much more easier to create a ephemeral environment that is cost-effective, less complex and helps you dynamically bring up the resources and hibernate when not in use. &amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;how-to-set-up-an-ephemeral-environment-in-k8s-manually&#34;&gt;How to Set Up an Ephemeral Environment in K8s Manually?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Setting up an ephemeral environment, especially within a Kubernetes ecosystem, involves several key steps that ensure agility, efficiency, and cost-effectiveness. Below, we detail a straightforward approach to creating and managing these temporary environments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 1: Define Your Infrastructure Requirements&lt;/strong&gt;&lt;br&gt;Before you create an ephemeral environment, it‚Äôs essential to understand the specific requirements of the application or feature being tested. This includes the necessary computing resources, the required services, and any dependencies that need to be replicated from the production environment.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 2: Automate the Environment Setup&lt;/strong&gt;&lt;br&gt;Automation is crucial in managing ephemeral environments to ensure they can be spun up and torn down efficiently. Tools like Terraform or Ansible can be used to script the creation of your infrastructure. In Kubernetes, you might automate setting up namespaces, deploying container images, and configuring network policies through CI/CD pipelines.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Using Kubernetes Namespaces&lt;/strong&gt;&lt;br&gt;In Kubernetes, namespaces provide a way to divide cluster resources between multiple users. Each ephemeral environment can be created in its namespace, isolating its running processes and resources from other environments&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl create namespace your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 3: Deploy Your Application&lt;/strong&gt;&lt;br&gt;Once the namespace is ready, deploy your application using Kubernetes manifests or Helm charts. This step often involves setting up the necessary config maps and secrets to configure the application according to the environment&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl apply -f your-application-deployment.yaml -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Or using Helm&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;helm install your-application-release your-helm-chart/ -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 4: Configure Autoscaling and Monitoring&lt;/strong&gt;&lt;br&gt;To optimize costs and resource usage, configure autoscaling for your application workloads. Kubernetes Horizontal Pod Autoscaler (HPA) or a more advanced tool like KEDA can be used to automatically adjust the number of pods based on traffic or other metrics&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;apiVersion: autoscaling/v2beta2&#xA;kind: HorizontalPodAutoscaler&#xA;metadata:&#xA;  name: your-application-hpa&#xA;  namespace: your-environment-name&#xA;spec:&#xA;  scaleTargetRef:&#xA;    apiVersion: apps/v1&#xA;    kind: Deployment&#xA;    name: your-application-deployment&#xA;  minReplicas: 1&#xA;  maxReplicas: 10&#xA;  metrics:&#xA;  - type: Resource&#xA;    resource:&#xA;      name: cpu&#xA;      target:&#xA;        type: Utilization&#xA;        averageUtilization: 50&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Monitoring is also essential to track the performance and health of your temporary environment. Tools like Prometheus for monitoring and Grafana for visualization can be integrated to monitor the environment‚Äôs metrics.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 5: Implement Cleanup Procedures&lt;/strong&gt;&lt;br&gt;To ensure that resources are not wasted, set up automatic cleanup procedures to tear down the environment after use. This can be scheduled using cron jobs or integrated into your CI/CD pipeline to destroy the environment once the testing is complete:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl delete namespace your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Or a more controlled cleanup with Helm&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;helm uninstall your-application-release -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 6: Documentation and Training&lt;/strong&gt;&lt;br&gt;Finally, document the entire process and provide training for your teams. This ensures that everyone understands how to efficiently use ephemeral environments, which helps in maximizing the benefits while minimizing potential disruptions or misuse.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Manually creating and deleting the namespaces, and integrating it within pipelines can be something a big pain when it comes to developer productivity. Integrating different tools such as Grafana, Prometheus, Jenkins, ArgoCD, KEDA, etc can be a tedius task for DevOps / SRE engineers as well. With the the involvement of custom scripting, again the complexities increases, high rish to human errors. With Devtron‚Äôs simplified workflow, it becomes a lot more easier to automate the process, and improve the developer productivity while reducing its high dependencies from DevOps/ SRE teams.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;how-devtron-simplifies-ephemeral-environments&#34;&gt;How Devtron Simplifies Ephemeral Environments?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Devtron enhances the management of ephemeral environments through its modern dashboard, simplified workflows, automation and effective cost-management strategies. Here are key features:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Namespace Utilization&lt;/strong&gt;: In Kubernetes, namespaces provide logical separation, allowing multiple ephemeral environments within the same cluster without additional cost. Devtron leverages this to minimize the overhead associated with setting up and tearing down environments.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Cost Management&lt;/strong&gt;: Devtron implements strategies such as leveraging spot instances and right-sizing resources, ensuring that the infrastructure costs are kept to a minimum. For example, by using spot instances, organizations can save up to 70-90% compared to standard costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Automated Scaling&lt;/strong&gt;: Devtron employs tools like KEDA for event-driven autoscaling, ensuring resources are used efficiently. Environments can scale down automatically during inactivity and scale up when needed, further optimizing costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Simplified Workflow:&amp;nbsp;&lt;/strong&gt;Devtron provides an intuitive dashboard for all operating on Kubernetes, providing Kubernetes-native CI/CD pipelines, simplifying the heavy scripting and stitching up of different tools to complete an end-to-end workflow.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Along with that, there are many other factors which makes the entire process much more seamless, such as visibility of workloads, application metrics, configurations management, etc.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;setting-up-ephemeral-environments-with-devtron&#34;&gt;Setting-up Ephemeral Environments With Devtron&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Devtron is a Software Distribution Platform designed for Kubernetes. On its mission to democratize Kubernetes, ephemeral environments are one among the many other features, that make life easier. With Devtron‚Äôs intuitive dashboard, operations on Kubernetes become flawless, and it goes with ephemeral environments as well. To get started with ephemeral environment, follow the below steps.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 1: Install the keda-add-on-http from the chart‚Äôs marketplace. Navigate to the&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/deploy-chart/overview-of-charts?ref=devtron.ai&#34;&gt;charts store&lt;/a&gt;, search for Keda, and as you can see in the below image, you can see all charts related to Keda. Select the appropriate helm chart and deploy it. To add any helm chart which is not listed on the charts store,&amp;nbsp;&lt;a href=&#34;https://devtron.ai/blog/helm-chart-deployment/&#34;&gt;free feel to check out this blog&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-11-at-3.35.11-PM.png&#34; alt=&#34;keda-http-add-on&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 1] KEDA HTTP Add-on Controller&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 2: Once the controller has been successfully installed, you can see a consolidated view of the deployed helm chart, along with its resources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-11-at-3.40.27-PM.png&#34; alt=&#34;resources-grouped-view&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 2] Controller Successfully Deployed&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 3: Now, let‚Äôs move and configure the ephemeral environment for my microservice called,&amp;nbsp;&lt;code&gt;payment-svc&lt;/code&gt;. To configure the ephemeral environment for any application, the process remains the same and you should be able to configure/ clone the workflows for different applications. Navigate to&amp;nbsp;&lt;code&gt;Workflow Editor&lt;/code&gt;, add a workflow for the respective environment where you want to deploy your applications, in our case, its&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment as you can see in the below image. To understand more about workflows in Devtron, feel free to refer the&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/applications/creating-application/workflow/ci-pipeline?ref=devtron.ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-4.45.56-PM.png&#34; alt=&#34;workflow-editor&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 3] Adding Deployment Pipeline&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 4: Once the workflow has been created, Devtron automatically creates&amp;nbsp;&lt;code&gt;environment-overrides&lt;/code&gt;&amp;nbsp;for the deployment environment.&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/applications/creating-application/environment-overrides?ref=devtron.ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Environment overrides&lt;/a&gt;&amp;nbsp;help you manage your Kubernetes configuration for the specific environment in a more efficient way. Under the&amp;nbsp;&lt;code&gt;environment override&lt;/code&gt;&amp;nbsp;&amp;gt;&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment, we can add the relevant configurations required in the deployment template which would create the HttpScaledObject, responsible for bringing the environment up and running dynamically as it receives any HTTP request.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-4.46.31-PM.png&#34; alt=&#34;deployment-template&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 4] Configuring HTTPScaledObject&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 5: After providing the relevant configuration, navigate to&amp;nbsp;&lt;code&gt;Build &amp;amp; Deploy&lt;/code&gt;&amp;nbsp;section, select the relevant image, and deploy it in the&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment. Upon successful deployment, you can see the application status as Healthy, and all details about the deployment are as you can see in the below image.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.14.09-PM.png&#34; alt=&#34;application-details&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 5] Application Details&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;You can also see all the resources deployed along with the deployment in a resourced grouped view, and perform operations such as checking the logs, events, manifests, or exec into the terminals. You can notice in the below image that, we have a Deployment object but there isn‚Äôt any pod running as of now. This is because it automatically scaled down the workload since there is no HTTP request hitting the given hostname/ service.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.28.14-PM.png&#34; alt=&#34;deployment-replicaset&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 6] Deployment &amp;amp; ReplicaSet&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 6: In the Devtron dashboard, it automatically picks up the ingress host and shows it in&amp;nbsp;&lt;code&gt;URLs&lt;/code&gt;&amp;nbsp;section at the top right of the dashboard as you can see in Fig. 5, and if any request has been made into the hostname, it will automatically scale up the pod and it can serve the traffic as you can see in the below image.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.51.33-PM.png&#34; alt=&#34;scaled-up-pod&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 7] Dynamically Scaled-up Pod&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments offer a flexible, cost-effective solution for managing development stages, particularly in a dynamic and fast-paced software development landscape. Devtron‚Äôs approach not only simplifies the management of these environments but also enhances cost efficiency and deployment agility.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Organizations looking to streamline their development processes and reduce costs should consider implementing ephemeral environments, especially those already using Kubernetes. With Devtron, the transition is smoother, allowing teams to focus more on innovation and less on infrastructure management.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Feel free to join our&amp;nbsp;&lt;a href=&#34;https://discord.devtron.ai/?ref=devtron.ai&#34;&gt;Discord Community&lt;/a&gt;&amp;nbsp;if you have any questions. Would love to address any queries or questions. If you liked Devtron, do give it a&amp;nbsp;&lt;a href=&#34;https://github.com/devtron-labs/devtron?ref=devtron.ai&#34;&gt;Star ‚≠êÔ∏è on GitHub&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Thu, 11 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêNow what? Kubernetes troubleshooting with AI?„ÄëÊÄé‰πàÂäûÔºü Kubernetes ‰ΩøÁî® AI ËøõË°åÊïÖÈöúÊéíÈô§Ôºü</title>
      <link>https://www.cncf.io/blog/2024/07/11/now-what-kubernetes-troubleshooting-with-ai/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;Ambassador post originally published on &lt;a href=&#34;https://eminalemdar.medium.com/&#34;&gt;Medium &lt;/a&gt;by Emin Alemdar&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;cb51&#34;&gt;We all know that Kubernetes troubleshooting is difficult and it can get pretty complex from time to time. We can easily get lost in the logs, jumping between pods, searching through events and what have you. Most importantly, finding a meaningful explanation can be an extremely huge problem because not all of the logs are easily understandable. We also have to accept that it is time consuming. Of course we can use an external observability tool or even an observability stack with all of the components but still not all of the outputs are easy to understand and even read. At the end of the day, diagnose times and of course the troubleshooting times are extending drastically as a result.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;358a&#34;&gt;But let‚Äôs talk about the elephant in the room here. Can AI help us here? Especially for Kubernetes troubleshooting. Because we started using&amp;nbsp;&lt;strong&gt;AI&lt;/strong&gt;&amp;nbsp;and mostly&amp;nbsp;&lt;strong&gt;LLMs&lt;/strong&gt;&amp;nbsp;everyday not just in our jobs but in our daily activities as well. There are of course some&amp;nbsp;&lt;strong&gt;AIOps&lt;/strong&gt;&amp;nbsp;tools out there that help us implement the observability solution with the help of AI but almost all of those tools consume huge amounts of GPU resources and that increases the underlying cost and of course the maintainability issue here.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;140d&#34;&gt;So, in this blog post, I want to introduce a&amp;nbsp;&lt;strong&gt;CNCF Sandbox&lt;/strong&gt;&amp;nbsp;project to you which is a really powerful tool designed to simplify Kubernetes management using AI and natural language processing. The project is called&amp;nbsp;&lt;strong&gt;K8sGPT&lt;/strong&gt;&amp;nbsp;and K8sGPT is a tool for scanning your Kubernetes clusters, diagnosing and triaging issues in simple English. So basically, K8sGPT integrates with various AI backends, including OpenAI, Azure OpenAI, and Amazon Bedrock, to provide clear and actionable insights into your Kubernetes environment and it provides these insights in a user friendly format.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*rWs5LOWvxt5df7gW&#34; alt=&#34;Image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 class=&#34;wp-block-heading&#34; id=&#34;439b&#34;&gt;Key Features of K8sGPT&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;6ba7&#34;&gt;Before sharing some examples of K8sGPT usage, I want to share some specific key features of it. Let‚Äôs break them down into bullet points.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Workload Health Analysis&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;969d&#34;&gt;Of course we should start with this because that is one of the most important reasons this tool is here. K8sGPT scans your Kubernetes clusters to identify critical issues affecting your workloads. It transforms complex diagnostic data and logs into simple, understandable suggestions, making it easier to maintain cluster health. Yes, K8sGPT also provides suggestions for the problems it detects in the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;AI-Powered Diagnostics&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a06c&#34;&gt;This one is obvious. K8sGPT leverages AI and LLMs. K8sGPT provides detailed explanations of detected issues in plain English. This feature is powered by integrations with leading AI platforms like OpenAI. Basically, by using these AI platforms, K8sGPT transforms the diagnostic data into a very human friendly format.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Built-in and Custom Analysers&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;5583&#34;&gt;K8sGPT includes several built-in analyzers for various Kubernetes resources, such as Pods, Services, and Nodes. But of course with additional integrations, you can also create custom analysers to meet specific needs. K8sGPT has integrations with AWS, Prometheus, KEDA and Trivy. With these native integrations, you can have more detailed analysis. Of course I believe this list is going to be extended and more integrations will be added here in the future.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Continuous Monitoring and Integration&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;d67b&#34;&gt;You can also deploy K8sGPT as a Kubernetes Operator within a Kubernetes cluster. K8sGPT can continuously monitor the environment and integrate seamlessly with existing tools like Prometheus and Alertmanager in the cluster. You can see the components that the K8sGPT Operator instals and manages from the diagram below.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*loluayfTjzR3OyVv&#34; alt=&#34;image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Security CVE Review&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;e023&#34;&gt;With the Trivy integration, K8sGPT helps in identifying and addressing security vulnerabilities within your Kubernetes clusters. Once you activate the Trivy integration, Trivy Kubernetes Operator will be installed into the Kubernetes cluster and make it possible for K8sGPT to interact with the results of the Operator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;fbd6&#34;&gt;I also want to specifically talk about the&amp;nbsp;&lt;strong&gt;AWS&lt;/strong&gt;&amp;nbsp;integration here. I‚Äôm pretty sure you‚Äôre already familiar with the&amp;nbsp;&lt;strong&gt;AWS Controllers for Kubernetes (ACK)&lt;/strong&gt;&amp;nbsp;project but if not, I also have a blog post about that and you can check that out from&amp;nbsp;&lt;a href=&#34;https://medium.com/@eminalemdar/manage-your-aws-resources-from-kubernetes-with-ack-3cf06a4b0770&#34;&gt;here&lt;/a&gt;. In short, ACK allows you to manage AWS services from your Kubernetes clusters with CRDs. This integration also helps K8sGPT to interact with the AWS resources managed by ACK. As a result, you can use K8sGPT to analyse and manage not only your Kubernetes resources but also your AWS resources that are under the management of ACK.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;01bf&#34;&gt;Let‚Äôs play with K8sGPT&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;4756&#34;&gt;Installation of K8sGPT CLI is extremely easy and you can use popular packet managers like Brew to install it on your machine. After the installation, you will need to authenticate with your chosen AI provider. I‚Äôm using OpenAI here with GPT4 but you can of course choose the appropriate one for you:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;k8sgpt auth list&lt;br&gt;Default:&lt;br&gt;&amp;gt; openai&lt;br&gt;Active:&lt;br&gt;&amp;gt; openai&lt;br&gt;Unused:&lt;br&gt;&amp;gt; localai&lt;br&gt;&amp;gt; azureopenai&lt;br&gt;&amp;gt; cohere&lt;br&gt;&amp;gt; amazonbedrock&lt;br&gt;&amp;gt; amazonsagemaker&lt;br&gt;&amp;gt; google&lt;br&gt;&amp;gt; noopai&lt;br&gt;&amp;gt; huggingface&lt;br&gt;&amp;gt; googlevertexai&lt;br&gt;&amp;gt; oci&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;ddc5&#34;&gt;You can use&amp;nbsp;&lt;code&gt;k8sgpt auth add&lt;/code&gt;&amp;nbsp;command to add the provider backend authentication.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;0e6f&#34;&gt;After this step, it‚Äôs pretty straightforward actually. Let‚Äôs start the first analysis with the&amp;nbsp;&lt;code&gt;k8sgpt analyse&lt;/code&gt;&amp;nbsp;command. But before running the command let me just deploy a broken Pod to see the actual diagnosis part.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl apply -f - &amp;lt;&amp;lt;EOF&lt;br&gt;apiVersion: v1&lt;br&gt;kind: Pod&lt;br&gt;metadata:&lt;br&gt;name: broken-pod&lt;br&gt;namespace: default&lt;br&gt;spec:&lt;br&gt;containers:&lt;br&gt;- name: broken-pod&lt;br&gt;image: nginx:1.a.b.c&lt;br&gt;livenessProbe:&lt;br&gt;httpGet:&lt;br&gt;path: /&lt;br&gt;port: 81&lt;br&gt;initialDelaySeconds: 3&lt;br&gt;periodSeconds: 3&lt;br&gt;EOF&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;8645&#34;&gt;As you can see the image part of this Pod definition is wrong. So let me run the&amp;nbsp;&lt;code&gt;k8sgpt analyse&lt;/code&gt;&amp;nbsp;command and see the output.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*mSTa1ykjgzbeBas2&#34; alt=&#34;Code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;616a&#34;&gt;We can see the error but not in detail but there is a flag for us to use here. If we add the&amp;nbsp;&lt;code&gt;k8sgpt analyse --explain&lt;/code&gt;&amp;nbsp;flag here, K8sGPT will connect to the AI provider and LLM here.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*xAdRU8htHZ3VxrtY&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;637f&#34;&gt;So now we have some more details about the issue and also some recommendations as well. I know this is a very basic example but let‚Äôs extend this. I will now enable two integrations, AWS and Trivy. If we look at the filters we can use now, we see the added filters coming from the integrations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:390/0*s-zY2Cb0irELZNms&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a48f&#34;&gt;Let‚Äôs start with the EKS filter and see if there‚Äôs anything wrong with that.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:466/0*uwSRiMmwhveF5-k5&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;2054&#34;&gt;Brilliant! No problems here but let‚Äôs see the results of the Vulnerability Report from Trivy.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*5tX1K5Wi2SOZNAv2&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;3f28&#34;&gt;Wow! That‚Äôs a lot! Let‚Äôs dive in to understand some of these issues.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*smPQrxM4w4VOtjP0&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*YtfG2_J9R1f4oWMk&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a139&#34;&gt;As you can see from the screenshots, We have very detailed information about each issue and again some really good recommendations on how to solve the problems.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;ae22&#34;&gt;Let me also deploy some other resources to the cluster and see the information about those.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*5rbT8-9aQ-bXGYcz&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;2953&#34;&gt;As you can see from the broken resources, I now have a very good understanding of what is really happening in my cluster here.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;bdbf&#34;&gt;Some final words here. Of course we all have some questions around security of these AI backends but there is also another option here called anonymise which basically anonymise the data before sending it to the AI backend.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;2ade&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;25df&#34;&gt;In the future K8sGPT might turn into one of the tools that‚Äôs going to transform Kubernetes management, problem diagnosis and of course troubleshooting by making it more accessible and efficient. Whether you‚Äôre an SRE, DevOps Engineer or Platform Engineer, K8sGPT can help you reduce troubleshooting times. It is extremely easy to get started with K8sGPT, you can just install it on your Kubernetes environment or on your local machine, authenticate with your chosen AI backend, and connect it to your cluster. In my opinion, you should definitely give K8sGPT a try.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://eminalemdar.medium.com/?source=post_page-----9c68d26e00ac--------------------------------&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Â§ß‰ΩøÂ∏ñÂ≠êÊúÄÂàùÁî± Emin Alemdar Âú® &lt;a href=&#34;https://eminalemdar.medium.com/&#34;&gt;Medium&lt;/a&gt; ‰∏äÂèëÂ∏É&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;cb51&#34;&gt;Êàë‰ª¨ÈÉΩÁü•ÈÅì Kubernetes ÊïÖÈöúÊéíÈô§ÂæàÂõ∞ÈöæÔºåËÄå‰∏îÊúâÊó∂‰ºöÂèòÂæóÈùûÂ∏∏Â§çÊùÇ„ÄÇÊàë‰ª¨ÂæàÂÆπÊòìËø∑Â§±Âú®Êó•Âøó‰∏≠ÔºåÂú® Pod ‰πãÈó¥Ë∑≥ËΩ¨ÔºåÊêúÁ¥¢‰∫ã‰ª∂‰ª•Âèä‰Ω†Êã•ÊúâÁöÑ‰∏úË•ø„ÄÇÊúÄÈáçË¶ÅÁöÑÊòØÔºåÊâæÂà∞ÊúâÊÑè‰πâÁöÑËß£ÈáäÂèØËÉΩÊòØ‰∏Ä‰∏™ÊûÅÂÖ∂Â∑®Â§ßÁöÑÈóÆÈ¢òÔºåÂõ†‰∏∫Âπ∂ÈùûÊâÄÊúâÊó•ÂøóÈÉΩÊòì‰∫éÁêÜËß£„ÄÇÊàë‰ª¨ËøòÂøÖÈ°ªÊâøËÆ§ËøôÂæàËÄóÊó∂„ÄÇÂΩìÁÑ∂ÔºåÊàë‰ª¨ÂèØ‰ª•‰ΩøÁî®Â§ñÈÉ®ÂèØËßÇÂØüÊÄßÂ∑•ÂÖ∑ÔºåÁîöËá≥ÂèØ‰ª•‰ΩøÁî®ÂåÖÂê´ÊâÄÊúâÁªÑ‰ª∂ÁöÑÂèØËßÇÂØüÊÄßÂ†ÜÊ†àÔºå‰ΩÜ‰ªçÁÑ∂Âπ∂ÈùûÊâÄÊúâËæìÂá∫ÈÉΩÊòì‰∫éÁêÜËß£ÁîöËá≥ÈòÖËØª„ÄÇÂΩíÊ†πÁªìÂ∫ïÔºåËØäÊñ≠Êó∂Èó¥‰ª•ÂèäÊïÖÈöúÊéíÈô§Êó∂Èó¥ÈÉΩ‰ºöÂõ†Ê≠§ËÄåÂ§ßÂπÖÂª∂Èïø„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;358a&#34;&gt;‰ΩÜÊòØÊàë‰ª¨Êù•Ë∞àË∞àÊàøÈó¥ÈáåÁöÑÂ§ßË±°„ÄÇ‰∫∫Â∑•Êô∫ËÉΩÂèØ‰ª•Âú®ËøôÊñπÈù¢Â∏ÆÂä©Êàë‰ª¨ÂêóÔºüÁâπÂà´ÊòØÂØπ‰∫é Kubernetes ÊïÖÈöúÊéíÈô§„ÄÇÂõ†‰∏∫Êàë‰ª¨ÊØèÂ§©ÈÉΩÂºÄÂßã‰ΩøÁî®&lt;strong&gt;‰∫∫Â∑•Êô∫ËÉΩ&lt;/strong&gt;ÔºåÂπ∂‰∏î‰∏ªË¶ÅÊòØ&lt;strong&gt;Ê≥ïÂ≠¶Á°ïÂ£´&lt;/strong&gt;Ôºå‰∏ç‰ªÖÂú®Êàë‰ª¨ÁöÑÂ∑•‰Ωú‰∏≠ÔºåËÄå‰∏îÂú®Êàë‰ª¨ÁöÑÊó•Â∏∏Ê¥ªÂä®‰∏≠„ÄÇÂΩìÁÑ∂ÔºåÊúâ‰∏Ä‰∫õ&lt;strong&gt;AIOps&lt;/strong&gt;Â∑•ÂÖ∑ÂèØ‰ª•Â∏ÆÂä©Êàë‰ª¨Âú®‰∫∫Â∑•Êô∫ËÉΩÁöÑÂ∏ÆÂä©‰∏ãÂÆûÁé∞ÂèØËßÇÊµãÊÄßËß£ÂÜ≥ÊñπÊ°àÔºå‰ΩÜÂá†‰πéÊâÄÊúâËøô‰∫õÂ∑•ÂÖ∑ÈÉΩ‰ºöÊ∂àËÄóÂ§ßÈáèÁöÑ GPU ËµÑÊ∫êÔºåËøô‰ºöÂ¢ûÂä†ÊΩúÂú®ÊàêÊú¨ÔºåÂΩìÁÑ∂ËøôÈáåÁöÑÂèØÁª¥Êä§ÊÄßÈóÆÈ¢ò„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;140d&#34;&gt;Âõ†Ê≠§ÔºåÂú®ËøôÁØáÂçöÊñá‰∏≠ÔºåÊàëÊÉ≥ÂêëÊÇ®‰ªãÁªç‰∏Ä‰∏™&lt;strong&gt;CNCF Sandbox&lt;/strong&gt;È°πÁõÆÔºåËøôÊòØ‰∏Ä‰∏™ÈùûÂ∏∏Âº∫Â§ßÁöÑÂ∑•ÂÖ∑ÔºåÊó®Âú®‰ΩøÁî®‰∫∫Â∑•Êô∫ËÉΩÂíåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊù•ÁÆÄÂåñ Kubernetes ÁÆ°ÁêÜ„ÄÇËØ•È°πÁõÆÂêç‰∏∫&lt;strong&gt;K8sGPT&lt;/strong&gt;ÔºåK8sGPT ÊòØ‰∏Ä‰∏™Áî®ÁÆÄÂçïÁöÑËã±ËØ≠Êâ´Êèè Kubernetes ÈõÜÁæ§„ÄÅËØäÊñ≠ÂíåÂàÜÁ±ªÈóÆÈ¢òÁöÑÂ∑•ÂÖ∑„ÄÇÂõ†Ê≠§ÔºåÂü∫Êú¨‰∏äÔºåK8sGPT ‰∏éÂêÑÁßç AI ÂêéÁ´ØÈõÜÊàêÔºåÂåÖÊã¨ OpenAI„ÄÅAzure OpenAI Âíå Amazon BedrockÔºå‰∏∫ÊÇ®ÁöÑ Kubernetes ÁéØÂ¢ÉÊèê‰æõÊ∏ÖÊô∞‰∏îÂèØÊìç‰ΩúÁöÑËßÅËß£ÔºåÂπ∂‰ª•Áî®Êà∑ÂèãÂ•ΩÁöÑÊ†ºÂºèÊèê‰æõËøô‰∫õËßÅËß£„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*rWs5LOWvxt5df7gW&#34; alt=&#34;Image&#34;referrerpolicy =&#34;Êó†ÂºïËçê&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 class=&#34;wp-block-heading&#34; id=&#34;439b&#34;&gt;K8sGPT ÁöÑ‰∏ªË¶ÅÁâπÊÄß&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;6ba7&#34;&gt;Âú®ÂàÜ‰∫´‰∏Ä‰∫õ K8sGPT ‰ΩøÁî®Á§∫‰æã‰πãÂâçÔºåÊàëÊÉ≥ÂÖàÂàÜ‰∫´‰∏Ä‰∏ãÂÆÉÁöÑ‰∏Ä‰∫õÂÖ∑‰ΩìÂÖ≥ÈîÆÂäüËÉΩ„ÄÇËÆ©Êàë‰ª¨Â∞ÜÂÆÉ‰ª¨ÂàÜËß£‰∏∫Ë¶ÅÁÇπ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Â∑•‰ΩúË¥üËΩΩÂÅ•Â∫∑ÂàÜÊûê&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;969d&#34;&gt;ÂΩìÁÑ∂Êàë‰ª¨Â∫îËØ•‰ªéËøô‰∏™ÂºÄÂßãÔºåÂõ†‰∏∫ËøôÊòØËøô‰∏™Â∑•ÂÖ∑Âá∫Áé∞ÁöÑÊúÄÈáçË¶ÅÂéüÂõ†‰πã‰∏Ä„ÄÇ K8sGPT Êâ´ÊèèÊÇ®ÁöÑ Kubernetes ÈõÜÁæ§‰ª•ËØÜÂà´ÂΩ±ÂìçÊÇ®Â∑•‰ΩúË¥üËΩΩÁöÑÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇÂÆÉÂ∞ÜÂ§çÊùÇÁöÑËØäÊñ≠Êï∞ÊçÆÂíåÊó•ÂøóËΩ¨Êç¢‰∏∫ÁÆÄÂçï„ÄÅÊòì‰∫éÁêÜËß£ÁöÑÂª∫ËÆÆÔºå‰ªéËÄåÊõ¥ËΩªÊùæÂú∞Áª¥Êä§ÈõÜÁæ§ÂÅ•Â∫∑Áä∂ÂÜµ„ÄÇÊòØÁöÑÔºåK8sGPT ËøòÈíàÂØπÂÆÉÂú®ÈõÜÁæ§‰∏≠Ê£ÄÊµãÂà∞ÁöÑÈóÆÈ¢òÊèê‰æõÂª∫ËÆÆ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;‰∫∫Â∑•Êô∫ËÉΩÈ©±Âä®ÁöÑËØäÊñ≠&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a06c&#34;&gt;Ëøô‰∏ÄÁÇπÊòØÊòæËÄåÊòìËßÅÁöÑ„ÄÇ K8sGPT Âà©Áî®‰∫∫Â∑•Êô∫ËÉΩÂíåÊ≥ïÂ≠¶Á°ïÂ£´„ÄÇ K8sGPT Áî®ÁÆÄÂçïÁöÑËã±ËØ≠Êèê‰æõ‰∫ÜÊ£ÄÊµãÂà∞ÁöÑÈóÆÈ¢òÁöÑËØ¶ÁªÜËß£Èáä„ÄÇÊ≠§ÂäüËÉΩÁî±‰∏é OpenAI Á≠âÈ¢ÜÂÖà AI Âπ≥Âè∞ÁöÑÈõÜÊàêÊèê‰æõÊîØÊåÅ„ÄÇÂü∫Êú¨‰∏äÔºåÈÄöËøá‰ΩøÁî®Ëøô‰∫õ‰∫∫Â∑•Êô∫ËÉΩÂπ≥Âè∞ÔºåK8sGPT Â∞ÜËØäÊñ≠Êï∞ÊçÆËΩ¨Êç¢‰∏∫ÈùûÂ∏∏‰∫∫ÊÄßÂåñÁöÑÊ†ºÂºè„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;ÂÜÖÁΩÆÂíåËá™ÂÆö‰πâÂàÜÊûêÂô®&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;5583&#34;&gt;K8sGPT ÂåÖÂê´Â§ö‰∏™ÈíàÂØπÂêÑÁßç Kubernetes ËµÑÊ∫êÔºà‰æãÂ¶Ç Pod„ÄÅÊúçÂä°ÂíåËäÇÁÇπÔºâÁöÑÂÜÖÁΩÆÂàÜÊûêÂô®„ÄÇÂΩìÁÑ∂ÔºåÈÄöËøáÈ¢ùÂ§ñÁöÑÈõÜÊàêÔºåÊÇ®ËøòÂèØ‰ª•ÂàõÂª∫Ëá™ÂÆö‰πâÂàÜÊûêÂô®Êù•Êª°Ë∂≥ÁâπÂÆöÈúÄÊ±Ç„ÄÇ K8sGPT ‰∏é AWS„ÄÅPrometheus„ÄÅKEDA Âíå Trivy ÈõÜÊàê„ÄÇÈÄöËøáËøô‰∫õÂéüÁîüÈõÜÊàêÔºåÊÇ®ÂèØ‰ª•ËøõË°åÊõ¥ËØ¶ÁªÜÁöÑÂàÜÊûê„ÄÇÂΩìÁÑ∂ÔºåÊàëÁõ∏‰ø°Ëøô‰∏™ÂàóË°®Â∞Ü‰ºöÊâ©Â±ïÔºåÂπ∂‰∏îÂ∞ÜÊù•‰ºöÊ∑ªÂä†Êõ¥Â§öÈõÜÊàê„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;ÊåÅÁª≠ÁõëÊéßÂíåÈõÜÊàê&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;d67b&#34;&gt;ÊÇ®ËøòÂèØ‰ª•Â∞Ü K8sGPT ‰Ωú‰∏∫ Kubernetes Operator Âú® Kubernetes ÈõÜÁæ§‰∏≠ÈÉ®ÁΩ≤„ÄÇ K8sGPT ÂèØ‰ª•ÊåÅÁª≠ÁõëÊéßÁéØÂ¢ÉÔºåÂπ∂‰∏éÈõÜÁæ§‰∏≠Áé∞ÊúâÁöÑ Prometheus„ÄÅAlertmanager Á≠âÂ∑•ÂÖ∑Êó†ÁºùÈõÜÊàê„ÄÇÊÇ®ÂèØ‰ª•‰ªé‰∏ãÂõæ‰∏≠ÁúãÂà∞ K8sGPT Operator ÂÆâË£ÖÂíåÁÆ°ÁêÜÁöÑÁªÑ‰ª∂„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*loluayfTjzR3OyVv&#34; alt=&#34;image&#34;referrerpolicy =&#34;Êó†ÂºïËçê&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;ÂÆâÂÖ® CVE ÂÆ°Ê†∏&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;e023&#34;&gt;ÈÄöËøá Trivy ÈõÜÊàêÔºåK8sGPT ÊúâÂä©‰∫éËØÜÂà´ÂíåËß£ÂÜ≥ Kubernetes ÈõÜÁæ§ÂÜÖÁöÑÂÆâÂÖ®ÊºèÊ¥û„ÄÇ‰∏ÄÊó¶ÊøÄÊ¥ª Trivy ÈõÜÊàêÔºåTrivy Kubernetes Operator Â∞ÜË¢´ÂÆâË£ÖÂà∞ Kubernetes ÈõÜÁæ§‰∏≠ÔºåÂπ∂‰Ωø K8sGPT ËÉΩÂ§ü‰∏é Operator ÁöÑÁªìÊûúËøõË°å‰∫§‰∫í„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;fbd6&#34;&gt;ÊàëËøòÊÉ≥Âú®Ê≠§‰∏ìÈó®ËÆ®ËÆ∫ &lt;strong&gt;AWS&lt;/strong&gt; ÈõÜÊàê„ÄÇÊàëÂæàÁ°ÆÂÆöÊÇ®Â∑≤ÁªèÁÜüÊÇâ&lt;strong&gt;AWS Controllers for Kubernetes (ACK)&lt;/strong&gt;È°πÁõÆÔºå‰ΩÜÂ¶ÇÊûú‰∏çÁÜüÊÇâÔºåÊàëËøòÊúâ‰∏ÄÁØáÂÖ≥‰∫éËØ•È°πÁõÆÁöÑÂçöÊñáÔºåÊÇ®ÂèØ‰ª•‰ªé&lt;a href= ‚Äúhttps://medium.com/@eminalemdar/manage-your-aws-resources-from-kubernetes-with-ack-3cf06a4b0770&#34;&gt;Ê≠§Â§Ñ&lt;/a&gt;„ÄÇÁÆÄËÄåË®Ä‰πãÔºåACK ÂÖÅËÆ∏ÊÇ®‰ΩøÁî® CRD ‰ªé Kubernetes ÈõÜÁæ§ÁÆ°ÁêÜ AWS ÊúçÂä°„ÄÇËøôÁßçÈõÜÊàêËøòÊúâÂä©‰∫é K8sGPT ‰∏é ACK ÁÆ°ÁêÜÁöÑ AWS ËµÑÊ∫êËøõË°å‰∫§‰∫í„ÄÇÂõ†Ê≠§ÔºåÊÇ®‰∏ç‰ªÖÂèØ‰ª•‰ΩøÁî® K8sGPT Êù•ÂàÜÊûêÂíåÁÆ°ÁêÜ Kubernetes ËµÑÊ∫êÔºåËøòÂèØ‰ª•ÂàÜÊûêÂíåÁÆ°ÁêÜ ACK ÁÆ°ÁêÜ‰∏ãÁöÑ AWS ËµÑÊ∫ê„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;01bf&#34;&gt;ËÆ©Êàë‰ª¨Êù•Áé©‰∏Ä‰∏ã K8sGPT&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;4756&#34;&gt;K8sGPT CLI ÁöÑÂÆâË£ÖÈùûÂ∏∏ÁÆÄÂçïÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî® Brew Á≠âÊµÅË°åÁöÑÊï∞ÊçÆÂåÖÁÆ°ÁêÜÂô®Â∞ÜÂÖ∂ÂÆâË£ÖÂú®ÊÇ®ÁöÑËÆ°ÁÆóÊú∫‰∏ä„ÄÇÂÆâË£ÖÂêéÔºåÊÇ®ÈúÄË¶ÅÂêëÊÇ®ÈÄâÊã©ÁöÑ AI Êèê‰æõÂïÜËøõË°åË∫´‰ªΩÈ™åËØÅ„ÄÇÊàëÂú®ËøôÈáå‰ΩøÁî® OpenAI Âíå GPT4Ôºå‰ΩÜÊÇ®ÂΩìÁÑ∂ÂèØ‰ª•ÈÄâÊã©ÈÄÇÂêàÊÇ®ÁöÑÔºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;k8sgpt Ë∫´‰ªΩÈ™åËØÅÂàóË°®&lt;br&gt;ÈªòËÆ§Ôºö&lt;br&gt;&gt; openai&lt;br&gt;Ê¥ªÂä®Ôºö&lt;br&gt;&gt; openai&lt;br&gt;Êú™‰ΩøÁî®Ôºö&lt;br&gt; &gt; localai&lt;br&gt;&gt; azureopenai&lt;br&gt;&gt; cohere&lt;br&gt;&gt; amazonbedrock&lt;br&gt;&gt; amazonsagemaker&lt;br&gt;&gt; google&lt;br&gt;&gt; noopai&lt;br&gt;&gt; Huggingface&lt;br&gt;&gt; googlevertexai&lt;br&gt;&gt; oci&lt;/code &gt;&lt;/Ââç&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;ddc5&#34;&gt;ÊÇ®ÂèØ‰ª•‰ΩøÁî®&lt;code&gt;k8sgpt auth add&lt;/code&gt;ÂëΩ‰ª§Ê∑ªÂä†Êèê‰æõÂïÜÂêéÁ´ØË∫´‰ªΩÈ™åËØÅ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;0e6f&#34;&gt;ÂÆåÊàêËøô‰∏ÄÊ≠•ÂêéÔºåÂÆûÈôÖ‰∏äÈùûÂ∏∏ÁÆÄÂçï„ÄÇËÆ©Êàë‰ª¨‰ΩøÁî® &lt;code&gt;k8sgpt analysis&lt;/code&gt; ÂëΩ‰ª§ÂºÄÂßãÁ¨¨‰∏ÄÊ¨°ÂàÜÊûê„ÄÇ‰ΩÜÂú®ËøêË°åÂëΩ‰ª§‰πãÂâçÔºåËÆ©ÊàëÈÉ®ÁΩ≤‰∏Ä‰∏™ÊçüÂùèÁöÑ Pod Êù•Êü•ÁúãÂÆûÈôÖÁöÑËØäÊñ≠ÈÉ®ÂàÜ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl apply -f - &lt;&lt;EOF&lt;br&gt;apiÁâàÊú¨Ôºöv1&lt;br&gt;ÁßçÁ±ªÔºöPod&lt;br&gt;ÂÖÉÊï∞ÊçÆÔºö&lt;br&gt;ÂêçÁß∞ÔºöÊçüÂùè-pod&lt;br&gt;ÂëΩÂêçÁ©∫Èó¥ÔºöÈªòËÆ§&lt;br&gt;ËßÑÊ†ºÔºö&lt;br&gt;ÂÆπÂô®Ôºö&lt;br&gt;- ÂêçÁß∞Ôºöbroken-pod&lt;br&gt;ÂõæÂÉèÔºönginx:1.a.b.c&lt;br&gt;livenessProbe:&lt;br&gt;httpGet:&lt;br&gt;Ë∑ØÂæÑÔºö/&lt;br&gt;Á´ØÂè£Ôºö81&lt;br&gt;initialDelaySecondsÔºö3&lt;br&gt;periodSecondsÔºö3&lt;br&gt;EOF&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;8645&#34;&gt;Â¶ÇÊÇ®ÊâÄËßÅÔºåÊ≠§ Pod ÂÆö‰πâÁöÑÂõæÂÉèÈÉ®ÂàÜÊòØÈîôËØØÁöÑ„ÄÇÈÇ£‰πàËÆ©ÊàëËøêË°å &lt;code&gt;k8sgpt analysis&lt;/code&gt; ÂëΩ‰ª§Âπ∂Êü•ÁúãËæìÂá∫„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*mSTa1ykjgzbeBas2&#34; alt=&#34;‰ª£Á†Å&#34;referrerpolicy =&#34;Êó†ÂºïËçê&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;616a&#34;&gt;Êàë‰ª¨ÂèØ‰ª•ÁúãÂà∞ÈîôËØØÔºå‰ΩÜÁúã‰∏çÂà∞ËØ¶ÁªÜ‰ø°ÊÅØÔºå‰ΩÜËøôÈáåÊúâ‰∏Ä‰∏™Ê†áÂøó‰æõÊàë‰ª¨‰ΩøÁî®„ÄÇÂ¶ÇÊûúÊàë‰ª¨Âú®Ê≠§Â§ÑÊ∑ªÂä† &lt;code&gt;k8sgptanalyze--explain&lt;/code&gt; Ê†áÂøóÔºåK8sGPT Â∞ÜÂú®Ê≠§Â§ÑËøûÊé•Âà∞ AI Êèê‰æõÂïÜÂíå LLM„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*xAdRU8htHZ3VxrtY&#34; alt=&#34;code&#34;referrerpolicy =&#34;Êó†ÂºïËçê&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;637f&#34;&gt;Áé∞Âú®Êàë‰ª¨Êúâ‰∫ÜÊúâÂÖ≥ËØ•ÈóÆÈ¢òÁöÑÊõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØ‰ª•Âèä‰∏Ä‰∫õÂª∫ËÆÆ„ÄÇÊàëÁü•ÈÅìËøôÊòØ‰∏Ä‰∏™ÈùûÂ∏∏Âü∫Êú¨ÁöÑÁ§∫‰æãÔºå‰ΩÜËÆ©Êàë‰ª¨Êâ©Â±ï‰∏Ä‰∏ã„ÄÇÊàëÁé∞Âú®Â∞ÜÂêØÁî®‰∏§‰∏™ÈõÜÊàêÔºöAWS Âíå Trivy„ÄÇÂ¶ÇÊûúÊàë‰ª¨Êü•ÁúãÁé∞Âú®ÂèØ‰ª•‰ΩøÁî®ÁöÑËøáÊª§Âô®ÔºåÊàë‰ª¨‰ºöÁúãÂà∞Ê∑ªÂä†ÁöÑËøáÊª§Âô®Êù•Ëá™ÈõÜÊàê„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:390/0*s-zY2Cb0irELZNms&#34; alt=&#34;code ‚Äúreferrerpolicy =‚Äúno-referrer‚Äù&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a48f&#34;&gt;ËÆ©Êàë‰ª¨‰ªé EKS ËøáÊª§Âô®ÂºÄÂßãÔºåÁúãÁúãÊòØÂê¶Êúâ‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:466/0*uwSRiMmwhveF5-k5&#34; alt=&#34;code ‚Äúreferrerpolicy =‚Äúno-referrer‚Äù&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;2054&#34;&gt;Â§™Ê£í‰∫ÜÔºÅËøôÈáåÊ≤°ÊúâÈóÆÈ¢òÔºå‰ΩÜËÆ©Êàë‰ª¨ÁúãÁúã Trivy ÁöÑÊºèÊ¥ûÊä•ÂëäÁöÑÁªìÊûú„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*5tX1K5Wi2SOZNAv2&#34; alt=&#34;code&#34;referrerpolicy =‚ÄúÊó†ÂºïÁî®ËÄÖ‚Äù&gt;&lt;/Âõæ&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;3f28&#34;&gt;ÂìáÔºÅÂ•ΩÂ§öÂïäÔºÅËÆ©Êàë‰ª¨Ê∑±ÂÖ•‰∫ÜËß£ÂÖ∂‰∏≠‰∏Ä‰∫õÈóÆÈ¢ò„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*smPQrxM4w4VOtjP0&#34; alt=&#34;code&#34;referrerpolicy =‚ÄúÊó†ÂºïÁî®ËÄÖ‚Äù&gt;&lt;/Âõæ&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*YtfG2_J9R1f4oWMk&#34; alt=&#34;code&#34;referrerpolicy =‚ÄúÊó†ÂºïÁî®ËÄÖ‚Äù&gt;&lt;/Âõæ&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a139&#34;&gt;Ê≠£Â¶ÇÊÇ®‰ªéÂ±èÂπïÊà™Âõæ‰∏≠ÁúãÂà∞ÁöÑÔºåÊàë‰ª¨Êèê‰æõ‰∫ÜÊúâÂÖ≥ÊØè‰∏™ÈóÆÈ¢òÁöÑÈùûÂ∏∏ËØ¶ÁªÜÁöÑ‰ø°ÊÅØÔºåÂπ∂‰∏îËøòÊèê‰æõ‰∫Ü‰∏Ä‰∫õÂÖ≥‰∫éÂ¶Ç‰ΩïËß£ÂÜ≥ÈóÆÈ¢òÁöÑÈùûÂ∏∏Â•ΩÁöÑÂª∫ËÆÆ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;ae22&#34;&gt;ËÆ©Êàë‰πüÂ∞Ü‰∏Ä‰∫õÂÖ∂‰ªñËµÑÊ∫êÈÉ®ÁΩ≤Âà∞ÈõÜÁæ§Âπ∂Êü•ÁúãÊúâÂÖ≥Ëøô‰∫õËµÑÊ∫êÁöÑ‰ø°ÊÅØ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*5rbT8-9aQ-bXGYcz&#34; alt= ‚Äú‰ª£Á†Å‚Äùreferrerpolicy =‚Äúno-referrer‚Äù&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;2953&#34;&gt;Ê≠£Â¶ÇÊÇ®‰ªéÊçüÂùèÁöÑËµÑÊ∫ê‰∏≠ÁúãÂà∞ÁöÑÈÇ£Ê†∑ÔºåÊàëÁé∞Âú®ÂØπÈõÜÁæ§‰∏≠ÂÆûÈôÖÂèëÁîüÁöÑÊÉÖÂÜµÊúâ‰∫ÜÂæàÂ•ΩÁöÑ‰∫ÜËß£„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;bdbf&#34;&gt;ÊúÄÂêé‰∏Ä‰∫õËØù„ÄÇÂΩìÁÑ∂ÔºåÊàë‰ª¨ÂØπËøô‰∫õ‰∫∫Â∑•Êô∫ËÉΩÂêéÁ´ØÁöÑÂÆâÂÖ®ÊÄßÈÉΩÊúâ‰∏Ä‰∫õÁñëÈóÆÔºå‰ΩÜËøôÈáåËøòÊúâÂè¶‰∏Ä‰∏™ÈÄâÈ°πÔºåÁß∞‰∏∫ÂåøÂêçÔºåÂÆÉÂü∫Êú¨‰∏äÂú®Â∞ÜÊï∞ÊçÆÂèëÈÄÅÂà∞‰∫∫Â∑•Êô∫ËÉΩÂêéÁ´Ø‰πãÂâçÂØπÊï∞ÊçÆËøõË°åÂåøÂêçÂåñ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;2ade&#34;&gt;ÁªìËÆ∫&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;25df&#34;&gt;Â∞ÜÊù•ÔºåK8sGPT ÂèØËÉΩ‰ºöÊàê‰∏∫‰∏ÄÁßçÂ∑•ÂÖ∑ÔºåÈÄöËøá‰ΩøÂÖ∂Êõ¥Êòì‰∫éËÆøÈóÆÂíåÊõ¥È´òÊïàÔºåÊù•ÊîπÂèò Kubernetes ÁÆ°ÁêÜ„ÄÅÈóÆÈ¢òËØäÊñ≠ÂíåÊïÖÈöúÊéíÈô§„ÄÇÊó†ËÆ∫ÊÇ®ÊòØ SRE„ÄÅDevOps Â∑•Á®ãÂ∏àËøòÊòØÂπ≥Âè∞Â∑•Á®ãÂ∏àÔºåK8sGPT ÈÉΩÂèØ‰ª•Â∏ÆÂä©ÊÇ®ÂáèÂ∞ëÊïÖÈöúÊéíÈô§Êó∂Èó¥„ÄÇ K8sGPT ÂÖ•Èó®ÈùûÂ∏∏ÁÆÄÂçïÔºåÊÇ®Âè™ÈúÄÂ∞ÜÂÖ∂ÂÆâË£ÖÂú® Kubernetes ÁéØÂ¢ÉÊàñÊú¨Âú∞ËÆ°ÁÆóÊú∫‰∏äÔºå‰ΩøÁî®ÊÇ®ÈÄâÊã©ÁöÑ AI ÂêéÁ´ØËøõË°åË∫´‰ªΩÈ™åËØÅÔºåÁÑ∂ÂêéÂ∞ÜÂÖ∂ËøûÊé•Âà∞ÈõÜÁæ§Âç≥ÂèØ„ÄÇÂú®ÊàëÁúãÊù•Ôºå‰Ω†ÁªùÂØπÂ∫îËØ•Â∞ùËØï‰∏Ä‰∏ã K8sGPT„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://eminalemdar.medium.com/?source=post_page-----9c68d26e00ac---------------------- ----------&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Wed, 10 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêAs we reach mid-year 2024, a look at CNCF, Linux Foundation, and top 30 open source project velocity„ÄëÂà∞‰∫Ü 2024 Âπ¥Âπ¥‰∏≠ÔºåÊàë‰ª¨Êù•ÁúãÁúã CNCF„ÄÅLinux Âü∫Èáë‰ºöÂíåÊéíÂêçÂâç 30 ÁöÑÂºÄÊ∫êÈ°πÁõÆÈÄüÂ∫¶</title>
      <link>https://www.cncf.io/blog/2024/07/11/as-we-reach-mid-year-2024-a-look-at-cncf-linux-foundation-and-top-30-open-source-project-velocity/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;Staff post by Chris Aniszczyk&amp;nbsp;&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Date/Time:&lt;/strong&gt; July 11 at 8am&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For the last several years we have tracked open source project velocity, which has enabled us to monitor the trends and technologies that resonate with developers and end users. For comparison, have a look at past timeframes from our &lt;a href=&#34;https://www.cncf.io/?s=Velocity&#34;&gt;blogs&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Here are the main takeaways I see from these charts:&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Kubernetes continues to mature with its consistent and largest contributor base&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;OpenTelemetry continues to grow its contributor base and remains the second highest velocity project; they recently added &lt;a href=&#34;https://opentelemetry.io/blog/2024/profiling/&#34;&gt;profiling&lt;/a&gt; as a new signal type&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Backstage grows solving an important pain point around developer experience&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;GitOps continues to be important in the cloud native ecosystem, where projects like Argo and Flux continue to cultivate large communities&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Crossplane grew its contributor base by over 20% in the last year reflecting a desire for open source control planes in the era of open source relicensing issues&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KeyCloak joined CNCF last year as an incubating project and has a large community pushing open source identity and access management forward&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;In many cases, CNCF projects &lt;a href=&#34;https://www.cncf.io/case-studies/openai/&#34;&gt;underpin large scale AI infrastructure&lt;/a&gt; and we have Kubeflow appearing on the top 30 CNCF project list for the first time in 2023.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;CNCF projects ‚Äì Last 12 months &lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;interactive map&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXeXEdq4E15JkoUMLwMRdFVFC0jeXT7El7VLVnVChbBOZk-v2I-03MLI5R8HMhgOrnlMS8LC-dQlKrKI4m8ybZHMzQ4Khm4_3dm5bboQhepmg4N9mf2IVWD4dOE4kxEv2T1rmQ0J33ATxZsT4Oi3XsxZh01x?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;cncf projects&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Linux Foundation Projects ‚Äì Last 12 months &lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507&#34;&gt;interactive map&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXcBhcGzavp5kG5-l6g2MW-hNTvn2BIl2HIkb07TFm2dhUOJEymAhE18jAxE2_hgqitU9vJCRSDZ18EzppV9auq0eHq-qyGiYoT-JxQ5znQRMK_rq-mA_naXmtdQrNeuAeI8paEXZtUSxWxXLbsgRSBZi74?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;LF projects&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Top 30 open source projects ‚Äì Last 12 months&lt;/strong&gt; (&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507&#34;&gt;interactive map&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdrzMUoz3xCWNweqwfxhzfI5koLh1Z68AbH4oWMjsQsH6FoBfZDx4bpsY2Cwyaxx7ZT9exDO1qtNy-oP00BuSczhhCZYCreLEDjdv0bQssLvSGEDkuVSaNCy2GQyjB0GUJIo4amk3C_St4m7tqM9zj8wAsv?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;Open source projects&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: We use bubble charts to show three axes of data: commits, authors, and comments/pull requests, and plot on a log-log chart to show the data across large scales.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;The bubble‚Äôs area is proportional to the number of authors&amp;nbsp;&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;The y-axis is the total number of pull requests and issues&amp;nbsp;&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;The x-axis is the number of commits&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;All of the the &lt;a href=&#34;https://github.com/cncf/velocity#current-reports&#34;&gt;current&lt;/a&gt; and &lt;a href=&#34;https://github.com/cncf/velocity#past-reports&#34;&gt;past&lt;/a&gt; reports are available on GitHub, as well as a list and charts on the Google sheets below:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;All CNCF projects for July 2023-July 2024&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507&#34;&gt;All Linux Foundation projects for July 2023-July 2024&lt;/a&gt;&amp;nbsp;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507&#34;&gt;Top 30 open source projects for July 2023-July 2024&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;All of the scripts used to generate this data are at &lt;a href=&#34;https://github.com/cncf/velocity&#34;&gt;https://github.com/cncf/velocity&lt;/a&gt; (under an Apache 2.0 &lt;a href=&#34;https://www.cncf.io/blog/2017/02/01/cncf-recommends-aslv2/&#34;&gt;license&lt;/a&gt;). If you see any errors, please open an issue there.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Past blog posts about project velocity:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2024/01/17/a-look-back-at-cncf-linux-foundation-and-top-30-open-source-project-velocity-in-2023/&#34;&gt;A look back at CNCF, Linux Foundation, and top 30 open source project velocity in 2023&amp;nbsp;&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/10/27/october-2023-where-we-are-with-velocity-of-cncf-lf-and-top-30-open-source-projects/&#34;&gt;October 2023: where we are with velocity of CNCF, LF, and top 30 open source projects&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/01/11/a-look-at-the-2022-velocity-of-cncf-linux-foundation-and-top-30-open-source-projects/&#34;&gt;A look at the 2022 velocity of CNCF, Linux Foundation, and top 30 open source projects&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2022/08/23/mid-year-update-on-2022-cncf-linux-foundation-and-open-source-velocity/&#34;&gt;Mid-year update on 2022 CNCF, Linux Foundation, and open source velocity&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/12/15/end-of-year-update-on-cncf-and-open-source-velocity-in-2021/&#34;&gt;End of year update on CNCF and open source velocity in 2021&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/08/02/update-on-cncf-and-open-source-project-velocity-2020/&#34;&gt;Update on CNCF and Open Source Project Velocity 2020&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2017/06/05/30-highest-velocity-open-source-projects/&#34;&gt;The 30 highest velocity open source projects&lt;/a&gt;&amp;nbsp;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Chris Aniszczyk ÁöÑÂëòÂ∑•Â∏ñÂ≠ê&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Êó•Êúü/Êó∂Èó¥Ôºö&lt;/strong&gt;7 Êúà 11 Êó•‰∏äÂçà 8 ÁÇπ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Âú®ËøáÂéªÁöÑÂá†Âπ¥ÈáåÔºåÊàë‰ª¨‰∏ÄÁõ¥Âú®Ë∑üË∏™ÂºÄÊ∫êÈ°πÁõÆÁöÑÈÄüÂ∫¶ÔºåËøô‰ΩøÊàë‰ª¨ËÉΩÂ§üÁõëÊéß‰∏éÂºÄÂèë‰∫∫ÂëòÂíåÊúÄÁªàÁî®Êà∑‰∫ßÁîüÂÖ±È∏£ÁöÑË∂ãÂäøÂíåÊäÄÊúØ„ÄÇ‰∏∫‰∫ÜËøõË°åÊØîËæÉÔºåËØ∑Êü•ÁúãÊàë‰ª¨ÁöÑ&lt;a href=&#34;https://www.cncf.io/?s=Velocity&#34;&gt;ÂçöÂÆ¢&lt;/a&gt;‰∏≠ËøáÂéªÁöÑÊó∂Èó¥ËåÉÂõ¥„ÄÇ¬†&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‰ª•‰∏ãÊòØÊàë‰ªéËøô‰∫õÂõæË°®‰∏≠ÁúãÂà∞ÁöÑ‰∏ªË¶ÅÁªìËÆ∫Ôºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Kubernetes Âá≠ÂÄüÂÖ∂‰∏ÄËá¥‰∏îÊúÄÂ§ßÁöÑË¥°ÁåÆËÄÖÁæ§‰Ωì‰∏çÊñ≠Ëµ∞ÂêëÊàêÁÜü&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;OpenTelemetry ÁöÑË¥°ÁåÆËÄÖÂü∫Á°Ä‰∏çÊñ≠Êâ©Â§ßÔºåÂπ∂‰∏î‰ªçÁÑ∂ÊòØÁ¨¨‰∫åÈ´òÈÄüÂ∫¶‚Äã‚ÄãÈ°πÁõÆÔºõ‰ªñ‰ª¨ÊúÄËøëÊ∑ªÂä†‰∫Ü&lt;a href=&#34;https://opentelemetry.io/blog/2024/profiling/&#34;&gt;ÂàÜÊûê&lt;/a&gt;‰Ωú‰∏∫Êñ∞ÁöÑ‰ø°Âè∑Á±ªÂûã&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;ÂêéÂè∞‰∏çÊñ≠ÂèëÂ±ïÔºåËß£ÂÜ≥‰∫ÜÂºÄÂèëËÄÖ‰ΩìÈ™åÊñπÈù¢ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÁóõÁÇπ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;GitOps Âú®‰∫ëÂéüÁîüÁîüÊÄÅÁ≥ªÁªü‰∏≠‰ªçÁÑ∂ÂèëÊå•ÁùÄÈáçË¶Å‰ΩúÁî®ÔºåArgo Âíå Flux Á≠âÈ°πÁõÆÁªßÁª≠ÂüπËÇ≤Â§ßÂûãÁ§æÂå∫&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;ÂéªÂπ¥ÔºåCrossplane ÁöÑË¥°ÁåÆËÄÖÊï∞ÈáèÂ¢ûÈïø‰∫Ü 20% ‰ª•‰∏äÔºåÂèçÊò†Âá∫Âú®ÂºÄÊ∫êÈáçÊñ∞ËÆ∏ÂèØÈóÆÈ¢òÊó∂‰ª£ÂØπÂºÄÊ∫êÊéßÂà∂Âπ≥Èù¢ÁöÑÊ∏¥Êúõ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KeyCloak ÂéªÂπ¥‰Ωú‰∏∫Â≠µÂåñÈ°πÁõÆÂä†ÂÖ• CNCFÔºåÊã•ÊúâÊé®Âä®ÂºÄÊ∫êË∫´‰ªΩÂíåËÆøÈóÆÁÆ°ÁêÜÂêëÂâçÂèëÂ±ïÁöÑÂ§ßÂûãÁ§æÂå∫&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Âú®ËÆ∏Â§öÊÉÖÂÜµ‰∏ãÔºåCNCF È°πÁõÆ&lt;a href=&#34;https://www.cncf.io/case-studies/openai/&#34;&gt;ÊîØÊíëÂ§ßËßÑÊ®°‰∫∫Â∑•Êô∫ËÉΩÂü∫Á°ÄËÆæÊñΩ&lt;/a&gt;ÔºåÂπ∂‰∏î Kubeflow Âá∫Áé∞Âú®Ââç 30 Âêç‰∏≠2023Âπ¥È¶ñÊ¨°CNCFÈ°πÁõÆÂêçÂçï„ÄÇ&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;CNCF È°πÁõÆ ‚Äì ËøáÂéª 12 ‰∏™Êúà&lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;‰∫§‰∫íÂºèÂú∞Âõæ&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXeXEdq4E15JkoUMLwMRdFVFC0jeXT7El7VLVnVChbBOZk-v2I-03MLI5R8HMhgOrnlMS8LC-dQlKrKI4m8ybZHMzQ4Khm4_ 3dm5bboQhepmg4N9mf2IVWD4dOE4kxEv2T1rmQ0J33ATxZsT4Oi3XsxZh01x?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;cncf È°πÁõÆ&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Linux Âü∫Èáë‰ºöÈ°πÁõÆ ‚Äì ËøáÂéª 12 ‰∏™Êúà&lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507 &#34;&gt;‰∫§‰∫íÂºèÂú∞Âõæ&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXcBhcGzavp5kG5-l6g2MW-hNTvn2BIl2HIkb07TFm2dhUOJEymAhE18jAxE2_hgqitU9vJCRSDZ18EzppV9auq0eHq- qyGiYoT-JxQ5znQRMK_rq-mA_naXmtdQrNeuAeI8paEXZtUSxWxXLbsgRSBZi74?key= CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;LF È°πÁõÆ&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ââç 30 ‰∏™ÂºÄÊ∫êÈ°πÁõÆ ‚Äì ËøáÂéª 12 ‰∏™Êúà&lt;/strong&gt; (&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507 &#34;&gt;‰∫§‰∫íÂºèÂú∞Âõæ&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;imgËß£Á†Å=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdrzMUoz3xCWNweqwfxhzfI5koLh1Z68AbH4oWMjsQsH6FoBfZDx4bpsY2Cwyaxx7ZT9exDO1qtNy-oP00BuSczhhCZY CreLEDjdv0bQssLvSGEDkuVSaNCy2GQyjB0GUJIo4amk3C_St4m7tqM9zj8wAsv?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;ÊâìÂºÄÊ∫êÈ°πÁõÆ&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Ê≥®ÊÑè&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;ÔºöÊàë‰ª¨‰ΩøÁî®Ê∞îÊ≥°ÂõæÊù•ÊòæÁ§∫Êï∞ÊçÆÁöÑ‰∏â‰∏™ËΩ¥ÔºöÊèê‰∫§„ÄÅ‰ΩúËÄÖÂíåËØÑËÆ∫/ÊãâÂèñËØ∑Ê±ÇÔºåÂπ∂Âú®ÂØπÊï∞Êó•Âøó‰∏äÁªòÂà∂ÂõæË°®ÊòæÁ§∫Â§ßËåÉÂõ¥ÂÜÖÁöÑÊï∞ÊçÆ„ÄÇ¬†&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;Ê∞îÊ≥°Èù¢ÁßØ‰∏é‰ΩúËÄÖÊï∞ÈáèÊàêÊ≠£ÊØî&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;y ËΩ¥ÊòØÊãâÂèñËØ∑Ê±ÇÂíåÈóÆÈ¢òÁöÑÊÄªÊï∞&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;x ËΩ¥ÊòØÊèê‰∫§Ê¨°Êï∞&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊâÄÊúâ&lt;a href=&#34;https://github.com/cncf/velocity#current-reports&#34;&gt;ÂΩìÂâç&lt;/a&gt;Âíå&lt;a href=&#34;https://github.com/cncf/ velocity#past-reports&#34;&gt;ËøáÂéªÁöÑ&lt;/a&gt;Êä•ÂëäÂèØÂú® GitHub ‰∏äÊâæÂà∞Ôºå‰∏ãÈù¢ÁöÑ Google Â∑•‰ΩúË°®‰∏äËøòÊèê‰æõÂàóË°®ÂíåÂõæË°®Ôºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;2023 Âπ¥ 7 ÊúàËá≥ 2024 Âπ¥ 7 ÊúàÁöÑÊâÄÊúâ CNCF È°πÁõÆ&lt;/a&gt;&lt;/Êùé&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507&#34;&gt;2023 Âπ¥ 7 ÊúàËá≥ 2024 Âπ¥ 7 ÊúàÁöÑÊâÄÊúâ Linux Âü∫Èáë‰ºöÈ°πÁõÆ&lt;/a &gt;¬†&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507&#34;&gt;2023 Âπ¥ 7 ÊúàËá≥ 2024 Âπ¥ 7 ÊúàÊéíÂêçÂâç 30 ÁöÑÂºÄÊ∫êÈ°πÁõÆ&lt;/a&gt; &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Áî®‰∫éÁîüÊàêÊ≠§Êï∞ÊçÆÁöÑÊâÄÊúâËÑöÊú¨Âùá‰Ωç‰∫é &lt;a href=&#34;https://github.com/cncf/velocity&#34;&gt;https://github.com/cncf/velocity&lt;/a&gt;ÔºàÂú®Apache 2.0 &lt;a href=&#34;https://www.cncf.io/blog/2017/02/01/cncf-recommends-aslv2/&#34;&gt;ËÆ∏ÂèØËØÅ&lt;/a&gt;Ôºâ„ÄÇÂ¶ÇÊûúÊÇ®ÂèëÁé∞‰ªª‰ΩïÈîôËØØÔºåËØ∑Âú®ÈÇ£ÈáåÊèêÂá∫ÈóÆÈ¢ò„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ËøáÂéªÊúâÂÖ≥È°πÁõÆÈÄüÂ∫¶ÁöÑÂçöÂÆ¢ÊñáÁ´†Ôºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2024/01/17/a-look-back-at-cncf-linux-foundation-and-top-30-open-source- project-velocity-in-2023/&#34;&gt;CNCF„ÄÅLinux Âü∫Èáë‰ºöÂíå 2023 Âπ¥ÊéíÂêçÂâç 30 ÁöÑÂºÄÊ∫êÈ°πÁõÆÈÄüÂ∫¶ÂõûÈ°æ&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/10/27/october-2023-where-we-are-with-velocity-of-cncf-lf-and-top- 30-open-source-projects/&#34;&gt;2023 Âπ¥ 10 ÊúàÔºöCNCF„ÄÅLF ÂíåÂâç 30 ‰∏™ÂºÄÊ∫êÈ°πÁõÆÁöÑÂèëÂ±ïÈÄüÂ∫¶&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/01/11/a-look-at-the-2022-velocity-of-cncf-linux-foundation-and-top- 30-open-source-projects/&#34;&gt;CNCF„ÄÅLinux Âü∫Èáë‰ºöÂíåÂâç 30 ‰∏™ÂºÄÊ∫êÈ°πÁõÆ 2022 Âπ¥ÈÄüÂ∫¶‰∏ÄËßà&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2022/08/23/mid-year-update-on-2022-cncf-linux-foundation-and-open-source-velocity/ &#34;&gt;2022 Âπ¥ CNCF„ÄÅLinux Âü∫Èáë‰ºöÂíåÂºÄÊ∫êÈÄüÂ∫¶ÁöÑÂπ¥‰∏≠Êõ¥Êñ∞&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/12/15/end-of-year-update-on-cncf-and-open-source-velocity-in-2021/ &#34;&gt;CNCF Âπ¥ÁªàÊõ¥Êñ∞Âíå 2021 Âπ¥ÂºÄÊ∫êÈÄüÂ∫¶&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/08/02/update-on-cncf-and-open-source-project-velocity-2020/&#34;&gt;CNCF Âíå2020 Âπ¥ÂºÄÊ∫êÈ°πÁõÆÈÄüÂ∫¶&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2017/06/05/30-highest-velocity-open-source-projects/&#34;&gt;30 ‰∏™ÈÄüÂ∫¶ÊúÄÂø´ÁöÑÂºÄÊ∫êÈ°πÁõÆ&lt;/a &gt;¬†&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Wed, 10 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêEmbracing the future: our online store moves to a print-on-demand model„ÄëÊã•Êä±Êú™Êù•ÔºöÊàë‰ª¨ÁöÑÂú®Á∫øÂïÜÂ∫óËΩ¨ÂêëÊåâÈúÄÊâìÂç∞Ê®°Âºè</title>
      <link>https://www.cncf.io/blog/2024/07/08/embracing-the-future-our-online-store-moves-to-a-print-on-demand-model/</link>
      <description>„Äê&lt;p&gt;In today‚Äôs fast-paced digital world, businesses must evolve and adapt to meet their customers‚Äô changing needs. We are excited to announce that our &lt;a href=&#34;https://store.cncf.io/&#34;&gt;online store&lt;/a&gt; is transitioning to a Print On Demand (POD) model. This significant change brings numerous benefits for us and, more importantly, our vibrant community.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;What is Print On Demand?&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Print On Demand is a fulfillment method in which items are printed as soon as an order is placed rather than stored in inventory. This model allows for greater flexibility and customization, ensuring each product is made specifically for the person who orders it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;The Benefits of Moving to Print-On-Demand&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Sustainability&lt;/strong&gt;:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reduced Waste&lt;/strong&gt;: Traditional inventory systems often result in overproduction and excess stock, leading to waste. With POD, we only produce what is needed, minimizing our environmental footprint.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Eco-Friendly Materials&lt;/strong&gt;: Many POD services use sustainable materials and eco-friendly printing processes, reducing environmental impact.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reduced Overhead Costs&lt;/strong&gt;: By eliminating the need for warehousing and managing excess inventory, we can focus on improving other aspects of our service, such as customer support and product quality.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;How This Benefits Our Community&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Our shift to a Print-on-Demand model is a testament to our commitment to our community. By reducing waste and promoting sustainability, we can allow more flexibility in our offerings and ensure we always have inventory.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We are excited about this new chapter and look forward to providing you an even better shopping experience. Your support and feedback have been invaluable in making this transition possible. Together, we can positively impact both the environment and the creative community.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Thank you for being a part of our journey. Explore our new&lt;a href=&#34;https://store.cncf.io/collections/all-projects&#34;&gt; Print-On-Demand offerings today&lt;/a&gt; and discover the endless possibilities!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In the Future&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the coming months, we look forward to adding customization to our products and sourcing print-on-demand options with global partners to reduce shipping costs.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Stay tuned for more updates and exciting new products!&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;Âú®ÂΩì‰ªäÂø´ËäÇÂ•èÁöÑÊï∞Â≠ó‰∏ñÁïå‰∏≠Ôºå‰ºÅ‰∏öÂøÖÈ°ª‰∏çÊñ≠ÂèëÂ±ïÂíåÈÄÇÂ∫îÔºå‰ª•Êª°Ë∂≥ÂÆ¢Êà∑‰∏çÊñ≠ÂèòÂåñÁöÑÈúÄÊ±Ç„ÄÇÊàë‰ª¨ÂæàÈ´òÂÖ¥Âú∞ÂÆ£Â∏ÉÔºåÊàë‰ª¨ÁöÑ&lt;a href=&#34;https://store.cncf.io/&#34;&gt;Âú®Á∫øÂïÜÂ∫ó&lt;/a&gt;Ê≠£Âú®ËΩ¨Âèò‰∏∫ÊåâÈúÄÊâìÂç∞ (POD) Ê®°Âºè„ÄÇËøô‰∏ÄÈáçÂ§ßÂèòÂåñ‰∏∫Êàë‰ª¨Â∏¶Êù•‰∫Ü‰ºóÂ§öÂ•ΩÂ§ÑÔºåÊõ¥ÈáçË¶ÅÁöÑÊòØÔºå‰∏∫Êàë‰ª¨ÂÖÖÊª°Ê¥ªÂäõÁöÑÁ§æÂå∫Â∏¶Êù•‰∫Ü‰ºóÂ§öÂ•ΩÂ§Ñ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;‰ªÄ‰πàÊòØÊåâÈúÄÊâìÂç∞Ôºü&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊåâÈúÄÊâìÂç∞ÊòØ‰∏ÄÁßçÂ±•Ë°åÊñπÊ≥ïÔºåÂú®‰∏ãËÆ¢ÂçïÂêéÁ´ãÂç≥ÊâìÂç∞ÂïÜÂìÅÔºåËÄå‰∏çÊòØÂ∞ÜÂÖ∂Â≠òÂÇ®Âú®Â∫ìÂ≠ò‰∏≠„ÄÇËøôÁßçÊ®°ÂºèÊèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑÁÅµÊ¥ªÊÄßÂíåÂÆöÂà∂ÊÄßÔºåÁ°Æ‰øùÊØè‰ª∂‰∫ßÂìÅÈÉΩÊòØ‰∏ìÈó®‰∏∫ËÆ¢Ë¥≠ËÄÖÂà∂ÈÄ†ÁöÑ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;ËΩ¨ÂêëÊåâÈúÄÊâìÂç∞ÁöÑÂ•ΩÂ§Ñ&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;ÂèØÊåÅÁª≠ÊÄß&lt;/strong&gt;Ôºö&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;ÂáèÂ∞ëÊµ™Ë¥π&lt;/strong&gt;Ôºö‰º†ÁªüÂ∫ìÂ≠òÁ≥ªÁªüÈÄöÂ∏∏‰ºöÂØºËá¥Áîü‰∫ßËøáÂâ©ÂíåÂ∫ìÂ≠òËøáÂâ©Ôºå‰ªéËÄåÂØºËá¥Êµ™Ë¥π„ÄÇÈÄöËøá PODÔºåÊàë‰ª¨Âè™Áîü‰∫ßÊâÄÈúÄÁöÑ‰∫ßÂìÅÔºåÊúÄÂ§ßÈôêÂ∫¶Âú∞ÂáèÂ∞ëÂØπÁéØÂ¢ÉÁöÑÂΩ±Âìç„ÄÇ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;ÁéØ‰øùÊùêÊñô&lt;/strong&gt;ÔºöËÆ∏Â§ö POD ÊúçÂä°ÈÉΩ‰ΩøÁî®ÂèØÊåÅÁª≠ÊùêÊñôÂíåÁéØ‰øùÂç∞Âà∑Â∑•Ëâ∫ÔºåÂáèÂ∞ëÂØπÁéØÂ¢ÉÁöÑÂΩ±Âìç„ÄÇ&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;ÊïàÁéáÔºö&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Èôç‰ΩéÁÆ°ÁêÜÊàêÊú¨&lt;/strong&gt;ÔºöÈÄöËøáÊ∂àÈô§‰ªìÂÇ®ÂíåÁÆ°ÁêÜËøáÂâ©Â∫ìÂ≠òÁöÑÈúÄË¶ÅÔºåÊàë‰ª¨ÂèØ‰ª•‰∏ìÊ≥®‰∫éÊîπÂñÑÊúçÂä°ÁöÑÂÖ∂‰ªñÊñπÈù¢Ôºå‰æãÂ¶ÇÂÆ¢Êà∑ÊîØÊåÅÂíå‰∫ßÂìÅË¥®Èáè„ÄÇ&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;ËøôÂØπÊàë‰ª¨ÁöÑÁ§æÂå∫Êúâ‰ΩïÂ•ΩÂ§Ñ&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Êàë‰ª¨ÂêëÊåâÈúÄÊâìÂç∞Ê®°ÂºèÁöÑËΩ¨ÂèòËØÅÊòé‰∫ÜÊàë‰ª¨ÂØπÁ§æÂå∫ÁöÑÊâøËØ∫„ÄÇÈÄöËøáÂáèÂ∞ëÊµ™Ë¥πÂíå‰øÉËøõÂèØÊåÅÁª≠ÂèëÂ±ïÔºåÊàë‰ª¨ÂèØ‰ª•Âú®Êàë‰ª¨ÁöÑ‰∫ßÂìÅ‰∏≠Êèê‰æõÊõ¥Â§ßÁöÑÁÅµÊ¥ªÊÄßÔºåÂπ∂Á°Æ‰øùÊàë‰ª¨ÂßãÁªàÊúâÂ∫ìÂ≠ò„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Êàë‰ª¨ÂØπËøô‰∏™Êñ∞ÁØáÁ´†ÊÑüÂà∞ÂÖ¥Â•ãÔºåÂπ∂ÊúüÂæÖ‰∏∫ÊÇ®Êèê‰æõÊõ¥Â•ΩÁöÑË¥≠Áâ©‰ΩìÈ™å„ÄÇÊÇ®ÁöÑÊîØÊåÅÂíåÂèçÈ¶àÂØπ‰∫éÂÆûÁé∞Ëøô‰∏ÄËΩ¨ÂèòËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÂèØ‰ª•ÈΩêÂøÉÂçèÂäõÔºåÂØπÁéØÂ¢ÉÂíåÂàõÊÑèÁ§æÂå∫‰∫ßÁîüÁßØÊûÅÂΩ±Âìç„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊÑüË∞¢ÊÇ®ÂèÇ‰∏éÊàë‰ª¨ÁöÑÊóÖÁ®ã„ÄÇÁ´ãÂç≥Êé¢Á¥¢Êàë‰ª¨Êñ∞ÁöÑ&lt;a href=&#34;https://store.cncf.io/collections/all-projects&#34;&gt;ÊåâÈúÄÊâìÂç∞‰∫ßÂìÅ&lt;/a&gt;Âπ∂ÂèëÁé∞Êó†ÈôêÁöÑÂèØËÉΩÊÄßÔºÅ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Êú™Êù•&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Âú®Êé•‰∏ãÊù•ÁöÑÂá†‰∏™Êúà‰∏≠ÔºåÊàë‰ª¨ÊúüÂæÖ‰∏∫Êàë‰ª¨ÁöÑ‰∫ßÂìÅÂ¢ûÂä†ÂÆöÂà∂ÂäüËÉΩÔºåÂπ∂‰∏éÂÖ®ÁêÉÂêà‰Ωú‰ºô‰º¥‰∏ÄËµ∑ÈááË¥≠ÊåâÈúÄÊâìÂç∞ÈÄâÈ°πÔºå‰ª•Èôç‰ΩéËøêËæìÊàêÊú¨„ÄÇ¬†&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Êï¨ËØ∑ÂÖ≥Ê≥®Êõ¥Â§öÊõ¥Êñ∞Âíå‰ª§‰∫∫ÂÖ¥Â•ãÁöÑÊñ∞‰∫ßÂìÅÔºÅ&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Sun, 07 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêKubestronaut in Orbit: Eleni Grosdouli„ÄëÂú®ËΩ®ÈÅì‰∏äÁöÑ KubetronautÔºöEleni Grosdouli</title>
      <link>https://www.cncf.io/blog/2024/07/16/kubestronaut-in-orbit-eleni-grosdouli/</link>
      <description>„Äê&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1800&#34; height=&#34;945&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5.jpg&#34; alt=&#34;Eleni headshot&#34; class=&#34;wp-image-114334&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5.jpg 1800w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-300x158.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-1024x538.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-768x403.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-194x102.jpg 194w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-388x204.jpg 388w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-776x408.jpg 776w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-1552x816.jpg 1552w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-900x473.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-381x200.jpg 381w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-762x400.jpg 762w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-590x310.jpg 590w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-1180x620.jpg 1180w&#34; sizes=&#34;(max-width: 1800px) 100vw, 1800px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Get to know Eleni&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This week‚Äôs Kubstronaut in Orbit, Eleni Grosdouli, brings diverse experiences to her role as a DevOps Consulting Engineer at Cisco Systems. She‚Äôs the go-to person for DevOps and Kubernetes Automation, with a passion for networking, security, endpoint management, and endpoint security,&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you‚Äôd like to be a Kubestronaut like Eleni, find more details on the&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt; CNCF Kubestronaut &lt;/a&gt;page.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;When did you get started with Kubernetes‚Äìwhat was your first project?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;My journey started back in 2019. At the time, I was a network consulting engineer involved in network automation projects. After learning about automation technologies, I was involved in container creation images and Docker. The next logical step was to work with an open source offering for container orchestration. That‚Äôs when I became familiar with Kubernetes. Setting up a home lab was just the beginning towards the exciting journey of the Kubestronaut certification.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What are the primary CNCF projects you work on or use today? What projects have you enjoyed the most in your career?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cilium/cilium&#34;&gt;Cilium&lt;/a&gt; as CNI and Service mesh&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kyverno&#34;&gt;Kyverno&lt;/a&gt; for applying policies&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/projectsveltos&#34;&gt;Sveltos&lt;/a&gt; for Kubernetes add-on and applications deployment&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/argoproj/argo-cd&#34;&gt;ArgoCD&lt;/a&gt; for continuous deployments on specific topics&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/aquasecurity/kube-bench/&#34;&gt;Kubebench&lt;/a&gt;, &lt;a href=&#34;https://github.com/trivy&#34;&gt;Trivy&lt;/a&gt;, &lt;a href=&#34;https://github.com/falco&#34;&gt;Falco&lt;/a&gt;, &lt;a href=&#34;http://kubesec.io/&#34;&gt;Kubesec.io&lt;/a&gt; for security and compliance&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/prometheus&#34;&gt;Prometheus&lt;/a&gt;, &lt;a href=&#34;https://github.com/grafana&#34;&gt;Grafana&lt;/a&gt;, &lt;a href=&#34;https://github.com/cortexproject/cortex&#34;&gt;Cortex&lt;/a&gt; for observability&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How have the certs helped you in your career?&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I would say a lot. As most certifications are hands-on, there is no way to pass the exam unless you know what you are doing. It is proof to your employees and the customers you interact with that you know what exactly you are talking about.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How has CNCF helped you or influenced your career?&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Getting familiar with all these open source projects. Understanding how the community works and how I can assist in the efforts, was a mindset change!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What are some other books/sites/courses you recommend for people who want to work with k8s?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;All training material from Sander Van Vugt on O‚ÄôReilly Media&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The Kodekloud team for the DevOps track including the certification preparation material&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The ‚ÄòThe Linux DevOps Handbook‚Äô By Damian Wojs≈Çaw, Grzegorz Adamowicz&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The ‚ÄòLearning Helm‚Äô By Matt Butcher, Matt Farina, Josh Dolitsky&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The ‚ÄòProgramming Kubernetes‚Äô By Michael Hausenblas, Stefan Schimanski&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The ‚ÄòGetting Started with Grafana: Real-Time Dashboards for IT and Business Operations‚Äô By Ronald McCollam&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And many more!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What do you do in your free time?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I love nature and like to exercise! Usually, you will find me somewhere hiking and enjoying nature or practising rugby in a nearby local field. Spending quality time with family and friends is very important to me.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What would you tell someone who is just starting their K8s certification journey? Any tips or tricks?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Practise, practise, practise. Hands-on and working on projects is what will do the trick! Be confident to put yourself out there, surpass your limits and learn something new every day!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Today the cloud native ecosystem is way more than Kubernetes. Do you plan to get other cloud native certifications from the CNCF?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If it is hands-on, yes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How did you get involved with cloud native and Kubernetes in general?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When I started working with Kubernetes back in 2019-2020.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;figure class =‚Äúwp-block-image size-full‚Äù&gt;&lt;imgÂä†ËΩΩ=‚Äúlazy‚ÄùËß£Á†Å=‚ÄúÂºÇÊ≠•‚ÄùÂÆΩÂ∫¶=‚Äú1800‚ÄùÈ´òÂ∫¶=‚Äú945‚Äùsrc=‚Äúhttps://www.cncf.io/ wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5.jpg&#34; alt=&#34;Eleni headshot&#34; class=&#34;wp-image-114334&#34; srcset=&#34;https://www.cncf.io/wp-ÂÜÖÂÆπ/uploads/2024/07/Kubestronaut-in-Orbit-5.jpg 1800wÔºåhttps://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-300x158.jpg 300wÔºåhttps://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-1024x538.jpg 1024wÔºåhttps://www.cncf.io/wp-content/uploads /2024/07/Kubestronaut-in-Orbit-5-768x403.jpg 768wÔºåhttps://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-194x102.jpg 194w Ôºåhttps://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-388x204.jpg 388wÔºåhttps://www.cncf.io/wp-content/uploads/ 2024/07/Kubestronaut-in-Orbit-5-776x408.jpg 776wÔºåhttps://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-1552x816.jpg 1552wÔºå https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-900x473.jpg 900wÔºåhttps://www.cncf.io/wp-content/uploads/2024 /07/Kubestronaut-in-Orbit-5-381x200.jpg 381wÔºåhttps://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-762x400.jpg 762wÔºåhttps ://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-590x310.jpg 590wÔºåhttps://www.cncf.io/wp-content/uploads/2024/ 07/Kubestronaut-in-Orbit-5-1180x620.jpg 1180w‚ÄúÂ∞∫ÂØ∏=‚ÄùÔºàÊúÄÂ§ßÂÆΩÂ∫¶Ôºö1800pxÔºâ100vwÔºå1800px‚Äúreferrerpolicy=‚Äúno-referrer‚Äù&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;‰∫ÜËß£ Eleni&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Êú¨Âë® Orbit ÁöÑ Kubstronaut Eleni Grosdouli ‰∏∫Â•πÊãÖ‰ªª Cisco Systems ÁöÑ DevOps Âí®ËØ¢Â∑•Á®ãÂ∏àÂ∏¶Êù•‰∫Ü‰∏∞ÂØåÁöÑÁªèÈ™å„ÄÇÂ•πÊòØ DevOps Âíå Kubernetes Ëá™Âä®ÂåñÁöÑÂÖ≥ÈîÆ‰∫∫Áâ©ÔºåÂØπÁΩëÁªú„ÄÅÂÆâÂÖ®„ÄÅÁ´ØÁÇπÁÆ°ÁêÜÂíåÁ´ØÁÇπÂÆâÂÖ®ÂÖÖÊª°ÁÉ≠ÊÉÖÔºå&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Â¶ÇÊûúÊÇ®ÊÉ≥Êàê‰∏∫ÂÉè Eleni ‰∏ÄÊ†∑ÁöÑ KubestronautÔºåËØ∑Âú®&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt;CNCF Kubestronaut&lt;/a&gt;È°µÈù¢‰∏äÊâæÂà∞Êõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ¬†&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ÊÇ®‰ªÄ‰πàÊó∂ÂÄôÂºÄÂßã‰ΩøÁî® Kubernetes - ÊÇ®ÁöÑÁ¨¨‰∏Ä‰∏™È°πÁõÆÊòØ‰ªÄ‰πàÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊàëÁöÑÊóÖÁ®ãÂßã‰∫é 2019 Âπ¥„ÄÇÂΩìÊó∂ÔºåÊàëÊòØ‰∏ÄÂêçÂèÇ‰∏éÁΩëÁªúËá™Âä®ÂåñÈ°πÁõÆÁöÑÁΩëÁªúÂí®ËØ¢Â∑•Á®ãÂ∏à„ÄÇÂú®Â≠¶‰π†‰∫ÜËá™Âä®ÂåñÊäÄÊúØ‰πãÂêéÔºåÊàëÂèÇ‰∏é‰∫ÜÂÆπÂô®ÂàõÂª∫ÈïúÂÉèÂíåDocker„ÄÇ‰∏ã‰∏Ä‰∏™Âêà‰πéÈÄªËæëÁöÑÊ≠•È™§ÊòØ‰ΩøÁî®Áî®‰∫éÂÆπÂô®ÁºñÊéíÁöÑÂºÄÊ∫ê‰∫ßÂìÅ„ÄÇÂ∞±Âú®ÈÇ£Êó∂ÊàëÂºÄÂßãÁÜüÊÇâ Kubernetes„ÄÇÂª∫Á´ãÂÆ∂Â∫≠ÂÆûÈ™åÂÆ§Âè™ÊòØ Kubetronaut ËÆ§ËØÅÊøÄÂä®‰∫∫ÂøÉÁöÑÊóÖÁ®ãÁöÑÂºÄÂßã„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ÊÇ®Áé∞Âú®‰ªé‰∫ãÊàñ‰ΩøÁî®ÁöÑ‰∏ªË¶Å CNCF È°πÁõÆÊòØ‰ªÄ‰πàÔºüÂú®ÊÇ®ÁöÑËÅå‰∏öÁîüÊ∂Ø‰∏≠ÔºåÊÇ®ÊúÄÂñúÊ¨¢Âì™‰∫õÈ°πÁõÆÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cilium/cilium&#34;&gt;Cilium&lt;/a&gt; ‰Ωú‰∏∫ CNI ÂíåÊúçÂä°ÁΩëÊ†º&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kyverno&#34;&gt;Kyverno&lt;/a&gt; Áî®‰∫éÂ∫îÁî®Á≠ñÁï•&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/projectsveltos&#34;&gt;Sveltos&lt;/a&gt;ÔºåÁî®‰∫é Kubernetes Êèí‰ª∂ÂíåÂ∫îÁî®Á®ãÂ∫èÈÉ®ÁΩ≤&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/argoproj/argo-cd&#34;&gt;ArgoCD&lt;/a&gt;ÔºåÁî®‰∫éÁâπÂÆö‰∏ªÈ¢òÁöÑÊåÅÁª≠ÈÉ®ÁΩ≤&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/aquasecurity/kube-bench/&#34;&gt;Kubebench&lt;/a&gt;„ÄÅ&lt;a href=&#34;https://github.com/trivy&#34;&gt;Trivy&lt;/a&gt; &gt;„ÄÅ&lt;a href=&#34;https://github.com/falco&#34;&gt;Falco&lt;/a&gt;„ÄÅ&lt;a href=&#34;http://kubesec.io/&#34;&gt;Kubesec.io&lt;/a&gt; Á°Æ‰øùÂÆâÂÖ®ÊÄßÂíåÂêàËßÑÊÄß&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/prometheus&#34;&gt;ÊôÆÁΩóÁ±≥‰øÆÊñØ&lt;/a&gt;„ÄÅ&lt;a href=&#34;https://github.com/grafana&#34;&gt;Grafana&lt;/a&gt;„ÄÅ&lt;a href =&#34;https://github.com/cortexproject/cortex&#34;&gt;Cortex&lt;/a&gt; Áî®‰∫éÂèØËßÇÂØüÊÄß&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ëøô‰∫õËØÅ‰π¶ÂØπÊÇ®ÁöÑËÅå‰∏öÁîüÊ∂ØÊúâ‰ΩïÂ∏ÆÂä©Ôºü¬†&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊàëÊÉ≥ËØ¥ÂæàÂ§ö„ÄÇÁî±‰∫éÂ§ßÂ§öÊï∞ËÆ§ËØÅÈÉΩÊòØÂÆûË∑µÊÄßÁöÑÔºåÈô§ÈùûÊÇ®Áü•ÈÅìËá™Â∑±Âú®ÂÅö‰ªÄ‰πàÔºåÂê¶ÂàôÊó†Ê≥ïÈÄöËøáËÄÉËØï„ÄÇËøôÂêëÊÇ®ÁöÑÂëòÂ∑•Âíå‰∏éÊÇ®‰∫íÂä®ÁöÑÂÆ¢Êà∑ËØÅÊòéÊÇ®Á°ÆÂàáÂú∞Áü•ÈÅìËá™Â∑±Âú®ËØ¥‰ªÄ‰πà„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;CNCF Â¶Ç‰ΩïÂ∏ÆÂä©ÊàñÂΩ±ÂìçÊÇ®ÁöÑËÅå‰∏öÁîüÊ∂ØÔºü¬†&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÁÜüÊÇâÊâÄÊúâËøô‰∫õÂºÄÊ∫êÈ°πÁõÆ„ÄÇ‰∫ÜËß£Á§æÂå∫Â¶Ç‰ΩïËøê‰Ωú‰ª•ÂèäÊàëÂ¶Ç‰ΩïÂçèÂä©Ëøô‰∫õÂ∑•‰ΩúÔºåÊòØ‰∏ÄÁßçÂøÉÊÄÅÁöÑÊîπÂèòÔºÅ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ÊÇ®‰∏∫ÊÉ≥Ë¶Å‰ΩøÁî® k8s ÁöÑ‰∫∫Êé®Ëçê‰∫ÜÂì™‰∫õÂÖ∂‰ªñ‰π¶Á±ç/ÁΩëÁ´ô/ËØæÁ®ãÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;O‚ÄôReilly Media ‰∏ä Sander Van Vugt Êèê‰æõÁöÑÊâÄÊúâÂüπËÆ≠ÊùêÊñô&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Kodekloud Âõ¢ÈòüË¥üË¥£ DevOps ËΩ®ÈÅìÔºåÂåÖÊã¨ËÆ§ËØÅÂáÜÂ§áÊùêÊñô&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;„ÄäLinux DevOps ÊâãÂÜå„ÄãÔºå‰ΩúËÄÖÔºöDamian Wojs≈Çaw„ÄÅGrzegorz Adamowicz&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;‚ÄúÂ≠¶‰π†Â§¥Áõî‚ÄùÔºå‰ΩúËÄÖÔºöMatt Butcher„ÄÅMatt Farina„ÄÅJosh Dolitsky&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Michael Hausenblas„ÄÅStefan Schimanski ÁöÑ„ÄäKubernetes ÁºñÁ®ã„Äã&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Ronald McCollam ÁöÑ„ÄäGrafana ÂÖ•Èó®ÔºöIT Âíå‰∏öÂä°ËøêËê•ÂÆûÊó∂‰ª™Ë°®Êùø„Äã&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ËøòÊúâÊõ¥Â§öÔºÅ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;‰Ω†Á©∫Èó≤Êó∂Èó¥ÂÅö‰ªÄ‰πàÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÊàëÁÉ≠Áà±Â§ßËá™ÁÑ∂ÔºåÂñúÊ¨¢ËøêÂä®ÔºÅÈÄöÂ∏∏Ôºå‰Ω†‰ºöÂèëÁé∞ÊàëÂú®Êüê‰∏™Âú∞ÊñπÂæíÊ≠•ÊóÖË°åÔºå‰∫´ÂèóÂ§ßËá™ÁÑ∂ÔºåÊàñËÄÖÂú®ÈôÑËøëÁöÑÂΩìÂú∞Âú∫Âú∞ÁªÉ‰π†Ê©ÑÊ¶ÑÁêÉ„ÄÇ‰∏éÂÆ∂‰∫∫ÂíåÊúãÂèãÂÖ±Â∫¶ÁæéÂ•ΩÊó∂ÂÖâÂØπÊàëÊù•ËØ¥ÈùûÂ∏∏ÈáçË¶Å„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ÊÇ®‰ºöÂØπÂàöÂàöÂºÄÂßã K8s ËÆ§ËØÅ‰πãÊóÖÁöÑ‰∫∫ËØ¥‰∫õ‰ªÄ‰πàÔºüÊúâ‰ªÄ‰πàÊèêÁ§∫ÊàñÊäÄÂ∑ßÂêóÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÁªÉ‰π†ÔºåÁªÉ‰π†ÔºåÁªÉ‰π†„ÄÇ‰∫≤Ëá™Âä®ÊâãÂπ∂‰ªé‰∫ãÈ°πÁõÆÊâçÊòØÊàêÂäüÁöÑÁßòËØÄÔºÅËá™‰ø°Âú∞Â±ïÁé∞Ëá™Â∑±ÔºåË∂ÖË∂äËá™Â∑±ÁöÑÊûÅÈôêÔºåÊØèÂ§©Â≠¶‰π†Êñ∞‰∏úË•øÔºÅ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Â¶Ç‰ªäÔºå‰∫ëÂéüÁîüÁîüÊÄÅÁ≥ªÁªüÁöÑÊÑè‰πâËøú‰∏çÊ≠¢ Kubernetes„ÄÇÊÇ®ÊòØÂê¶ËÆ°ÂàíËé∑Âæó CNCF ÁöÑÂÖ∂‰ªñ‰∫ëÂéüÁîüËÆ§ËØÅÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Â¶ÇÊûúÊòØÂä®ÊâãÁöÑËØùÔºåÊòØÁöÑ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ÊÇ®ÊòØÂ¶Ç‰ΩïÊ∂âË∂≥‰∫ëÂéüÁîüÂíå Kubernetes ÁöÑÔºü&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÂΩìÊàëÂú® 2019-2020 Âπ¥ÂºÄÂßã‰ΩøÁî® Kubernetes Êó∂„ÄÇ&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Mon, 15 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>„ÄêWebAssembly components: the next wave of cloud native computing„ÄëWebAssembly ÁªÑ‰ª∂Ôºö‰∫ëÂéüÁîüËÆ°ÁÆóÁöÑ‰∏ã‰∏ÄÊ≥¢Êµ™ÊΩÆ</title>
      <link>https://www.cncf.io/blog/2024/07/09/webassembly-components-the-next-wave-of-cloud-native-computing/</link>
      <description>„Äê&lt;p&gt;&lt;em&gt;Member post by Liam Randall, Cosmonic CEO and CNCF Ambassador and Bailey Hayes, Cosmonic CTO, Bytecode Alliance TSC director, and WASI SG co-chair&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The advent of containers marked an inflection point for computing in the 21st century‚Äîa paradigm shift (&lt;a href=&#34;https://en.wikipedia.org/wiki/Paradigm_shift&#34;&gt;per Thomas Kuhn&lt;/a&gt;) that gave rise to the entire cloud native landscape. In 2024, the arrival of WebAssembly components represents a new inflection point, and the next paradigm shift is already underway.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Why components are made for the cloud&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly (Wasm) has been around for about a decade now‚Äîmuch as Linux kernel namespaces were in use for over a decade before the debut of Docker (and 15 years before Kubernetes reached the mainstream). Like the Linux namespace, the core WebAssembly standard has provided a firm foundation to build on.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the case of Wasm, that means we have a bytecode format and virtual instruction set architecture (ISA) that enable us to compile code from any language to a common standard, without needing to build for a particular kernel or architecture. Over the last decade, Wasm‚Äôs flexibility has proven itself not only in the browser, but‚Ä¶pretty much everywhere else as well. Today, Wasm is used every where from &lt;a href=&#34;https://www.amazon.science/blog/how-prime-video-updates-its-app-for-more-than-8-000-device-types&#34;&gt;Amazon Prime Video&lt;/a&gt; to &lt;a href=&#34;https://www.youtube.com/watch?v=ms5_0wOl79I&#34;&gt;Google Earth&lt;/a&gt; to &lt;a href=&#34;https://www.cncf.io/blog/2022/11/17/better-together-a-kubernetes-and-wasm-case-study/&#34;&gt;Adobe&lt;/a&gt; to &lt;a href=&#34;https://www.cncf.io/blog/2024/01/05/bringing-webassembly-to-telecoms-with-cncf-wasmcloud/&#34;&gt;telecoms like Orange&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Core Wasm is particularly well-suited to the cloud. In addition to their flexibility, Wasm binaries are tiny, sandboxed, and efficient, allowing for much greater density and speed of download or startup in cloud environments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly &lt;strong&gt;components&lt;/strong&gt; take all of this one step further. (Or many big leaps further, as we‚Äôll see.) Components are WebAssembly binaries conforming to an additional specification called the Component Model. Components bring all the same benefits of ordinary Wasm modules (as the Component Model is built on top of the Core WebAssembly specification), but they are also &lt;strong&gt;interoperable&lt;/strong&gt; and &lt;strong&gt;composable&lt;/strong&gt; with other components:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Interoperability&lt;/strong&gt; means that components can communicate over strictly-defined interfaces. There is a common set of standardized interfaces called the &lt;a href=&#34;https://github.com/WebAssembly/WASI/&#34;&gt;&lt;strong&gt;WASI&lt;/strong&gt;&lt;/a&gt;, consisting of high-level APIs (at various stages of proposal and standardization) for functionality like HTTP, CLI, blob storage, key-value storage, and much more. Developers can use their favorite libraries in their favorite languages, and once they compile the code to a Wasm component, other components can make use of the functions they expose‚Äîregardless of the language and libraries used to write &lt;em&gt;those&lt;/em&gt; components.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Composability&lt;/strong&gt; means that multiple components can be combined into a single component. Functions exposed (or ‚Äúexported‚Äù) by one component on a given interface can be used (or ‚Äúimported‚Äù) by another component, and those two components can be compiled together into a single binary. For a microservice‚Äîor any two pieces talking, really‚Äîcomposition is much more efficient than sending data over a network boundary where calls within a composed component happen with the same process in nanoseconds vs a network request in milliseconds. If you need to link components dynamically, you can achieve a compositional effect with distributed components using an open source transport protocol like &lt;a href=&#34;https://github.com/wrpc/wrpc&#34;&gt;wRPC (WIT over RPC)&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In short, if core WebAssembly binaries are flat-surfaced, fundamental building blocks, components are studded construction bricks‚Äîdesigned for building sophisticated, interconnected applications in new ways.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1086&#34; height=&#34;506&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/components.jpg&#34; alt=&#34;components&#34; class=&#34;wp-image-113994&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/components.jpg 1086w, https://www.cncf.io/wp-content/uploads/2024/07/components-300x140.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/components-1024x477.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/components-768x358.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/components-900x419.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/components-429x200.jpg 429w, https://www.cncf.io/wp-content/uploads/2024/07/components-858x400.jpg 858w&#34; sizes=&#34;(max-width: 1086px) 100vw, 1086px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If a WASI API doesn‚Äôt exist for your use case, no problem‚Äîyou can write your own interface in the open &lt;strong&gt;WebAssembly Interface Type (WIT)&lt;/strong&gt; interface description language. Open standards make the ecosystem infinitely extensible.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly enthusiasts (like us) often share this quote from Docker founder Solomon Hykes:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;If WASM+WASI existed in 2008, we wouldn&#39;t have needed to create Docker.&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We love that quote! But if we use it without explaining how components work, we run the risk of obscuring what is so transformative about them. It‚Äôs a mistake to think of components as a more efficient alternative to containers. Yes, they are efficient, portable delivery mechanisms for cloud native workloads‚Äîbut components are also an entirely new paradigm that unlocks entirely new models of computing.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;The next wave is already here&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The last couple of years have built to a convergence moment in which key pieces have fallen into place (or rather, been wrestled into place by many, many people working across the ecosystem). The most important of these is the release of WASI 0.2 and the Component Model in January 2024. With a common model and common APIs in place, the community has wasted no time building and updating a wide array of open source tools and native support in standard language libraries. For just a handful of examples:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasmtime&#34;&gt;&lt;strong&gt;Wasmtime&lt;/strong&gt;&lt;/a&gt;: Standalone runtime for WebAssembly components&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasm-tools&#34;&gt;&lt;strong&gt;&lt;code&gt;wasm-tools&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;: A multi-functional tool for interacting with components (read WIT interfaces, compose, and more)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://wasmcloud.com/&#34;&gt;&lt;strong&gt;wasmCloud&lt;/strong&gt;&lt;/a&gt;: Build and run components anywhere, including edge and distributed environments&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wit-deps&#34;&gt;&lt;strong&gt;&lt;code&gt;wit-deps&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;: Manage WIT dependencies for a component project&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasi-virt&#34;&gt;&lt;strong&gt;WASI Virt&lt;/strong&gt;&lt;/a&gt;: Use composition to virtualize a component within another encapsulating component&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Better yet, components increasingly integrate with common cloud native tools and standards:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;OCI has emerged as &lt;a href=&#34;https://tag-runtime.cncf.io/wgs/wasm/deliverables/wasm-oci-artifact/&#34;&gt;the standard for packaging components in registries&lt;/a&gt;.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Platforms like wasmCloud &lt;a href=&#34;https://wasmcloud.com/docs/kubernetes&#34;&gt;integrate with Kubernetes&lt;/a&gt;, OpenTelemetry, and Open Policy Agent.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Teams don‚Äôt have to start from square one to run components in production, but can use their existing cloud native tooling. For those brand new to components, the community is developing more and more resources like &lt;a href=&#34;https://component-model.bytecodealliance.org/&#34;&gt;The Component Model Book&lt;/a&gt; and &lt;a href=&#34;https://wasi.dev/&#34;&gt;WASI.dev&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;All the while, the component development experience is growing more and more polished across more and more languages. In the JavaScript ecosystem, &lt;a href=&#34;https://github.com/bytecodealliance/jco&#34;&gt;&lt;code&gt;jco&lt;/code&gt;&lt;/a&gt; enables JavaScript developers to write idiomatic code and compile to WebAssembly, while Rustaceans can compile directly to the &lt;a href=&#34;https://doc.rust-lang.org/nightly/rustc/platform-support/wasm32-wasip2.html&#34;&gt;&lt;code&gt;wasm32-wasip2&lt;/code&gt; target&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Around the world, teams are already unlocking new possibilities with components. In resource-constrained environments like edge devices on factory floors, manufacturing analytics company &lt;a href=&#34;https://www.youtube.com/watch?v=fQdkNGZqYZA&amp;amp;list=PLj6h78yzYM2MQteKoXxICTWiUdZYEw6RI&amp;amp;index=6&#34;&gt;MachineMetrics&lt;/a&gt; is using components to process high-frequency data directly on factory-floor devices and stream it to the cloud in real-time‚Äîrunning in places that containers typically don‚Äôt reach, all the while integrating with Kubernetes. Only a handful of months after the release of WASI 0.2, components are already changing what is possible.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Accelerating innovation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Developers‚Äîand ecosystems as a whole‚Äîdon‚Äôt necessarily adopt new technologies because they are theoretically more efficient or secure. New ways of working emerge when a technology enables us to be more effective, more productive, more innovative. WebAssembly components are doing just that‚Äîbreaking down language silos and revealing new opportunities.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Today, we‚Äôre about a decade out from the first days of Wasm. In the timeline of containers, this is where we reached an inflection point; the Component Model and WASI 0.2 are ushering in the same sort of paradigm shift. The tooling is there for developers to not just build with WebAssembly, but to be more productive, more effective, and more innovative. The common interfaces of WASI make component development cycles incredibly rapid, and components themselves incredibly flexible. Teams will take components to places that we can‚Äôt predict, and from here, the next wave of cloud native computing will only become more transformative.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;„Äë&lt;p&gt;&lt;em&gt;Cosmonic È¶ñÂ∏≠ÊâßË°åÂÆòÂÖº CNCF Â§ß‰Ωø Liam Randall Âíå Cosmonic È¶ñÂ∏≠ÊäÄÊúØÂÆò„ÄÅÂ≠óËäÇÁ†ÅËÅîÁõü TSC ÊÄªÁõëÂÖº WASI SG ËÅîÂêà‰∏ªÂ∏≠ Bailey Hayes ÂèëË°®ÁöÑ‰ºöÂëòÂ∏ñÂ≠ê&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÂÆπÂô®ÁöÑÂá∫Áé∞Ê†áÂøóÁùÄ 21 ‰∏ñÁ∫™ËÆ°ÁÆóÁöÑÊãêÁÇπ‚Äî‚ÄîËåÉÂºèËΩ¨ÂèòÔºà&lt;a href=&#34;https://en.wikipedia.org/wiki/Paradigm_shift&#34;&gt;ÊØè Thomas Kuhn&lt;/a&gt;ÔºâËøôÂÇ¨Áîü‰∫ÜÊï¥‰∏™‰∫ëÂéüÁîüÊôØËßÇ„ÄÇ 2024 Âπ¥ÔºåWebAssembly ÁªÑ‰ª∂ÁöÑÂà∞Êù•‰ª£Ë°®ÁùÄ‰∏Ä‰∏™Êñ∞ÁöÑÊãêÁÇπÔºå‰∏ã‰∏Ä‰∏™ËåÉÂºèËΩ¨ÂèòÂ∑≤ÁªèÂºÄÂßã„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;‰∏∫‰ªÄ‰πàÁªÑ‰ª∂ÊòØ‰∏∫‰∫ëËÄåËÆæËÆ°ÁöÑ&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly (Wasm) Â∑≤ÁªèÂ≠òÂú®‰∫ÜÂ§ßÁ∫¶ÂçÅÂπ¥ÔºåÂ∞±ÂÉè Linux ÂÜÖÊ†∏ÂëΩÂêçÁ©∫Èó¥Âú® Docker Âá∫Áé∞‰πãÂâçÂ∑≤Áªè‰ΩøÁî®‰∫ÜÂçÅÂ§öÂπ¥‰∏ÄÊ†∑Ôºà‰ª•Âèä Kubernetes Êàê‰∏∫‰∏ªÊµÅ‰πãÂâçÁöÑ 15 Âπ¥Ôºâ„ÄÇ‰∏é Linux ÂëΩÂêçÁ©∫Èó¥‰∏ÄÊ†∑ÔºåÊ†∏ÂøÉ WebAssembly Ê†áÂáÜÊèê‰æõ‰∫ÜÂùöÂÆûÁöÑÂü∫Á°Ä„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Â∞± Wasm ËÄåË®ÄÔºåËøôÊÑèÂë≥ÁùÄÊàë‰ª¨Êã•ÊúâÂ≠óËäÇÁ†ÅÊ†ºÂºèÂíåËôöÊãüÊåá‰ª§ÈõÜÊû∂ÊûÑ (ISA)Ôºå‰ΩøÊàë‰ª¨ËÉΩÂ§üÂ∞Ü‰ªª‰ΩïËØ≠Ë®ÄÁöÑ‰ª£Á†ÅÁºñËØë‰∏∫ÈÄöÁî®Ê†áÂáÜÔºåËÄåÊó†ÈúÄÈíàÂØπÁâπÂÆöÂÜÖÊ†∏ÊàñÊû∂ÊûÑËøõË°åÊûÑÂª∫„ÄÇÂú®ËøáÂéªÁöÑÂçÅÂπ¥‰∏≠ÔºåWasm ÁöÑÁÅµÊ¥ªÊÄß‰∏ç‰ªÖÂú®ÊµèËßàÂô®‰∏≠ÂæóÂà∞‰∫ÜËØÅÊòéÔºåËÄå‰∏î......Âá†‰πéÂú®ÂÖ∂‰ªñ‰ªª‰ΩïÂú∞ÊñπÈÉΩÂæóÂà∞‰∫ÜËØÅÊòé„ÄÇÂ¶Ç‰ªäÔºåWasm Â∑≤Ë¢´ÂπøÊ≥õ‰ΩøÁî® &lt;a href=&#34;https://www.amazon.science/blog/how-prime-video-updates-its-app-for-more-than-8-000-device-types &#34;&gt;Amazon Prime Video&lt;/a&gt; Ëá≥ &lt;a href=&#34;https://www.youtube.com/watch?v=ms5_0wOl79I&#34;&gt;Google Âú∞ÁêÉ&lt;/a&gt; Ëá≥ &lt;a href=&#34;https://www. cncf.io/blog/2022/11/17/better-together-a-kubernetes-and-wasm-case-study/&#34;&gt;Adobe&lt;/a&gt; Ëá≥ &lt;a href=&#34;https://www.cncf.io /blog/2024/01/05/bringing-web assembly-to-telecoms-with-cncf-wasmcloud/&#34;&gt;ÂÉè Orange ËøôÊ†∑ÁöÑÁîµ‰ø°ÂÖ¨Âè∏&lt;/a&gt;„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Core Wasm ÁâπÂà´ÈÄÇÂêà‰∫ë„ÄÇÈô§‰∫ÜÁÅµÊ¥ªÊÄß‰πãÂ§ñÔºåWasm ‰∫åËøõÂà∂Êñá‰ª∂‰ΩìÁßØÂ∞è„ÄÅÊ≤ôÁÆ±Âåñ‰∏îÈ´òÊïàÔºåÂèØÂú®‰∫ëÁéØÂ¢É‰∏≠ÂÆûÁé∞Êõ¥È´òÁöÑ‰∏ãËΩΩÊàñÂêØÂä®ÂØÜÂ∫¶ÂíåÈÄüÂ∫¶„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly &lt;strong&gt;ÁªÑ‰ª∂&lt;/strong&gt;‰ΩøËøô‰∏ÄÂàáÊõ¥Ëøõ‰∏ÄÊ≠•„ÄÇ ÔºàÊàñËÄÖÊàë‰ª¨Â∞ÜÁúãÂà∞Êõ¥Â§öÁöÑÂ∑®Â§ßÈ£ûË∑É„ÄÇÔºâÁªÑ‰ª∂ÊòØÁ¨¶ÂêàÁß∞‰∏∫ÁªÑ‰ª∂Ê®°ÂûãÁöÑÈôÑÂä†ËßÑËåÉÁöÑ WebAssembly ‰∫åËøõÂà∂Êñá‰ª∂„ÄÇÁªÑ‰ª∂Â∏¶Êù•‰∫ÜÊôÆÈÄö Wasm Ê®°ÂùóÁöÑÊâÄÊúâÁõ∏Âêå‰ºòÂäøÔºàÂõ†‰∏∫ÁªÑ‰ª∂Ê®°ÂûãÊûÑÂª∫Âú® Core WebAssembly ËßÑËåÉ‰πã‰∏äÔºâÔºå‰ΩÜÂÆÉ‰ª¨‰πüÂèØ‰ª•‰∏éÂÖ∂‰ªñÁªÑ‰ª∂&lt;strong&gt;‰∫íÊìç‰Ωú&lt;/strong&gt;Âíå&lt;strong&gt;ÂèØÁªÑÂêà&lt;/strong&gt; Ôºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;‰∫íÊìç‰ΩúÊÄß&lt;/strong&gt;ÊÑèÂë≥ÁùÄÁªÑ‰ª∂ÂèØ‰ª•ÈÄöËøá‰∏•Ê†ºÂÆö‰πâÁöÑÊé•Âè£ËøõË°åÈÄö‰ø°„ÄÇÊúâ‰∏ÄÁªÑÈÄöÁî®ÁöÑÊ†áÂáÜÂåñÊé•Âè£ÔºåÁß∞‰∏∫ &lt;a href=&#34;https://github.com/WebAssembly/WASI/&#34;&gt;&lt;strong&gt;WASI&lt;/strong&gt;&lt;/a&gt;ÔºåÁî±È´òÁ∫ß APIÔºà‰Ωç‰∫éÊèêÊ°àÂíåÊ†áÂáÜÂåñÁöÑÂêÑ‰∏™Èò∂ÊÆµÔºâÔºåÁî®‰∫é HTTP„ÄÅCLI„ÄÅblob Â≠òÂÇ®„ÄÅÈîÆÂÄºÂ≠òÂÇ®Á≠âÂäüËÉΩ„ÄÇÂºÄÂèë‰∫∫ÂëòÂèØ‰ª•Áî®‰ªñ‰ª¨ÂñúÊ¨¢ÁöÑËØ≠Ë®Ä‰ΩøÁî®‰ªñ‰ª¨ÂñúÊ¨¢ÁöÑÂ∫ìÔºå‰∏ÄÊó¶‰ªñ‰ª¨Â∞Ü‰ª£Á†ÅÁºñËØë‰∏∫ Wasm ÁªÑ‰ª∂ÔºåÂÖ∂‰ªñÁªÑ‰ª∂Â∞±ÂèØ‰ª•‰ΩøÁî®ÂÆÉ‰ª¨ÂÖ¨ÂºÄÁöÑÂäüËÉΩ - Êó†ËÆ∫Áî®‰∫éÁºñÂÜôËøô‰∫õÂäüËÉΩÁöÑËØ≠Ë®ÄÂíåÂ∫ìÊòØ‰ªÄ‰πà&lt;em&gt;&lt;/em&gt;ÁªÑ‰ª∂„ÄÇ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;ÂèØÁªÑÂêàÊÄß&lt;/strong&gt;ÊÑèÂë≥ÁùÄÂ§ö‰∏™ÁªÑ‰ª∂ÂèØ‰ª•ÁªÑÂêàÊàê‰∏Ä‰∏™ÁªÑ‰ª∂„ÄÇÁªôÂÆöÊé•Âè£‰∏äÁöÑ‰∏Ä‰∏™ÁªÑ‰ª∂ÂÖ¨ÂºÄÔºàÊàñ‚ÄúÂØºÂá∫‚ÄùÔºâÁöÑÂáΩÊï∞ÂèØ‰ª•Áî±Âè¶‰∏ÄÁªÑ‰ª∂‰ΩøÁî®ÔºàÊàñ‚ÄúÂØºÂÖ•‚ÄùÔºâÔºåÂπ∂‰∏îËøô‰∏§‰∏™ÁªÑ‰ª∂ÂèØ‰ª•‰∏ÄËµ∑ÁºñËØëÊàêÂçï‰∏™‰∫åËøõÂà∂Êñá‰ª∂„ÄÇÂØπ‰∫éÂæÆÊúçÂä°ÔºàÊàñËÄÖ‰ªª‰Ωï‰∏§‰∏™Ê≠£Âú®ËØ¥ËØùÁöÑÈÉ®ÂàÜÔºâÊù•ËØ¥ÔºåÁªÑÂêàÊØîÈÄöËøáÁΩëÁªúËæπÁïåÂèëÈÄÅÊï∞ÊçÆË¶ÅÈ´òÊïàÂæóÂ§öÔºåÂú®ÁΩëÁªúËæπÁïå‰∏≠ÔºåÁªÑÂêàÁªÑ‰ª∂ÂÜÖÁöÑË∞ÉÁî®Âú®Á∫≥ÁßíÂÜÖÈÄöËøáÁõ∏ÂêåÁöÑËøõÁ®ãËøõË°åÔºåËÄåÁΩëÁªúËØ∑Ê±ÇÂàôÂú®ÊØ´ÁßíÂÜÖÂèëÁîü„ÄÇÂ¶ÇÊûúÊÇ®ÈúÄË¶ÅÂä®ÊÄÅÈìæÊé•ÁªÑ‰ª∂ÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî®ÂºÄÊ∫ê‰º†ËæìÂçèËÆÆÔºàÂ¶Ç &lt;a href=&#34;https://github.com/wrpc/wrpc&#34;&gt;wRPC (WIT over RPC)&lt;/‰∏Ä‰∏™&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÁÆÄËÄåË®Ä‰πãÔºåÂ¶ÇÊûúÊ†∏ÂøÉ WebAssembly ‰∫åËøõÂà∂Êñá‰ª∂ÊòØÂπ≥Èù¢ÁöÑÂü∫Êú¨ÊûÑÂª∫ÂùóÔºåÈÇ£‰πàÁªÑ‰ª∂Â∞±ÊòØÈï∂ÂµåÁöÑÊûÑÈÄ†ÂùóÔºåÊó®Âú®‰ª•Êñ∞ÁöÑÊñπÂºèÊûÑÂª∫Â§çÊùÇÁöÑ‰∫íËøûÂ∫îÁî®Á®ãÂ∫è„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =‚Äúwp-block-image size-full‚Äù&gt;&lt;imgÂä†ËΩΩ=‚Äúlazy‚ÄùËß£Á†Å=‚ÄúÂºÇÊ≠•‚ÄùÂÆΩÂ∫¶=‚Äú1086‚ÄùÈ´òÂ∫¶=‚Äú506‚Äùsrc=‚Äúhttps://www.cncf.io/ wp-content/uploads/2024/07/components.jpg&#34; alt=&#34;ÁªÑ‰ª∂&#34; class=&#34;wp-image-113994&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07 /components.jpg 1086wÔºåhttps://www.cncf.io/wp-content/uploads/2024/07/components-300x140.jpg 300wÔºåhttps://www.cncf.io/wp-content/uploads/2024 /07/components-1024x477.jpg 1024wÔºåhttps://www.cncf.io/wp-content/uploads/2024/07/components-768x358.jpg 768wÔºåhttps://www.cncf.io/wp-content /uploads/2024/07/components-900x419.jpg 900wÔºåhttps://www.cncf.io/wp-content/uploads/2024/07/components-429x200.jpg 429wÔºåhttps://www.cncf.io /wp-content/uploads/2024/07/components-858x400.jpg 858w‚ÄúÂ∞∫ÂØ∏=‚ÄùÔºàÊúÄÂ§ßÂÆΩÂ∫¶Ôºö1086pxÔºâ100vwÔºå1086px‚Äúreferrerpolicy=‚Äúno-referrer‚Äù&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Â¶ÇÊûúÊÇ®ÁöÑÁî®‰æã‰∏çÂ≠òÂú® WASI APIÔºå‰πüÊ≤°ÊúâÈóÆÈ¢ò - ÊÇ®ÂèØ‰ª•‰ΩøÁî®ÂºÄÊîæÁöÑ &lt;strong&gt;WebAssembly Êé•Âè£Á±ªÂûã (WIT)&lt;/strong&gt; Êé•Âè£ÊèèËø∞ËØ≠Ë®ÄÁºñÂÜôËá™Â∑±ÁöÑÊé•Âè£„ÄÇÂºÄÊîæÊ†áÂáÜ‰ΩøÁîüÊÄÅÁ≥ªÁªüÊó†ÈôêÊâ©Â±ï„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly Áà±Â•ΩËÄÖÔºàÂÉèÊàë‰ª¨‰∏ÄÊ†∑ÔºâÁªèÂ∏∏ÂàÜ‰∫´ Docker ÂàõÂßã‰∫∫ Solomon Hykes ÁöÑËøôÂè•ËØùÔºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;Â¶ÇÊûú WASM+WASI Âú® 2008 Âπ¥Â∞±Â≠òÂú®ÔºåÊàë‰ª¨Â∞±‰∏çÈúÄË¶ÅÂàõÂª∫ Docker„ÄÇ&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Êàë‰ª¨ÂñúÊ¨¢ËøôÂè•ËØùÔºÅ‰ΩÜÂ¶ÇÊûúÊàë‰ª¨Âú®‰∏çËß£ÈáäÁªÑ‰ª∂Â¶Ç‰ΩïÂ∑•‰ΩúÁöÑÊÉÖÂÜµ‰∏ã‰ΩøÁî®ÂÆÉÔºåÊàë‰ª¨Â∞±ÊúâÂèØËÉΩÊé©ÁõñÂÆÉ‰ª¨ÁöÑÂèòÈù©ÊÄß„ÄÇÂ∞ÜÁªÑ‰ª∂ËßÜ‰∏∫ÊØîÂÆπÂô®Êõ¥ÊúâÊïàÁöÑÊõø‰ª£ÊñπÊ°àÊòØÈîôËØØÁöÑ„ÄÇÊòØÁöÑÔºåÂÆÉ‰ª¨ÊòØ‰∫ëÂéüÁîüÂ∑•‰ΩúË¥üËΩΩÁöÑÈ´òÊïà„ÄÅÂèØÁßªÊ§çÁöÑ‰∫§‰ªòÊú∫Âà∂Ôºå‰ΩÜÁªÑ‰ª∂‰πüÊòØ‰∏ÄÁßçÂÖ®Êñ∞ÁöÑËåÉ‰æãÔºåÂèØ‰ª•Ëß£ÈîÅÂÖ®Êñ∞ÁöÑËÆ°ÁÆóÊ®°Âûã„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;‰∏ã‰∏ÄÊ≥¢Êµ™ÊΩÆÂ∑≤ÁªèÂà∞Êù•&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ËøáÂéªÂá†Âπ¥Â∑≤ÁªèÂΩ¢Êàê‰∫Ü‰∏Ä‰∏™ËûçÂêàÊó∂ÂàªÔºåÂÖ≥ÈîÆÈÉ®ÂàÜÂ∑≤ÁªèÂ∞±‰ΩçÔºàÊàñËÄÖÊõ¥Á°ÆÂàáÂú∞ËØ¥ÔºåÊòØÁî±Êï¥‰∏™ÁîüÊÄÅÁ≥ªÁªü‰∏≠ÁöÑËÆ∏Â§ö‰∫∫Âä™ÂäõÂ∞±‰ΩçÔºâ„ÄÇÂÖ∂‰∏≠ÊúÄÈáçË¶ÅÁöÑÊòØ 2024 Âπ¥ 1 ÊúàÂèëÂ∏ÉÁöÑ WASI 0.2 ÂíåÁªÑ‰ª∂Ê®°Âûã„ÄÇÊúâ‰∫ÜÈÄöÁî®Ê®°ÂûãÂíåÈÄöÁî® APIÔºåÁ§æÂå∫Á´ãÂç≥ÊûÑÂª∫ÂíåÊõ¥Êñ∞‰∫ÜÂêÑÁßçÂºÄÊ∫êÂ∑•ÂÖ∑ÂíåÊ†áÂáÜÁöÑÊú¨Êú∫ÊîØÊåÅËØ≠Ë®ÄÂ∫ì„ÄÇ‰ªÖ‰∏æÂá†‰∏™‰æãÂ≠êÔºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasmtime&#34;&gt;&lt;strong&gt;Wasmtime&lt;/strong&gt;&lt;/a&gt;ÔºöWebAssembly ÁªÑ‰ª∂ÁöÑÁã¨Á´ãËøêË°åÊó∂&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasm-tools&#34;&gt;&lt;strong&gt;&lt;code&gt;wasm-tools&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;ÔºöÂ§öÂäüËÉΩÂ∑•ÂÖ∑Áî®‰∫é‰∏éÁªÑ‰ª∂‰∫§‰∫íÔºàÈòÖËØª WIT Êé•Âè£„ÄÅÊí∞ÂÜôÁ≠âÔºâ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://wasmcloud.com/&#34;&gt;&lt;strong&gt;wasmCloud&lt;/strong&gt;&lt;/a&gt;ÔºöÂú®‰ªª‰ΩïÂú∞ÊñπÊûÑÂª∫ÂíåËøêË°åÁªÑ‰ª∂ÔºåÂåÖÊã¨ËæπÁºòÂíåÂàÜÂ∏ÉÂºèÁéØÂ¢É&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wit-deps&#34;&gt;&lt;strong&gt;&lt;code&gt;wit-deps&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;ÔºöÁÆ°ÁêÜ WIT ‰æùËµñÈ°πÁªÑ‰ª∂È°πÁõÆ&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasi-virt&#34;&gt;&lt;strong&gt;WASI Virt&lt;/strong&gt;&lt;/a&gt;Ôºö‰ΩøÁî®ÁªÑÂêàÂú®Âè¶‰∏Ä‰∏™Â∞ÅË£ÖÁªÑ‰ª∂‰∏≠ËôöÊãüÂåñ‰∏Ä‰∏™ÁªÑ‰ª∂&lt;/li &gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Êõ¥Â•ΩÁöÑÊòØÔºåÁªÑ‰ª∂Ë∂äÊù•Ë∂äÂ§öÂú∞‰∏éÂ∏∏ËßÅÁöÑ‰∫ëÂéüÁîüÂ∑•ÂÖ∑ÂíåÊ†áÂáÜÈõÜÊàêÔºö&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;OCI Â∑≤Êàê‰∏∫&lt;a href=&#34;https://tag-runtime.cncf.io/wgs/wasm/deliverables/wasm-oci-artifact/&#34;&gt;Âú®Ê≥®ÂÜåË°®‰∏≠ÊâìÂåÖÁªÑ‰ª∂ÁöÑÊ†áÂáÜ&lt;/a&gt;„ÄÇ &lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;wasmCloud Á≠âÂπ≥Âè∞&lt;a href=&#34;https://wasmcloud.com/docs/kubernetes&#34;&gt;‰∏é Kubernetes ÈõÜÊàê&lt;/a&gt;„ÄÅOpenTelemetry ÂíåÂºÄÊîæÁ≠ñÁï•‰ª£ÁêÜ„ÄÇ&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Âõ¢Èòü‰∏çÂøÖ‰ªéÂ§¥ÂºÄÂßãÂú®Áîü‰∫ß‰∏≠ËøêË°åÁªÑ‰ª∂ÔºåËÄåÊòØÂèØ‰ª•‰ΩøÁî®Áé∞ÊúâÁöÑ‰∫ëÂéüÁîüÂ∑•ÂÖ∑„ÄÇÂØπ‰∫éÈÇ£‰∫õÂÖ®Êñ∞ÁöÑÁªÑ‰ª∂ÔºåÁ§æÂå∫Ê≠£Âú®ÂºÄÂèëË∂äÊù•Ë∂äÂ§öÁöÑËµÑÊ∫êÔºå‰æãÂ¶Ç &lt;a href=&#34;https://component-model.bytecodealliance.org/&#34;&gt;ÁªÑ‰ª∂Ê®°ÂûãÊâãÂÜå&lt;/a&gt; Âíå &lt;a href=&#34;https ://wasi.dev/&#34;&gt;WASI.dev&lt;/a&gt;„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‰∏éÊ≠§ÂêåÊó∂ÔºåÁªÑ‰ª∂ÂºÄÂèë‰ΩìÈ™åÂú®Ë∂äÊù•Ë∂äÂ§öÁöÑËØ≠Ë®Ä‰∏≠ÂèòÂæóË∂äÊù•Ë∂äÂÆåÂñÑ„ÄÇÂú® JavaScript ÁîüÊÄÅÁ≥ªÁªü‰∏≠Ôºå&lt;a href=&#34;https://github.com/bytecodealliance/jco&#34;&gt;&lt;code&gt;jco&lt;/code&gt;&lt;/a&gt; ‰Ωø JavaScript ÂºÄÂèë‰∫∫ÂëòËÉΩÂ§üÁºñÂÜôÊÉØÁî®‰ª£Á†ÅÂπ∂ÁºñËØë‰∏∫ WebAssemblyÔºåËÄå Rustaceans ÂèØ‰ª•Áõ¥Êé•ÁºñËØëÂà∞ &lt;a href=&#34;https://doc.rust-lang.org/nightly/rustc/platform-support/wasm32-wasip2.html&#34;&gt;&lt;code&gt;wasm32-wasip2&lt;/code&gt; ÁõÆÊ†á&lt;/a &gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‰∏ñÁïåÂêÑÂú∞ÁöÑÂõ¢ÈòüÂ∑≤ÁªèÂú®Âà©Áî®ÁªÑ‰ª∂Ëß£ÈîÅÊñ∞ÁöÑÂèØËÉΩÊÄß„ÄÇÂú®Â∑•ÂéÇËΩ¶Èó¥ËæπÁºòËÆæÂ§áÁ≠âËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠ÔºåÂà∂ÈÄ†ÂàÜÊûêÂÖ¨Âè∏ &lt;a href=&#34;https://www.youtube.com/watch?v=fQdkNGZqYZA&amp;list=PLj6h78yzYM2MQteKoXxICTWiUdZYEw6RI&amp;index=6&#34;&gt;MachineMetrics&lt;/a&gt; Ê≠£Âú®‰ΩøÁî®ÁªÑ‰ª∂Êù•Áõ¥Êé•Âú®Â∑•ÂéÇËΩ¶Èó¥ËÆæÂ§á‰∏äÂ§ÑÁêÜÈ´òÈ¢ëÊï∞ÊçÆÂπ∂Â∞ÜÂÖ∂ÂÆûÊó∂‰º†ËæìÂà∞‰∫ëÁ´Ø‚Äî‚ÄîÂú®ÂÆπÂô®ÈÄöÂ∏∏Êó†Ê≥ïÂà∞ËææÁöÑÂú∞ÊñπËøêË°åÔºåÂêåÊó∂‰∏é Kubernetes ÈõÜÊàê„ÄÇ WASI 0.2 ÂèëÂ∏É‰ªÖÂá†‰∏™ÊúàÂêéÔºåÁªÑ‰ª∂Â∞±Â∑≤ÁªèÊîπÂèò‰∫Ü‰∏ÄÂàáÂèØËÉΩ„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Âä†ÈÄüÂàõÊñ∞&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;ÂºÄÂèëËÄÖ‰ª•ÂèäÊï¥‰∏™ÁîüÊÄÅÁ≥ªÁªü‰∏ç‰∏ÄÂÆö‰ºöÈááÁî®Êñ∞ÊäÄÊúØÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Âú®ÁêÜËÆ∫‰∏äÊõ¥È´òÊïàÊàñÊõ¥ÂÆâÂÖ®„ÄÇÂΩìÊäÄÊúØ‰ΩøÊàë‰ª¨ÂèòÂæóÊõ¥ÊúâÊïà„ÄÅÊõ¥ÊúâÁîü‰∫ßÂäõ„ÄÅÊõ¥ÂÖ∑ÂàõÊñ∞ÊÄßÊó∂ÔºåÊñ∞ÁöÑÂ∑•‰ΩúÊñπÂºèÂ∞±‰ºöÂá∫Áé∞„ÄÇ WebAssembly ÁªÑ‰ª∂Ê≠£Âú®ËøôÊ†∑ÂÅö‚Äî‚ÄîÊâìÁ†¥ËØ≠Ë®ÄÂ≠§Â≤õÂπ∂Êè≠Á§∫Êñ∞ÁöÑÊú∫‰ºö„ÄÇ&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;‰ªäÂ§©ÔºåË∑ù Wasm ËØûÁîüÂ∑≤ÊúâÂçÅÂπ¥‰∫Ü„ÄÇÂú®ÂÆπÂô®ÁöÑÊó∂Èó¥ËΩ¥‰∏äÔºåËøôÊòØÊàë‰ª¨Âà∞Ëææ‰∏Ä‰∏™ÊãêÁÇπÁöÑÂú∞ÊñπÔºõÁªÑ‰ª∂Ê®°ÂûãÂíå WASI 0.2 Ê≠£Âú®ËøéÊù•ÂêåÊ†∑ÁöÑËåÉÂºèËΩ¨Âèò„ÄÇËØ•Â∑•ÂÖ∑‰∏ç‰ªÖÂèØ‰ª•ËÆ©ÂºÄÂèë‰∫∫Âëò‰ΩøÁî® WebAssembly ËøõË°åÊûÑÂª∫ÔºåËøòÂèØ‰ª•ÊèêÈ´òÁîü‰∫ßÂäõ„ÄÅÊïàÁéáÂíåÂàõÊñ∞ÊÄß„ÄÇ WASI ÁöÑÈÄöÁî®Êé•Âè£‰ΩøÁªÑ‰ª∂ÂºÄÂèëÂë®ÊúüÂèòÂæóÂºÇÂ∏∏Âø´ÈÄüÔºåÂπ∂‰∏îÁªÑ‰ª∂Êú¨Ë∫´ÈùûÂ∏∏ÁÅµÊ¥ª„ÄÇÂõ¢ÈòüÂ∞ÜÊääÁªÑ‰ª∂Â∏¶Âà∞Êàë‰ª¨Êó†Ê≥ïÈ¢ÑÊµãÁöÑÂú∞ÊñπÔºå‰ªéËøôÈáåÂºÄÂßãÔºå‰∏ã‰∏ÄÊ≥¢‰∫ëÂéüÁîüËÆ°ÁÆóÂè™‰ºöÂèòÂæóÊõ¥ÂÖ∑ÂèòÈù©ÊÄß„ÄÇ&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Mon, 08 Jul 2024 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>