<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>CNCF - Blog</title>
    <link>http://rsshub.rssforever.com/cncf</link>
    <description>CNCF - Blog - Powered by RSSHub</description>
    <managingEditor>contact@rsshub.app (RSSHub)</managingEditor>
    <item>
      <title>【Kubestronaut in Orbit: David Mukuzi】在轨道上的 Kubetronaut：David Mukuzi</title>
      <link>https://www.cncf.io/blog/2024/12/17/kubestronaut-in-orbit-david-mukuzi/</link>
      <description>【&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1650&#34; height=&#34;866&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1.jpg&#34; alt=&#34;kubestronaut &#34; class=&#34;wp-image-122044&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1.jpg 1650w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-300x157.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-1024x537.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-768x403.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-194x102.jpg 194w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-388x204.jpg 388w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-776x408.jpg 776w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-1552x816.jpg 1552w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-900x472.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-381x200.jpg 381w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-762x400.jpg 762w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-590x310.jpg 590w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-1180x620.jpg 1180w&#34; sizes=&#34;auto, (max-width: 1650px) 100vw, 1650px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Get to know David&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This week’s Kubestronaut in Orbit, David Mukuzi, is a DevOps Engineer in Nairobi, Kenya. David is driven by a deep-rooted enthusiasm for continuous learning and exploration of emerging technologies. He enjoys working with collaborative teams to build reliable, high-performing, and accessible solutions. David is focused on understanding and addressing customer challenges, with innovation as well as a practical approach to problem solving.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you’d like to be a Kubestronaut like David, get more details on the &lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt;CNCF Kubestronaut&lt;/a&gt; page.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;When did you get started with Kubernetes and/or cloud-native? What was your first project?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I started working with Kubernetes in 2018 – the company I worked for was running Kubernetes workloads both on bare metal and in the cloud.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;What are the primary CNCF projects you work on or use today?&amp;nbsp; What projects have you enjoyed the most in your career?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I get to interact with a majority of the &lt;a href=&#34;https://www.cncf.io/projects/&#34;&gt;Graduated CNCF projects &lt;/a&gt;daily, I’ve enjoyed &lt;a href=&#34;https://www.cncf.io/projects/kubernetes/&#34;&gt;Kubernetes&lt;/a&gt; and &lt;a href=&#34;https://www.cncf.io/projects/coredns/&#34;&gt;CoreDNS&lt;/a&gt; the most.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;How have the certs or CNCF helped you in your career?&amp;nbsp;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The practical and hands-on aspects of the certifications helped me to put the knowledge into practice and contributing to different projects have provided additional practice and learning.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;What are some other books/sites/courses you recommend for people who want to work with k8s?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.oreilly.com/library/view/networking-and-kubernetes/9781492081647/&#34;&gt;Networking and Kubernetes&lt;/a&gt; by James Strong and Vallery Lancey was helpful.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;What do you do in your free time?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I enjoy cooking and working out.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;What would you tell someone who is just starting their K8s certification journey? Any tips or tricks?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Practice breaking things and get hands-on experience.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;Today the Cloud native ecosystem is way more than Kubernetes. Do you plan to get other cloud native certifications from the CNCF ?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Now that I have all the Kubernetes certs I’m planning to take the&lt;a href=&#34;https://www.cncf.io/training/certification/pca/&#34;&gt; Prometheus Certified Associate (PCA)&lt;/a&gt; and the&lt;a href=&#34;https://www.cncf.io/training/certification/ica/&#34;&gt; Istio Certified Associate (ICA)&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“1650”高度=“866”src=“https://www.cncf.io/ wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1.jpg&#34; alt=&#34;kubestronaut&#34; class=&#34;wp-image-122044&#34; srcset =“https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1.jpg 1650w，https://www.cncf.io/wp-content/ uploads/2024/12/Kubestronaut-in-Orbit-12-1-300x157.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-1024x537.jpg 1024w，https://www.cncf.io/wp-content/uploads /2024/12/Kubestronaut-in-Orbit-12-1-768x403.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-194x102.jpg 194w，https://www.cncf.io/wp-content/uploads /2024/12/Kubestronaut-in-Orbit-12-1-388x204.jpg 388w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-776x408.jpg 776w，https://www.cncf.io/wp-content/uploads /2024/12/Kubestronaut-in-Orbit-12-1-1552x816.jpg 1552w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-900x472.jpg 900w，https://www.cncf.io/wp-content/uploads /2024/12/Kubestronaut-in-Orbit-12-1-381x200.jpg 381w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-762x400.jpg 762w，https://www.cncf.io/wp-content/uploads /2024/12/Kubestronaut-in-Orbit-12-1-590x310.jpg 590w, https://www.cncf.io/wp-content/uploads/2024/12/Kubestronaut-in-Orbit-12-1-1180x620.jpg 1180w“尺寸=”自动，（最大宽度：1650px）100vw，1650px “referrerpolicy =“no-referrer”&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;了解大卫&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;本周 Orbit 的 Kubetronaut David Mukuzi 是肯尼亚内罗毕的一名 DevOps 工程师。 David 对不断学习和探索新兴技术有着根深蒂固的热情。他喜欢与协作团队合作构建可靠、高性能且易于访问的解决方案。 David 专注于通过创新和解决问题的实用方法来理解和解决客户挑战。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果您想成为像 David 这样的 Kubetronaut，请在 &lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt;CNCF Kubestronaut&lt;/a&gt; 页面获取更多详细信息。&lt; /p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;您什么时候开始使用 Kubernetes 和/或云原生？您的第一个项目是什么？&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我于 2018 年开始使用 Kubernetes - 我工作的公司在裸机和云中运行 Kubernetes 工作负载。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;您现在从事或使用的主要 CNCF 项目是什么？  在您的职业生涯中您最喜欢哪些项目？&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我每天都可以与大多数&lt;a href=&#34;https://www.cncf.io/projects/&#34;&gt;毕业的 CNCF 项目&lt;/a&gt;进行互动，我很享受&lt;a href=&#34;https ://www.cncf.io/projects/kubernetes/&#34;&gt;Kubernetes&lt;/a&gt; 和 &lt;a href=&#34;https://www.cncf.io/projects/coredns/&#34;&gt;CoreDNS&lt;/a&gt; 最多。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;证书或 CNCF 对您的职业生涯有何帮助？ &lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;认证的实践和动手方面帮助我将知识付诸实践，并为不同的项目做出贡献，提供了额外的实践和学习。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;您为想要使用 k8s 的人推荐了哪些其他书籍/网站/课程？&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.oreilly.com/library/view/networking-and-kubernetes/9781492081647/&#34;&gt;James Strong 和 Vallery Lancey 的《网络和 Kubernetes》&lt;/a&gt; 很有帮助。&lt;/ p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;你空闲时间做什么？&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我喜欢烹饪和锻炼。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;您会对刚刚开始 K8s 认证之旅的人说些什么？有什么提示或技巧吗？&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;练习打破事物并获得实践经验。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;如今，云原生生态系统远不止 Kubernetes。您是否计划获得 CNCF 的其他云原生认证？&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;现在我已经拥有所有 Kubernetes 证书，我计划参加&lt;a href=&#34;https://www.cncf.io/training/certification/pca/&#34;&gt;Prometheus Certified Associate (PCA)&lt;/a &gt; 和&lt;a href=&#34;https://www.cncf.io/training/certification/ica/&#34;&gt;Istio 认证工程师 (ICA)&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Mon, 16 Dec 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Announcing 1000 Kubestronauts】宣布 1000 名 Kubetronauts</title>
      <link>https://www.cncf.io/blog/2024/12/20/announcing-1000-kubestronauts/</link>
      <description>【&lt;p&gt;CNCF is excited to share that since launching the Kubestronauts program less than a year ago, over 1000 &lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt;Kubestronauts&lt;/a&gt; have joined the program. A special welcome to our 1,000th Kubestronaut, Remy Mollandin!&amp;nbsp;Each of these 1000+ Kubestronauts have active certifications in all of CNCF’s Kubernetes certifications: &lt;a href=&#34;https://www.cncf.io/training/certification/kcna/&#34;&gt;&lt;strong&gt;KCNA&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.cncf.io/training/certification/kcsa/&#34;&gt;&lt;strong&gt;KCSA&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.cncf.io/training/certification/cka/&#34;&gt;&lt;strong&gt;CKA&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.cncf.io/training/certification/ckad/&#34;&gt;&lt;strong&gt;CKAD&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.cncf.io/training/certification/cks/&#34;&gt;&lt;strong&gt;CKS&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Unique Value of the Kubestronaut Program&amp;nbsp;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;CNCF’s Kubestronaut program is designed to drive the growth of Kubernetes and open source cloud native technologies by providing training, learning resources, networking opportunities, and professional development opportunities to help participants grow their cloud-native careers. There is no other comparable program in the world like this.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Becoming a Kubestronaut not only adds you to an elite group, but you also get:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;A Kubestronaut jacket&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;A Credly badge&amp;nbsp;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Access to the dedicated/private Kubestronaut Slack channel and mailing list&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Coupons for 50% off five certifications each year – for yourself or to share&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;20% off three CNCF events (KubeCon + CloudNativeCon or KubeDays) a year&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;“We’re very pleased to celebrate this major milestone—reaching 1,000 Kubestronauts in less than a year since the program’s inception! This achievement highlights the tremendous enthusiasm within the community to learn and grow with Kubernetes. We appreciate all our Kubestronauts and look forward to learning from them to ensure we produce better cloud native education materials for the world.” –Chris Aniszczyk, CTO, CNCF&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;What Kubestronauts say about the program:&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In automated environments, and especially with regards to security, it is extremely important to be aware of what’s currently being offered, what gets deprecated, and what best practices to follow. Continuous learning with this certification program will “push” you to get better. – &lt;a href=&#34;https://www.cncf.io/blog/2024/10/22/kubestronaut-in-orbit-maria-salcedo/&#34;&gt;Maria Salcedo&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The Kubernetes certifications are hands-on exams that will help you acquire not only knowledge of Kubernetes, but also the basic skills to build and troubleshoot application environments on Kubernetes by actually doing the work.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;–&lt;a href=&#34;https://www.cncf.io/blog/2024/09/10/kubestronaut-in-orbit-daiki-takasao/&#34;&gt;Daiki Takasao&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Learn more about how to become a &lt;a href=&#34;https://www.cncf.io/training/kubestronaut/kubestronaut-faq/&#34;&gt;Kubestronaut&lt;/a&gt;&amp;nbsp; and read about the highlighted &lt;a href=&#34;https://www.cncf.io/lf-author-category/kubestronaut/&#34;&gt;Kubestronauts in Orbit&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Discover more about our Kubestronauts&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Kubestronauts come from around the world with participants in 86 countries and every continent except Antarctica. Fourteen Kubestronauts are the only ones in their entire country! Although India and United States have the most Kubestronauts per country, some cities have more Kubestronauts than others.&amp;nbsp; Our top 5 cities for Kubestronauts are:&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Singapore, Singapore&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Seoul, South Korea&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;London, United Kingdom&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Oslo, Norway&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Tokyo, Japan&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&amp;nbsp;Our top five countries by number of Kubestronauts are:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;India&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;United States&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Germany&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The Netherlands&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;France&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We’d like to thank all the Kubestronauts for being committed members of the CNCF Open Source community.&amp;nbsp; Learn more about becoming a &lt;a href=&#34;https://www.cncf.io/training/kubestronaut/kubestronaut-faq/&#34;&gt;Kubestronaut&lt;/a&gt; and explore stories in our &lt;a href=&#34;https://www.cncf.io/lf-author-category/kubestronaut/&#34;&gt;Kubestronauts in Orbit&lt;/a&gt; series.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;CNCF 很高兴与大家分享，自启动 Kubetronauts 计划不到一年以来，已有超过 1000 名 &lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt;Kubestronauts&lt;/a&gt; 加入程序。特别欢迎我们的第 1,000 名 Kubetronaut，Remy Mollandin！ 这 1000 多名 Kubetronaut 中的每一位都拥有 CNCF 的所有 Kubernetes 认证的有效认证：&lt;a href=&#34;https://www.cncf.io/training/certification/kcna/&#34;&gt;&lt;strong&gt;KCNA&lt;/strong&gt;&lt;/a &gt;、&lt;a href=&#34;https://www.cncf.io/training/certification/kcsa/&#34;&gt;&lt;strong&gt;KCSA&lt;/strong&gt;&lt;/a&gt;、&lt;a href=&#34;https://www.cncf.io/training/certification/cka/&#34;&gt;&lt;strong&gt;CKA&lt;/strong&gt;&lt;/a&gt;，&lt;a href=&#34;https://www.cncf.io/training /certification/ckad/&#34;&gt;&lt;strong&gt;CKAD&lt;/strong&gt;&lt;/a&gt;、&lt;a href=&#34;https://www.cncf.io/training/certification/cks/&#34;&gt;&lt;strong&gt;CKS&lt;/strong &gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Kubestronaut 计划的独特价值&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;CNCF 的 Kubetronaut 计划旨在通过提供培训、学习资源、网络机会和专业发展机会来帮助参与者发展云原生职业，从而推动 Kubernetes 和开源云原生技术的发展。世界上没有其他类似的程序。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;成为 Kubetronaut 不仅能让您跻身精英团队，而且您还可以获得：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Kubestronaut 夹克&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Credly 徽章&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;访问专用/私人 Kubetronaut Slack 频道和邮件列表&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;每年获得五项认证可享受 50% 折扣的优惠券 - 供自己使用或与他人分享&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;每年三场 CNCF 活动（KubeCon + CloudNativeCon 或 KubeDays）可享受 20% 折扣&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;“我们非常高兴庆祝这一重要里程碑——自该计划启动以来不到一年的时间里，Kubestronaut 人数就达到了 1,000 名！这一成就凸显了社区内与 Kubernetes 一起学习和成长的巨大热情。我们感谢所有 Kubetronaut，并期待向他们学习，以确保我们为世界制作更好的云原生教育材料。” –Chris Aniszczyk，CNCF 首席技术官&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Kubestronauts 对该计划的评价：&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在自动化环境中，尤其是在安全方面，了解当前提供的内容、已弃用的内容以及应遵循的最佳实践非常重要。通过此认证计划持续学习将“推动”您变得更好。 – &lt;a href=&#34;https://www.cncf.io/blog/2024/10/22/kubestronaut-in-orbit-maria-salcedo/&#34;&gt;玛丽亚·萨尔塞多&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Kubernetes 认证是实践考试，不仅可以帮助您获得 Kubernetes 知识，还可以帮助您通过实际工作在 Kubernetes 上构建应用程序环境并进行故障排除的基本技能。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;–&lt;a href=&#34;https://www.cncf.io/blog/2024/09/10/kubestronaut-in-orbit-daiki-takasao/&#34;&gt;大辉高佐&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;了解更多了解如何成为 &lt;a href=&#34;https://www.cncf.io/training/kubestronaut/kubestronaut-faq/&#34;&gt;Kubestronaut&lt;/a&gt;，并阅读突出显示的 &lt;a href=&#34;https:// www.cncf.io/lf-author-category/kubetronaut/&#34;&gt;Kubestronauts 在轨道上&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;了解有关我们 Kubestronauts 的更多信息&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Kubestronauts 来自世界各地，参与者遍布 86 个国家和除南极洲以外的各大洲。整个国家仅有 14 名 Kubesteronauts！尽管印度和美国拥有最多的 Kubestronaus 国家，但有些城市的 Kubestronauts 数量多于其他城市。  最适合 Kubestronauts 的 5 个城市是：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol类=“wp-block-list”&gt;&#xA;&lt;li&gt;新加坡，新加坡&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;韩国首尔&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;英国伦敦&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;挪威奥斯陆&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;日本东京&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt; Kubestronaut 数量排名前五的国家/地区是：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol类=“wp-block-list”&gt;&#xA;&lt;li&gt;印度&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;美国&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;德国&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;荷兰&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;法国&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们要感谢所有 Kubetronauts 成为 CNCF 开源社区的忠实成员。  了解有关成为 &lt;a href=&#34;https://www.cncf.io/training/kubestronaut/kubestronaut-faq/&#34;&gt;Kubestronaut&lt;/a&gt; 的更多信息，并在我们的 &lt;a href=&#34;https://www 中探索故事.cncf.io/lf-author-category/kubetronaut/&#34;&gt;Kubestronauts in Orbit&lt;/a&gt; 系列。&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Thu, 19 Dec 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【The Node Resource Interface says “hi” to WebAssembly】节点资源接口向 WebAssembly 打招呼</title>
      <link>https://www.cncf.io/blog/2024/12/19/the-node-resource-interface-says-hi-to-webassembly/</link>
      <description>【&lt;p&gt;&lt;em&gt;Community blog post by Sascha Grunert, CRI-O maintainer&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The&amp;nbsp;&lt;a href=&#34;https://github.com/containerd/nri&#34;&gt;Node Resource Interface (NRI)&lt;/a&gt;&amp;nbsp;allows users to write plugins for&amp;nbsp;&lt;a href=&#34;https://opencontainers.org/&#34;&gt;Open Container Initiative (OCI)&lt;/a&gt;&amp;nbsp;compatible runtimes like&amp;nbsp;&lt;a href=&#34;https://cri-o.io/&#34;&gt;CRI-O&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://containerd.io/&#34;&gt;containerd&lt;/a&gt;. These plugins are capable of making controlled changes to containers at dedicated points in their life cycle. For example, by using the NRI it is possible to allocate extra node resources on container creation, which can be released again after the container got removed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;A plugin is written as daemon-like process which serves&amp;nbsp;&lt;a href=&#34;https://github.com/containerd/nri/blob/eaf78a9/pkg/api/api.proto&#34;&gt;a predefined&lt;/a&gt;&amp;nbsp;API based on&amp;nbsp;&lt;a href=&#34;https://github.com/containerd/ttrpc&#34;&gt;ttRPC&lt;/a&gt;&amp;nbsp;(&lt;a href=&#34;https://grpc.io/&#34;&gt;gRPC&lt;/a&gt;&amp;nbsp;for low-memory environments). This means in detail, that the NRI implementation in the runtime (CRI-O, containerd) will communicate using a UNIX Domain Socket (UDS) with each plugin and provide them with all required event data. For example, events can be container or pod sandbox&amp;nbsp;&lt;em&gt;creation&lt;/em&gt;,&amp;nbsp;&lt;em&gt;stopping&lt;/em&gt;&amp;nbsp;or&amp;nbsp;&lt;em&gt;removal&lt;/em&gt;, while corresponding data are the&amp;nbsp;&lt;em&gt;name&lt;/em&gt;,&amp;nbsp;&lt;em&gt;namespace&lt;/em&gt;&amp;nbsp;or corresponding&amp;nbsp;&lt;em&gt;annotations&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;On one hand plugins written as daemons have the benefit of persisting the current state out of the box, while on the other hand they come with a performance and management overhead. For that reason, the NRI also supports&amp;nbsp;&lt;a href=&#34;https://github.com/opencontainers/runtime-spec/blob/9ceba9f/config.md#posix-platform-hooks&#34;&gt;OCI hook&lt;/a&gt;-like&amp;nbsp;&lt;a href=&#34;https://github.com/containerd/nri/tree/693d64e/plugins/v010-adapter&#34;&gt;binary plugins&lt;/a&gt;&amp;nbsp;which get executed for each event. Combining the concept of small binary plugins with a universal standard like&amp;nbsp;&lt;a href=&#34;https://webassembly.org/&#34;&gt;WebAssembly (Wasm)&lt;/a&gt;&amp;nbsp;empowers the NRI to run on the edge and universally on all imaginable platforms.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;How it works&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/saschagrunert/nri/blob/844792ca69ee242510f3693ec05365ba26394c05/blog.md#how-it-works&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The required change for the NRI landed with&amp;nbsp;&lt;a href=&#34;https://github.com/containerd/nri/pull/121&#34;&gt;Pull Request containerd/nri#121&lt;/a&gt;. This change adds a&amp;nbsp;&lt;a href=&#34;https://github.com/knqyf263/go-plugin&#34;&gt;go-plugin&lt;/a&gt;&amp;nbsp;mechanism to the NRI. Each plugin gets compiled to Wasm, which means that it is size-efficient, memory-safe, automatically sandboxed and highly portable out of the box! The plugin system works in the same way as the NRI by using&amp;nbsp;&lt;a href=&#34;https://protobuf.dev/&#34;&gt;Protocol Buffers&lt;/a&gt;. This means, that the NRI can reuse the existing API for ttRPC, while the communication will happen in memory and not over the Remote Procedure Call (RPC).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One key benefit is that WebAssembly is designed as a portable compilation target for programming languages. Plugins compiled to Wasm can be used anywhere, which means that there is no requirement for multi architecture binaries. Beside that, the Wasm stack machine is designed to be encoded in a size and time efficient binary format, which make them great targets for binary execution.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Demo&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/saschagrunert/nri/blob/844792ca69ee242510f3693ec05365ba26394c05/blog.md#demo&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Unfortunately, the native golang (&lt;code&gt;go&lt;/code&gt;) compiler does not have full WebAssembly support yet, which means the plugins have to be compiled using the alternative&amp;nbsp;&lt;a href=&#34;https://tinygo.org/&#34;&gt;tinygo&lt;/a&gt;&amp;nbsp;compiler. An&amp;nbsp;&lt;a href=&#34;https://github.com/containerd/nri/blob/dd57194/plugins/wasm/plugin.go&#34;&gt;example Wasm plugin&lt;/a&gt;&amp;nbsp;within the NRI repository, can be compiled locally using:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;make $(pwd)/build/bin/wasm&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Or within a container image:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;make $(pwd)/build/bin/wasm TINYGO_DOCKER=1&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the future it may be possible to cross compile plugins using&amp;nbsp;&lt;code&gt;GOOS=wasip1 GOARCH=wasm go build&lt;/code&gt;, but that is not implemented yet (see&amp;nbsp;&lt;a href=&#34;https://github.com/knqyf263/go-plugin/issues/58&#34;&gt;knqyf263/go-plugin#58&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The resulting file should be a valid WebAssembly binary:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;file build/bin/wasm&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;build/bin/wasm: WebAssembly (wasm) binary module version 0x1 (MVP)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To try out the binary, we have to put it into the default local NRI directory. We also need to prefix the binary by a chosen index, which later refers to the plugin execution order:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo mkdir -p /opt/nri/plugins&#xA;sudo cp build/bin/wasm /opt/nri/plugins/10-wasm&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cri-o/cri-o&#34;&gt;CRI-O&lt;/a&gt;&amp;nbsp;v1.32 (which has been not released yet as time of writing) or it’s recent&amp;nbsp;&lt;a href=&#34;https://github.com/cri-o/cri-o/commits/main&#34;&gt;&lt;code&gt;main&lt;/code&gt;&lt;/a&gt;&amp;nbsp;branch can be used to verify that the plugin got loaded successfully:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo ./bin/crio&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;…&#xA;INFO[…] Create NRI interface&#xA;INFO[…] runtime interface created&#xA;INFO[…] Registered domain &#34;k8s.io&#34; with NRI&#xA;INFO[…] runtime interface starting up...&#xA;INFO[…] starting plugins...&#xA;INFO[…] discovered plugin 10-wasm&#xA;INFO[…] starting pre-installed NRI plugin &#34;wasm&#34;...&#xA;INFO[…] Found WASM plugin: /opt/nri/plugins/10-wasm&#xA;INFO[…] WASM: Got configure request&#xA;INFO[…] Synchronizing NRI (plugin) with current runtime state&#xA;INFO[…] synchronizing plugin 10-wasm&#xA;INFO[…] WASM: Got synchronize request&#xA;INFO[…] pre-installed NRI plugin &#34;10-wasm&#34; synchronization success&#xA;INFO[…] plugin invocation order&#xA;INFO[…]   #1: &#34;10-wasm&#34; (external:10-wasm[0])&#xA;…&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The partial logs above outline that the&amp;nbsp;&lt;code&gt;10-wasm&lt;/code&gt;&amp;nbsp;plugin got loaded and the WebAssembly plugin received a&amp;nbsp;&lt;code&gt;configure&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;synchronize&lt;/code&gt;&amp;nbsp;request. Log lines prefixed with&amp;nbsp;&lt;code&gt;WASM:&lt;/code&gt;&amp;nbsp;are directly invoked&amp;nbsp;&lt;a href=&#34;https://github.com/containerd/nri/blob/d138684/plugins/wasm/plugin.go#L39&#34;&gt;from the plugin itself&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;func (p *plugin) Configure(ctx context.Context, req *api.ConfigureRequest) (*api.ConfigureResponse, error) {&#xA;log(ctx, &#34;Got configure request&#34;)&#xA;return nil, nil&#xA;}&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The logging itself is achieved by a so-called&amp;nbsp;&lt;em&gt;host function&lt;/em&gt;. This function can be used to pass data back to the host (the NRI) and process them there (log to&amp;nbsp;&lt;code&gt;stderr&lt;/code&gt;). The plugin just has to fulfill the host&amp;nbsp;&lt;a href=&#34;https://github.com/containerd/nri/blob/d138684/plugins/wasm/plugin.go#L31-L36&#34;&gt;&lt;code&gt;log&lt;/code&gt;&amp;nbsp;function&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;func log(ctx context.Context, msg string) {&#xA;api.NewHostFunctions().Log(ctx, &amp;amp;api.LogRequest{&#xA;Msg:   &#34;WASM: &#34; + msg,&#xA;Level: api.LogRequest_LEVEL_INFO,&#xA;})&#xA;}&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And the NRI can fulfill the&amp;nbsp;&lt;a href=&#34;https://github.com/containerd/nri/blob/d138684/pkg/adaptation/plugin.go#L699-L715&#34;&gt;logging functionality&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;func (wasmHostFunctions) Log(ctx context.Context, request *api.LogRequest) (*api.Empty, error) {&#xA;switch request.GetLevel() {&#xA;case api.LogRequest_LEVEL_INFO:&#xA;log.Infof(ctx, request.GetMsg())&#xA;case api.LogRequest_LEVEL_WARN:&#xA;log.Warnf(ctx, request.GetMsg())&#xA;case api.LogRequest_LEVEL_ERROR:&#xA;log.Errorf(ctx, request.GetMsg())&#xA;default:&#xA;log.Debugf(ctx, request.GetMsg())&#xA;}&#xA;&#xA;return &amp;amp;api.Empty{}, nil&#xA;}&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If the plugin is loaded into memory and CRI-O now creates&amp;nbsp;&lt;a href=&#34;https://github.com/cri-o/cri-o/blob/e83973d/test/testdata/sandbox_config.json&#34;&gt;an example sandbox&lt;/a&gt;, then the WebAssembly instance will get executed accordingly by invoking the correct entry point:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo crictl runp test/testdata/sandbox_config.json&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;…&#xA;INFO[…] Running pod sandbox: test.crio/podsandbox1/POD  id=…&#xA;…&#xA;INFO[…] WASM: Got state change request with event: RUN_POD_SANDBOX&#xA;INFO[…] WASM: Got run pod sandbox request&#xA;…&#xA;INFO[…] Ran pod sandbox … with infra container: test.crio/podsandbox1/POD  id=…&#xA;…&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly NRI plugins allow to distribute functionality independently from the target platform in a secure and performant way. That makes them awesome for edge scenarios or for being distributed as OCI artifacts. For the future, it is imaginable to have a (semi) automatic reload functionality for the loaded in-memory plugins, but that is something we are currently elaborating.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Thank you for reading this blog post! If you have any questions or comments feel free to open an issue in the&amp;nbsp;&lt;a href=&#34;https://github.com/containerd/nri/issues/new&#34;&gt;NRI repository&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/saschagrunert/nri/blob/844792ca69ee242510f3693ec05365ba26394c05/blog.md#the-node-resource-interface-says-hi-to-webassembly&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;CRI-O 维护者 Sascha Grunert 的社区博客文章&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/containerd/nri&#34;&gt;节点资源接口 (NRI)&lt;/a&gt; 允许用户为&lt;a href=&#34;https://opencontainers.org 编写插件/&#34;&gt;开放容器计划 (OCI)&lt;/a&gt; 兼容的运行时，例如 &lt;a href=&#34;https://cri-o.io/&#34;&gt;CRI-O&lt;/a&gt; 和 &lt;a href=&#34;https://containerd.io/&#34;&gt;containerd&lt;/a&gt;。这些插件能够在容器生命周期的特定点对容器进行受控更改。例如，通过使用NRI，可以在容器创建时分配额外的节点资源，这些资源可以在容器被删除后再次释放。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;插件被编写为类似守护进程的进程，它服务于&lt;a href=&#34;https://github.com/containerd/nri/blob/eaf78a9/pkg/api/api.proto&#34;&gt;预定义的&lt;/a&gt;基于&lt;a href=&#34;https://github.com/containerd/ttrpc&#34;&gt;ttRPC&lt;/a&gt;（&lt;a href=&#34;https://grpc.io/&#34;&gt;gRPC&lt;/a&gt;）的 API内存环境）。具体来说，这意味着运行时（CRI-O、containerd）中的 NRI 实现将使用 UNIX 域套接字 (UDS) 与每个插件进行通信，并为它们提供所有必需的事件数据。例如，事件可以是容器或 pod 沙箱&lt;em&gt;创建&lt;/em&gt;、&lt;em&gt;停止&lt;/em&gt;或&lt;em&gt;删除&lt;/em&gt;，而相应的数据是&lt;em&gt;名称&lt;/em&gt; 、&lt;em&gt;命名空间&lt;/em&gt;或相应的&lt;em&gt;注释&lt;/em&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;一方面，作为守护进程编写的插件具有开箱即用地保留当前状态的优点，而另一方面，它们会带来性能和管理开销。因此，NRI 还支持类似 &lt;a href=&#34;https://github.com/opencontainers/runtime-spec/blob/9ceba9f/config.md#posix-platform-hooks&#34;&gt;OCI 挂钩&lt;/a&gt;针对每个事件执行的&lt;a href=&#34;https://github.com/containerd/nri/tree/693d64e/plugins/v010-adapter&#34;&gt;二进制插件&lt;/a&gt;。将小型二进制插件的概念与 &lt;a href=&#34;https://web assembly.org/&#34;&gt;WebAssembly (Wasm)&lt;/a&gt; 等通用标准相结合，使 NRI 能够在边缘并在所有可以想象的平台上普遍运行。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;它是如何工作的&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/saschagrunert/nri/blob/844792ca69ee242510f3693ec05365ba26394c05/blog.md#how-it-works&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;NRI 所需的更改通过&lt;a href=&#34;https://github.com/containerd/nri/pull/121&#34;&gt;拉取请求 containerd/nri#121&lt;/a&gt; 实现。此更改为 NRI 添加了&lt;a href=&#34;https://github.com/knqyf263/go-plugin&#34;&gt;go-plugin&lt;/a&gt; 机制。每个插件都会编译为 Wasm，这意味着它具有大小高效、内存安全、自动沙箱和开箱即用的高度可移植性！该插件系统的工作方式与 NRI 相同，均使用&lt;a href=&#34;https://protobuf.dev/&#34;&gt;协议缓冲区&lt;/a&gt;。这意味着，NRI 可以重用现有的 ttRPC API，而通信将在 mem 中进行而非通过远程过程调用 (RPC)。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;一个主要好处是 WebAssembly 被设计为编程语言的可移植编译目标。编译为 Wasm 的插件可以在任何地方使用，这意味着不需要多架构二进制文件。除此之外，Wasm 堆栈机被设计为以大小和时间高效的二进制格式进行编码，这使它们成为二进制执行的绝佳目标。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;演示&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/saschagrunert/nri/blob/844792ca69ee242510f3693ec05365ba26394c05/blog.md#demo&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;不幸的是，原生 golang (&lt;code&gt;go&lt;/code&gt;) 编译器尚不具备完整的 WebAssembly 支持，这意味着必须使用替代方案来编译插件&lt;a href=&#34;https://tinygo.org /&#34;&gt;tinygo&lt;/a&gt; 编译器。 NRI 存储库中的&lt;a href=&#34;https://github.com/containerd/nri/blob/dd57194/plugins/wasm/plugin.go&#34;&gt;示例 Wasm 插件&lt;/a&gt;，可以使用以下命令进行本地编译：&lt; /p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;make $(pwd)/build/bin/wasm&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;或者在容器镜像内：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;make $(pwd)/build/bin/wasm TINYGO_DOCKER=1&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;将来可能可以使用&lt;code&gt;GOOS=wasip1 GOARCH=wasm go build&lt;/code&gt;交叉编译插件，但这尚未实现（请参阅&lt;a href=&#34;https://github.com/goos/wasip1/GOARCH&#34;）。 com/knqyf263/go-plugin/issues/58&#34;&gt;knqyf263/go-plugin#58&lt;/a&gt;）。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;生成的文件应该是有效的 WebAssembly 二进制文件：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;文件 build/bin/wasm&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;build/bin/wasm：WebAssembly (wasm) 二进制模块版本 0x1 (MVP)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;要试用该二进制文件，我们必须将其放入默认的本地 NRI 目录中。我们还需要在二进制文件中添加所选索引的前缀，该索引稍后引用插件执行顺序：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo mkdir -p /opt/nri/plugins&#xA;sudo cp build/bin/wasm /opt/nri/plugins/10-wasm&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cri-o/cri-o&#34;&gt;CRI-O&lt;/a&gt; v1.32（截至撰写本文时尚未发布）或最近发布的版本&lt;a href=&#34;https://github.com/cri-o/cri-o/commits/main&#34;&gt;&lt;code&gt;main&lt;/code&gt;&lt;/a&gt; 分支可用于验证插件是否已成功加载：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo ./bin/crio&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;...&#xA;INFO[…] 创建 NRI 接口&#xA;INFO[…] 运行时接口已创建&#xA;INFO[…] 已向 NRI 注册域名“k8s.io”&#xA;信息[...]运行时界面正在启动...&#xA;信息[...]正在启动插件...&#xA;INFO[…] 发现插件 10-wasm&#xA;INFO[…] 开始预装 NRI 插件“wasm”…&#xA;INFO[…] 找到 WASM 插件：/opt/nri/plugins/10-wasm&#xA;INFO[…] WASM：收到配置请求&#xA;INFO[…] 正在将 NRI（插件）与当前运行时状态同步&#xA;INFO[…] 同步插件 10-wasmINFO[…] WASM：收到同步请求&#xA;INFO[…]预装NRI插件“10-wasm”同步成功&#xA;INFO[…] 插件调用顺序&#xA;信息[…]#1：“10-wasm”（外部：10-wasm[0]）&#xA;……&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;上面的部分日志概述了 &lt;code&gt;10-wasm&lt;/code&gt; 插件已加载，并且 WebAssembly 插件收到了&lt;code&gt;配置&lt;/code&gt;和&lt;code&gt;同步&lt;/code&gt;请求。以 &lt;code&gt;WASM:&lt;/code&gt; 为前缀的日志行直接从&lt;a href=&#34;https://github.com/containerd/nri/blob/d138684/plugins/wasm/plugin.go#L39&#34;&gt;调用插件本身&lt;/a&gt;：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;func (p *plugin) 配置(ctx context.Context, req *api.ConfigureRequest) (*api.ConfigureResponse, error) {&#xA;log(ctx, &#34;收到配置请求&#34;)&#xA;返回零，零&#xA;}&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;日志记录本身是通过所谓的&lt;em&gt;主机功能&lt;/em&gt;来实现的。此函数可用于将数据传递回主机（NRI）并在那里进行处理（记录到&lt;code&gt;stderr&lt;/code&gt;）。该插件只需满足主机&lt;a href=&#34;https://github.com/containerd/nri/blob/d138684/plugins/wasm/plugin.go#L31-L36&#34;&gt;&lt;code&gt;log&lt;/code&gt;功能&lt;/a&gt;：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;func log(ctx context.Context, msg string) {&#xA;api.NewHostFunctions().Log(ctx, &amp;api.LogRequest{&#xA;消息：“WASM：”+消息，&#xA;级别：api.LogRequest_LEVEL_INFO，&#xA;})&#xA;}&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;NRI 可以实现&lt;a href=&#34;https://github.com/containerd/nri/blob/d138684/pkg/adaptation/plugin.go#L699-L715&#34;&gt;日志记录功能&lt;/a&gt;： &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;func (wasmHostFunctions) Log(ctx context.Context, request *api.LogRequest) (*api.Empty, error) {&#xA;切换 request.GetLevel() {&#xA;案例 api.LogRequest_LEVEL_INFO：&#xA;log.Infof(ctx, request.GetMsg())&#xA;案例 api.LogRequest_LEVEL_WARN：&#xA;log.Warnf(ctx, request.GetMsg())&#xA;案例 api.LogRequest_LEVEL_ERROR：&#xA;log.Errorf(ctx, request.GetMsg())&#xA;默认：&#xA;log.Debugf(ctx, request.GetMsg())&#xA;}&#xA;&#xA;返回 &amp;api.Empty{}, nil&#xA;}&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果插件已加载到内存中，并且 CRI-O 现在会创建 &lt;a href=&#34;https://github.com/cri-o/cri-o/blob/e83973d/test/testdata/sandbox_config.json&#34;&gt;一个示例沙箱&lt;/a&gt;，然后 WebAssembly 实例将通过调用正确的入口点来相应执行：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo crictl runp test/testdata/sandbox_config.json&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;...&#xA;INFO[…] 运行 pod 沙箱：test.crio/podsandbox1/POD id=…&#xA;……&#xA;INFO[…] WASM：收到带有事件的状态更改请求：RUN_POD_SANDBOX&#xA;INFO[…] WASM：已运行 pod 沙箱请求&#xA;……&#xA;INFO[…] Ran pod sandbox…与基础容器：test.crio/podsandbox1/POD id=…&#xA;……&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly NRI 插件允许以安全且高性能的方式独立于目标平台分发功能。这使得它们非常适合边缘场景或作为 OCI 工件进行分发s。对于未来，可以想象为加载的内存插件提供（半）自动重新加载功能，但这是我们目前正在阐述的内容。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;感谢您阅读这篇博文！如果您有任何问题或意见，请随时在 &lt;a href=&#34;https://github.com/containerd/nri/issues/new&#34;&gt;NRI 存储库&lt;/a&gt;中提出问题。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/saschagrunert/nri/blob/844792ca69ee242510f3693ec05365ba26394c05/blog.md#the-node-resource-interface-says-hi-to-web assembly&#34;&gt;&lt;/a&gt;&lt; /p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Wed, 18 Dec 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Cloud native solutions for SMBs: unlocking scalability and resilience】面向中小型企业的云原生解决方案：释放可扩展性和弹性</title>
      <link>https://www.cncf.io/blog/2024/12/20/cloud-native-solutions-for-smbs-unlocking-scalability-and-resilience/</link>
      <description>【&lt;p&gt;&lt;em&gt;Ambassador post by &lt;a href=&#34;https://www.linkedin.com/in/ar4mirez/&#34;&gt;Angel Ramirez&lt;/a&gt;, CEO of Cuemby and CNCF ambassador.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As the technology landscape evolves, businesses must embrace innovations that enable them to adapt and thrive. Cloud-native technologies, championed by the CNCF community, have emerged as essential tools for achieving scalability, agility, and resilience. For small and medium-sized businesses (SMBs), these solutions offer a unique opportunity to streamline operations, reduce costs, and accelerate growth.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Drawing from Cuemby’s experience in implementing cloud-native practices, this article explores the transformative potential of these technologies and their practical impact on SMBs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;The Business Case for Cloud Native Technologies&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;Cost Efficiency with Kubernetes and CNCF Projects&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Migrating to cloud-native solutions helps SMBs reallocate IT budgets more effectively. Kubernetes, a CNCF graduated project, eliminates the need for costly hardware investments by orchestrating containerized applications across cloud environments. Businesses only pay for the resources they use, significantly lowering operational costs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Take a mid-sized company with an annual IT budget of $5 million: moving to the cloud could result in savings ranging from $750,000 to $1.25 million annually (Flexera, 2023). These freed-up resources can then be strategically redirected toward growth-driving activities, such as marketing, product development, or customer acquisition.&lt;br&gt;&lt;br&gt;But the savings don’t stop there. Cloud solutions operate on a pay-as-you-go model, meaning businesses only pay for the resources they actually use. This is especially advantageous for companies with fluctuating demand, such as seasonal industries. For example, organizations in retail or travel could cut costs by up to 30% during low-demand periods by scaling down their usage (Deloitte, 2022). This flexibility not only prevents overinvestment but also optimizes operational expenses.&lt;br&gt;&lt;br&gt;In addition to infrastructure savings, maintenance costs are significantly reduced. On-premises systems often require 15% to 20% of the IT budget for hardware refreshes and updates (TechRepublic, 2021). With cloud migration, these tasks are handled by the cloud provider, enabling businesses to focus on their core objectives rather than routine IT upkeep.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;Scalability Without Limits&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Cloud-native platforms like Kubernetes enable businesses to scale operations in minutes rather than months. According to CNCF reports, organizations adopting Kubernetes see a 60% reduction in time-to-market for new services.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;During seasonal peaks, industries such as retail have relied on Kubernetes to dynamically scale workloads. For example, during high-demand periods like Black Friday, businesses using cloud platforms report improved performance with up to 40% less downtime compared to those relying on on-premises infrastructure (McKinsey, 2021). This ensures that operations remain uninterrupted, enhancing customer satisfaction and protecting revenue during crucial periods.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Practical Use Cases for SMBs&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;Simplifying Maintenance with Open Source Tools&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Real-time monitoring and logging are essential for minimizing downtime. Tools like Prometheus and Fluentd, widely adopted CNCF projects, offer robust solutions for automating these tasks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Small businesses often lack the capacity to maintain and update traditional IT infrastructure. By migrating to the cloud, they can eliminate 15% to 20% of their IT budget typically allocated to hardware maintenance and upgrades (TechRepublic, 2021). Cloud platforms reduce downtime by leveraging built-in redundancies and failover mechanisms. For example, a Gartner 2023 study highlighted that businesses using cloud infrastructure experience 35% fewer unplanned outages compared to those with traditional on-premises systems (Gartner, 2023).&lt;br&gt;&lt;br&gt;This delegation of maintenance tasks empowers businesses to concentrate on growth activities such as customer engagement, product innovation, and scaling operations. For instance, a small e-commerce company that shifted to a cloud-based system reduced its IT overhead by 35%, enabling it to allocate more resources to digital marketing and inventory expansion (Flexera, 2023).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;Collaboration and Flexibility for Distributed Teams&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Cloud computing transforms collaboration, especially for businesses adopting remote or hybrid work models. According to a 2023 Gartner survey, organizations using cloud-based collaboration tools like Google Workspace or Microsoft Teams improved productivity by 25% and reduced project turnaround times by 18%.&lt;br&gt;&lt;br&gt;Small businesses with distributed teams particularly benefit from real-time collaboration features. Employees can access the same version of files, ensuring consistency and reducing errors. In one case, a startup leveraging cloud solutions for remote work reported a 30% increase in employee satisfaction due to the flexibility of accessing work resources from any device, anywhere (Deloitte, 2022). Additionally, cloud-based systems support secure access, ensuring sensitive data remains protected during remote operations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;Transforming Business Models Through Cloud&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;Rapid Prototyping&amp;nbsp;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In highly competitive industries like technology and e-commerce, speed is essential. The cloud goes beyond rapid scalability by enabling businesses to create sandbox environments, where new ideas can be tested, refined, and deployed with minimal risk. For example, retail companies can simulate and optimize seasonal promotions, while technology firms can rapidly prototype new applications without the delays of traditional infrastructure. This agility not only shortens time-to-market cycles but also empowers businesses to experiment and innovate with confidence, staying ahead of evolving market demands.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;Aligning Technology with Strategic Goals&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Adopting the latest technologies is only part of the equation—using them strategically is where the cloud truly transforms businesses. A cloud-enabled digital transformation roadmap ensures every tool and system aligns with long-term business objectives. Leveraging predictive analytics and AI-driven insights in the cloud allows companies to proactively optimize supply chains, improve operational efficiency, and deliver highly personalized customer experiences. This strategic alignment ensures that technology supports growth, innovation, and competitive advantage rather than becoming a siloed resource.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;How Cloud Computing Has Changed Business&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;Advanced Security and Compliance: Protecting What Matters Most&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Cloud providers like AWS, Microsoft Azure, and Google Cloud collectively invest billions annually into cutting-edge security measures, including encryption, real-time threat monitoring, and advanced firewalls. In 2021 alone, Alphabet, Amazon, Meta, Apple and Microsoft spent a combined&amp;nbsp; $2.4 billion on funding or acquiring 23 cybersecurity companies (Statista). These investments ensure small and large businesses alike benefit from enterprise-grade security without the need for their own IT infrastructure.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Cloud platforms simplify compliance with regulations like GDPR, SOC 2, and HIPAA. A 2022 study by Forrester Research found that 68% of organizations using cloud services reduced the time spent on compliance-related tasks by 40%, streamlining audits and ensuring adherence to global standards (Forrester, 2022). Businesses using cloud-based tools can ensure compliance with automated data management and reporting features, mitigating risks and building trust among stakeholders.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;Resilience Through Disaster Recovery: Continuity Without Compromise&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Disruptions such as natural disasters, cyberattacks, or hardware failures can severely impact business operations. Cloud-based disaster recovery solutions offer cost-effective and efficient means to safeguard data and maintain business continuity. For example, AWS Elastic Disaster Recovery provides scalable and reliable recovery options, enabling businesses to resume operations swiftly after a disruption (Amazon Web Services).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;By utilizing cloud-based disaster recovery services, businesses can ensure their data remains accessible and protected, even in the face of unforeseen events. This resilience is invaluable for maintaining customer trust and sustaining operations during challenging times.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Embrace a Cloud-Native Future&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The journey to cloud migration is a call to action for businesses to reimagine their operations and embrace a future where agility and innovation define success. As companies adopt cloud solutions to drive digital transformation, they are also laying the groundwork for resilience and competitiveness in a rapidly evolving market.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To thrive in this cloud-first world, it’s essential to build strong partnerships. Collaborating with experts who understand the complexities of cloud adoption can make the transition seamless and ensure your business maximizes the value of this transformative technology. Whether you’re looking to streamline processes or scale operations, the right partner can help design a roadmap tailored to your needs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Cuemby is dedicated to helping organizations navigate this transformation. By combining technical expertise with practical insights, we empower businesses to achieve resilience, scalability, and innovation. Discover how Cuemby’s cloud-native solutions can help your business succeed. Visit &lt;a href=&#34;https://www.cuemby.com/&#34;&gt;https://www.cuemby.com&lt;/a&gt; to learn more, or schedule a consultation to explore how cloud computing can optimize your operations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For more inspiration and resources, explore the CNCF blog and news section to stay updated on the latest trends and advancements in the cloud-native ecosystem. Let’s build a scalable and resilient future together.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;Cuemby 首席执行官兼 CNCF 大使 &lt;a href=&#34;https://www.linkedin.com/in/ar4mirez/&#34;&gt;Angel Ramirez&lt;/a&gt; 发表大使帖子。&lt;/em&gt;&lt;/em&gt;&lt;/a&gt; p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;随着技术格局的发展，企业必须拥抱创新，使其能够适应并蓬勃发展。 CNCF 社区倡导的云原生技术已成为实现可扩展性、敏捷性和弹性的重要工具。对于中小型企业 (SMB)，这些解决方案提供了简化运营、降低成本和加速增长的独特机会。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;本文借鉴 Cuemby 在实施云原生实践方面的经验，探讨了这些技术的变革潜力及其对中小企业的实际影响。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;云原生技术的商业案例&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;Kubernetes 和 CNCF 项目的成本效率&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;迁移到云原生解决方案可帮助中小型企业更有效地重新分配 IT 预算。 Kubernetes 是 CNCF 毕业项目，通过跨云环境编排容器化应用程序，消除了昂贵的硬件投资。企业只需为他们使用的资源付费，从而显着降低运营成本。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;以一家年度 IT 预算为 500 万美元的中型公司为例：迁移到云可能每年节省 75 万美元到 125 万美元（Flexera，2023 年）。然后，可以将这些释放的资源战略性地重新用于推动增长的活动，例如营销、产品开发或客户获取。&lt;br&gt;&lt;br&gt;但节省的费用还不止于此。云解决方案采用按需付费模式，这意味着企业只需为实际使用的资源付费。这对于需求波动的公司（例如季节性行业）尤其有利。例如，零售或旅游组织可以通过减少使用量在需求低迷时期将成本削减高达 30%（Deloitte，2022）。这种灵活性不仅可以防止过度投资，还可以优化运营费用。&lt;br&gt;&lt;br&gt;除了节省基础设施之外，还可以显着降低维护成本。本地系统通常需要 15% 到 20% 的 IT 预算用于硬件更新和更新（TechRepublic，2021）。通过云迁移，这些任务由云提供商处理，使企业能够专注于其核心目标，而不是日常 IT 维护。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;无限制的可扩展性&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Kubernetes 等云原生平台使企业能够在几分钟而不是几个月内扩展运营。根据 CNCF 报告，采用 Kubernetes 的组织发现新服务的上市时间缩短了 60%。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在季节性高峰期间，零售等行业依赖 Kubernetes 来动态扩展工作负载。例如，在黑色星期五等高需求时期，使用云平台的企业报告性能有所改善与依赖本地基础设施的企业相比，停机时间减少高达 40%（麦肯锡，2021 年）。这可确保运营不间断，提高客户满意度并在关键时期保护收入。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;中小型企业的实际用例&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;使用开源工具简化维护&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;实时监控和记录对于最大限度地减少停机时间至关重要。 Prometheus 和 Fluentd 等广泛采用的 CNCF 项目等工具为自动化这些任务提供了强大的解决方案。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;小型企业通常缺乏维护和更新传统 IT 基础设施的能力。通过迁移到云，他们可以减少通常分配给硬件维护和升级的 IT 预算的 15% 到 20%（TechRepublic，2021）。云平台通过利用内置的冗余和故障转移机制来减少停机时间。例如，Gartner 2023 年的一项研究强调，与使用传统本地系统的企业相比，使用云基础设施的企业所经历的意外中断减少了 35%（Gartner，2023）。&lt;br&gt;&lt;br&gt;这种维护任务的委派使企业能够专注于增长活动，例如客户参与、产品创新和规模化运营。例如，一家小型电子商务公司转向基于云的系统，其 IT 开销减少了 35%，使其能够将更多资源分配给数字营销和库存扩张（Flexera，2023）。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;分布式团队的协作和灵活性&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;云计算改变了协作方式，特别是对于采用远程或混合工作模式的企业而言。根据 Gartner 2023 年的一项调查，使用 Google Workspace 或 Microsoft Teams 等基于云的协作工具的组织将工作效率提高了 25%，并将项目周转时间缩短了 18%。&lt;br&gt;&lt;br&gt;拥有分布式团队的小型企业尤其受益于实时时间协作功能。员工可以访问相同版本的文件，确保一致性并减少错误。在一个案例中，一家利用云解决方案进行远程工作的初创公司报告称，由于可以从任何地方的任何设备灵活地访问工作资源，员工满意度提高了 30%（Deloitte，2022）。此外，基于云的系统支持安全访问，确保敏感数据在远程操作期间保持受到保护。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;通过云转变业务模式&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;快速原型制作&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在科技和电子商务等竞争激烈的行业中，速度至关重要。云超越了快速可扩展性，使企业能够创建沙盒环境，在沙盒环境中可以以最小的风险测试、完善和部署新想法。例如，零售公司可以模拟和优化季节性促销，而科技公司可以快速制作新应用程序的原型。阳离子，而不会出现传统基础设施的延迟。这种敏捷性不仅缩短了上市周期，而且使企业能够充满信心地进行试验和创新，从而领先于不断变化的市场需求。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;使技术与战略目标保持一致&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;采用最新技术只是其中的一部分——战略性地使用它们才是云真正改变业务的地方。支持云的数字化转型路线图可确保每个工具和系统都符合长期业务目标。利用云中的预测分析和人工智能驱动的洞察力，企业可以主动优化供应链、提高运营效率并提供高度个性化的客户体验。这种战略联盟确保技术支持增长、创新和竞争优势，而不是成为孤立的资源。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;云计算如何改变业务&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;高级安全性和合规性：保护最重要的内容&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;AWS、Microsoft Azure 和 Google Cloud 等云提供商每年总共投资数十亿美元用于尖端安全措施，包括加密、实时威胁监控和高级防火墙。仅 2021 年，Alphabet、亚马逊、Meta、苹果和微软就总共花费了 24 亿美元资助或收购 23 家网络安全公司 (Statista)。这些投资可确保小型和大型企业都受益于企业级安全性，而无需拥有自己的 IT 基础设施。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;云平台简化了 GDPR、SOC 2 和 HIPAA 等法规的合规性。 Forrester Research 2022 年的一项研究发现，68% 使用云服务的组织将合规相关任务所花费的时间减少了 40%，从而简化了审计并确保遵守全球标准（Forrester，2022）。使用基于云的工具的企业可以确保遵守自动化数据管理和报告功能，从而降低风险并在利益相关者之间建立信任。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;通过灾难恢复实现弹性：不妥协的连续性&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;自然灾害、网络攻击或硬件故障等中断可能会严重影响业务运营。基于云的灾难恢复解决方案提供了经济高效的方法来保护数据并保持业务连续性。例如，AWS Elastic Disaster Recovery 提供可扩展且可靠的恢复选项，使企业能够在中断后迅速恢复运营（Amazon Web Services）。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;通过利用基于云的灾难恢复服务，即使面对不可预见的事件，企业也可以确保其数据仍然可访问并受到保护。这种弹性对于在充满挑战的时期维持客户信任和维持运营非常宝贵。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;拥抱 Cloud-Nat我的未来&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;云迁移之旅呼吁企业采取行动，重新构想其运营并拥抱敏捷性和创新决定成功的未来。随着公司采用云解决方案来推动数字化转型，他们也为快速发展的市场中的弹性和竞争力奠定了基础。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;要在这个云优先的世界中蓬勃发展，建立牢固的合作伙伴关系至关重要。与了解云采用复杂性的专家合作可以实现无缝过渡，并确保您的企业最大限度地发挥这一变革性技术的价值。无论您是希望简化流程还是扩大运营规模，合适的合作伙伴都可以帮助您设计适合您需求的路线图。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Cuemby 致力于帮助组织应对这一转型。通过将技术专业知识与实践见解相结合，我们帮助企业实现弹性、可扩展性和创新。了解 Cuemby 的云原生解决方案如何帮助您的业务取得成功。请访问 &lt;a href=&#34;https://www.cuemby.com/&#34;&gt;https://www.cuemby.com&lt;/a&gt; 了解更多信息，或安排咨询以探索云计算如何优化您的运营。&lt; /p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如需更多灵感和资源，请探索 CNCF 博客和新闻部分，了解云原生生态系统的最新趋势和进展。让我们共同构建一个可扩展且有弹性的未来。&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Thu, 19 Dec 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【OpenTelemetry.io 2024 review】OpenTelemetry.io 2024 回顾</title>
      <link>https://www.cncf.io/blog/2024/12/20/opentelemetry-io-2024-review/</link>
      <description>【&lt;p&gt;&lt;em&gt;Project post originally published on the &lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/&#34;&gt;OpenTelemetry blog&lt;/a&gt; by&amp;nbsp;&lt;a href=&#34;https://github.com/svrnm&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Severin Neumann&lt;/a&gt;&amp;nbsp;(Cisco),&amp;nbsp;&lt;a href=&#34;https://github.com/chalin/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Patrice Chalin&lt;/a&gt;&amp;nbsp;(CNCF),&amp;nbsp;&lt;a href=&#34;https://github.com/tiffany76&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Tiffany Hrabusa&lt;/a&gt;&amp;nbsp;(Grafana Labs)&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As 2024 draws to a close, we reflect on the year and share some insights and accomplishments from&amp;nbsp;&lt;a href=&#34;https://docs.google.com/document/d/1wW0jLldwXN8Nptq2xmgETGbGn9eWP8fitvD5njM-xZY&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;SIG Communications&lt;/a&gt;, the team responsible for managing this website, blog, and documentation.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;key-achievements-of-2024&#34;&gt;Key achievements of 2024&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#key-achievements-of-2024&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Several key accomplishments stand out in our efforts to make OpenTelemetry documentation more accessible, user-friendly, and impactful for our global community.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;multilingual-documentation&#34;&gt;Multilingual documentation&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#multilingual-documentation&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;A major accomplishment this year was achieving multilingual support with the launch of our&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/blog/2024/docs-localized/&#34;&gt;localized documentation&lt;/a&gt;. Thanks to the efforts of localization teams, over 120 pages were translated from English into other languages. The available translations include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://opentelemetry.io/zh/&#34;&gt;Chinese&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://opentelemetry.io/fr/&#34;&gt;French&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://opentelemetry.io/ja/&#34;&gt;Japanese&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://opentelemetry.io/pt/&#34;&gt;Portuguese&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://opentelemetry.io/es/&#34;&gt;Spanish&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;A big thank you to everyone who contributed to this initiative. These translations make OpenTelemetry more accessible, enhancing the user experience for our global audience.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;ia-improvements&#34;&gt;Information Architecture (IA) improvements&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#ia-improvements&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To&amp;nbsp;&lt;strong&gt;improve readership experience&lt;/strong&gt;&amp;nbsp;and make OpenTelemetry&amp;nbsp;&lt;strong&gt;documentation more intuitive and accessible&lt;/strong&gt;, we undertook important updates to our Information Architecture (IA) this year. These changes were driven by the need to better organize content, clarify the purpose of key sections, and provide a more structured and user-friendly experience for end-users and developers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Key IA updates include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Renaming the&amp;nbsp;&lt;code&gt;Instrumentation&lt;/code&gt;&amp;nbsp;section to&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/docs/languages/&#34;&gt;Language APIs &amp;amp; SDKs&lt;/a&gt;&amp;nbsp;to better reflect its purpose and set clearer expectations for users.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Moving&amp;nbsp;&lt;code&gt;Automatic Instrumentation&lt;/code&gt;&amp;nbsp;into the new&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/docs/zero-code/&#34;&gt;Zero-code Instrumentation&lt;/a&gt;&amp;nbsp;section to more clearly distinguish between instrumentation APIs &amp;amp; SDKs and tools like the Java agent, used to inject telemetry.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Following these updates, the Java SIG&amp;nbsp;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/discussions/4853&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;proposed&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/pulls?q=is%3Apr+java+is%3Aclosed+label%3Asig%3Ajava+merged%3A2024-01-01..2024-12-31+author%3Ajack-berg&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;reorganized their documentation&lt;/a&gt;, introducing substantial improvements to the structure and clarity of the content. The bulk of this effort is reflected in these PRs:&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/pull/4966&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Refactor Java SDK and configuration #4966&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/pull/5276&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Refactor Java instrumentation #5276&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/pull/5590&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Move performance to Java agent, merge Javadoc into API page #5590&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;Kudos to&amp;nbsp;&lt;a href=&#34;https://github.com/jack-berg&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Jack Berg&lt;/a&gt;&amp;nbsp;and the&amp;nbsp;&lt;a href=&#34;https://docs.google.com/document/d/1D7ZD93LxSWexHeztHohRp5yeoTzsi9Dj1HRm7Tad-hM&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Java SIG&lt;/a&gt;&amp;nbsp;for their exemplary leadership in improving language-SIG documentation!&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Next year, we aim to redesign how OpenTelemetry is introduced to beginners, ensuring a smoother and more accessible learning experience. If you’re passionate about making OpenTelemetry easier to understand and use, we’d love your contributions —&amp;nbsp;&lt;a href=&#34;https://github.com/open-telemetry/community/pull/2427&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;join us&lt;/a&gt;&amp;nbsp;in this collaborative effort.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;year-in-numbers&#34;&gt;Year in numbers&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#year-in-numbers&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;contributions&#34;&gt;Contributions&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#contributions&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In&amp;nbsp;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/releases/tag/2022.12&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;December 2022&lt;/a&gt;, we started&amp;nbsp;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/releases&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;monthly releases&lt;/a&gt;&amp;nbsp;of the website so that we could regularly summarize activities and highlight significant contributions. These releases allow us to track progress over time and perform long-term comparisons.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance, comparing the periods&amp;nbsp;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/compare/2022.12...2023.11&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;December 2022 to November 2023&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/compare/2023.12...2024.11&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;December 2023 to November 2024&lt;/a&gt;, we observed an upward trend in contributions:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Commits&lt;/strong&gt;&amp;nbsp;increased 33% from 1,011 to 1,340&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Contributors&lt;/strong&gt;&amp;nbsp;grew 15% from 92 to 106&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The only metric that declined was the&amp;nbsp;&lt;strong&gt;number of files changed&lt;/strong&gt;, which decreased from 1,864 to 1,624 (13%)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Since the repository’s inception in April 2019, the community has seen remarkable growth, with:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;3,824 merged pull requests (3,982 commits) by&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;768 contributors&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Thank you to every contributor for helping to build and improve the OpenTelemetry website. Your efforts make a difference!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;which-pages-were-the-most-popular&#34;&gt;Which pages were the most popular?&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#which-pages-were-the-most-popular&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;According to our publicly available&amp;nbsp;&lt;a href=&#34;https://lookerstudio.google.com/s/tSTKxK1ECeU&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;analytics&lt;/a&gt;&amp;nbsp;data,&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/&#34;&gt;opentelemetry.io&lt;/a&gt;&amp;nbsp;was viewed&amp;nbsp;&lt;strong&gt;12 million&lt;/strong&gt;&amp;nbsp;times across 4 million sessions this year. This marks a&amp;nbsp;&lt;strong&gt;16% increase&lt;/strong&gt;&amp;nbsp;over last year’s nearly 10 million views and over 3 million sessions.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The most popular pages and sections of the documentation were:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Page/Section&lt;/th&gt;&lt;th&gt;Views&lt;/th&gt;&lt;th&gt;%&amp;nbsp;&lt;sup&gt;&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://opentelemetry.io/docs/what-is-opentelemetry/&#34;&gt;What is OpenTelemetry?&lt;/a&gt;&lt;/td&gt;&lt;td&gt;290K&lt;/td&gt;&lt;td&gt;2.4%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://opentelemetry.io/docs/collector&#34;&gt;Collector&lt;/a&gt;&lt;/td&gt;&lt;td&gt;1.3M&lt;/td&gt;&lt;td&gt;10.5%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://opentelemetry.io/docs/what-is-opentelemetry/&#34;&gt;Concepts&lt;/a&gt;&lt;/td&gt;&lt;td&gt;1.2M&lt;/td&gt;&lt;td&gt;9.8%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://opentelemetry.io/docs/demo/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt;&lt;td&gt;829K&lt;/td&gt;&lt;td&gt;6.7%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://opentelemetry.io/ecosystem/&#34;&gt;Ecosystem&lt;/a&gt;&lt;/td&gt;&lt;td&gt;500K&lt;/td&gt;&lt;td&gt;4.0%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;trivia&#34;&gt;Fun trivia&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#trivia&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Did you know that:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;“OpenTelemetry” occurs 7.3K times in the English website pages, making it the 3rd most frequent word after “the” and “to.” The word “collector” is used 3.2K times, putting it in 11th place!&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/docs/collector/&#34;&gt;Collector landing page&lt;/a&gt;&amp;nbsp;has been the most updated file since its creation, with 91 changes.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;With 511 commits (27K additions, and 10K deletions) the&amp;nbsp;&lt;a href=&#34;https://github.com/opentelemetrybot&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;opentelemetrybot&lt;/a&gt;&amp;nbsp;is the fourth most active contributor. Go bots!&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The record for the PR with the most comments this year—and of all time is held by:&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/pull/5575&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Generative AI updates blog post (#5575)&lt;/a&gt;, with 150 comments!&lt;/li&gt;&lt;/ul&gt;A close second goes to:&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/pull/5380&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Portuguese translation of Go instrumentation&lt;/a&gt;, with 146 comments&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;amazing-community&#34;&gt;Amazing Community&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#amazing-community&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;With&amp;nbsp;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/pulls?q=is%3Apr+is%3Amerged+merged%3A2024-01-01..2024-12-31&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;1.3K PRs&lt;/a&gt;, we collectively contributed an equally impressive number of reviews to ensure that content is accurate, valuable, aligned with our documentation goals, and easy to read and understand.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In addition to PRs, contributors created nearly&amp;nbsp;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/issues?q=is%3Aissue+created%3A2024-01-01..2024-12-31&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;500 issues&lt;/a&gt;&amp;nbsp;and engaged in many&amp;nbsp;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/discussions&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;discussions&lt;/a&gt;, reporting bugs, suggesting improvements, and driving collaboration. Each of these efforts reflects our community’s dedication to maintaining the quality of OpenTelemetry docs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We are fortunate to have many contributors who take on responsibilities, including:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Approvers and maintainers&lt;/strong&gt;&amp;nbsp;from other SIGs who co-own parts of the docs&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Localization teams&lt;/strong&gt;&amp;nbsp;who oversee translations into various languages&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;The OpenTelemetry community&lt;/strong&gt;, whose contributions make all the difference — every drive-by edit and typo fix counts!&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;SIG Communications team members&lt;/strong&gt;, for their contributions and for orchestrating it all!&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Thank you to everyone who contributed their time and expertise to OpenTelemetry docs this year!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;join-us-in-2025&#34;&gt;Join us in 2025&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#join-us-in-2025&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;A big shout-out to everyone for making 2024 a successful year! We look forward to continuing our collaboration in 2025.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Whether you’re an end user, a contributor, or simply enthusiastic about OpenTelemetry, we welcome your participation. You can&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/docs/contributing/&#34;&gt;get involved&lt;/a&gt;&amp;nbsp;by raising&amp;nbsp;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/issues&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;issues&lt;/a&gt;, participating in&amp;nbsp;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/discussions&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;discussions&lt;/a&gt;, or&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/docs/contributing/pull-requests/&#34;&gt;submitting PRs&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;You can also join us:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;On the&amp;nbsp;&lt;a href=&#34;https://slack.cncf.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;CNCF Slack&lt;/a&gt;&amp;nbsp;at any one of the many&amp;nbsp;&lt;code&gt;#otel&lt;/code&gt;-prefixed channels.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;In&amp;nbsp;&lt;a href=&#34;https://docs.google.com/document/d/1wW0jLldwXN8Nptq2xmgETGbGn9eWP8fitvD5njM-xZY&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Comms meetings&lt;/a&gt;, held every other Monday at 10:00 AM Pacific time.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Together, we can make 2025 another amazing year for&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/&#34;&gt;opentelemetry.io&lt;/a&gt;!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Percentage of the site-total 12M views.&amp;nbsp;&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#fnref:1&#34;&gt;↩︎&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;项目帖子最初由 &lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/&#34;&gt;OpenTelemetry 博客&lt;/a&gt;发布://github.com/svrnm&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Severin Neumann&lt;/a&gt;（思科），&lt;a href=&#34;https://github.com/chalin/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;帕特里斯·查林&lt;/a&gt;（CNCF），&lt;a href=&#34;https://github.com/tiffany76&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;蒂芙尼Hrabusa&lt;/a&gt;（Grafana Labs）&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;2024 年即将结束，我们回顾这一年并分享&lt;a href=&#34;https://docs.google.com/document/d/1wW0jLldwXN8Nptq2xmgETGbGn9eWP8fitvD5njM-xZY&#34; target=&#34;_blank&#34; 的一些见解和成就rel=&#34;noreferrer noopener&#34;&gt;SIG Communications&lt;/a&gt;，负责管理此网站、博客的团队，和文档。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;key-achievements-of-2024&#34;&gt;2024 年的主要成就&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/ #key-achievements-of-2024&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们努力使 OpenTelemetry 文档更易于访问、用户友好且对我们的全球社区具有影响力，并取得了一些突出的关键成就。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;multilingual-documentation&#34;&gt;多语言文档 &lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#multilingual-documentation&#34;&gt; &lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;今年的一项重大成就是通过推出&lt;a href=&#34;https://opentelemetry.io/blog/2024/docs-localized/&#34;&gt;本地化文档&lt;/a&gt;实现了多语言支持。在本地化团队的努力下，超过 120 页的内容已从英语翻译成其他语言。可用的翻译包括：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://opentelemetry.io/zh/&#34;&gt;中文&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://opentelemetry.io/fr/&#34;&gt;法语&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://opentelemetry.io/ja/&#34;&gt;日语&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://opentelemetry.io/pt/&#34;&gt;葡萄牙语&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://opentelemetry.io/es/&#34;&gt;西班牙语&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;非常感谢所有为此计划做出贡献的人。这些翻译使 OpenTelemetry 更易于访问，从而增强了全球受众的用户体验。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;ia-improvements&#34;&gt;信息架构 (IA) 改进&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#ia -改进&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;为了&lt;strong&gt;改善读者体验&lt;/strong&gt;并让 OpenTelemetry &lt;strong&gt;文档更加直观和易于访问&lt;/strong&gt;，我们今年对信息架构 (IA) 进行了重要更新。这些变化是由于需要更好地组织内容、阐明关键部分的目的以及为最终用户和开发人员提供更加结构化和用户友好的体验。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;关键信息架构更新包括：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;重命名&lt;code&gt;仪器&lt;/code&gt; 部分添加到&lt;a href=&#34;https://opentelemetry.io/docs/languages/&#34;&gt;语言 API 和 SDK&lt;/a&gt;，以更好地反映其目的并为用户设定更明确的期望。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;将&lt;code&gt;自动检测&lt;/code&gt;移至新的&lt;a href=&#34;https://opentelemetry.io/docs/zero-code/&#34;&gt;零代码检测&lt;/a&gt;部分，以更清晰地区分在仪器 API 和 SDK 以及 Java 代理等工具之间，用于注入遥测。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;在这些更新之后，Java SIG &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/discussions/4853&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;提议&lt;/ a&gt; 和 &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/pulls?q=is%3Apr+java+is%3Aclose+label%3Asig%3Ajava+merged%3A2024-01-01..2024- 12-31+author%3Ajack-berg&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;重新组织了他们的文档&lt;/a&gt;，对内容的结构和清晰度。这些工作的大部分反映在这些 PR 中：&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/pull/4966&#34; target=&#34;_blank&#34; rel=&#34; noreferrer noopener&#34;&gt;重构 Java SDK 和配置#4966&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/pull/5276&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;重构 Java 检测#5276&lt;/a&gt;&lt;/li&gt;&lt;li &gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/pull/5590&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;移动性能对于 Java 代理，将 Javadoc 合并到 API 页面 #5590&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;感谢 &lt;a href=&#34;https://github.com/jack-berg&#34; target=&#34;_blank&#34; rel=&#34; noreferrer noopener&#34;&gt;杰克·伯格&lt;/a&gt;和&lt;a href=&#34;https://docs.google.com/document/d/1D7ZD93LxSWexHeztHohRp5yeoTzsi9Dj1HRm7Tad-hM&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Java SIG&lt;/a&gt; 在改进语言 SIG 文档方面发挥着模范带头作用！ &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;明年，我们的目标是重新设计向初学者介绍 OpenTelemetry 的方式，确保更顺畅、更容易获得的学习体验。如果您热衷于让 OpenTelemetry 更易于理解和使用，我们非常乐意您的贡献 — &lt;a href=&#34;https://github.com/open-telemetry/community/pull/2427&#34; target=&#34;_blank&#34; rel =&#34;noreferrer noopener&#34;&gt;加入我们&lt;/a&gt;，参与这项合作。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;year-in-numbers&#34;&gt;数字年份&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#year-数字内&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;contributions&#34;&gt;贡献&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#contributions&#34;&gt;&lt;/a&gt;&lt; /h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/releases/tag/2022.12&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;2022 年 12 月&lt;/a&gt;，我们开始&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/releases&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;网络每月发布&lt;/a&gt;网站，以便我们能够定期总结活动并突出重大贡献。这些版本使我们能够跟踪一段时间内的进展并进行长期比较。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;例如，比较周期&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/compare/2022.12...2023.11&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt; 2022 年 12 月至 2023 年 11 月&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/compare/2023.12...2024.11&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;2023 年 12 月至 2024 年 11 月&lt;/a&gt;，我们观察到贡献呈上升趋势：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;提交次数&lt;/strong&gt;从 1,011 次增加到 1,340 次，增加了 33%&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;贡献者&lt;/strong&gt;增长了 15%，从 92 名增加到 106 名&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;唯一下降的指标是&lt;strong&gt;更改的文件数量&lt;/strong&gt;，从 1,864 个减少到 1,624 个 (13%)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;自 2019 年 4 月存储库成立以来，社区取得了显着的增长，其中：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;3,824 个合并拉取请求（3,982 次提交）&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;768 名贡献者&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;感谢每一位贡献者帮助构建和改进 OpenTelemetry 网站。您的努力会有所作为！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;which-pages-were-the-most-popular&#34;&gt;哪些页面最受欢迎？&lt;a href=&#34;https://opentelemetry.io/blog/2024 /year-in-review/#which-pages-were-the-most-popular&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;根据我们公开的&lt;a href=&#34;https://lookerstudio.google.com/s/tSTKxK1ECeU&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;分析&lt;/a&gt;数据，&lt;a href=今年，“https://opentelemetry.io/&#34;&gt;opentelemetry.io&lt;/a&gt; 的观看次数为 400 万次，浏览量为&lt;strong&gt;1200 万次&lt;/strong&gt;。这比去年近 1000 万次观看次数和超过 300 万次会话增加了&lt;strong&gt;16%&lt;/strong&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;文档中最受欢迎的页面和部分是：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;页面/部分&lt;/th&gt;&lt;th&gt;视图&lt;/th&gt;&lt;th &gt;% &lt;sup&gt;&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody &gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://opentelemetry.io/docs/what-is-opentelemetry/&#34;&gt;什么是OpenTelemetry？&lt;/a&gt;&lt;/td&gt;&lt;td&gt;290K&lt;/td&gt;&lt;td&gt;2.4%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://opentelemetry.io/ docs/collector&#34;&gt;收集器&lt;/a&gt;&lt;/td&gt;&lt;td&gt;1.3M&lt;/td&gt;&lt;td&gt;10.5%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://opentelemetry.io/docs/what-is-opentelemetry/&#34;&gt;概念&lt;/a&gt;&lt;/td&gt;&lt;td&gt;120万&lt;/td&gt;&lt;td&gt;9.8%&lt;/td&gt;&lt;/ tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://opentelemetry.io/docs/demo/&#34;&gt;演示&lt;/a&gt;&lt;/td&gt;&lt;td&gt;829K&lt;/td&gt;&lt;td&gt;6.7%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;&lt;a href=&#34;https://opentelemetry.io/ecosystem/&#34;&gt;生态系统&lt;/a&gt;&lt;/td&gt;&lt;td&gt;500K&lt;/td&gt;&lt;td&gt;4.0%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt; /表&gt;&lt;/图&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;trivia&#34;&gt;有趣的问答&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#trivia&#34;&gt;&lt;/a&gt; &lt;/h3&gt;&lt;p&gt;你知道吗：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;“OpenTelemetry”在英文网站页面中出现了 7.3K 次，使其成为继“the”和“to”之后第三个最常见的单词。 “收藏家”这个词被使用了 3.2K 次，排在第 11 位！&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://opentelemetry.io/docs/collector/&#34;&gt;收集器登录页面&lt;/a&gt;是自创建以来最新更新的文件，共进行了 91 项更改。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/opentelemetrybot&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;opentelemetrybot&lt;/a&gt; 共进行了 511 次提交（27K 添加和 10K 删除），排名第四最活跃的贡献者。机器人来吧！&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;今年评论最多的 PR 记录（也是历史上的记录）保持者：&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/ pull/5575&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;生成式 AI 更新博客文章 (#5575)&lt;/a&gt;，有 150 条评论！&lt;/li&gt;&lt;/ul&gt;紧随其后的是：&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/pull/5380&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Go 工具的葡萄牙语翻译&lt;/a&gt;，已有 146 条评论&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;amazing-community&#34;&gt;精彩社区&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#amazing-community&#34;&gt; &lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;与&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/pulls?q=is%3Apr+is%3Amerged+merged%3A2024-01-01..2024-12- 31&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;1.3K PR&lt;/a&gt;，我们共同贡献了同样令人印象深刻的评论数量，以确保内容准确、有价值、符合我们的文档目标，并且易于阅读和理解。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;除了 PR 之外，贡献者还创建了近&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/issues?q=is%3Aissue+created%3A2024-01-01..2024 -12-31&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;500 个问题&lt;/a&gt;并参与了许多&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/discussions&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;讨论&lt;/a&gt;、报告错误、提出改进建议和推动协作。每一项努力都体现了我们社区对维护 OpenTelemetry 文档质量的奉献精神。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们很幸运有许多承担责任的贡献者，包括：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;批准者和维护者&lt;/strong&gt;来自共同拥有部分文档的其他 SIG&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;本地化团队&lt;/strong&gt;负责监督各种语言的翻译&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;OpenTelemetry 社区&lt;/strong&gt;，他们的贡献使一切变得不同 - 每一次路过式编辑和拼写错误修复都很重要！&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;SIG Communications 团队成员&lt;/strong&gt;，感谢他们的贡献和精心策划的一切！&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;感谢所有贡献时间的人今年将向 OpenTelemetry 文档提供专业知识！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;join-us-in-2025&#34;&gt;2025 年加入我们&lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/ #join-us-in-2025&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;向大家大声喊叫，让 2024 年成为成功的一年！我们期待在 2025 年继续合作。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;无论您是最终用户、贡献者，还是只是对 OpenTelemetry 充满热情，我们都欢迎您的参与。您可以通过筹集资金&lt;a href=&#34;https://opentelemetry.io/docs/contributing/&#34;&gt;参与&lt;/a&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/问题” target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;问题&lt;/a&gt;，参与&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry.io/discussions&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;讨论&lt;/a&gt;，或&lt;a href=&#34;https://opentelemetry.io/docs/contributing/pull-requests/&#34;&gt;提交 PR&lt;/a&gt;。&lt; /p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;您也可以加入我们：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;在&lt;a href=&#34;https://slack.cncf.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;CNCF Slack&lt;/a&gt;上的任意一个&lt;code&gt;#otel &lt;/code&gt;-前缀频道。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/1wW0jLldwXN8Nptq2xmgETGbGn9eWP8fitvD5njM-xZY&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;通讯会议&lt;/a&gt;，每隔一周星期一举行太平洋时间上午 10:00。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;只要我们齐心协力，我们就能让 2025 年成为 &lt;a href=&#34;https://opentelemetry.io/&#34;&gt;opentelemetry.io&lt;/a&gt; 又一个精彩的一年！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol类=“wp-block-list”&gt;&#xA;&lt;li&gt;网站总浏览量 1200 万次的百分比。 &lt;a href=&#34;https://opentelemetry.io/blog/2024/year-in-review/#fnref:1&#34;&gt;↩︎&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Thu, 19 Dec 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Decoding the pod termination lifecycle in Kubernetes: a comprehensive guide】解读 Kubernetes 中的 Pod 终止生命周期：综合指南</title>
      <link>https://www.cncf.io/blog/2024/12/19/decoding-the-pod-termination-lifecycle-in-kubernetes-a-comprehensive-guide/</link>
      <description>【&lt;p&gt;&lt;em&gt;Member post by &lt;a href=&#34;https://www.linkedin.com/in/rohit-raveendran-1529b1131&#34;&gt;Rohit Raveendran&lt;/a&gt;, Facets.Cloud&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Introduction&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;What happens behind the scenes when a Kubernetes pod shuts down? In Kubernetes, understanding the intricacies of pod termination is crucial for maintaining the stability and efficiency of your applications. When a pod is terminated, it’s not just a simple shutdown; it involves a well-defined lifecycle that ensures minimal disruption and data loss. This process, known as graceful termination, is vital for handling in-progress requests and performing necessary clean-up tasks before a pod is finally removed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This guide examines each lifecycle phase during pod termination, detailing the mechanisms for graceful handling, resource optimization strategies, persistent data management, and troubleshooting techniques for common termination issues. By the end of this blog, you will have a thorough understanding of how to effectively manage pod termination in your Kubernetes environment, ensuring smooth and efficient operations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;The Lifecycle Phases of Pod Termination&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In Kubernetes, graceful termination means that the system gives the pods time to finish serving in-progress requests and shut down cleanly before removing them. This helps to avoid disruption and loss of data. Kubernetes supports graceful termination of pods, and it’s achieved through the following steps:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When a pod is asked to terminate, Kubernetes updates the object state and marks it as “Terminating”.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Kubernetes also sends a SIGTERM signal to the main process in each container of the pod.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The SIGTERM signal is an indication that the processes in the containers should stop. The processes have a grace period (default is 30 seconds) to shut down properly.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If a process is still running after the grace period, Kubernetes sends a SIGKILL signal to force the process to terminate.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To gracefully terminate the pod you can either add terminationGracePeriodSeconds to your spec or add&amp;nbsp; kubectl delete pods &amp;lt;pod&amp;gt; –grace-period=&amp;lt;seconds&amp;gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The below contains a time series graph for graceful termination:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;936&#34; height=&#34;460&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Picture-1.png&#34; alt=&#34;Termination Grace Period&#34; class=&#34;wp-image-122520&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Picture-1.png 936w, https://www.cncf.io/wp-content/uploads/2024/12/Picture-1-300x147.png 300w, https://www.cncf.io/wp-content/uploads/2024/12/Picture-1-768x377.png 768w, https://www.cncf.io/wp-content/uploads/2024/12/Picture-1-900x442.png 900w, https://www.cncf.io/wp-content/uploads/2024/12/Picture-1-407x200.png 407w, https://www.cncf.io/wp-content/uploads/2024/12/Picture-1-814x400.png 814w&#34; sizes=&#34;auto, (max-width: 936px) 100vw, 936px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The “preStop” hook is executed just before a container is terminated.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When a pod’s termination is requested, Kubernetes will run the preStop hook (if it’s defined), send a SIGTERM signal to the container, and then wait for a grace period before sending a SIGKILL signal.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Here’s an example of how you might define a preStop hook in your pod configuration:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: my-pod&#xA;spec:&#xA;  containers:&#xA;  - name: my-container&#xA;    image: my-image&#xA;    lifecycle:&#xA;      preStop:&#xA;        exec:&#xA;          command: [&#34;/bin/sh&#34;, &#34;-c&#34;, &#34;echo Hello from the preStop hook&#34;]&#xA;```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this example, the preStop hook runs a command that prints a message to the container’s standard output.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The preStop hook is a great place to put code that:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Waits for connections to close&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Cleans up resources&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Notifies other components of the shutdown&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;NOTE: Remember that Kubernetes will run the preStop hook, then wait for the grace period (default 30 seconds) before forcibly terminating the container. If your preStop hook takes longer than the grace period, Kubernetes will interrupt it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The above image explains it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Factors Influencing Pod Termination&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;1. How resource constraints impact pod termination decisions.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Pod Eviction due to Resource Constraints&lt;/strong&gt;: Kubernetes nodes monitor for conditions that indicate system resources are running low. When resources such as CPU, memory, disk space, or filesystem inodes reach certain thresholds, the node enters a condition of ‘DiskPressure’ or ‘MemoryPressure.’ In such cases, the kubelet (the Kubernetes agent running on each node) tries to reclaim resources by evicting pods. Pods with a lower quality of service, such as BestEffort or Burstable pods exceeding their request, are evicted first.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Pod Termination due to Out of Memory / CrashloopBackoff &lt;/strong&gt;: If a node is severely low on memory, the Linux kernel’s Out of Memory (OOM) Killer process kicks in and starts terminating processes to free up memory. In a Kubernetes context, this could lead to abrupt termination of your pods. This is not a graceful termination, so it’s usually a situation to avoid. You can mitigate such scenarios by setting appropriate resource requests and limits for your pods.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Pod Termination due to Unschedulable Pods&lt;/strong&gt;: When you create a pod, if there is not enough resource in any of the nodes to meet the resource request of the pod, the pod remains in a state of Pending. This could lead to pod termination if it remains unscheduled for a long time.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;2. Strategies for optimizing resource allocation to mitigate potential issues.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Set Resource Requests and Limits&lt;/strong&gt;: For each pod, you can specify resource requests and limits. The request is the amount of resource that Kubernetes guarantees for the pod, and the limit is the maximum amount of resource the pod can use. By setting these appropriately, you can ensure that your pods have the resources they need and also prevent them from using up too many resources and affecting other pods.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;resources:&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; limits:&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; cpu: &#34;1&#34;&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; memory: 1000Mi&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; requests:&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; cpu: 200m&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; memory: 500Mi&#xA;```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Use Quality of Service Classes&lt;/strong&gt;: Kubernetes assigns pods to Quality of Service (QoS) classes (Guaranteed, Burstable, or BestEffort) based on their resource requests and limits. Pods with higher QoS classes are less likely to be evicted. You can optimize resource allocation by adjusting these settings to assign your most critical pods to the Guaranteed class.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Guaranteed: All containers in the pod have CPU and memory requests and limits, and they’re equal.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;resources:&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; requests:&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; memory: &#34;64Mi&#34;&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; cpu: &#34;250m&#34;&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; limits:&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; memory: &#34;64Mi&#34;&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; cpu: &#34;250m&#34;&#xA;```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Burstable: At least one container in the pod has a CPU or memory request. Or if limits and more than the requests&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;resources:&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; requests:&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; memory: &#34;64Mi&#34;&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; cpu: &#34;250m&#34;&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; limits:&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; memory: &#34;128Mi&#34;&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; cpu: &#34;500m&#34;&#xA;```&#xA;BestEffort: No containers in the pod have any CPU or memory requests or limits.&#xA;&#xA;```yaml&#xA;&#xA;resources:{}&#xA;&#xA;```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Namespace Resource Quotas&lt;/strong&gt;: You can use ResourceQuotas to limit the total amount of resources that can be used in a namespace. This can prevent runaway consumption by a single team or project. Within a namespace, you can also use LimitRanges to set defaults and constraints on the resource requests and limits for individual pods.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;apiVersion: v1&#xA;kind: ResourceQuota&#xA;metadata:&#xA;  name: compute-resources&#xA;spec:&#xA;  hard:&#xA;    pods: &#34;10&#34;&#xA;    requests.cpu: &#34;1&#34;&#xA;    requests.memory: 1Gi&#xA;    limits.cpu: &#34;2&#34;&#xA;    limits.memory: 2Gi&#xA; ```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Node Affinity Rules&lt;/strong&gt;: These rules allow you to influence where pods are scheduled based on the labels of nodes. For example, you could use node affinity to ensure that high-memory pods are scheduled on high-memory nodes.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: with-node-affinity&#xA;spec:&#xA;  affinity:&#xA;    nodeAffinity:&#xA;      requiredDuringSchedulingIgnoredDuringExecution:&#xA;        nodeSelectorTerms:&#xA;        - matchExpressions:&#xA;          - key: disktype&#xA;            operator: In&#xA;            values:&#xA;            - ssd&#xA;  containers:&#xA;  - name: nginx-container&#xA;    image: nginx&#xA;```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Taints and Tolerations&lt;/strong&gt;: Taints allow a node to repel a set of pods, and tolerations can be added to pods to allow (but not require) them to be scheduled onto nodes with matching taints&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; kubectl taint nodes node1 key=value:NoSchedule&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: my-pod&#xA;spec:&#xA;  containers:&#xA;  - name: my-container&#xA;    image: my-image&#xA;  tolerations:&#xA;  - key: &#34;key&#34;&#xA;    operator: &#34;Equal&#34;&#xA;    value: &#34;value&#34;&#xA;    effect: &#34;NoSchedule&#34;&#xA;```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Autoscaling: &lt;/strong&gt;Using HPA and VPA to scale and right size deployments&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Handling Persistent Data and Storage&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In Kubernetes, a StatefulSet ensures that each Pod gets a unique and sticky identity, which is important in maintaining the state for applications like databases. When it comes to Pod termination, Kubernetes handles it a bit differently in StatefulSets compared to other Pod controllers like Deployments or ReplicaSets.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Here are the steps how Kubernetes handles pod termination in StatefulSets:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Scaling Down/ Deleted : When a StatefulSet is scaled down or commissioned to be destroyed, Kubernetes will terminate the Pods in reverse order, starting from the highest to the lowest, ensuring that the state is maintained. For example, if you have pods named web-0, web-1, web-2, and scale down, web-2 would be deleted first, then web-1, and so on.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Graceful Termination: Just like other pods, StatefulSet pods also go through the graceful termination process. When a pod is deleted, Kubernetes will send a SIGTERM signal to allow the pod to shut down gracefully. It allows the pod to finish processing any in-flight requests and prepare for shutdown before it is removed. If the process doesn’t exit within the grace period (default 30 seconds), a SIGKILL signal will be sent to forcibly shut it down. You can check the above explanations&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;PreStop Hook: If a preStop lifecycle hook is defined in the pod specification within a StatefulSet, it will be executed before the pod is terminated. This provides an opportunity to perform any cleanup or final actions before the pod is deleted.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Pod Identity: With StatefulSets, the identity of the pod is preserved across rescheduling. If a node fails and a pod needs to be rescheduled, the replacement pod will have the same network identity (name and hostname) and if persistent volumes are used, the same data from the same PersistentVolumeClaim (PVC).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Pod Storage: If a pod in a StatefulSet has attached storage, the storage (PVC) is not removed when the pod is deleted. This ensures that if a new pod is scheduled with the same identity, it can resume where the old one left off.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Troubleshooting Pod Termination Issues&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Identification of common issues related to pod termination:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Pods stuck in Terminating state&lt;/strong&gt;: Sometimes, pods can get stuck in the Terminating state due to various reasons like storage issues, finalizers stuck in deletion, or network issues.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Pod Disruption Budgets&lt;/strong&gt;: If you have set a Pod Disruption Budget (PDB) which limits the number of pods of a replicated application that are down simultaneously from voluntary disruptions, it can prevent voluntary termination of pods.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Practical troubleshooting tips and solutions:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Pods stuck in Terminating state: For pods stuck in the Terminating state, you can force delete the pod with kubectl delete pod &amp;lt;PODNAME&amp;gt; –grace-period=0 –force. However, this should be a last resort as it can cause data loss. It’s better to identify the root cause by checking the pod description (kubectl describe pod &amp;lt;PODNAME&amp;gt;) and looking for any errors.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Pod Disruption Budgets: If you have a PDB that’s preventing pod termination, you may need to reconsider your PDB settings. If necessary, you can delete the PDB, but be aware that this may impact the availability of your application. Always consider the implications and plan downtime accordingly.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;A solid understanding of pod termination in Kubernetes keeps applications running smoothly as workloads grow. By mastering Kubernetes tools, configuring resources well, and using best practices, you can create a resilient environment. Solutions like Facets enhance these efforts, automating terminations, managing resources, and meeting scaling needs with ease.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ultimately, a thoughtful approach to Kubernetes pod termination not only improves the stability and scalability of your infrastructure but also empowers teams to deliver higher-quality services faster, with minimal disruption to end-users.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;成员帖子，作者：Facets.Cloud &lt;a href=&#34;https://www.linkedin.com/in/rohit-raveendran-1529b1131&#34;&gt;Rohit Raveendran&lt;/a&gt;&lt;/em&gt;&lt;/p &gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;简介&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;当 Kubernetes Pod 关闭时，幕后会发生什么？在 Kubernetes 中，了解 Pod 终止的复杂性对于维护应用程序的稳定性和效率至关重要。当 pod 终止时，这不仅仅是简单的关闭；它涉及一个明确定义的生命周期，以确保最大限度地减少中断和数据丢失。这个过程称为“优雅终止”，对于处理正在进行的请求以及在 Pod 最终删除之前执行必要的清理任务至关重要。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;本指南检查 Pod 终止期间的每个生命周期阶段，详细介绍了优雅处理机制、资源优化策略、持久数据管理以及常见终止问题的故障排除技术。读完本博客后，您将彻底了解如何在 Kubernetes 环境中有效管理 pod 终止，确保平稳高效的操作。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Pod 终止的生命周期阶段&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在 Kubernetes 中，优雅终止意味着系统给 pod 有时间来完成处理中的请求，并在删除它们之前彻底关闭。这有助于避免数据中断和丢失。 Kubernetes 支持 pod 的优雅终止，它是通过以下步骤实现的：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;当 Pod 被要求终止时，Kubernetes 会更新对象状态并将其标记为“终止”。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Kubernetes 还会向 pod 的每个容器中的主进程发送 SIGTERM 信号。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SIGTERM 信号指示容器中的进程应该停止。这些进程有一个宽限期（默认为 30 秒）来正确关闭。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果进程在宽限期后仍在运行，Kubernetes 会发送 SIGKILL 信号强制进程终止。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;要正常终止 pod，您可以在规范中添加终止GracePeriodSeconds 或添加 kubectl 删除 pod &lt;pod&gt; –grace-period=&lt;seconds&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;下面包含优雅终止的时间序列图：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“936”高度=“460”src=“https://www.cncf.io/ wp-content/uploads/2024/12/Picture-1.png&#34; alt=&#34;终止宽限期&#34; class=&#34;wp-image-122520&#34; srcset =“https://www.cncf.io/wp-content/uploads/2024/12/Picture-1.png 936w，https://www.cncf.io/wp-content/uploads/2024/12/图片-1-300x147.png 300w, https://www.cncf.io/wp-content/uploads/2024/12/Picture-1-768x377.png 768w，https://www.cncf.io/wp-content/uploads/2024/12/Picture -1-900x442.png 900w, https://www.cncf.io/wp-content/uploads/2024/12/Picture-1-407x200.png 407w，https://www.cncf.io/wp-content/uploads/2024/12/Picture -1-814x400.png 814w&#34; 尺寸 = &#34;自动,（最大宽度：936px）100vw，936px“referrerpolicy =“no-referrer”&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;“preStop”钩子在容器终止之前执行。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;当请求终止 pod 时，Kubernetes 将运行 preStop 挂钩（如果已定义），向容器发送 SIGTERM 信号，然后等待一段宽限期，然后再发送 SIGKILL 信号。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;以下是如何在 pod 配置中定义 preStop 挂钩的示例：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;api版本：v1&#xA;种类: 豆荚&#xA;元数据：&#xA;  名称：我的豆荚&#xA;规格：&#xA;  容器：&#xA;  - 名称：我的容器&#xA;    图像：我的图像&#xA;    生命周期：&#xA;      预停止：&#xA;        执行：&#xA;          命令：[“/bin/sh”，“-c”，“从 preStop 挂钩回显 Hello”]&#xA;```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在此示例中，preStop 挂钩运行一个命令，将消息打印到容器的标准输出。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;preStop 钩子是放置以下代码的好地方：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;等待连接关闭&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;清理资源&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;通知其他组件关闭&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;注意：请记住，Kubernetes 将运行 preStop 挂钩，然后等待宽限期（默认 30 秒），然后再强制终止容器。如果你的 preStop hook 花费的时间超过了宽限期，Kubernetes 将中断它。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;上图解释了这一点。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;影响 Pod 终止的因素&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;1.资源限制如何影响 Pod 终止决策。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;由于资源限制而被驱逐&lt;/strong&gt;：Kubernetes 节点监控表明系统资源不足的情况。当 CPU、内存、磁盘空间或文件系统 inode 等资源达到特定阈值时，节点会进入“DiskPressure”或“MemoryPressure”状态。在这种情况下，kubelet（在每个节点上运行的 Kubernetes 代理）会尝试回收通过驱逐 pod 来获取资源。服务质量较低的 Pod（例如超出其请求的 BestEffort 或 Burstable Pod）将首先被驱逐。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;由于内存不足/CrashloopBackoff 导致 Pod 终止&lt;/strong&gt;：如果节点内存严重不足，Linux 内核的内存不足 (OOM) Killer 进程就会启动并开始终止进程​​以释放内存。在 Kubernetes 环境中，这可能会导致 pod 突然终止。这不是一个优雅的终止，因此通常是需要避免的情况。您可以通过为 Pod 设置适当的资源请求和限制来缓解此类情况。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;由于 Pod 无法调度而导致 Pod 终止&lt;/strong&gt;：当您创建 pod 时，如果任何节点上都没有足够的资源来满足 pod 的资源请求，则 pod 会处于以下状态：待办的。如果 Pod 长时间未调度，这可能会导致 Pod 终止。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;2.优化资源的策略分配以缓解潜在问题。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;设置资源请求和限制&lt;/strong&gt;：对于每个 Pod，您可以指定资源请求和限制。 request 是 Kubernetes 为 pod 保证的资源量，limit 是 pod 可以使用的最大资源量。通过适当设置这些，您可以确保您的 Pod 拥有所需的资源，并防止它们占用过多资源并影响其他 Pod。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;资源：&#xA;      限制：&#xA;        中央处理器：“1”&#xA;        内存：1000Mi&#xA;      要求：&#xA;        中央处理器：200m&#xA;        内存：500Mi&#xA;```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;使用服务质量类&lt;/strong&gt;：Kubernetes 根据 Pod 的资源请求和限制将其分配到服务质量 (QoS) 类（Guaranteed、Burstable 或 BestEffort）。具有较高 QoS 类别的 Pod 被驱逐的可能性较小。您可以通过调整这些设置来优化资源分配，将最关键的 pod 分配给Guaranteed 类。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;保证：Pod 中的所有容器都有 CPU 和内存请求和限制，并且它们是相等的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;资源：&#xA;      要求：&#xA;        内存：“64Mi”&#xA;        中央处理器：“250m”&#xA;      限制：&#xA;        内存：“64Mi”&#xA;        中央处理器：“250m”&#xA;```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;突发：Pod 中至少有一个容器有 CPU 或内存请求。或者如果限制且超过请求&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;资源：&#xA;      要求：&#xA;        内存：“64Mi”&#xA;        中央处理器：“250m”&#xA;      限制：&#xA;        内存：“128Mi”&#xA;        中央处理器：“500m”&#xA;````&#xA;BestEffort：Pod 中的容器没有任何 CPU 或内存请求或限制。&#xA;&#xA;```yaml&#xA;&#xA;资源：{}&#xA;&#xA;```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;命名空间资源配额&lt;/strong&gt;：您可以使用 ResourceQuotas 来限制命名空间中可使用的资源总量。这可以防止单个团队或项目的消耗失控。在命名空间内，您还可以使用 LimitRanges 设置资源请求的默认值和约束以及单个 Pod 的限制。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;api版本：v1&#xA;种类：资源配额&#xA;元数据：&#xA;  名称：计算资源&#xA;规格：&#xA;  难的：&#xA;    豆荚：“10”&#xA;    请求.cpu:“1”&#xA;    请求内存：1Gi&#xA;    限制.cpu：“2”&#xA;    内存限制：2Gi&#xA; ```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;节点关联性规则&lt;/strong&gt;：这些规则允许您根据节点标签影响 Pod 的调度位置。例如，您可以使用节点亲和性来确保高内存 Pod 调度在高内存节点上。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;api版本：v1&#xA;种类: 豆荚&#xA;元数据：&#xA;  名称：与节点亲和力&#xA;规格：&#xA;  亲和力：&#xA;    节点亲和力：&#xA;      requiredDuringSchedulingIgnoredDuringExecution：&#xA;        节点选择器条款：&#xA;        - 匹配表达式：&#xA;          - 键：磁盘类型&#xA;            运算符：In&#xA;            价值观：&#xA;            - 固态硬盘&#xA;  容器：&#xA;  - 名称：nginx-容器&#xA;    图片：nginx&#xA;```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;污点和容忍&lt;/strong&gt;：污点允许节点排斥一组 pod，并且可以向 pod 添加容忍，以允许（但不要求）将它们调度到具有匹配污点的节点上&lt;/李&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;            kubectl 污点节点 node1 key=value:NoSchedule&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;```yaml&#xA;api版本：v1&#xA;种类: 豆荚&#xA;元数据：&#xA;  名称：我的豆荚&#xA;规格：&#xA;  容器：&#xA;  - 名称：我的容器&#xA;    图像：我的图像&#xA;  容忍度：&#xA;  - 键：“键”&#xA;    运算符：“等于”&#xA;    值：“值”&#xA;    效果：“无时间表”&#xA;```&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;自动扩展：&lt;/strong&gt;使用 HPA 和 VPA 来扩展并调整部署规模&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;处理持久数据和存储&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在 Kubernetes 中，StatefulSet 可确保每个 Pod 获得唯一且粘性的身份，这对于维护数据库等应用程序的状态非常重要。当涉及到 Pod 终止时，与 Deployments 或 ReplicaSets 等其他 Pod 控制器相比，Kubernetes 在 StatefulSet 中的处理方式略有不同。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;以下是 Kubernetes 如何处理 StatefulSets 中 pod 终止的步骤：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;缩小/删除：当 StatefulSet 缩小或委托销毁时，Kubernetes 会按照相反的顺序（从最高到最低）终止 Pod，以确保状态得以维持。例如，如果您有名为 web-0、web-1、web-2 的 Pod，并且缩小规模，则将首先删除 web-2，然后删除 web-1，依此类推。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;优雅终止：与其他 Pod 一样，StatefulSet Pod 也会经历优雅终止过程。当 pod 被删除时，Kubernetes 将发送一个 SIGTERM 信号以允许 pod 正常关闭。它允许 Pod 完成处理任何正在进行的请求，并在删除之前准备关闭。如果进程在宽限期内（默认30秒）没有退出，则会发送SIGKILL信号强制关闭它。您可以查看上面的说明&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;你l 类=“wp-block-list”&gt;&#xA;&lt;li&gt;PreStop Hook：如果在 StatefulSet 内的 Pod 规范中定义了 preStop 生命周期钩子，则它将在 Pod 终止之前执行。这提供了在删除 Pod 之前执行任何清理或最终操作的机会。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Pod 身份：使用 StatefulSets，Pod 的身份在重新调度时得以保留。如果节点发生故障并且需要重新调度 Pod，则替换 Pod 将具有相同的网络标识（名称和主机名），并且如果使用持久卷，则来自相同 PersistentVolumeClaim (PVC) 的相同数据。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Pod 存储：如果 StatefulSet 中的 pod 附加了存储，则删除 pod 时不会删除存储 (PVC)。这确保了如果使用相同身份调度新 pod，它可以从旧 pod 停止的位置恢复。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;排除 Pod 终止问题&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;识别与 Pod 终止相关的常见问题：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Pod 陷入终止状态&lt;/strong&gt;：有时，Pod 可能会因为存储问题、终结器陷入删除或网络问题等各种原因而陷入终止状态。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Pod 中断预算&lt;/strong&gt;：如果您设置了 Pod 中断预算 (PDB)，该预算限制复制应用程序中因自愿中断而同时关闭的 Pod 数量，则可以防止 Pod 自愿终止.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;实用的故障排除技巧和解决方案：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Pod 陷入 Terminate 状态：对于陷入 Terminate 状态的 Pod，您可以使用 kubectl delete pod &lt;PODNAME&gt; –grace-period=0 –force 强制删除 pod。但是，这应该是最后的手段，因为它可能会导致数据丢失。最好通过检查 pod 描述 (kubectl describe pod &lt;PODNAME&gt;) 并查找任何错误来确定根本原因。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Pod 中断预算：如果您的 PDB 阻止 Pod 终止，您可能需要重新考虑您的 PDB 设置。如有必要，您可以删除 PDB，但请注意，这可能会影响应用程序的可用性。始终考虑其影响并相应地计划停机时间。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;结论&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;对 Kubernetes 中 Pod 终止的深入了解可以使应用程序随着工作负载的增长而平稳运行。通过掌握 Kubernetes 工具、配置好资源并使用最佳实践，您可以创建一个弹性环境。 Facets 等解决方案增强了这些工作，自动终止、管理资源并轻松满足扩展需求。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;最终，深思熟虑的 Kubernetes Pod 终止方法不仅可以提高基础设施的稳定性和可扩展性，还使团队能够更快地提供更高质量的服务，同时将影响降到最低。向最终用户提供。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Wed, 18 Dec 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Running a production-ready Raspbery Pi Kubernetes cluster at home】在家运行生产就绪的 Raspbery Pi Kubernetes 集群</title>
      <link>https://www.cncf.io/blog/2024/12/27/running-a-production-ready-raspbery-pi-kubernetes-cluster-at-home/</link>
      <description>【&lt;p&gt;&lt;em&gt;Ambassador post originally published on &lt;a href=&#34;https://geraldonit.com/2024/12/16/production-ready-raspbery-pi-kubernetes-cluster/&#34;&gt;Gerald on IT &lt;/a&gt;by Gerald Venzl&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;em&gt;In this guide, I’ll cover how to run a production-ready Raspberry Pi Kubernetes Cluster using K3s.&lt;/em&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;background&#34;&gt;Background&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you are like me, you probably have a bunch of (older) Raspberry Pi models lying around not doing much because you replaced them with newer models. So, instead of just having them collect dust, why not create your own little Kubernetes cluster and deploy something on them, or just use it to learn Kubernetes?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;setup&#34;&gt;Setup&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;hardware&#34;&gt;Hardware&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;1 x Raspberry Pi Model 4 B with 4 GB RAM&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;1 x Raspberry Pi Model 4 B with 2 GB RAM&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;4 x Raspberry Pi Model 3 B+ with 1 GB of RAM&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;1 x Netgear ProSafe 8 Port Gigabit unmanaged switch&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;1 x Corsair Flash Voyager GTX 256GB USB 3.1 Premium Flash Drive&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;Note: This hardware setup is what I have available. At no point is this the recommendation for building your own cluster. If you have newer, more powerful Raspberry Pi models, you are probably better off using them instead.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://geraldonit.com/wp-content/uploads/2024/12/k3s-cluster-topology.png&#34; alt=&#34;image&#34; class=&#34;wp-image-6245&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;software&#34;&gt;Software&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;All Raspberry Pis are running Raspberry Pi OS (with desktop)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Release date:&amp;nbsp;November 19th 2024&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;System:&amp;nbsp;64-bit&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Kernel version:&amp;nbsp;6.6&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Debian version:&amp;nbsp;12 (bookworm)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;why-k3s&#34;&gt;Why K3s?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;K3s is a lightweight, CNCF-certified, and fully compliant Kubernetes distribution. It ships as a single binary, requires half the memory, supports other data stores, and more. As&amp;nbsp;&lt;a href=&#34;https://docs.k3s.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;their website&lt;/a&gt;&amp;nbsp;says, it’s:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-style-smaller-quote has-gray-200-background-color has-background is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p&gt;Great for:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Edge&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Homelab&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Internet of Things (IoT)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Continuous Integration (CI)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Development&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Single board computers (ARM)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Air-gapped environments&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Embedded K8s&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Situations where a PhD in K8s clusterology is infeasible&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;  – &lt;a href=&#34;https://docs.k3s.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;K3s Documentation&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Another advantage of K3s for Raspberry Pis is that it allows for data stores other than&amp;nbsp;&lt;code&gt;etcd&lt;/code&gt;. That’s great because, as&amp;nbsp;&lt;a href=&#34;https://docs.k3s.io/installation/requirements#disks&#34;&gt;their website&lt;/a&gt;&amp;nbsp;says,&amp;nbsp;&lt;code&gt;etcd&lt;/code&gt;&amp;nbsp;is write-intensive and the SD cards can usually not handle the IO load:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;blockquote class=&#34;wp-block-quote is-style-smaller-quote has-gray-200-background-color has-background is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p&gt;K3s performance depends on the performance of the database. To ensure optimal speed, we recommend using an SSD when possible.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If deploying K3s on a Raspberry Pi or other ARM devices, it is recommended that you use an external SSD. etcd is write intensive; SD cards and eMMC cannot handle the IO load.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– &lt;a href=&#34;https://docs.k3s.io/installation/requirements#disks&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;K3s Documentation – Requirements -&amp;gt; Hardware -&amp;gt; Disks&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In my case, I am using an external MariaDB database running on the&amp;nbsp;&lt;a href=&#34;https://www.corsair.com/us/en/p/data-storage/cmfvygtx3c-256gb/flash-voyager-gtx-usb-3-1-256gb-premium-flash-drive-cmfvygtx3c-256gb&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Corsair Flash Voyager GTX 256GB USB 3.1 Premium Flash Drive&lt;/a&gt;. For comparison, the&amp;nbsp;&lt;a href=&#34;https://www.amazon.com/SanDisk-Extreme-microSDXC-Memory-Adapter/dp/B09X7CRKRZ/ref=sr_1_1?crid=2WFZDBMVRT0HP&amp;amp;dib=eyJ2IjoiMSJ9.ux20OaUNF6XBPajRAA5x7UFt2T38_NMfoGlpb-uQPK1_89xjiZxpbbPkaUaV8G2tQw_5FW_O22WD_gawx6y4opdJ0NFIO759qaHG5G8vbpkNQi5vlLuXTd41nASoHxZgIik331NU0usAH42GN_ptsKhWSdVm_jCNrA_t85IsKSKyV3Llpx5zt3m1nVVjS2Q0VzxBMt_ygdX7eBYZiQ7HrA0gFaLDBv-xLd7MZpAvYjU.Dx2XvPX09HARgnZYlYmAeENyINErLjMjnes3mkiI_9s&amp;amp;dib_tag=se&amp;amp;keywords=sanDisk%2B256GB%2BExtreme%2BmicroSDXC%2BUHS-I&amp;amp;qid=1733096767&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;SanDisk 256GB Extreme microSDXC UHS-I&amp;nbsp;&lt;/a&gt;card offers a write rate of 130MB/s and a read rate of 190MB/s, while the Voyager GTX USB 3.1 provides a read and write rate of 440MB/s. However, it comes at 2.5 times the price of a microSD card.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;cgroups&#34;&gt;cgroups&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Kubernetes requires the&amp;nbsp;&lt;code&gt;cgroups&lt;/code&gt;&amp;nbsp;(control groups) Linux kernel feature. Unfortunately, the memory subsystem of this feature is not enabled by default in the latest Raspberry Pi OS image. To verify whether it is, you can do a&amp;nbsp;&lt;code&gt;cat /proc/cgroups&lt;/code&gt;&amp;nbsp;and see whether there is a&amp;nbsp;&lt;code&gt;1&lt;/code&gt;&amp;nbsp;in the&amp;nbsp;&lt;code&gt;enabled&lt;/code&gt;&amp;nbsp;column for the&amp;nbsp;&lt;code&gt;memory&lt;/code&gt;&amp;nbsp;row:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-rbp-0:~ $ cat /proc/cgroups&#xA;#subsys_name    hierarchy    num_cgroups    enabled&#xA;cpuset                  0             58          1&#xA;cpu                     0             58          1&#xA;cpuacct                 0             58          1&#xA;blkio                   0             58          1&#xA;memory                  0             58          0&#xA;devices                 0             58          1&#xA;freezer                 0             58          1&#xA;net_cls                 0             58          1&#xA;perf_event              0             58          1&#xA;net_prio                0             58          1&#xA;pids                    0             58          1&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you see a&amp;nbsp;&lt;code&gt;0&lt;/code&gt;&amp;nbsp;like in the output above, you have to enable the memory subsystem. This is done by adding&amp;nbsp;&lt;code&gt;cgroup_enable=memory&lt;/code&gt;&amp;nbsp;to the&amp;nbsp;&lt;code&gt;/boot/firmware/cmdline.txt&lt;/code&gt;&amp;nbsp;file and then reboot the system. The quickest way to do this is via these commands (&lt;strong&gt;note&lt;/strong&gt;:&amp;nbsp;&lt;code&gt;sudo reboot&lt;/code&gt;&amp;nbsp;will reboot your Raspberry Pi):&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo sh -c &#39;echo &#34; cgroup_enable=memory&#34; &amp;gt;&amp;gt; /boot/firmware/cmd&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Note:&amp;nbsp;&lt;/strong&gt;&lt;code&gt;cgroup_enable=cpuset&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;cgroup_memory=1&lt;/code&gt;&amp;nbsp;are no longer required.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Once the system is up again, doublecheck the entry for the&amp;nbsp;&lt;code&gt;memory&lt;/code&gt;&amp;nbsp;subsystem, which should now show a&amp;nbsp;&lt;code&gt;1&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-rbp-0:~ $ cat /proc/cgroups&#xA;#subsys_name    hierarchy    num_cgroups    enabled&#xA;cpuset                  0             94          1&#xA;cpu                     0             94          1&#xA;cpuacct                 0             94          1&#xA;blkio                   0             94          1&#xA;memory                  0             94          1&#xA;devices                 0             94          1&#xA;freezer                 0             94          1&#xA;net_cls                 0             94          1&#xA;perf_event              0             94          1&#xA;net_prio                0             94          1&#xA;pids                    0             94          1&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-400-background-color has-background&#34;&gt;&lt;strong&gt;Repeat the above step on every Raspberry Pi before continuing.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;static-ip-address-configuration&#34;&gt;Static IP address configuration&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Static IP addresses make things easy for cluster management and communication. It ensures that devices always have the same IP address, which makes it easier to identify a given node and prevent communication disruption between nodes due to changing IP addresses. The latest Raspberry Pi OS has a new NetworkManager and associated command line utilities. For an interactive, text-based UI, use the&amp;nbsp;&lt;code&gt;nmtui&lt;/code&gt;&amp;nbsp;(network manager text user interface) command. For scripting purposes, you can use the&amp;nbsp;&lt;code&gt;nmcli&lt;/code&gt;&amp;nbsp;(network manager command line interface) to assign static IP addresses for the Raspberry Pis. You should find an already preconfigured&amp;nbsp;&lt;code&gt;Wired connection 1&lt;/code&gt;&amp;nbsp;on the ethernet device&amp;nbsp;&lt;code&gt;eth0&lt;/code&gt;. You can verify that via&amp;nbsp;&lt;code&gt;nmcli con show&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-rbp-0:~ $ nmcli con show&#xA;NAME                UUID                                  TYPE      DEVICE&#xA;preconfigured       999a68d9-b3e1-4437-bf86-1c9a5f775159  wifi      wlan0&#xA;lo                  db18dd9c-94fe-4fac-8c66-d6ddc3406900  loopback  lo&#xA;Wired connection 1  68f30a89-ef57-3ee9-8238-9310f0829f21  ethernet  eth0&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To change the configuration for the ethernet connection to have a static IP address, use&amp;nbsp;&lt;code&gt;sudo nmcli con mod&lt;/code&gt;. The&amp;nbsp;&lt;code&gt;NN&lt;/code&gt;&amp;nbsp;reflects the digits you want to use for the Raspberry Pi. In my case, it’s going to be&amp;nbsp;&lt;code&gt;10&lt;/code&gt;,&amp;nbsp;&lt;code&gt;11&lt;/code&gt;,&amp;nbsp;&lt;code&gt;12&lt;/code&gt;,&amp;nbsp;&lt;code&gt;13&lt;/code&gt;,&amp;nbsp;&lt;code&gt;14&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;15&lt;/code&gt;&amp;nbsp;on the given node:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo nmcli c mod &#34;Wired connection 1&#34; ipv4.addresses &#34;192.168.0.NN/24&#34; ipv4.method manual&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you also want to set a Gateway address to reach the outside network and/or internet and DNS entries for name resolution, you can do so with the following commands:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo nmcli con mod &#34;Wired connection 1&#34; ipv4.gateway 192.168.0.1&#xA;sudo nmcli con mod &#34;Wired connection 1&#34; ipv4.dns &#34;192.168.0.1, 1.1.1.1, 8.8.8.8&#34;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&amp;nbsp;In my case, I will reach the outside world via the WiFi connection. The ethernet connection is purely for cluster communication&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-400-background-color has-background&#34;&gt;&lt;strong&gt;Repeat the above step on every Raspberry Pi before continuing.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;installing-k3s&#34;&gt;Installing K3s&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The simplest way to install K3s is by running&amp;nbsp;&lt;code&gt;curl -sfL https://get.k3s.io | sh -&lt;/code&gt;. However, because I’m using an external MariaDB database as the cluster data store, things are a bit different. Instead of using&amp;nbsp;&lt;code&gt;etcd&lt;/code&gt;, the installation needs to connect to the MariaDB database. This can be done by supplying the&amp;nbsp;&lt;code&gt;--datastore-endpoint&lt;/code&gt;&amp;nbsp;parameter or&amp;nbsp;&lt;code&gt;K3S_DATASTORE_ENDPOINT&lt;/code&gt;&amp;nbsp;environment variable during the installation. For more details, see&amp;nbsp;&lt;a href=&#34;https://docs.k3s.io/datastore?ext-db=MySQL+%2F+MariaDB&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Cluster Datastore in the K3s documentation&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;creating-the-mariadb-database-and-user-for-kubernetes&#34;&gt;Creating the MariaDB database and user for Kubernetes&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;K3s is capable of connecting to the MariaDB socket at&amp;nbsp;&lt;code&gt;/var/run/mysqld/mysqld.sock&lt;/code&gt;&amp;nbsp;using the&amp;nbsp;&lt;code&gt;root&lt;/code&gt;&amp;nbsp;user if just&amp;nbsp;&lt;code&gt;mysql://&lt;/code&gt;&amp;nbsp;is provided as the datastore-endpoint. That means that the database needs to run on the same host as the control plane and socket connectivity for&amp;nbsp;&lt;code&gt;root&lt;/code&gt;&amp;nbsp;has to be enabled in the MariaDB configuration. Alternatively, one can create a user and database manually, which is what I will do. The user and database will both be called&amp;nbsp;&lt;code&gt;kubernetes&lt;/code&gt;. Here are the four SQL statements you will need for that:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;CREATE DATABASE kubernetes;&#xA;CREATE USER &#39;kubernetes&#39;@&#39;&amp;lt;your IP address range&amp;gt;&#39; IDENTIFIED BY &#39;&amp;lt;your password&amp;gt;&#39;;&#xA;GRANT ALL PRIVILEGES ON kubernetes.* TO &#39;kubernetes&#39;@&#39;&amp;lt;your IP address range&amp;gt;&#39;;&#xA;FLUSH PRIVILEGES;&#xA;gvenzl@gvenzl-rbp-0:~ $ sudo mysql&#xA;Welcome to the MariaDB monitor.  Commands end with ; or \g.&#xA;Your MariaDB connection id is 36&#xA;Server version: 10.11.6-MariaDB-0+deb12u1 Debian 12&#xA; &#xA;Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.&#xA; &#xA;Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.&#xA; &#xA;MariaDB [(none)]&amp;gt; CREATE DATABASE kubernetes;&#xA;Query OK, 1 row affected (0.001 sec)&#xA; &#xA;MariaDB [(none)]&amp;gt; CREATE USER &#39;kubernetes&#39;@&#39;192.168.10.%&#39; IDENTIFIED BY &#39;*********&#39;;&#xA;Query OK, 0 rows affected (0.005 sec)&#xA; &#xA;MariaDB [(none)]&amp;gt; GRANT ALL PRIVILEGES ON kubernetes.* TO &#39;kubernetes&#39;@&#39;192.168.10.%&#39;;&#xA;Query OK, 0 rows affected (0.002 sec)&#xA; &#xA;MariaDB [(none)]&amp;gt; FLUSH PRIVILEGES;&#xA;Query OK, 0 rows affected (0.002 sec)&#xA; &#xA;MariaDB [(none)]&amp;gt; exit;&#xA;Bye&#xA;gvenzl@gvenzl-rbp-0:~ $&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;running-the-k3s-setup-script-on-the-control-plane-raspberry-pi&#34;&gt;Running the K3s setup script on the control plane Raspberry Pi&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To start the K3s installation, a slightly different variation from the above setup script needs to be run to include the&amp;nbsp;&lt;code&gt;--datastore-endpoint&lt;/code&gt;&amp;nbsp;parameter:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;curl -sfL https://get.k3s.io | sh -s - --datastore-endpoint mysql://&amp;lt;username&amp;gt;:&amp;lt;password&amp;gt;@tcp(&amp;lt;hostname&amp;gt;:3306)/&amp;lt;database-name&amp;gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In my case, this is going to look like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;curl -sfL https://get.k3s.io | sh -s - --datastore-endpoint &#34;mysql://kubernetes:*******@tcp(192.168.10.10:3306)/kubernetes&#34;&#xA;gvenzl@gvenzl-rbp-0:~ $ curl -sfL https://get.k3s.io | sh -s - --datastore-endpoint &#34;mysql://kubernetes:*********@tcp(192.168.10.10:3306)/kubernetes&#34;&#xA;[INFO]  Finding release for channel stable&#xA;[INFO]  Using v1.30.6+k3s1 as release&#xA;[INFO]  Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.30.6+k3s1/sha256sum-arm64.txt&#xA;[INFO]  Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.30.6+k3s1/k3s-arm64&#xA;[INFO]  Verifying binary download&#xA;[INFO]  Installing k3s to /usr/local/bin/k3s&#xA;[INFO]  Finding available k3s-selinux versions&#xA;sh: 416: [: k3s-selinux-1.6-1.el9.noarch.rpm: unexpected operator&#xA;[INFO]  Creating /usr/local/bin/kubectl symlink to k3s&#xA;[INFO]  Creating /usr/local/bin/crictl symlink to k3s&#xA;[INFO]  Creating /usr/local/bin/ctr symlink to k3s&#xA;[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh&#xA;[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh&#xA;[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env&#xA;[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service&#xA;[INFO]  systemd: Enabling k3s unit&#xA;Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service.&#xA;[INFO]  Host iptables-save/iptables-restore tools not found&#xA;[INFO]  Host ip6tables-save/ip6tables-restore tools not found&#xA;[INFO]  systemd: Starting k3s&#xA;gvenzl@gvenzl-rbp-0:~ $&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Once the script has finished, verify the control plane setup via&amp;nbsp;&lt;code&gt;sudo kubectl get nodes&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-rbp-0:~ $ sudo kubectl get nodes&#xA;NAME           STATUS   ROLES                  AGE     VERSION&#xA;gvenzl-rbp-0   Ready    control-plane,master   2m48s   v1.30.6+k3s1&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;adding-nodes-to-the-cluster&#34;&gt;Adding nodes to the cluster&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To add the additional Pis to the cluster, you must first retrieve the cluster token in the&amp;nbsp;&lt;code&gt;/var/lib/rancher/k3s/server/token&lt;/code&gt;&amp;nbsp;file, which is needed for the agent installation. You can do that via the following command:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo cat /var/lib/rancher/k3s/server/token&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And will get a token that looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-rbp-0:~ $ sudo cat /var/lib/rancher/k3s/server/token&#xA;K103bf5abb471fc2f7bcda85fa95a60c0f934a22a858c6ae943f4d7e0ee4091bc11::server:f3d376e3274a174a38b3b97d224aac6d&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Once you have retrieved the token, connect to the other Raspberry Pis and execute the following command:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;curl -sfL https://get.k3s.io | K3S_URL=https://&amp;lt;control plane node IP&amp;gt;:6443 K3S_TOKEN=&amp;lt;server token&amp;gt; sh -&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For example:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;curl -sfL https://get.k3s.io | K3S_URL=https://192.168.10.10:6443 K3S_TOKEN=K103bf5abb471fc2f7bcda85fa95a60c0f934a22a858c6ae943f4d7e0ee4091bc11::server:f3d376e3274a174a38b3b97d224aac6d sh -&#xA;gvenzl@gvenzl-rbp-1:~ $ curl -sfL https://get.k3s.io | K3S_URL=https://192.168.10.10:6443 K3S_TOKEN=K103bf5abb471fc2f7bcda85fa95a60c0f934a22a858c6ae943f4d7e0ee4091bc11::server:f3d376e3274a174a38b3b97d224aac6d sh -&#xA;[INFO]  Finding release for channel stable&#xA;[INFO]  Using v1.30.6+k3s1 as release&#xA;[INFO]  Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.30.6+k3s1/sha256sum-arm64.txt&#xA;[INFO]  Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.30.6+k3s1/k3s-arm64&#xA;[INFO]  Verifying binary download&#xA;[INFO]  Installing k3s to /usr/local/bin/k3s&#xA;[INFO]  Finding available k3s-selinux versions&#xA;sh: 416: [: k3s-selinux-1.6-1.el9.noarch.rpm: unexpected operator&#xA;[INFO]  Creating /usr/local/bin/kubectl symlink to k3s&#xA;[INFO]  Creating /usr/local/bin/crictl symlink to k3s&#xA;[INFO]  Creating /usr/local/bin/ctr symlink to k3s&#xA;[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh&#xA;[INFO]  Creating uninstall script /usr/local/bin/k3s-agent-uninstall.sh&#xA;[INFO]  env: Creating environment file /etc/systemd/system/k3s-agent.service.env&#xA;[INFO]  systemd: Creating service file /etc/systemd/system/k3s-agent.service&#xA;[INFO]  systemd: Enabling k3s-agent unit&#xA;Created symlink /etc/systemd/system/multi-user.target.wants/k3s-agent.service → /etc/systemd/system/k3s-agent.service.&#xA;[INFO]  Host iptables-save/iptables-restore tools not found&#xA;[INFO]  Host ip6tables-save/ip6tables-restore tools not found&#xA;[INFO]  systemd: Starting k3s-agent&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Once the installation has finished on all nodes, you have your K3s cluster up and running. You can verify that by running&amp;nbsp;&lt;code&gt;sudo kubectl get nodes&lt;/code&gt;&amp;nbsp;on the control plane one more time:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-rbp-0:~ $ sudo kubectl get nodes&#xA;NAME           STATUS   ROLES                  AGE     VERSION&#xA;gvenzl-rbp-0   Ready    control-plane,master   16m     v1.30.6+k3s1&#xA;gvenzl-rbp-1   Ready    &amp;lt;none&amp;gt;                 5m42s   v1.30.6+k3s1&#xA;gvenzl-rbp-2   Ready    &amp;lt;none&amp;gt;                 3m14s   v1.30.6+k3s1&#xA;gvenzl-rbp-3   Ready    &amp;lt;none&amp;gt;                 2m36s   v1.30.6+k3s1&#xA;gvenzl-rbp-4   Ready    &amp;lt;none&amp;gt;                 2m7s    v1.30.6+k3s1&#xA;gvenzl-rbp-5   Ready    &amp;lt;none&amp;gt;                 42s     v1.30.6+k3s1&#xA;gvenzl@gvenzl-rbp-0:~ $&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Congratulations, you now have a K3s cluster ready for action!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;bonus-access-your-cluster-from-the-outside-with-kubectl&#34;&gt;Bonus: Access your cluster from the Outside with&amp;nbsp;&lt;code&gt;kubectl&lt;/code&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you want to access the cluster from, e.g., your local MacBook with&amp;nbsp;&lt;code&gt;kubectl&lt;/code&gt;, you will need to save a copy of the&amp;nbsp;&lt;code&gt;/etc/rancher/k3s/k3s.yaml&lt;/code&gt;&amp;nbsp;file locally as&amp;nbsp;&lt;code&gt;~/.kube/config&lt;/code&gt;&amp;nbsp;(the&amp;nbsp;&lt;code&gt;k3s.yaml&lt;/code&gt;&amp;nbsp;file needs to be called&amp;nbsp;&lt;code&gt;config&lt;/code&gt;) and replace the value of the&amp;nbsp;&lt;code&gt;server&lt;/code&gt;&amp;nbsp;field with the IP address or name of the K3s server.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;kubectl&lt;/code&gt;&amp;nbsp;itself can be installed via Homebrew:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-mac ~ % brew install kubectl&#xA;==&amp;gt; Downloading https://ghcr.io/v2/homebrew/core/kubernetes-cli/manifests/1.31.3&#xA;Already downloaded: /Users/gvenzl/Library/Caches/Homebrew/downloads/f8fd19d10e239038f339af3c9b47978cb154932f089fbf6b7d67ea223df378de--kubernetes-cli-1.31.3.bottle_manifest.json&#xA;==&amp;gt; Fetching kubernetes-cli&#xA;==&amp;gt; Downloading https://ghcr.io/v2/homebrew/core/kubernetes-cli/blobs/sha256:fd154ae205719c58f90bdb2a51c63e428c3bf941013557908ccd322d7488fb67&#xA;Already downloaded: /Users/gvenzl/Library/Caches/Homebrew/downloads/ec1af5c100c16e5e4dc51cff36ce98eb1e257a312ce5a501fae7a07724e59bf9--kubernetes-cli--1.31.3.sonoma.bottle.tar.gz&#xA;==&amp;gt; Pouring kubernetes-cli--1.31.3.sonoma.bottle.tar.gz&#xA;==&amp;gt; Caveats&#xA;zsh completions have been installed to:&#xA;  /usr/local/share/zsh/site-functions&#xA;==&amp;gt; Summary&#xA;🍺  /usr/local/Cellar/kubernetes-cli/1.31.3: 237 files, 61.3MB&#xA;==&amp;gt; Running `brew cleanup kubernetes-cli`...&#xA;Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.&#xA;Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Next, create the&amp;nbsp;&lt;code&gt;~/.kube&lt;/code&gt;&amp;nbsp;folder and save a copy of&amp;nbsp;&lt;code&gt;k3s.yaml&lt;/code&gt;&amp;nbsp;as&amp;nbsp;&lt;code&gt;config&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-mac ~ % mkdir ~/.kube&#xA;gvenzl@gvenzl-mac ~ % scp root@gvenzl-rbp-0:k3s.yaml ~/.kube/config&#xA;root@gvenzl-rbp-0&#39;s password:&#xA;k3s.yaml                               100% 2965   221.2KB/s   00:00&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And replace the&amp;nbsp;&lt;code&gt;server&lt;/code&gt;&amp;nbsp;parameter with the IP address or hostname of your control plane node:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-mac ~ % sed -i &#39;&#39; &#39;s|server: .*|server: https://gvenzl-rbp-0:6443|g&#39; ~/.kube/config&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Once you have done that, you can control your cluster locally too:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-mac ~ % kubectl get nodes&#xA;NAME           STATUS   ROLES                  AGE   VERSION&#xA;gvenzl-rbp-0   Ready    control-plane,master   39m   v1.30.6+k3s1&#xA;gvenzl-rbp-1   Ready    &amp;lt;none&amp;gt;                 28m   v1.30.6+k3s1&#xA;gvenzl-rbp-2   Ready    &amp;lt;none&amp;gt;                 25m   v1.30.6+k3s1&#xA;gvenzl-rbp-3   Ready    &amp;lt;none&amp;gt;                 25m   v1.30.6+k3s1&#xA;gvenzl-rbp-4   Ready    &amp;lt;none&amp;gt;                 24m   v1.30.6+k3s1&#xA;gvenzl-rbp-5   Ready    &amp;lt;none&amp;gt;                 23m   v1.30.6+k3s1&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;大使帖子最初发布于 &lt;a href=&#34;https://geraldonit.com/2024/12/16/product-ready-raspbery-pi-kubernetes-cluster/&#34;&gt;Gerald on IT &lt;/a &gt;作者：杰拉尔德·文茨尔&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;em&gt;在本指南中，我将介绍如何使用 K3s 运行生产就绪的 Raspberry Pi Kubernetes 集群。&lt;/em&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;background&#34;&gt;​​背景&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果您像我一样，您可能有一堆（较旧的）Raspberry Pi 模型，但没有做太多事情，因为您用较新的模型替换了它们。因此，与其让它们积满灰尘，为什么不创建自己的小型 Kubernetes 集群并在其上部署一些东西，或者只是用它来学习 Kubernetes？&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;setup&#34;&gt;设置&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;hardware&#34;&gt;硬件&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;1 个 Raspberry Pi 型号 4 B，带 4 GB RAM&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;1 个 Raspberry Pi 型号 4 B，带 2 GB RAM&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;4 个 Raspberry Pi 型号 3 B+，带 1 GB RAM&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;1 个 Netgear ProSafe 8 端口千兆非托管交换机&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;1 个 Corsair Flash Voyager GTX 256GB USB 3.1 高级闪存盘&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;​​注意：此硬件设置是我可用的。这绝不是构建您自己的集群的建议。如果您有更新、功能更强大的 Raspberry Pi 型号，那么最好使用它们。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://geraldonit.com/wp-content/uploads/2024/12/k3s-cluster-topology.png&#34; alt= “图像”类=“wp-image-6245”referrerpolicy=“no-referrer”&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;software&#34;&gt;软件&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;所有 Raspberry Pi 都运行 Raspberry Pi OS（带桌面）&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;发布日期：2024 年 11 月 19 日&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;系统：64 位&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;内核版本：6.6&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Debian 版本：12（书呆子）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;why-k3s&#34;&gt;为什么选择 K3？&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;K3s 是一个轻量级、经过 CNCF 认证且完全兼容 Kubernetes 的发行版。它作为单个二进制文件提供，需要一半的内存，支持其他数据存储等等。正如&lt;a href=&#34;https://docs.k3s.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;他们的网站&lt;/a&gt;所说，它是：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-style-smaller-quote has-gray-200-background-color has-background is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p&gt;非常适合：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;边缘&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;家庭实验室&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;物联网 (IoT)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;持续集成 (CI)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;开发&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;单板计算机 (ARM)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;气隙环境&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;嵌入式 K8&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;无法获得 K8s 集群学博士学位的情况&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt; – &lt;a href=&#34;https://docs.k3s.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;K3s 文档&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/块引用&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;安适用于 Raspberry Pi 的 K3s 的另一个优势是它允许除 &lt;code&gt;etcd&lt;/code&gt; 之外的数据存储。这很棒，因为正如&lt;a href=&#34;https://docs.k3s.io/installation/requirements#disks&#34;&gt;他们的网站&lt;/a&gt;所说，&lt;code&gt;etcd&lt;/code&gt; 是写入密集型的，而且 SD卡通常无法处理 IO 负载：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;blockquote class=&#34;wp-block-quote is-style-smaller-quote has-gray-200-background-color has-background is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p&gt;K3s的性能取决于数据库的性能。为了确保最佳速度，我们建议尽可能使用 SSD。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果在 Raspberry Pi 或其他 ARM 设备上部署 K3s，建议您使用外部 SSD。 etcd 是写密集型的； SD卡和eMMC无法处理IO负载。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– &lt;a href=&#34;https://docs.k3s.io/installation/requirements#disks&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;K3s 文档 – 要求 -&gt; 硬件 -&gt; 磁盘&lt;/a &gt;&lt;/p&gt;&#xA;&lt;/块引用&gt;&#xA;&lt;/块引用&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;就我而言，我使用的是在 &lt;a href=&#34;https://www.corsair.com/us/en/p/data-storage/cmfvygtx3c-256gb/flash-voyager- 上运行的外部 MariaDB 数据库gtx-usb-3-1-256gb-premium-flash-drive-cmfvygtx3c-256gb&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Corsair Flash Voyager GTX 256GB USB 3.1 高级闪存盘&lt;/a&gt;。为了进行比较，&lt;a href=&#34;https://www.amazon.com/SanDisk-Extreme-microSDXC-Memory-Adapter/dp/B09X7CRKRZ/ref=sr_1_1?crid=2WFZDBMVRT0HP&amp;dib=eyJ2I joiMSJ9.ux20OaUNF6XBPajRAA5x7UFt2T38_NMfoGlpb-uQPK1_89xjiZxpbbPkaUaV8G2tQw_5FW_O22WD_gawx6y4opdJ0NFIO759qaHG5G8vbpkNQi5vlLu XTd41nASoHxZgIik331NU0usAH42GN_ptsKhWSdVm_jCNrA_t85IsKSKyV3Llpx5zt3m1nVVjS2Q0VzxBMt_ygdX7eBYZiQ7HrA0gFaLDBv-xLd7MZpAvYjuU.Dx 2XvPX09HARgnZYlYmAeENyINErLjMjnes3mkiI_9s&amp;dib_tag=se&amp;关键字=sanDisk%2B256GB%2BExtreme%2BmicroSDXC%2BUHS-I&amp;qid=1733096767&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;闪迪 256GB Extreme microSDXC UHS-I 卡提供 130MB/s 的写入速率和 190MB/s 的读取速率，而 Voyager GTX USB 3.1 则提供 190MB/s 的读取速率写入速率为 440MB/s。然而，它的价格是 microSD 卡的 2.5 倍。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;installation&#34;&gt;安装&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;cgroups&#34;&gt;cgroups&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Kubernetes 需要 &lt;code&gt;cgroups&lt;/code&gt;（控制组）Linux 内核功能。不幸的是，在最新的 Raspberry Pi OS 映像中，默认情况下未启用此功能的内存子系统。要验证是否存在，您可以执行 &lt;code&gt;cat /proc/cgroups&lt;/code&gt; 并查看 &lt;code&gt;enabled&lt;/code&gt; 列中是否存在 &lt;code&gt;1&lt;/code&gt; &lt;代码&gt;内存&lt;/code&gt;行：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-rbp-0:~ $ cat /proc/cgroups&#xA;#subsys_name 层次结构 num_cgroups 已启用&#xA;中央处理器组 0 581&#xA;中央处理器 0 58 1&#xA;cpuacct 0 58 1&#xA;BLKIO 0 58 1&#xA;内存 0 58 0&#xA;设备 0 58 1&#xA;冷冻柜 0 58 1&#xA;网络_cls 0 58 1&#xA;性能事件 0 58 1&#xA;网络优先级 0 58 1&#xA;pid 0 58 1&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果您在上面的输出中看到 &lt;code&gt;0&lt;/code&gt;，则必须启用内存子系统。这是通过将 &lt;code&gt;cgroup_enable=memory&lt;/code&gt; 添加到 &lt;code&gt;/boot/firmware/cmdline.txt&lt;/code&gt; 文件中，然后重新启动系统来完成的。最快的方法是通过以下命令（&lt;strong&gt;注意&lt;/strong&gt;：&lt;code&gt;sudo restart&lt;/code&gt; 将重新启动您的 Raspberry Pi）：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo sh -c &#39;echo &#34; cgroup_enable=memory&#34; &gt;&gt; /boot/firmware/cmd&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;&lt;code&gt;cgroup_enable=cpuset&lt;/code&gt; 和 &lt;code&gt;cgroup_memory=1&lt;/code&gt; 不再需要。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;系统再次启动后，请仔细检查&lt;code&gt;内存&lt;/code&gt;子系统的条目，该子系统现在应显示&lt;code&gt;1&lt;/code&gt;：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-rbp-0:~ $ cat /proc/cgroups&#xA;#subsys_name 层次结构 num_cgroups 已启用&#xA;中央处理器组 0 94 1&#xA;中央处理器 0 94 1&#xA;cpuacct 0 94 1&#xA;BLKIO 0 94 1&#xA;内存 0 94 1&#xA;设备 0 94 1&#xA;冷冻柜 0 94 1&#xA;网络_cls 0 94 1&#xA;性能事件 0 94 1&#xA;网络优先级 0 94 1&#xA;pid 0 94 1&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-400-background-color has-background&#34;&gt;​​&lt;strong&gt;在继续之前，在每个 Raspberry Pi 上重复上述步骤。&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;static-ip-address-configuration&#34;&gt;静态IP地址配置&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;静态 IP 地址使集群管理和通信变得容易。它确保设备始终具有相同的 IP 地址，这使得更容易识别给定节点并防止由于 IP 地址更改而导致节点之间的通信中断。最新的 Raspberry Pi 操作系统有一个新的 NetworkManager 和相关的命令行实用程序。对于基于文本的交互式 UI，请使用 &lt;code&gt;nmtui&lt;/code&gt;（网络管理器文本用户界面）命令。出于编写脚本的目的，您可以使用&lt;code&gt;nmcli&lt;/code&gt;（网络管理器命令行界面）为 Raspberry Pi 分配静态 IP 地址。你应在以太网设备 &lt;code&gt;eth0&lt;/code&gt; 上找到已预配置的&lt;code&gt;有线连接 1&lt;/code&gt;。您可以通过&lt;code&gt;nmcli con show&lt;/code&gt;进行验证：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-rbp-0:~ $ nmcli con show&#xA;名称 UUID 类型 设备&#xA;预配置 999a68d9-b3e1-4437-bf86-1c9a5f775159 wifi wlan0&#xA;罗 db18dd9c-94fe-4fac-8c66-d6ddc3406900 环回罗&#xA;有线连接 1 68f30a89-ef57-3ee9-8238-9310f0829f21 以太网 eth0&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;要将以太网连接的配置更改为静态 IP 地址，请使用 &lt;code&gt;sudo nmcli con mod&lt;/code&gt;。 &lt;code&gt;NN&lt;/code&gt; 反映您想要用于 Raspberry Pi 的数字。就我而言，它将是 &lt;code&gt;10&lt;/code&gt;、&lt;code&gt;11&lt;/code&gt;、&lt;code&gt;12&lt;/code&gt;、&lt;code&gt;13&lt;/code&gt;、&lt;code&gt;14&lt;/code&gt;给定节点上的 code&gt; 和 &lt;code&gt;15&lt;/code&gt;：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo nmcli c mod &#34;有线连接1&#34; ipv4.addresses &#34;192.168.0.NN/24&#34; ipv4.method 手册&lt;/code&gt;&lt;/上一篇&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果您还想设置网关地址以访问外部网络和/或互联网以及 DNS 条目以进行名称解析，则可以使用以下命令来实现：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo nmcli con mod &#34;有线连接 1&#34; ipv4.gateway 192.168.0.1&#xA;sudo nmcli con mod &#34;有线连接1&#34; ipv4.dns &#34;192.168.0.1, 1.1.1.1, 8.8.8.8&#34;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：就我而言，我将通过 WiFi 连接与外界联系。以太网连接纯粹用于集群通信&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-400-background-color has-background&#34;&gt;​​&lt;strong&gt;在继续之前，在每个 Raspberry Pi 上重复上述步骤。&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;installing-k3s&#34;&gt;安装 K3s&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;安装 K3s 的最简单方法是运行 &lt;code&gt;curl -sfL https://get.k3s.io | sh-&lt;/code&gt;。但是，因为我使用外部 MariaDB 数据库作为集群数据存储，所以情况有点不同。安装需要连接到 MariaDB 数据库，而不是使用 &lt;code&gt;etcd&lt;/code&gt;。这可以通过在安装期间提供 &lt;code&gt;--datastore-endpoint&lt;/code&gt; 参数或 &lt;code&gt;K3S_DATASTORE_ENDPOINT&lt;/code&gt; 环境变量来完成。有关更多详细信息，请参阅 K3s 文档中的&lt;a href=&#34;https://docs.k3s.io/datastore?ext-db=MySQL+%2F+MariaDB&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;集群数据存储&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;creating-the-mariadb-database-and-user-for-kubernetes&#34;&gt;为 Kubernetes 创建 MariaDB 数据库和用户&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;K3s 能够使用 &lt;code&gt;root&lt;/code&gt; 用户连接到 &lt;code&gt;/var/run/mysqld/mysqld.sock&lt;/code&gt; 处的 MariaDB 套接字，只要 &lt;code&gt;mysql:// &lt;/code&gt; 作为数据存储端点提供。这意味着数据base 需要与控制平面运行在同一主机上，并且必须在 MariaDB 配置中启用 &lt;code&gt;root&lt;/code&gt; 的套接字连接。或者，可以手动创建用户和数据库，这就是我要做的。用户和数据库都称为&lt;code&gt;kubernetes&lt;/code&gt;。以下是您需要的四个 SQL 语句：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;创建数据库 kubernetes;&#xA;CREATE USER &#39;kubernetes&#39;@&#39;&lt;您的 IP 地址范围&gt;&#39; IDENTIFIED BY &#39;&lt;您的密码&gt;&#39;;&#xA;将 kubernetes.* 上的所有权限授予 &#39;kubernetes&#39;@&#39;&lt;您的 IP 地址范围&gt;&#39;;&#xA;同花顺特权；&#xA;gvenzl@gvenzl-rbp-0:~ $ sudo mysql&#xA;欢迎使用 MariaDB 监视器。  命令以 ; 结尾或\g。&#xA;您的 MariaDB 连接 ID 是 36&#xA;服务器版本：10.11.6-MariaDB-0+deb12u1 Debian 12&#xA; &#xA;版权所有 (c) 2000、2018，Oracle、MariaDB Corporation Ab 等。&#xA; &#xA;输入“帮助”；或“\h”寻求帮助。键入“\c”以清除当前输入语句。&#xA; &#xA;MariaDB [(无)]&gt; 创建数据库 kubernetes;&#xA;查询正常，1 行受影响（0.001 秒）&#xA; &#xA;MariaDB [(无)]&gt; 创建用户 &#39;kubernetes&#39;@&#39;192.168.10.%&#39; 标识为 &#39;************&#39;;&#xA;查询正常，0 行受影响（0.005 秒）&#xA; &#xA;MariaDB [(none)]&gt; 将 kubernetes.* 上的所有权限授予 &#39;kubernetes&#39;@&#39;192.168.10.%&#39;;&#xA;查询正常，0 行受影响（0.002 秒）&#xA; &#xA;MariaDB [(无)]&gt; 刷新权限；&#xA;查询正常，0 行受影响（0.002 秒）&#xA; &#xA;MariaDB [(无)]&gt; 退出；&#xA;再见&#xA;gvenzl@gvenzl-rbp-0:~ $&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;running-the-k3s-setup-script-on-the-control-plane-raspberry-pi&#34;&gt;在控制平面 Raspberry Pi 上运行 K3s 设置脚本&lt;/ H4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;要开始 K3s 安装，需要运行与上述设置脚本略有不同的变体以包含 &lt;code&gt;--datastore-endpoint&lt;/code&gt; 参数：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;curl -sfL https://get.k3s.io | sh -s - --datastore-endpoint mysql://&lt;用户名&gt;:&lt;密码&gt;@tcp(&lt;主机名&gt;:3306)/&lt;数据库名称&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;就我而言，它看起来像这样：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;curl -sfL https://get.k3s.io | sh -s - --datastore-endpoint &#34;mysql://kubernetes:********@tcp(192.168.10.10:3306)/kubernetes&#34;&#xA;gvenzl@gvenzl-rbp-0:~ $curl -sfL https://get.k3s.io | sh -s - --datastore-endpoint &#34;mysql://kubernetes:*********@tcp(192.168.10.10:3306)/kubernetes&#34;&#xA;[INFO] 寻找通道稳定版本&#xA;[INFO] 使用 v1.30.6+k3s1 作为版本&#xA;[信息] 下载哈希 https://github.com/k3s-io​​/k3s/releases/download/v1.30.6+k3s1/sha256sum-arm64.txt&#xA;[信息] 下载二进制文件 https://github.com/k3s-io​​/k3s/releases/download/v1.30.6+k3s1/k3s-arm64&#xA;[INFO] 验证二进制下载&#xA;[INFO] 将 k3s 安装到 /usr/local/bin/k3s&#xA;[INFO] 查找可用的 k3s-selinux 版本&#xA;sh：416：[：k3s-selinux-1.6-1.el9.noarch.rpm：意外的运算符&#xA;[INFO] 创建 /usr/local/bin/kubectl 到 k3s 的符号链接&#xA;[INFO] 创建 /usr/local/bin/crictl 到 k3s 的符号链接[INFO] 创建 /usr/local/bin/ctr 到 k3s 的符号链接&#xA;[信息] 创建killall脚本 /usr/local/bin/k3s-killall.sh&#xA;[信息] 创建卸载脚本 /usr/local/bin/k3s-uninstall.sh&#xA;[INFO] env：创建环境文件 /etc/systemd/system/k3s.service.env&#xA;[信息] systemd：创建服务文件 /etc/systemd/system/k3s.service&#xA;[INFO] systemd：启用 k3s 单元&#xA;创建符号链接 /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service。&#xA;[信息] 找不到主机 iptables-save/iptables-restore 工具&#xA;[信息] 找不到主机 ip6tables-save/ip6tables-restore 工具&#xA;[信息] systemd：启动 k3s&#xA;gvenzl@gvenzl-rbp-0:~ $&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;脚本完成后，通过&lt;code&gt;sudo kubectl getnodes&lt;/code&gt;验证控制平面设置：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-rbp-0:~ $ sudo kubectl 获取节点&#xA;姓名 状态 角色 年龄 版本&#xA;gvenzl-rbp-0 就绪控制平面，master 2m48s v1.30.6+k3s1&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;adding-nodes-to-the-cluster&#34;&gt;向集群添加节点&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;要将其他 Pi 添加到集群，您必须首先在 &lt;code&gt;/var/lib/rancher/k3s/server/token&lt;/code&gt; 文件中检索集群令牌，这是安装代理所需的。您可以通过以下命令来做到这一点：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;sudo cat /var/lib/rancher/k3s/server/token&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;并且会得到一个看起来像这样的令牌：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-rbp-0:~ $ sudo cat /var/lib/rancher/k3s/server/token&#xA;K103bf5abb471fc2f7bcda85fa95a60c0f934a22a858c6ae943f4d7e0ee4091bc11::server:f3d376e3274a174a38b3b97d224aac6d&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;检索到令牌后，连接到其他 Raspberry Pi 并执行以下命令：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;curl -sfL https://get.k3s.io | K3S_URL=https://&lt;控制平面节点IP&gt;:6443 K3S_TOKEN=&lt;服务器令牌&gt; sh -&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;例如：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;curl -sfL https://get.k3s.io | K3S_URL=https://192.168.10.10:6443 K3S_TOKEN=K103bf5abb471fc2f7bcda85fa95a60c0f934a22a858c6ae943f4d7e0ee4091bc11::server:f3d376e3274a174a38b3b97d224aac6d sh -&#xA;gvenzl@gvenzl-rbp-1:~ $curl -sfL https://get.k3s.io | K3S_URL=https://192.168.10.10:6443 K3S_TOKEN=K103bf5abb471fc2f7bcda85fa95a60c0f934a22a858c6ae943f4d7e0ee4091bc11::server:f3d376e3274a174a38b3b97d224aac6d sh -&#xA;[INFO] 寻找通道稳定版本&#xA;[INFO] 使用 v1.30.6+k3s1 作为版本&#xA;[信息] 下载哈希 https://github.com/k3s-io​​/k3s/releases/download/v1.30.6+k3s1/sha256sum-arm64.txt&#xA;[信息] 下载二进制文件 https://github.com/k3s-io​​/k3s/releases/download/v1.30.6+k3s1/k3s-arm64&#xA;[INFO] 验证二进制下载&#xA;[INFO] 将 k3s 安装到 /usr/local/bin/k3s&#xA;[INFO] 查找可用的 k3s-selinux 版本&#xA;sh：416：[：k3s-selinux-1.6-1.el9.noarch.rpm：意外的运算符&#xA;[我NFO] 创建 /usr/local/bin/kubectl 到 k3s 的符号链接&#xA;[INFO] 创建 /usr/local/bin/crictl 到 k3s 的符号链接&#xA;[INFO] 创建 /usr/local/bin/ctr 到 k3s 的符号链接&#xA;[信息] 创建killall脚本 /usr/local/bin/k3s-killall.sh&#xA;[信息] 创建卸载脚本 /usr/local/bin/k3s-agent-uninstall.sh&#xA;[INFO] env：创建环境文件 /etc/systemd/system/k3s-agent.service.env&#xA;[信息] systemd：创建服务文件 /etc/systemd/system/k3s-agent.service&#xA;[INFO] systemd：启用 k3s-agent 单元&#xA;创建符号链接 /etc/systemd/system/multi-user.target.wants/k3s-agent.service → /etc/systemd/system/k3s-agent.service。&#xA;[信息] 找不到主机 iptables-save/iptables-restore 工具&#xA;[信息] 找不到主机 ip6tables-save/ip6tables-restore 工具&#xA;[信息] systemd：启动 k3s-agent&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在所有节点上完成安装后，您的 K3s 集群就已启动并运行。您可以通过在控制平面上再次运行 &lt;code&gt;sudo kubectl getnodes&lt;/code&gt; 来验证这一点：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-rbp-0:~ $ sudo kubectl 获取节点&#xA;姓名 状态 角色 年龄 版本&#xA;gvenzl-rbp-0 就绪控制平面，主控 16m v1.30.6+k3s1&#xA;gvenzl-rbp-1 就绪 &lt;无&gt; 5m42s v1.30.6+k3s1&#xA;gvenzl-rbp-2 就绪 &lt;无&gt; 3m14s v1.30.6+k3s1&#xA;gvenzl-rbp-3 就绪 &lt;无&gt; 2m36s v1.30.6+k3s1&#xA;gvenzl-rbp-4 就绪 &lt;无&gt; 2m7s v1.30.6+k3s1&#xA;gvenzl-rbp-5 就绪 &lt;无&gt; 42s v1.30.6+k3s1&#xA;gvenzl@gvenzl-rbp-0:~ $&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;恭喜，您现在已经拥有一个可以使用的 K3s 集群！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;bonus-access-your-cluster-from-the-outside-with-kubectl&#34;&gt;奖励：使用 &lt;code&gt;kubectl&lt;/code&gt; 从外部访问您的集群&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果您想使用 &lt;code&gt;kubectl&lt;/code&gt; 从本地 MacBook 等设备访问集群，则需要保存 &lt;code&gt;/etc/rancher/k3s/k3s.yaml&lt; /code&gt; 在本地文件为 &lt;code&gt;~/.kube/config&lt;/code&gt;（&lt;code&gt;k3s.yaml&lt;/code&gt; 文件需要调用 &lt;code&gt;config&lt;/code&gt;）并替换&lt;code&gt;server&lt;/code&gt; 字段的值以及 K3s 服务器的 IP 地址或名称。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;kubectl&lt;/code&gt; 本身可以通过 Homebrew 安装：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-mac ~ %brew install kubectl&#xA;==&gt; 下载 https://ghcr.io/v2/homebrew/core/kubernetes-cli/manifests/1.31.3&#xA;已下载：/Users/gvenzl/Library/Caches/Homebrew/downloads/f8fd19d10e239038f339af3c9b47978cb154932f089fbf6b7d67ea223df378de--kubernetes-cli-1.31.3.bottle_manifest.json&#xA;==&gt; 获取 kubernetes-cli&#xA;==&gt;下载https://ghcr.io/v2/homebrew/core/kubernetes-cli/blobs/sha256:fd154ae205719c58f90bdb2a51c63e428c3bf941013557908ccd322d7488fb67&#xA;已下载：/Users/gvenzl/Library/Caches/Homebrew/downloads/ec1af5c100c16e5e4dc51cff36ce98eb1e257a312ce5a501fae7a07724e59bf9--kubernetes-cli--1.31.3.sonoma.bottle.tar.gz&#xA;==&gt;浇注kubernetes-cli--1.31.3.sonoma.bottle.tar.gz&#xA;==&gt; 注意事项&#xA;zsh 补全已安装到：&#xA;  /usr/local/share/zsh/site-functions&#xA;==&gt;总结&#xA;🍺 /usr/local/Cellar/kubernetes-cli/1.31.3：237 个文件，61.3MB&#xA;==&gt; 运行 `brew cleanup kubernetes-cli`...&#xA;通过设置 HOMEBREW_NO_INSTALL_CLEANUP 禁用此行为。&#xA;使用 HOMEBREW_NO_ENV_HINTS 隐藏这些提示（请参阅“manbrew”）。&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;接下来，创建 &lt;code&gt;~/.kube&lt;/code&gt; 文件夹并将 &lt;code&gt;k3s.yaml&lt;/code&gt; 的副本保存为 &lt;code&gt;config&lt;/code&gt;：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-mac ~ % mkdir ~/.kube&#xA;gvenzl@gvenzl-mac ~ % scp root@gvenzl-rbp-0:k3s.yaml ~/.kube/config&#xA;root@gvenzl-rbp-0 的密码：&#xA;k3s.yaml 100% 2965 221.2KB/s 00:00&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;并将 &lt;code&gt;server&lt;/code&gt; 参数替换为控制平面节点的 IP 地址或主机名：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-mac ~ % sed -i &#39;&#39; &#39;s|服务器：.*|服务器：https://gvenzl-rbp-0 :6443|g&#39; ~/.kube/config&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;完成此操作后，您也可以在本地控制集群：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;gvenzl@gvenzl-mac ~ % kubectl 获取节点&#xA;姓名 状态 角色 年龄 版本&#xA;gvenzl-rbp-0 就绪控制平面，master 39m v1.30.6+k3s1&#xA;gvenzl-rbp-1 就绪 &lt;无&gt; 28m v1.30.6+k3s1&#xA;gvenzl-rbp-2 就绪 &lt;无&gt; 25m v1.30.6+k3s1&#xA;gvenzl-rbp-3 就绪 &lt;无&gt; 25m v1.30.6+k3s1&#xA;gvenzl-rbp-4 就绪 &lt;无&gt; 24m v1.30.6+k3s1&#xA;gvenzl-rbp-5 就绪 &lt;无&gt; 23m v1.30.6+k3s1&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Thu, 26 Dec 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Managing large-scale Redis clusters on Kubernetes with an operator – Kuaishou’s approach】与算子一起管理Kubernetes上的大规模Redis集群——快手的方法</title>
      <link>https://www.cncf.io/blog/2024/12/17/managing-large-scale-redis-clusters-on-kubernetes-with-an-operator-kuaishous-approach/</link>
      <description>【&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks&#34;&gt;KubeBlocks&lt;/a&gt; by Yuxing Liu&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As a popular short-form video application, Kuaishou relies heavily on Redis to deliver low-latency responses to its users. Operating on private cloud infrastructure, automating the management of large-scale Redis clusters with minimal human intervention presents a significant challenge. A promising solution emerged: running Redis on Kubernetes using an Operator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While containerizing stateless services like applications and Nginx is now standard, running stateful services like databases and Redis on Kubernetes remains debated. Based on Kuaishou’s experience transforming Redis from physical machines to a cloud-native solution, this blog explores solutions and key considerations for managing stateful services on Kubernetes with the KubeBlocks Operator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;background&#34;&gt;Background&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#background&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As technology evolves, Kuaishou’s infrastructure is transitioning toward cloud-native technology stack. The infrastructure team delivers containers and Kubernetes to Application and PaaS systems. While stateless services at Kuaishou have almost fully adopted Kubernetes, the path toward cloud-native stateful services presents several challenges.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Taking Redis as an example, it is one of the most widely used stateful services at Kuaishou, characterized by its massive scale. Even small cost savings at this scale can deliver substantial financial benefits to the company. In its long-term planning, Kuaishou recognizes the significant potential of running Redis on Kubernetes, particularly in terms of cost optimization through improved resource utilization. This article shares insights from Kuaishou’s experience migrating Redis to Kubernetes, covering solutions, challenges encountered, and the corresponding strategies to address them.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;how-did-kuaishou-run-redis-on-kubernetes&#34;&gt;How Did Kuaishou Run Redis on Kubernetes?&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#how-did-kuaishou-run-redis-on-kubernetes&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;redis-deployment-architecture&#34;&gt;Redis Deployment Architecture&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#redis-deployment-architecture&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To meet the need for flexible shard management and support for hotspot migration and isolation, Kuaishou adopts a horizontally sharded, master-slave high-availability Redis architecture consisting of three components: Server, Sentinel, and Proxy.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://kubeblocks.io/assets/images/blog-redis-kuaishou-2-ccd82017ae8ebce4e4a8bb7289fc750f.png&#34; alt=&#34;Figure 2&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;analysis-what-does-kuaishou-require-from-a-redis-operator&#34;&gt;Analysis: What Does Kuaishou Require from a Redis Operator?&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#analysis-what-does-kuaishou-require-from-a-redis-operator&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;First, Redis Pod Management Requires a Layered Approach&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Redis Pod management needs to be handled in two layers: the first layer manages multiple shards, while the second layer manages multiple replicas within a single shard. It must support the dynamic scaling of the number of shards and the number of replicas per shard to adapt to varying workloads and usage scenarios.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This means that, in Operator’s implementation, a workload (such as a StatefulSet) is used to manage multiple replicas within each shard. On top of this, an additional layer (some CRD object) should be constructed to enable management of multiple shards within the entire Redis cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Second, Ensuring Data Consistency and Reliability During Failures and Day-2 Operations&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;During shard or replica lifecycle changes, data consistency and reliability must be ensured. For example, shard scaling requires data rebalancing, while instance scaling within a shard may require data backup and restoration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Thus, the Operator must support lifecycle hooks at both the shard and replica levels, enabling custom data management operations at different lifecycle stages.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Third, Topology Awareness for Service Discovery and Canary Releases&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The topology among multiple Redis Pods within a shard may dynamically change due to events like high-availability failovers, upgrades, or scaling operations. Service discovery and features like canary releases rely on the real-time topology.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To achieve this, the Operator must support dynamic topology awareness by introducing role detection and role labeling capabilities. This enables service discovery and canary releases based on the dynamic topology.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These requirements go beyond the capabilities of any existing open-source Redis Operator and would typically require developing a highly complex Kubernetes Operator to fulfill them. However, building a stable Operator with well-designed APIs from scratch is daunting for most platform teams, as it demands expertise in both Kubernetes and databases, along with extensive real-world testing.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;the-kubeblocks-solution-came-into-our-view&#34;&gt;The KubeBlocks Solution Came into Our View&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#the-kubeblocks-solution-came-into-our-view&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;After evaluating several solutions,&amp;nbsp;&lt;strong&gt;KubeBlocks&lt;/strong&gt;&amp;nbsp;caught our attention as an open-source Kubernetes database Operator. What makes KubeBlocks unique is its extensibility, offering an&amp;nbsp;&lt;strong&gt;Addon mechanism&lt;/strong&gt;&amp;nbsp;that allows you to use its API to describe the Day-1 and Day-2 characteristics and behaviors of a database, enabling its full lifecycle management on Kubernetes. As stated on its website, KubeBlocks’ vision is to “Run any database on Kubernetes.” This flexibility enables us to customize the KubeBlocks Redis Addon to fit our in-house Redis cluster deployment architecture.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;KubeBlocks’ API design also aligns well with our requirements for managing Redis clusters:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;1. InstanceSet: A More Powerful Workload Than StatefulSet&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;InstanceSet&lt;/strong&gt;&amp;nbsp;is a workload used within KubeBlocks to replace StatefulSet, designed specifically for managing database Pods. Like StatefulSet, InstanceSet supports managing multiple Pods (referred to as Instances). The key difference is that InstanceSet can track the&amp;nbsp;&lt;strong&gt;Role&lt;/strong&gt;&amp;nbsp;of each database Pod (e.g., primary, secondary). For different databases (as KubeBlocks supports multiple types), KubeBlocks allows customization of Pod roles, role detection methods, and the upgrade order based on roles during canary upgrades. The InstanceSet controller dynamically detects role changes during runtime and updates the role information as labels in the Pod metadata, enabling role-based Service selector.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;StatefulSet assigns each instance a globally ordered, incrementing identifier. This mechanism provides stable network and storage identities, with the topology within the cluster relying on these identifiers. However, as the topology dynamically changes during runtime, the fixed identifiers provided by StatefulSet may fall short of meeting requirements. For example, StatefulSet identifiers cannot have gaps, and deleting an intermediate identifier is not allowed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Kuaishou’s platform team has contributed several PRs to the KubeBlocks community, including enhancements such as allowing Pods within the same InstanceSet to have different configurations, decommissioning Pods with specific ordinals (without first decommissioning Pods with higher ordinals), and controlling upgrade concurrency. These improvements make InstanceSet more adaptable to Kuaishou’s requirements for managing large-scale Redis clusters in production environments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;2. Layered CRD &amp;amp; Controller Design: Component, Cluster Objects&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;KubeBlocks leverages a multi-layered CRD structure—&lt;strong&gt;Component&lt;/strong&gt;,&amp;nbsp;&lt;strong&gt;Cluster&lt;/strong&gt;—to manage the complex topology of database clusters. This design aligns seamlessly with Kuaishou’s Redis cluster deployment architecture:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Component&lt;/strong&gt;: Represents a group of Pods within the Redis cluster. For example, Proxy Pods form one Component, Sentinel Pods form another, and Redis-Server Pods are organized into one or more Components, each corresponding to a Shard. The number of Components dynamically changes based on the number of Shards.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;⛱️&amp;nbsp;&lt;strong&gt;Shard&lt;/strong&gt;: A specialized Component that defines the sharding behavior of horizontally scalable databases. Each Shard shares the same configuration. In Kuaishou’s Redis Cluster, for example, each Shard (Component) consists of a primary Pod and a replica Pod. Scaling out adds a new Shard (Component), while scaling in removes one, enabling shard-level scaling and lifecycle management.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cluster&lt;/strong&gt;: Represents the entire Redis cluster, integrating Proxy, Server, and Sentinel Components, while managing their startup topology and relationships.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This hierarchical design simplifies scaling, enhances lifecycle management, and provides the flexibility needed to support complex Redis deployment architecture in production.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Through close collaboration with the KubeBlocks community, we implemented the orchestration of a Redis cluster in the following ways:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://kubeblocks.io/assets/images/blog-redis-kuaishou-3-5b9c478f14e5551f8a75360203b57b06.png&#34; alt=&#34;Figure 3&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;There are three Components in a Redis Cluster:&amp;nbsp;&lt;code&gt;redis-server&lt;/code&gt;,&amp;nbsp;&lt;code&gt;redis-sentinel&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;redis-proxy&lt;/code&gt;. Within each Component, Pods are managed using&amp;nbsp;&lt;strong&gt;InstanceSet&lt;/strong&gt;&amp;nbsp;instead of&amp;nbsp;&lt;strong&gt;StatefulSet&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;using-kubernetes-federation-to-manage-ultra-large-scale-redis-clusters&#34;&gt;Using Kubernetes Federation to Manage Ultra-Large-Scale Redis Clusters&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#using-kubernetes-federation-to-manage-ultra-large-scale-redis-clusters&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;At Kuaishou, multiple applications operate in a multi-tenant manner within a single ultra-large-scale Redis cluster. For example, a single cluster may contain over 10,000 Pods, exceeding the capacity of a single Kubernetes cluster. As a result, we had to deploy a Redis cluster across multiple Kubernetes clusters. An important aspect is that we need to hide the complexity of managing multiple clusters from Redis application users.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;federated-k8s-cluster-architecture&#34;&gt;Federated K8s Cluster Architecture&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#federated-k8s-cluster-architecture&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Fortunately, Kuaishou’s Kubernetes infrastructure team provides a mature Kubernetes federation service, offering unified scheduling and a unified view:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Unified Scheduling&lt;/strong&gt;: Federation serves as a centralized resource dispatch entry, enabling resource scheduling across multiple member clusters.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Unified View&lt;/strong&gt;: Federation acts as a unified resource access point, allowing seamless retrieval of resources across both the federation and member clusters.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, the question becomes how can the Redis cluster management solution based on KubeBlocks be integrated into Kuaishou’s internal federation cluster architecture? Below is the overall architecture:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://kubeblocks.io/assets/images/blog-redis-kuaishou-4-38a5f290fdae275b468c48bd65c19691.png&#34; alt=&#34;Figure 4&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The Federation Kubernetes Cluster serves as the central control plane for managing multiple member clusters. It is responsible for cross-cluster orchestration, resource distribution, and lifecycle management of the Redis cluster. Its responsibilities include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Cross-Cluster Instance Distribution and Management: Ensures that Redis components (Proxy, Sentinel, Server) are distributed across member clusters based on resource requirements.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Concurrency Control&lt;/strong&gt;: Coordinates operations across clusters to ensure consistency and avoid conflicts.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Status Aggregation&lt;/strong&gt;: Collects and aggregates the status of all components from member clusters to provide a unified view.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Member K8s Clusters are the individual Kubernetes clusters where Redis Pods (instances) are deployed and managed. Each member cluster is responsible for running a subset of the overall Redis cluster. Its responsibilities include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Instance Management&lt;/strong&gt;: Localized management of Redis Pods (Proxy, Sentinel, Server) via InstanceSet.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, we divided the KubeBlocks Operator into two parts and deployed them in different Kubernetes clusters:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;The&amp;nbsp;&lt;strong&gt;InstanceSet Controller&lt;/strong&gt;&amp;nbsp;is deployed in member clusters to manage Pods locally.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The&amp;nbsp;&lt;strong&gt;Cluster Controller&lt;/strong&gt;&amp;nbsp;and&amp;nbsp;&lt;strong&gt;Component Controller&lt;/strong&gt;&amp;nbsp;are deployed in the federation cluster to handle global resource orchestration and coordination.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Once again, the layered CRD and Controller design of KubeBlocks is the key to enabling this deployment. If KubeBlocks had a monolithic CRD and Controller managing everything, splitting and deploying it separately in the Federation Kubernetes Cluster and Member Kubernetes Clusters would not have been possible.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;fed-instanceset-controller&#34;&gt;Fed-InstanceSet Controller&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#fed-instanceset-controller&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;There may be multiple Member Kubernetes Clusters, requiring the InstanceSet in the Federation Kubernetes Cluster to be partitioned into multiple InstanceSets, with one InstanceSet assigned to each Member Cluster. Additionally, the Instances (Pods) managed by the original InstanceSet need to be distributed across the new InstanceSets in the Member Clusters.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To handle this,&amp;nbsp;&lt;strong&gt;Kuaishou developed the Fed-InstanceSet Controller&lt;/strong&gt;&amp;nbsp;to manage interactions between the Federation Cluster and its Member Clusters. Its key responsibilities include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Scheduling Decisions&lt;/strong&gt;: Determining how many Instances each Member Cluster should deploy, based on predefined scheduling policies.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;InstanceSet Partitioning and Distribution&lt;/strong&gt;: Splitting the InstanceSet from the Federation Cluster and distributing the resulting InstanceSets to the appropriate Member Clusters.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To manage instance partitioning and ensure global uniqueness and proper ordering of Redis Instances in member Clusters, Kuaishou contributed a PR to the KubeBlocks community, adding an&amp;nbsp;&lt;strong&gt;Ordinals&lt;/strong&gt;&amp;nbsp;field to InstanceSet. This allows precise index assignment to instances.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The&amp;nbsp;&lt;strong&gt;Fed-InstanceSet Controller&lt;/strong&gt;&amp;nbsp;uses this field to assign unique index ranges to each Member Cluster, ensuring instance uniqueness and correct ordering across clusters.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://kubeblocks.io/assets/images/blog-redis-kuaishou-5-b038baa8377ca7861f18966529329fc5.png&#34; alt=&#34;Figure 5&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;discussion-are-stateful-services-fit-for-kubernetes&#34;&gt;Discussion: Are Stateful Services Fit for Kubernetes?&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#discussion-are-stateful-services-fit-for-kubernetes&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;benefits-and-risks-of-running-stateful-services-on-kubernetes&#34;&gt;Benefits and Risks of Running Stateful Services on Kubernetes&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#benefits-and-risks-of-running-stateful-services-on-kubernetes&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In our view, running stateful services on Kubernetes comes with notable benefits:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Improved resource utilization&lt;/strong&gt;: By merging multiple small resource pools for unified scheduling and enabling colocation of applications with Redis or Redis with other stateful services, resource usage is optimized, significantly reducing costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Enhanced operation efficiency&lt;/strong&gt;: With Kubernetes’s declarative APIs and the Operator pattern, it manages Redis services in an Infrastructure-as-Code (IaC) manner, reducing the need for manual intervention.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Lower maintenance costs&lt;/strong&gt;: Previously, Redis ran on physical machines, requiring dedicated personnel to manage the hardware infrastructure. By unifying the infrastructure onto containers and Kubernetes, infrastructure-related maintenance costs are reduced, and overall management efficiency is improved.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although running stateful services on Kubernetes offers significant benefits, the potential risks must be carefully evaluated, especially for stateful services like databases and Redis, which demand high levels of importance and stability. The challenges include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Performance degradation risk&lt;/strong&gt;: Running processes in containers, as opposed to directly on physical machines, introduces an additional layer, particularly due to the latency introduced by the overlay network. This raises concerns about potential service performance degradation.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Stability concerns&lt;/strong&gt;: Building the database platform (DBaaS) on the Kubernetes infrastructure raises concerns about whether the stability (availability and reliability) of databases or Redis might be affected.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Increased Operational Complexity&lt;/strong&gt;: In the event of an issue, would it require experts with both database and K8s technology expertise to effectively identify and resolve the problem?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://kubeblocks.io/assets/images/blog-redis-kuaishou-1-cc1d3428301b50c242ff122b274af82e.png&#34; alt=&#34;Figure 1&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The following sections explore these risks in more detail.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;mitigate-risks-of-running-redis-on-kubernetes&#34;&gt;Mitigate Risks of Running Redis on Kubernetes&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#mitigate-risks-of-running-redis-on-kubernetes&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;performance&#34;&gt;Performance&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#performance&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Containerizing Redis within a cloud-native architecture introduces an additional abstraction layer compared to traditional host-based deployments. However, industry benchmarks and Kuaishou’s internal testing show that performance differences are generally within 10%, which is often negligible in most use cases. While this variance is typically acceptable, organizations are advised to conduct their own performance testing to ensure the solution meets the specific needs of their workloads.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;stability&#34;&gt;Stability&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#stability&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Migrating stateful services to Kubernetes has greatly improved operational efficiency through automation. However, this also made the execution processes more opaque, with even small configuration changes potentially impacting many instances. To mitigate the stability risks from unexpected scenarios — such as pod evictions, human error, or Operator bugs— Kuaishou utilizes the&amp;nbsp;&lt;strong&gt;Admission Webhook&lt;/strong&gt;&amp;nbsp;mechanism within the Kubernetes API server to intercept and validate change requests. This approach allows Kuaishou to directly reject any unauthorized operations. Given the multi-cluster Kubernetes setup across multiple availability zones (AZs), it’s critical to ensure change control across clusters. To achieve this, Kuaishou developed an internal risk mitigation system called&amp;nbsp;&lt;strong&gt;kube-shield&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, it’s worth mentioning that Kuaishou has further enhanced availability and stability by improving support for fine-grained scheduling distribution and introducing load balancing features based on resource utilization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;operation-complexity&#34;&gt;Operation Complexity&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#operation-complexity&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Migrating from a host-based system to a Kubernetes-based environment, while ensuring ongoing maintenance, requires deep expertise in both Redis and K8s technologies. Relying solely on the Redis team or the K8s team for independent support would be challenging. Proper division of responsibilities not only enhances productivity but also allows each team to fully leverage their expertise in their respective domains.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For example, in Kuaishou’s cloud-native Redis solution:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Redis Team&lt;/strong&gt;: Focused on defining Redis cluster objects and encapsulating their operational expertise into declarative configurations.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Container Cloud Team&lt;/strong&gt;: Managed the Kubernetes side, including developing and maintaining the Operator, handling scheduling, and ensuring the cluster’s lifecycle.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://kubeblocks.io/assets/images/blog-redis-kuaishou-6-105d482f140f60a02013fd130ef4ef76.png&#34; alt=&#34;Figure 6&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#conclusion&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Cloud-native transformation for stateful services is a complex journey requiring careful evaluation of its pros and cons, and one filled with challenges. However, for Kuaishou, its value is self-evident. Starting with Redis, Kuaishou has worked closely with the KubeBlocks community to implement a cost-effective, cloud-native solution.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Looking forward, Kuaishou aims to build upon this experience to drive the cloud-native transformation of more stateful services, such as databases and middleware, thus reaping dual benefits in technology and cost efficiency.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;At KubeCon Hong Kong in August, Kuaishou and the KubeBlocks team delivered a joint presentation. If you’re interested, you can revisit&amp;nbsp;&lt;a href=&#34;https://kubeblocks.io/blog/migrate-redis-at-kuaishou-from-bare-metal-to-k8s&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;the talk&lt;/a&gt;&amp;nbsp;for further insights.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;About Author&lt;/strong&gt;: Yuxing Liu is the senior software engineer from Kuaishou. Yuxing has worked in the cloud-native teams of Alibaba Cloud and Kuaishou, focusing on the cloud-native field and gaining experience in open source, commercialization, and scaling of cloud-native technologies. Yuxing is also one of the maintainers of the CNCF/Dragonfly project and also one of the maintainers of the CNCF/Sealer project. Currently, he focuses on driving the cloud-native transformation of stateful business in Kuaishou.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;About Kuaishou&lt;/strong&gt;: Kuaishou is a leading content community and social platform in China and globally, committed to becoming the most customer-obsessed company in the world. Kuaishou uses its technological backbone, powered by cutting-edge AI technology, to continuously drive innovation and product enhancements that enrich its service offerings and application scenarios, creating exceptional customer value. Through short videos and live streams on Kuaishou’s platform, users can share their lives, discover goods and services they need and showcase their talent. By partnering closely with content creators and businesses, Kuaishou provides technologies, products, and services that cater to diverse user needs across a broad spectrum of entertainment, online marketing services, e-commerce, local services, gaming, and much more.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;会员帖子最初由 Yushing 在 &lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks&#34;&gt;KubeBlocks&lt;/a&gt; 上发布刘&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;作为流行的短视频应用，快手严重依赖 Redis 为用户提供低延迟响应。在私有云基础设施上运行，以最少的人工干预实现大规模 Redis 集群的自动化管理提出了重大挑战。一个有前途的解决方案出现了：使用 Operator 在 Kubernetes 上运行 Redis。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;虽然应用程序和 Nginx 等无状态服务的容器化现已成为标准，但在 Kubernetes 上运行数据库和 Redis 等有状态服务仍然存在争议。基于快手将Redis从物理机转变为云原生解决方案的经验，本博客探讨了使用KubeBlocks Operator管理Kubernetes上有状态服务的解决方案和关键注意事项。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;background&#34;&gt;​​背景&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#background &#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;随着技术的发展，快手的基础设施正在向云原生技术栈过渡。基础设施团队将容器和 Kubernetes 交付给应用程序和 PaaS 系统。虽然快手的无状态服务几乎全面采用了 Kubernetes，但通往云原生有状态服务的道路面临着一些挑战。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;以Redis为例，它是快手使用最广泛的状态服务之一，其特点是规模庞大。即使是这种规模的小额成本节省也可以为公司带来巨大的经济效益。在其长期规划中，快手认识到在 Kubernetes 上运行 Redis 的巨大潜力，特别是在通过提高资源利用率来优化成本方面。本文分享了快手Redis迁移到Kubernetes的经验，包括解决方案、遇到的挑战以及相应的应对策略。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;how-did-kuaishou-run-redis-on-kubernetes&#34;&gt;快手是如何在 Kubernetes 上运行 Redis 的？&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#how-did-kuaishou-run-redis-on-kubernetes&#34;&gt;&lt;/a&gt;&lt;/ H2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;redis-deployment-architecture&#34;&gt;Redis 部署架构&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s -with-kubeblocks#redis-deployment-architecture&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;为了满足灵活的分片管理以及支持热点迁移和隔离的需求，快手采用了水平分片、主从高可用的Redis架构，由Server、Sentinel和Proxy三个组件组成。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://kubeblocks.io/assets/images/blog-redis-kuaishou-2-ccd82017ae8ebce4e4a8bb7289fc750f.png&#34; alt=&#34;图2&#34;referrerpolicy=&#34;否-引用者&#34;&gt;&lt;/图&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;analysis-what-does-kuaishou-require-from-a-redis-operator&#34;&gt;分析：快手对 Redis Operator 有何要求？&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#analysis-what-does-kuaishou-require-from-a-redis-operator&#34;&gt;&lt;/一个&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;首先，Redis Pod 管理需要分层方法&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Redis Pod 管理需要分两层处理：第一层管理多个分片，第二层管理单个分片内的多个副本。它必须支持分片数量和每个分片副本数量的动态扩展，以适应不同的工作负载和使用场景。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;这意味着，在 Operator 的实现中，工作负载（例如 StatefulSet）用于管理每个分片内的多个副本。在此之上，应构建一个附加层（某些 CRD 对象）来管理整个 Redis 集群内的多个分片。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;第二，确保故障和第二天运营期间的数据一致性和可靠性&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在分片或副本生命周期发生变化时，必须保证数据的一致性和可靠性。例如，分片扩展需要数据重新平衡，而分片内的实例扩展可能需要数据备份和恢复。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;因此，Operator 必须支持分片和副本级别的生命周期挂钩，从而在不同的生命周期阶段启用自定义数据管理操作。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;三、服务发现和金丝雀发布的拓扑感知&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;由于高可用性故障转移、升级或扩展操作等事件，分片内多个 Redis Pod 之间的拓扑可能会动态变化。服务发现和金丝雀发布等功能依赖于实时拓扑。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;为了实现这一目标，Operator 必须通过引入角色检测和角色标记功能来支持动态拓扑感知。这使得基于动态拓扑的服务发现和金丝雀发布成为可能。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;这些要求超出了任何现有开源 Redis Operator 的能力，通常需要开发高度复杂的 Kubernetes Operator 来满足这些要求。然而，对于大多数平台团队来说，从头开始构建一个具有精心设计的 API 的稳定 Operator 是一项艰巨的任务，因为它需要 Kubernetes 和数据库方面的专业知识，以及广泛的实际测试。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;the-kubeblocks-solution-came-into-our-view&#34;&gt;KubeBlocks 解决方案进入我们的视野&lt;a href=&#34;https://kubeblocks.io/blog /manage-large-scale-redis-on-k8s-with-kubeblocks#the-kubeblocks-solution-came-into-our-view&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在评估了多种解决方案后，&lt;strong&gt;KubeBlocks&lt;/strong&gt; 作为开源 Kubernetes 数据库运营商引起了我们的注意。 KubeBlocks 的独特之处在于它的可扩展性，提供&amp;nbsp;&lt;strong&gt;插件机制&lt;/strong&gt;，允许您使用其 API 来描述数据库的 Day-1 和 Day-2 特征和行为，从而在 Kubernetes 上实现其完整的生命周期管理。正如其网站所述，KubeBlocks 的愿景是“在 Kubernetes 上运行任何数据库”。这种灵活性使我们能够自定义 KubeBlocks Redis Addon 以适应我们内部的 Redis 集群部署架构。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;KubeBlocks 的 API 设计也非常符合我们管理 Redis 集群的要求：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;强&gt;1。 InstanceSet：比StatefulSet更强大的工作负载&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;InstanceSet&lt;/strong&gt;是 KubeBlocks 中使用的一种工作负载，用于替代 StatefulSet，专为管理数据库 Pod 而设计。与 StatefulSet 一样，InstanceSet 支持管理多个 Pod（简称 Instance）。主要区别在于 InstanceSet 可以跟踪每个数据库 Pod 的&lt;strong&gt;角色&lt;/strong&gt;（例如，主要、辅助）。针对不同的数据库（KubeBlocks支持多种类型），KubeBlocks允许自定义Pod角色、角色检测方式以及金丝雀升级时根据角色的升级顺序。 InstanceSet 控制器在运行时动态检测角色变化，并将角色信息更新为 Pod 元数据中的标签，从而实现基于角色的服务选择器。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;StatefulSet 为每个实例分配一个全局排序的递增标识符。该机制提供稳定的网络和存储身份，集群内的拓扑依赖于这些标识符。然而，由于拓扑在运行时动态变化，StatefulSet提供的固定标识符可能无法满足要求。例如，StatefulSet标识符不能有间隙，并且不允许删除中间标识符。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;快手平台团队向 KubeBlocks 社区贡献了多个 PR，包括允许同一 InstanceSet 内的 Pod 有不同配置、退役特定序号的 Pod（无需先退役更高序号的 Pod）、控制升级并发等增强功能。这些改进使得InstanceSet更能适应快手在生产环境中管理大规模Redis集群的需求。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;强&gt;2。分层 CRD 和控制器设计：组件、集群对象&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;KubeBlocks 利用多层 CRD 结构（&lt;strong&gt;组件&lt;/strong&gt;、&lt;strong&gt;集群&lt;/strong&gt;）来管理数据库集群的复杂拓扑。这一设计与快手的Redis集群部署架构无缝对接：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;组件&lt;/strong&gt;：代表 Redis 集群中的一组 Pod。例如，Proxy Pod 构成一个 Component，Sentinel Pod 构成另一个 Component，Redis-Server Pod 则组织成一个或多个 Component，每个 Component 对应一个 Shard。组件数量 dy根据分片数量进行自然变化。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;​​⛱️ &lt;strong&gt;Shard&lt;/strong&gt;：定义水平可扩展数据库的分片行为的专用组件。每个分片共享相同的配置。以快手的Redis集群为例，每个Shard（组件）由一个主Pod和一个副本Pod组成。向外扩展会添加一个新的分片（组件），而向内扩展会删除一个新的分片（组件），从而实现分片级别的扩展和生命周期管理。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;集群&lt;/strong&gt;：代表整个 Redis 集群，集成了 Proxy、Server 和 Sentinel 组件，同时管理它们的启动拓扑和关系。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;这种分层设计简化了扩展，增强了生命周期管理，并提供了支持生产中复杂的 Redis 部署架构所需的灵活性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;通过与 KubeBlocks 社区的密切合作，我们通过以下方式实现了 Redis 集群的编排：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://kubeblocks.io/assets/images/blog-redis-kuaishou-3-5b9c478f14e5551f8a75360203b57b06.png&#34; alt=&#34;图3&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Redis 集群中有三个组件：&lt;code&gt;redis-server&lt;/code&gt;、&lt;code&gt;redis-sentinel&lt;/code&gt; 和 &lt;code&gt;redis-proxy&lt;/code&gt;。在每个组件中，Pod 是使用 &lt;strong&gt;InstanceSet&lt;/strong&gt; 而不是 &lt;strong&gt;StatefulSet&lt;/strong&gt; 进行管理的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;using-kubernetes-federation-to-manage-ultra-large-scale-redis-clusters&#34;&gt;使用 Kubernetes Federation 管理超大规模 Redis 集群&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#using-kubernetes-federation-to-manage-ultra-large-scale-redis-clusters&#34;&gt; &lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在快手，多个应用程序在单个超大规模Redis集群中以多租户方式运行。例如，单个集群可能包含超过 10,000 个 Pod，超出了单个 Kubernetes 集群的容量。因此，我们必须跨多个 Kubernetes 集群部署 Redis 集群。一个重要的方面是我们需要向 Redis 应用程序用户隐藏管理多个集群的复杂性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;federated-k8s-cluster-architecture&#34;&gt;联合 K8s 集群架构&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis- on-k8s-with-kubeblocks#federated-k8s-cluster-architecture&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;幸运的是，快手的 Kubernetes 基础设施团队提供了成熟的 Kubernetes 联邦服务，提供统一调度和统一视图：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;统一调度&lt;/strong&gt;：联邦作为集中的资源调度入口，实现跨多个成员集群的资源调度。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;统一视图&lt;/strong&gt;：联邦充当统一的资源访问点，允许跨联邦和成员集群无缝检索资源。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;那么问题来了，基于KubeBlocks的Redis集群管理方案如何融入到快手内部联邦集群架构中呢？整体架构如下：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://kubeblocks.io/assets/images/blog-redis-kuaishou-4-38a5f290fdae275b468c48bd65c19691.png&#34; alt=&#34;图4&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;联邦 Kubernetes 集群作为管理多个成员集群的中央控制平面。它负责Redis集群的跨集群编排、资源分配和生命周期管理。其职责包括：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;跨集群实例分发和管理：确保 Redis 组件（Proxy、Sentinel、Server）根据资源需求跨成员集群进行分发。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;并发控制&lt;/strong&gt;：协调跨集群的操作，以确保一致性并避免冲突。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;状态聚合&lt;/strong&gt;：收集并聚合成员集群中所有组件的状态，以提供统一的视图。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;成员 K8s 集群是部署和管理 Redis Pod（实例）的单个 Kubernetes 集群。每个成员集群负责运行整个 Redis 集群的一个子集。其职责包括：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;实例管理&lt;/strong&gt;：通过 InstanceSet 对 Redis Pod（Proxy、Sentinel、Server）进行本地化管理。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;因此，我们将 KubeBlocks Operator 分为两部分，并将它们部署在不同的 Kubernetes 集群中：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;InstanceSet 控制器&lt;/strong&gt;部署在成员集群中，用于在本地管理 Pod。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;集群控制器&lt;/strong&gt;和&lt;strong&gt;组件控制器&lt;/strong&gt;部署在联合集群中，用于处理全局资源编排和协调。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;KubeBlocks 的分层 CRD 和控制器设计再次成为实现此部署的关键。如果 KubeBlocks 有一个单一的 CRD 和控制器来管理一切，那么在联邦 Kubernetes 集群和成员 Kubernetes 集群中分开和部署它是不可能的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;fed-instanceset-controller&#34;&gt;Fed-InstanceSet 控制器&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on- k8s-with-kubeblocks#fed-instanceset-controller&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;可能存在多个Member Kubernetes Cluster，需要将Federation Kubernetes Cluster中的InstanceSet划分为多个InstanceSet，每个Member Cluster分配一个InstanceSet。附加ly，原始 InstanceSet 管理的实例（Pod）需要分布在成员集群中的新 InstanceSet 中。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;为了解决这个问题，&lt;strong&gt;快手开发了 Fed-InstanceSet 控制器&lt;/strong&gt;来管理联邦集群与其成员集群之间的交互。其主要职责包括：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;调度决策&lt;/strong&gt;：根据预定义的调度策略确定每个成员集群应部署多少个实例。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;InstanceSet 分区和分发&lt;/strong&gt;：从联邦集群中拆分 InstanceSet，并将生成的 InstanceSet 分发到适当的成员集群。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;为了管理实例分区并确保成员集群中 Redis 实例的全局唯一性和正确排序，快手向 KubeBlocks 社区贡献了一个 PR，向 InstanceSet 添加了一个 &lt;strong&gt;Ordinals&lt;/strong&gt; 字段。这允许为实例精确分配索引。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Fed-InstanceSet 控制器&lt;/strong&gt;使用此字段为每个成员集群分配唯一的索引范围，确保实例的唯一性和跨集群的正确排序。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://kubeblocks.io/assets/images/blog-redis-kuaishou-5-b038baa8377ca7861f18966529329fc5.png&#34; alt=&#34;图5&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;discussion-are-stateful-services-fit-for-kubernetes&#34;&gt;讨论：有状态服务适合 Kubernetes 吗？&lt;a href=&#34;https://kubeblocks.io /blog/manage-large-scale-redis-on-k8s-with-kubeblocks#discussion-are-stateful-services-fit-for-kubernetes&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;benefits-and-risks-of-running-stateful-services-on-kubernetes&#34;&gt;在 Kubernetes 上运行有状态服务的好处和风险&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#benefits-and-risks-of-running-stateful-services-on-kubernetes&#34;&gt;&lt;/一个&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们认为，在 Kubernetes 上运行有状态服务具有显着的好处：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;提高资源利用率&lt;/strong&gt;：通过合并多个小型资源池进行统一调度，并实现应用与 Redis 或 Redis 与其他有状态服务的共置，优化资源利用率，显着降低成本。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;提高运行效率&lt;/strong&gt;：借助 Kubernetes 的声明式 API 和 Operator 模式，以基础设施即代码 (IaC) 的方式管理 Redis 服务，减少人工干预的需要。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;更低的维护成本&lt;/strong&gt;：此前，Redis运行在物理机上，需要专门的人员来管理硬件基础设施。通过将基础设施统一到容器和 Kubernetes 上，降低了基础设施相关的维护成本，并提高了整体管理能力提高了处理效率。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;虽然在 Kubernetes 上运行有状态服务可以带来显着的好处，但必须仔细评估潜在的风险，特别是对于数据库和 Redis 等有状态服务，它们需要高度的重要性和稳定性。挑战包括：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol类=“wp-block-list”&gt;&#xA;&lt;li&gt;&lt;strong&gt;性能下降风险&lt;/strong&gt;：在容器中运行进程（而不是直接在物理机上运行）会引入额外的层，特别是由于覆盖网络引入的延迟。这引起了人们对潜在服务性能下降的担忧。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;稳定性问题&lt;/strong&gt;：在 Kubernetes 基础设施上构建数据库平台 (DBaaS) 会引发人们对数据库或 Redis 的稳定性（可用性和可靠性）是否会受到影响的担忧。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;运营复杂性增加&lt;/strong&gt;：如果出现问题，是否需要同时具备数据库和 K8s 技术专业知识的专家来有效识别和解决问题？&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://kubeblocks.io/assets/images/blog-redis-kuaishou-1-cc1d3428301b50c242ff122b274af82e.png&#34; alt=&#34;图1&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;以下部分将更详细地探讨这些风险。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;mitigate-risks-of-running-redis-on-kubernetes&#34;&gt;降低在 Kubernetes 上运行 Redis 的风险&lt;a href=&#34;https://kubeblocks.io/blog /manage-large-scale-redis-on-k8s-with-kubeblocks#mitigate-risks-of-running-redis-on-kubernetes&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;performance&#34;&gt;性能&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#performance &#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;与传统的基于主机的部署相比，在云原生架构中容器化 Redis 引入了额外的抽象层。然而，行业基准和快手的内部测试表明，性能差异通常在 10% 以内，这在大多数用例中通常可以忽略不计。虽然这种差异通常是可以接受的，但建议组织进行自己的性能测试，以确保解决方案满足其工作负载的特定需求。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;stability&#34;&gt;稳定性&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#stability &#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;将有状态服务迁移到 Kubernetes，通过自动化极大地提高了运营效率。然而，这也使得执行过程更加不透明，即使很小的配置更改也可能会影响许多实例。为了降低意外情况（例如 pod 驱逐、人为错误或 Operator bug）带来的稳定性风险，快手利用 Kubernetes API 服务器内的&lt;strong&gt;Admission Webhook&lt;/strong&gt; 机制来拦截和验证变更请求sts。这种做法可以让快手直接拒绝任何未经授权的操作。鉴于跨多个可用区 (AZ) 的多集群 Kubernetes 设置，确保跨集群的变更控制至关重要。为了实现这一目标，快手开发了一个名为&lt;strong&gt;kube-shield&lt;/strong&gt;的内部风险缓解系统。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;此外，值得一提的是，快手通过完善对细粒度调度分发的支持以及引入基于资源利用率的负载均衡功能，进一步增强了可用性和稳定性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;operation-complexity&#34;&gt;操作复杂度&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with- kubeblocks#operation-complexity&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;从基于主机的系统迁移到基于 Kubernetes 的环境，同时确保持续维护，需要在 Redis 和 K8s 技术方面拥有深厚的专业知识。仅仅依靠Redis团队或K8s团队的独立支持是有挑战性的。适当的职责分工不仅可以提高生产力，还可以让每个团队充分发挥各自领域的专业知识。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;以快手云原生Redis方案为例：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Redis 团队&lt;/strong&gt;：专注于定义 Redis 集群对象并将其操作专业知识封装到声明性配置中。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;容器云团队&lt;/strong&gt;：管理 Kubernetes 方面，包括开发和维护 Operator、处理调度以及确保集群的生命周期。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://kubeblocks.io/assets/images/blog-redis-kuaishou-6-105d482f140f60a02013fd130ef4ef76.png&#34; alt=&#34;图6&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;结论&lt;a href=&#34;https://kubeblocks.io/blog/manage-large-scale-redis-on-k8s-with-kubeblocks#conclusion &#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;有状态服务的云原生转型是一个复杂的过程，需要仔细评估其优缺点，并且充满挑战。然而，对于快手来说，其价值是不言而喻的。从Redis开始，快手与KubeBlocks社区密切合作，实现了经济高效的云原生解决方案。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;展望未来，快手希望以此经验为基础，推动数据库、中间件等更有状态服务的云原生转型，从而获得技术和成本效率的双重效益。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;8月份的香港KubeCon上，快手与KubeBlocks团队联合发表了演讲。如果您有兴趣，可以重新访问&lt;a href=&#34;https://kubeblocks.io/blog/migrate-redis-at-kuaishou-from-bare-metal-to-k8s&#34; target=&#34;_blank&#34; rel=&#34; noreferrer noopener&#34;&gt;演讲&lt;/a&gt;以获取更多见解。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;作者简介&lt;/strong&gt;：刘宇星是老大或者来自快手的软件工程师。宇星曾就职于阿里云、快手云原生团队，专注于云原生领域，在云原生技术的开源、商业化、规模化等方面积累了丰富的经验。宇兴也是 CNCF/Dragonfly 项目的维护者之一，也是 CNCF/Sealer 项目的维护者之一。目前主要负责推动快手有状态业务的云原生转型。&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;关于快手&lt;/strong&gt;：快手是中国乃至全球领先的内容社区和社交平台，致力于成为全球最以客户为中心的公司。快手以前沿人工智能技术为支撑，不断推动创新和产品升级，丰富服务内容和应用场景，创造卓越的客户价值。通过快手平台上的短视频和直播，用户可以分享自己的生活，发现自己需要的商品和服务，展示自己的才华。通过与内容创作者和企业密切合作，快手提供技术、产品和服务，满足娱乐、在线营销服务、电子商务、本地服务、游戏等广泛领域的多样化用户需求。&lt;/em &gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Mon, 16 Dec 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【What is Inference Parallelism and how it works】什么是推理并行性及其工作原理</title>
      <link>https://www.cncf.io/blog/2024/12/18/what-is-inference-parallelism-and-how-it-works/</link>
      <description>【&lt;p&gt;&lt;em&gt;Member post originally published on the &lt;a href=&#34;https://www.infracloud.io/blogs/inference-parallelism/&#34;&gt;InfraCloud blog&lt;/a&gt; by Aman Juneja, Principal Solutions Engineer at InfraCloud Technologies&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In recent years, we’ve witnessed two recurring trends: the release of increasingly powerful GPUs and the introduction of Large Language Models (LLMs) with billions or trillions of parameters and expansive context windows. Many businesses are leveraging these LLMs by fine-tuning them or building out apps with&amp;nbsp;&lt;a href=&#34;https://www.infracloud.io/blogs/retrieval-augmented-generation-using-data-with-llms/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;domain-specific knowledge using RAG&lt;/a&gt;&amp;nbsp;and deploying them on dedicated GPU servers. Now, when it comes to deploying these models on GPU, one thing to notice is the model size, i.e., the space required (for storing the parameters and context tokens) to load the model into the GPU memory is too high compared to the memory available on the GPU.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;model-size-vs-gpu-memory&#34;&gt;Model Size vs GPU Memory&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;There are methods to reduce model sizes by using optimization techniques like&amp;nbsp;&lt;a href=&#34;https://www.infracloud.io/blogs/exploring-ai-model-inference/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;quantization, pruning, distillation &amp;amp; compression, etc.&lt;/a&gt;&amp;nbsp;But if you notice in the below comparison table between the latest GPU memory and space requirement for 70B models (FP16 quantized), it’s almost impossible to handle multiple requests at a time, or in some GPUs, the model will not even fit on the memory.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;GPU&lt;/th&gt;&lt;th&gt;FP16 (TFLOPS)&lt;br&gt;with sparsity&lt;/th&gt;&lt;th&gt;GPU Memory&lt;br&gt;(GB)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;B200&lt;/td&gt;&lt;td&gt;4500&lt;/td&gt;&lt;td&gt;192&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;B100&lt;/td&gt;&lt;td&gt;3500&lt;/td&gt;&lt;td&gt;192&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;H200&lt;/td&gt;&lt;td&gt;1979&lt;/td&gt;&lt;td&gt;141&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;H100&lt;/td&gt;&lt;td&gt;1979&lt;/td&gt;&lt;td&gt;80&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;L4&lt;/td&gt;&lt;td&gt;242&lt;/td&gt;&lt;td&gt;24&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;L40S&lt;/td&gt;&lt;td&gt;733&lt;/td&gt;&lt;td&gt;48&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;L40&lt;/td&gt;&lt;td&gt;362&lt;/td&gt;&lt;td&gt;48&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;A100&lt;/td&gt;&lt;td&gt;624&lt;/td&gt;&lt;td&gt;80&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This is all with already applied FP16 quantization that incurs some loss of precision (which is usually acceptable in many of the generic use cases).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Models&lt;/th&gt;&lt;th&gt;KV Cache in GB for Parameters (FP16)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;llama3-8B&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;llama3-70B&lt;/td&gt;&lt;td&gt;140&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;llama-2-13B&lt;/td&gt;&lt;td&gt;26&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;llama2-70B&lt;/td&gt;&lt;td&gt;140&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;mistral-7B&lt;/td&gt;&lt;td&gt;14&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This brings us to the context of this blog post, i.e. how enterprises run large billion or trillion parameters LLM models on these modern datacenter GPUs. Are there any ways to split these models into smaller pieces and run only what is required at the moment, or can we distribute the parts of the model into different GPUs? I will try to answer these questions in this blog post with the current set of methods available to perform inference parallelization and also will try to highlight some of the tools/libraries that support these methods of parallelization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;inference-parallelism&#34;&gt;Inference parallelism&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Inference parallelism aims to distribute the computational workload of AI models, particularly deep learning models, across multiple processing units such as GPUs. This distribution allows for faster processing, reduced latency, and the ability to handle models that exceed the memory capacity of a single device.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Four primary methods have been developed to achieve inference parallelism, each with its strengths and applications:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Data Parallelism&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Tensor Parallelism&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Pipeline Parallelism&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Expert Parallelism&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;data-parallelism&#34;&gt;Data parallelism&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In data parallelism, we deploy multiple copies of models on different GPUs or GPU clusters. Each copy of the model independently processes the user request. In a simple analogy, this is like having multiple replicas of 1 microservice.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Now, a common question one might have is how it solves the problem of model size fitting into GPU memory, which we discussed at the start, and the short answer is that it doesn’t. This method is only recommended for smaller models that can fit into the GPU memory. In those cases, we can use multiple copies of the model deployed on different GPU instances and distribute the requests to different instances hence providing enough GPU resources for each request and also increasing the availability of the service. This will also increase the overall request throughput for the system as you have more instances to handle the traffic now.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.infracloud.io/assets/img/Blog/inference-parallelism/data-parallelism.webp&#34; alt=&#34;Data parallelism&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;a href=&#34;https://xzt102.github.io/publications/2021_WWW.pdf&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;(ImageSource)&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;tensor-parallelism&#34;&gt;Tensor parallelism&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In tensor parallelism, we split each layer of the model across different GPUs. A single user request will be shared across multiple GPUs and the result of each request’s GPU computations will be recombined over a&amp;nbsp;&lt;a href=&#34;https://www.infracloud.io/blogs/introduction-to-nvidia-network-operator/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;GPU-to-GPU network&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To understand it better, as the name suggests, we split the tensors into chunks along a particular dimension such that each device only holds 1/N chunk of the tensor. Computation is performed using this partial chunk to get partial output. These partial outputs are collected from all devices and then combined.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.infracloud.io/assets/img/Blog/inference-parallelism/tensore-parallelism.webp&#34; alt=&#34;Tensor parallelism&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;a href=&#34;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;(Image Source)&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As you might have noticed already, the bottleneck to the performance of tensor parallelism is the speed of the network between GPU-to-GPU. As each request will be computed across different GPUs and then combined, we need a high-performance network to ensure low latency numbers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.infracloud.io/assets/img/Blog/inference-parallelism/model-parallelism.webp&#34; alt=&#34;Model parallelism&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;a href=&#34;https://xzt102.github.io/publications/2021_WWW.pdf&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;(Image Source)&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;pipeline-parallelism&#34;&gt;Pipeline parallelism&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In pipeline parallelism, we distribute a group of model layers across different GPUs. Layer-based partitioning is the fundamental approach in pipeline parallelism. The model’s layers are grouped into continuous blocks, forming stages. This partitioning is typically done vertically through the network’s architecture. Computational balance is a key consideration. Ideally, each stage should have an approximately equal computational load to prevent bottlenecks. This often involves grouping layers of varying complexities to achieve balance. Memory usage optimization is another critical factor. Stages are designed to fit within the memory constraints of individual devices while maximizing utilization. Communication overhead minimization is also important. The partitioning aims to reduce the amount of data transferred between stages, as inter-device communication can be a significant performance bottleneck.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So for example, if you are deploying LLaMA3-8B model that has 32 layers on a 4 GPU instance, you can split and distribute 8 layers of model on each GPU. The processing of requests happens in a sequential manner where the computation starts at one GPU and continues to the next GPU with point-to-point communication.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Again, as multiple GPU instances are involved, the networking can become a huge bottleneck if we do not have high-speed network communication between the GPUs.This parallelism can increase the GPU throughput as every request will need fewer resources from each GPU and should be easily available, but it will end up increasing the overall latency as the request will be processed sequentially, and delay in any GPU computation or network component will cause an overall surge in latency.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.infracloud.io/assets/img/Blog/inference-parallelism/model-parallelism-layer-wise.webp&#34; alt=&#34;Model parallelism layer wise&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;a href=&#34;https://xzt102.github.io/publications/2021_WWW.pdf&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;(Image Source)&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;expert-parallelism&#34;&gt;Expert parallelism&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Expert parallelism, often implemented as a Mixture of Experts (MoE), is a technique that allows for the efficient use of large models during the inference process. It doesn’t solve the problem of fitting the models into the GPU memory but provides an option to have a broad capability model serving the requests based on the request context. In this technique, the model is divided into multiple expert sub-networks. Each expert is typically a neural network trained to handle specific types of inputs or subtasks within the broader problem domain. A gating network determines which expert to use for each input. Only a subset of experts is activated for any given input. Different experts can be distributed across different GPUs. Router/Gating network and active experts can operate in parallel. Inactive experts don’t consume computational resources. This greatly reduces the number of parameters that each request must interact with, as some experts are skipped. But like Tensor &amp;amp; Pipeline parallelism the overall request latency relies heavily on the GPU-to-GPU communication network. A request must be reconstituted back to their original GPUs after expert processing generating high networking communication over the GPU-to-GPU interconnect fabric.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This approach can lead to better utilization of hardware compared to Tensor parallelism as you don’t have to split the operations into smaller chunks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.infracloud.io/assets/img/Blog/inference-parallelism/moe-llm.webp&#34; alt=&#34;MoE LLM&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;a href=&#34;https://www.datasciencecentral.com/why-the-newest-llms-use-a-moe-mixture-of-experts-architecture/?ref=dailydev&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;(Image Source)&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Following is a summary and comparison of the methods we discussed. You can use it as a reference when planning to choose one for your use case.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Aspect&lt;/th&gt;&lt;th&gt;Data Parallelism&lt;/th&gt;&lt;th&gt;Tensor Parallelism&lt;/th&gt;&lt;th&gt;Pipeline Parallelism&lt;/th&gt;&lt;th&gt;Expert Parallelism&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Basic Concept&lt;/td&gt;&lt;td&gt;Splits input data across multiple devices&lt;/td&gt;&lt;td&gt;Splits individual tensors/layers across devices&lt;/td&gt;&lt;td&gt;Splits model into sequential stages across devices&lt;/td&gt;&lt;td&gt;Splits model into multiple expert sub-networks&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;How it Works&lt;/td&gt;&lt;td&gt;The same model is replicated on each device, processing different data chunks&lt;/td&gt;&lt;td&gt;Single layer/operation distributed across multiple devices&lt;/td&gt;&lt;td&gt;Different parts of the model pipeline on different devices&lt;/td&gt;&lt;td&gt;The router selects specific experts for each input&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Parallelization Unit&lt;/td&gt;&lt;td&gt;Batch of inputs&lt;/td&gt;&lt;td&gt;Individual tensors/layers&lt;/td&gt;&lt;td&gt;Model stages&lt;/td&gt;&lt;td&gt;Experts (sub-networks)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Scalability&lt;/td&gt;&lt;td&gt;Scales well with batch size&lt;/td&gt;&lt;td&gt;Scales well for very large models&lt;/td&gt;&lt;td&gt;Scales well for deep models&lt;/td&gt;&lt;td&gt;Scales well for wide models&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Memory Efficiency&lt;/td&gt;&lt;td&gt;Low (full model on each device)&lt;/td&gt;&lt;td&gt;High (only part of each layer on each device)&lt;/td&gt;&lt;td&gt;High (only part of the model on each device)&lt;/td&gt;&lt;td&gt;Medium to High (experts distributed across devices)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Communication Overhead&lt;/td&gt;&lt;td&gt;Low&lt;/td&gt;&lt;td&gt;Medium to High&lt;/td&gt;&lt;td&gt;Low (only between adjacent stages)&lt;/td&gt;&lt;td&gt;Medium (router communication and expert selection)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Load Balancing&lt;/td&gt;&lt;td&gt;Generally balanced if data is evenly distributed&lt;/td&gt;&lt;td&gt;Balanced within operations&lt;/td&gt;&lt;td&gt;Can be challenging, and requires careful stage design&lt;/td&gt;&lt;td&gt;Can be challenging, and requires effective routing&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Latency&lt;/td&gt;&lt;td&gt;Low for large batches&lt;/td&gt;&lt;td&gt;Can increase for small batches&lt;/td&gt;&lt;td&gt;Higher due to pipeline depth&lt;/td&gt;&lt;td&gt;Can be low if routing is efficient&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Throughput&lt;/td&gt;&lt;td&gt;High for large batches&lt;/td&gt;&lt;td&gt;Can be high for large models&lt;/td&gt;&lt;td&gt;High, especially for deep models&lt;/td&gt;&lt;td&gt;Can be very high for diverse inputs&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Typical Use Cases&lt;/td&gt;&lt;td&gt;Large batch inference, embarrassingly parallel tasks&lt;/td&gt;&lt;td&gt;Very large models that don’t fit on a single device&lt;/td&gt;&lt;td&gt;Deep models with sequential dependencies&lt;/td&gt;&lt;td&gt;Models with diverse sub-tasks or specializations&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Challenges&lt;/td&gt;&lt;td&gt;Limited by batch size, high memory usage&lt;/td&gt;&lt;td&gt;Complex implementation, potential communication bottlenecks&lt;/td&gt;&lt;td&gt;Pipeline bubble, difficulty in optimal stage partitioning&lt;/td&gt;&lt;td&gt;Load balancing, routing overhead, training instability&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Adaptability to Input Size&lt;/td&gt;&lt;td&gt;Highly adaptable&lt;/td&gt;&lt;td&gt;Less adaptable, fixed tensor partitioning&lt;/td&gt;&lt;td&gt;Less adaptable, fixed pipeline&lt;/td&gt;&lt;td&gt;Highly adaptable, different experts for different inputs&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Suitable Model Types&lt;/td&gt;&lt;td&gt;Most model types&lt;/td&gt;&lt;td&gt;Transformer-based models, very large neural networks&lt;/td&gt;&lt;td&gt;Deep sequential models&lt;/td&gt;&lt;td&gt;Multi-task models, language models with diverse knowledge&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Supported Inference Backends&lt;/td&gt;&lt;td&gt;TensonRT-LLM, vLLM, TGI&lt;/td&gt;&lt;td&gt;TensonRT-LLM, vLLM, TGI&lt;/td&gt;&lt;td&gt;TensonRT-LLM, vLLM, TGI&lt;/td&gt;&lt;td&gt;TensonRT-LLM, vLLM, TGI&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;parallelism-techniques-combined&#34;&gt;Parallelism techniques combined&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;By now, you might already be thinking that if using the above parallelism methods means we are reducing the overall consumption or utilization of GPU, then can we not combine or replicate these to increase the overall GPU throughput? Combining Inference parallelism methods can lead to more efficient and scalable systems, especially for large and complex models.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the table below, you can see 4 possible options but in actual scenarios based on the number of GPUs you have, this combination can grow to a very large number.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Data Parallelism (DP)&lt;br&gt;+&lt;br&gt;Pipeline Parallelism (PP)&lt;/th&gt;&lt;th&gt;Tensor Parallelism (TP)&lt;br&gt;+&lt;br&gt;Pipeline Parallelism (PP)&lt;/th&gt;&lt;th&gt;Expert Parallelism (EP)&lt;br&gt;+&lt;br&gt;Data Parallelism (DP)&lt;/th&gt;&lt;th&gt;Tensor Parallelism (TP)&lt;br&gt;+&lt;br&gt;Expert Parallelism (EP)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Split the model into stages (pipeline parallelism)&lt;/td&gt;&lt;td&gt;Divide the model into stages (pipeline parallelism)&lt;/td&gt;&lt;td&gt;Distribute experts across devices (expert parallelism)&lt;/td&gt;&lt;td&gt;Split large expert models across devices (tensor parallelism)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Replicate each stage across multiple devices (data parallelism)&lt;/td&gt;&lt;td&gt;Split large tensors within each stage across devices (tensor parallelism)&lt;/td&gt;&lt;td&gt;Process multiple inputs in parallel (data parallelism)&lt;/td&gt;&lt;td&gt;Split large expert models across devices (tensor parallelism)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So for example let’s say you have 64 GPU available and you are planning to deploy llama3-8b or Mistral 8*7B model on them. The following are some of the possible combinations of the parallelism methods. These are just examples to understand the parallelism combination strategies, for the actual use case you need to consider and benchmark other options as well.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;LLaMA 3-8B (64 GPUs)&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Strategy&lt;/th&gt;&lt;th&gt;GPU Allocation&lt;/th&gt;&lt;th&gt;Pros&lt;/th&gt;&lt;th&gt;Cons&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;PP8TP8&lt;/td&gt;&lt;td&gt;64 = 8 (pipeline) × 8 (tensor)&lt;/td&gt;&lt;td&gt;– Balanced distribution&lt;br&gt;– Reduced Communication&lt;/td&gt;&lt;td&gt;– Pipeline bubbles&lt;br&gt;– Increased latency&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PP4TP4DP4&lt;/td&gt;&lt;td&gt;64 = 4 (pipeline) × 4 (tensor) × 4 (data)&lt;/td&gt;&lt;td&gt;– Higher throughput&lt;br&gt;– Flexible for batch sizes&lt;/td&gt;&lt;td&gt;– Complex integration&lt;br&gt;– Requires large batches&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DP8TP8&lt;/td&gt;&lt;td&gt;64 = 8 (data) × 8 (tensor)&lt;/td&gt;&lt;td&gt;– Higher throughput&lt;br&gt;– No pipeline bubbles&lt;/td&gt;&lt;td&gt;– Large batch size needed&lt;br&gt;– High memory per GPU&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Mistral 8*7B (64 GPUs)&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Strategy&lt;/th&gt;&lt;th&gt;GPU Allocation&lt;/th&gt;&lt;th&gt;Pros&lt;/th&gt;&lt;th&gt;Cons&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;EP8TP8&lt;/td&gt;&lt;td&gt;64 = 8 (experts) × 8 (tensor per expert)&lt;/td&gt;&lt;td&gt;– Balanced memory distribution&lt;br&gt;– Efficient memory use&lt;/td&gt;&lt;td&gt;– Complex load balancing&lt;br&gt;– Potential compute underutilization&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;EP8TP4DP2&lt;/td&gt;&lt;td&gt;64 = 8 (experts) × 4 (tensor) × 2 (data)&lt;/td&gt;&lt;td&gt;– Higher throughput&lt;br&gt;– Balanced utilization&lt;/td&gt;&lt;td&gt;– Needs careful load balancing&lt;br&gt;– Large batch sizes required&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;EP8PP2TP4&lt;/td&gt;&lt;td&gt;64 = 8 (experts) × 2 (pipeline) × 4 (tensor)&lt;/td&gt;&lt;td&gt;– Supports deeper experts&lt;br&gt;– Flexible scaling&lt;/td&gt;&lt;td&gt;– Increased latency&lt;br&gt;– Complex synchronization&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;how-to-choose-one&#34;&gt;How to choose one?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Now we have covered 4 methods of Inference parallelism, and then we have multiple combinations of these methods, so a common question you might be having is how do you choose or identify which method to use. So, the choice of inference parallelism methods broadly depends on the following factors:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Model architecture&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Use case requirements (Latency vs Throughput)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Hardware configuration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;model-architecture&#34;&gt;Model architecture&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Model architecture plays a crucial role in determining the most effective inference parallelism strategy. You need to identify your model architecture and then choose the parallelism method or combination of them that fits well. Different model structures fit to different parallelization techniques:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Large, deep models (e.g., GPT-4, PaLM 2):&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Benefit from pipeline parallelism&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Can be split into stages across multiple GPUs&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Good for models with many sequential layers&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Wide models with large layer sizes (LLaMA 2, BLOOM):&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Ideal for tensor parallelism&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Allow splitting individual layers across GPUs&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Effective for models with very large matrix operations&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Ensemble models or Mixture of Experts (Mixtral 8x7B, Switch Transformers):&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Well-suited for expert parallelism&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Different experts can be distributed across GPUs&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Useful for models that use different sub-networks for various tasks&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Models with small computational graphs (GPT-3.5-turbo, Falcon-7B):&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Often works well with simple data parallelism&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Can replicate the entire model across GPUs&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Effective when the model fits entirely in GPU memory&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Attention-based models (e.g., Transformers):&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;Can benefit from attention slicing or multi-head parallelism&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Allow distributing attention computations across GPUs&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;use-case-requirements-latency-vs-throughput-tradeoff&#34;&gt;Use case requirements (Latency vs Throughput tradeoff)&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;You need to be familiar with your business requirements or use case, i.e., what matters to business more, the latency of the user requests, or utilization of the GPU. Or can you identify the mode of your application i.e., is it a real-time application where response time to the user request is the main driver for the user experience, or is it an offline system where response time to the user request is not the primary concern&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The reason we need to be aware of this is the tradeoff between latency and GPU throughput. If you want to reduce the latency to the user request you need to allocate more GPU resources to each request and your choice of parallelism method will rely on that. You can do optimal batching so that requests are not struggling to acquire the GPU resource which increases the overall latency.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, if latency is not the consideration, then your goal should be to achieve maximum throughput on the GPU by choosing the right parallelism method and appropriate batch sizes that utilize GPU resources to its maximum throughput.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Use Case&lt;/th&gt;&lt;th&gt;Preferred Parallelism&lt;/th&gt;&lt;th&gt;Latency vs Throughput Consideration&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Real-time chatbots&lt;/td&gt;&lt;td&gt;Data parallelism or Tensor parallelism&lt;/td&gt;&lt;td&gt;Low latency priority; moderate throughput&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Batch text processing&lt;/td&gt;&lt;td&gt;Pipeline parallelism&lt;/td&gt;&lt;td&gt;High throughput priority; latency less critical&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Content recommendation&lt;/td&gt;&lt;td&gt;Expert parallelism&lt;/td&gt;&lt;td&gt;High throughput for diverse inputs; moderate latency&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Sentiment analysis&lt;/td&gt;&lt;td&gt;Data parallelism&lt;/td&gt;&lt;td&gt;High throughput; latency less critical for bulk processing&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Voice assistants&lt;/td&gt;&lt;td&gt;Tensor parallelism or Pipeline parallelism&lt;/td&gt;&lt;td&gt;Very low latency priority; moderate throughput&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;hardware-configuration&#34;&gt;Hardware configuration&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Hardware configuration often dictates the feasible parallelism strategies, and the choice should be optimized for the specific inference workload and model architecture. Following are some hardware component choices that impact the overall parallelism choices.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Hardware Component&lt;/th&gt;&lt;th&gt;Impact on Parallelism Choice&lt;/th&gt;&lt;th&gt;Examples&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;GPU Memory Capacity&lt;/td&gt;&lt;td&gt;Determines feasibility of data parallelism and influences the degree of model sharding&lt;/td&gt;&lt;td&gt;NVIDIA A100 (80GB) allows larger model chunks than A100 (40GB)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Number of GPUs&lt;/td&gt;&lt;td&gt;Affects the degree of parallelism possible across all strategies&lt;/td&gt;&lt;td&gt;8 GPUs enable more parallelism than 4 GPUs&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GPU Interconnect Bandwidth&lt;/td&gt;&lt;td&gt;Influences efficiency of tensor and pipeline parallelism&lt;/td&gt;&lt;td&gt;NVLink offers higher bandwidth than PCIe, benefiting tensor parallelism&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CPU Capabilities&lt;/td&gt;&lt;td&gt;Impacts data preprocessing and postprocessing in parallelism strategies&lt;/td&gt;&lt;td&gt;High-core count CPUs can better handle data parallelism overhead&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;System Memory&lt;/td&gt;&lt;td&gt;Affects the ability to hold large datasets for data parallelism&lt;/td&gt;&lt;td&gt;1TB system RAM allows larger batch sizes than 256GB&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Storage Speed&lt;/td&gt;&lt;td&gt;Influences data loading speeds in data parallelism&lt;/td&gt;&lt;td&gt;NVMe SSDs provide faster data loading than SATA SSDs&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Network Bandwidth&lt;/td&gt;&lt;td&gt;Critical for distributed inference across multiple nodes&lt;/td&gt;&lt;td&gt;Networks based on Infiniband, RoCE are faster than conventional networks for GPU fabrics&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Specialized Hardware&lt;/td&gt;&lt;td&gt;Enables specific optimizations&lt;/td&gt;&lt;td&gt;Google TPUs are optimized for tensor operations&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;AI inference parallelism is a game-changer for running big AI models efficiently. We’ve looked at different ways to split up the work, like data parallelism, tensor parallelism, pipeline parallelism, and expert parallelism. Each method has its own pros and cons, and choosing the right one depends on your specific needs and setup. It’s exciting to see tools like&amp;nbsp;&lt;a href=&#34;https://nvidia.github.io/TensorRT-LLM/advanced/expert-parallelism.html#how-to-enable&#34; rel=&#34;noreferrer noopener&#34; target=&#34;_blank&#34;&gt;TensorRT-LLM&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://docs.vllm.ai/en/latest/serving/distributed_serving.html&#34; rel=&#34;noreferrer noopener&#34; target=&#34;_blank&#34;&gt;vLLM&lt;/a&gt;, and Hugging Face’s&amp;nbsp;&lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many#tensor-parallelism&#34; rel=&#34;noreferrer noopener&#34; target=&#34;_blank&#34;&gt;Text Generation Inference&lt;/a&gt;&amp;nbsp;making these advanced techniques easier for more people to use. As AI models keep getting bigger and more complex, knowing how to use these parallelism techniques will be super important. They’re not just about handling bigger models – they’re about running AI smarter and more efficiently. By using these methods well, we can do amazing things with AI, making it faster, cheaper, and more powerful for all kinds of uses.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The future of AI isn’t just about bigger models; it’s about finding clever ways to use them in the real world. With these parallelism techniques, we’re opening doors to AI applications that were once thought impossible. If you’re looking for experts who can help you scale or build your AI infrastructure, reach out to our&amp;nbsp;&lt;a href=&#34;https://www.infracloud.io/build-ai-cloud/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;AI &amp;amp; GPU Cloud experts&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you found this post valuable and informative, subscribe to our weekly newsletter for more posts like this. I’d love to hear your thoughts on this post, so do start a conversation on&amp;nbsp;&lt;a href=&#34;https://www.linkedin.com/in/aman-juneja-291588b9/&#34; rel=&#34;noreferrer noopener&#34; target=&#34;_blank&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;references&#34;&gt;References&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/#:~:text=NVIDIA%20Blackwell%3A%20A%20new%20platform,as%20GPT%201.8T%20MoE.&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Demystifying AI Inference Deployments for Trillion Parameter Large Language Models&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Tensor Parallelism Overview&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://xzt102.github.io/publications/2021_WWW.pdf&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Parallelizing DNN Training on GPUs: Challenges and Opportunities&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.datasciencecentral.com/why-the-newest-llms-use-a-moe-mixture-of-experts-architecture/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Why the newest LLMs use a MoE (Mixture of Experts) architecture&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;成员帖子最初由 InfraCloud 首席解决方案工程师 Aman Juneja 发布在 &lt;a href=&#34;https://www.infracloud.io/blogs/inference-parallelism/&#34;&gt;InfraCloud 博客&lt;/a&gt;上技术&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;近年来，我们见证了两个反复出现的趋势：日益强大的 GPU 的发布以及具有数十亿或数万亿参数和扩展上下文窗口的大型语言模型 (LLM) 的引入。许多企业正在通过微调这些 LLM 或使用 &lt;a href=&#34;https://www.infracloud.io/blogs/retrieval-augmented- Generation-using-data-with-llms/&#34; target= 构建应用程序来利用这些 LLM。 &#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;使用 RAG 并将其部署在专用 GPU 服务器上的特定领域知识。现在，当谈到在 GPU 上部署这些模型时，需要注意的一件事是模型大小，即将模型加载到 GPU 内存中所需的空间（用于存储参数和上下文标记）与内存相比太高可在 GPU 上使用。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;model-size-vs-gpu-memory&#34;&gt;模型大小与 GPU 内存&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;有一些方法可以通过使用优化技术来减小模型大小，例如 &lt;a href=&#34;https://www.infracloud.io/blogs/exploring-ai-model-inference/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;量化、剪枝、蒸馏和压缩等。&lt;/a&gt; 但是，如果您在下面的比较表中注意到 70B 模型（FP16 量化）的最新 GPU 内存和空间需求之间的关系，那么它几乎是不可能一次处理多个请求，或者在某些 GPU 中，模型甚至无法容纳在内存中。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;GPU&lt;/th&gt;&lt;th&gt;FP16 (TFLOPS)&lt;br&gt;稀疏&lt;/th&gt;&lt;th&gt;GPU内存&lt;br&gt;(GB)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;B200&lt;/td&gt;&lt;td&gt;4500&lt;/td&gt;&lt;td&gt;192&lt;/td&gt;&lt; /tr&gt;&lt;tr&gt;&lt;td&gt;B100&lt;/td&gt;&lt;td&gt; 3500&lt;/td&gt;&lt;td&gt;192&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;H200&lt;/td&gt;&lt;td&gt;1979&lt;/td&gt;&lt;td&gt;141&lt;/td&gt;&lt;/tr&gt;&lt;tr &gt;&lt;td&gt;H100&lt;/td&gt;&lt;td&gt;1979&lt;/td&gt;&lt;td&gt; 80&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;L4&lt;/td&gt;&lt;td&gt;242&lt;/td&gt;&lt;td&gt;24&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;L40S&lt;/td &gt;&lt;td&gt;733&lt;/td&gt;&lt;td&gt;48&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td &gt;L40&lt;/td&gt;&lt;td&gt;362&lt;/td&gt;&lt;td&gt;48&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;A100&lt;/td&gt;&lt;td&gt;624&lt;/td&gt;&lt;td&gt;80&lt; /td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;这都是已经应用的 FP16 量化导致的一些精度损失（这在许多通用用例中通常是可以接受的）。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;模型&lt;/th&gt;&lt;th&gt;参数的 KV 缓存（以 GB 为单位） (FP16)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;llama3-8B&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;llama3 -70B&lt;/td&gt;&lt;td&gt;140&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;llama-2-1 3B&lt;/td&gt;&lt;td&gt;26&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;llama2-70B&lt;/td&gt;&lt;td&gt;140&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;米斯特拉尔- 7B&lt;/td&gt;&lt;td&gt;14&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;这让我们了解了这篇博文的背景，即企业如何在这些现代数据中心 GPU 上运行数十亿或万亿参数的 LLM 模型。有没有什么方法可以将这些模型分成更小的部分并仅运行目前需要，或者我们可以将模型的各个部分分配到不同的 GPU 中吗？我将尝试在这篇博文中使用当前可用于执行推理并行化的方法集来回答这些问题，并将尝试重点介绍一些支持这些并行化方法的工具/库。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;inference-parallelism&#34;&gt;推理并行性&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;推理并行旨在跨多个处理单元（例如 GPU）分配 AI 模型（尤其是深度学习模型）的计算工作负载。这种分布可以实现更快的处理、减少延迟，并能够处理超出单个设备内存容量的模型。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;已经开发了四种主要方法来实现推理并行性，每种方法都有其优点和应用：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;数据并行&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;张量并行&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;管道并行性&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;专家并行&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;data-parallelism&#34;&gt;数据并行&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在数据并行方面，我们在不同的 GPU 或 GPU 集群上部署模型的多个副本。模型的每个副本独立处理用户请求。打个简单的比方，这就像拥有 1 个微服务的多个副本。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;现在，人们可能会遇到的一个常见问题是它如何解决模型大小适合 GPU 内存的问题，我们在一开始就讨论过这个问题，简短的回答是它没有。此方法仅建议用于可容纳 GPU 内存的较小模型。在这些情况下，我们可以使用部署在不同 GPU 实例上的模型的多个副本，并将请求分发到不同的实例，从而为每个请求提供足够的 GPU 资源，并提高服务的可用性。这也将增加系统的整体请求吞吐量，因为您现在有更多实例来处理流量。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://www.infracloud.io/assets/img/Blog/inference-parallelism/data-parallelism.webp&#34; alt= “数据并行性”referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;a href=&#34;https://xzt102.github.io/publications/2021_WWW.pdf&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;(ImageSource)&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;tensor-parallelism&#34;&gt;张量并行&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在张量并行中，我们将模型的每一层拆分到不同的 GPU 上。单个用户请求将在多个 GPU 之间共享，每个请求的 GPU 计算结果将通过 &lt;a href=&#34;https://www.infracloud.io/blogs/introduction-to-nvidia-network-operator/ 重新组合&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;GPU 到 GPU 网络&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;为了更好地理解它，顾名思义，我们将张量沿特定维度分成块，这样每个设备仅保存 1/N 块张量。使用该部分块执行计算以获得部分输出。这些部分输出是从所有设备收集然后组合的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://www.infracloud.io/assets/img/Blog/inference-parallelism/tensore-parallelism.webp&#34; alt= “张量并行度”referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;a href=&#34;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;（图片来源）&lt;/a &gt;&lt;/图标题&gt;&lt;/图&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;您可能已经注意到，张量并行性能的瓶颈是 GPU 到 GPU 之间的网络速度。由于每个请求都将在不同的 GPU 上计算然后组合，因此我们需要高性能网络来确保低延迟。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://www.infracloud.io/assets/img/Blog/inference-parallelism/model-parallelism.webp&#34; alt= “模型并行性”referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;a href=&#34;https://xzt102.github.io/publications/2021_WWW.pdf&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;（图片来源）&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;pipeline-parallelism&#34;&gt;管道并行&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在管道并行性中，我们将一组模型层分布在不同的 GPU 上。基于层的分区是管道并行的基本方法。该模型的层被分组为连续的块，形成阶段。这种划分通常是通过网络架构垂直完成的。计算平衡是一个关键考虑因素。理想情况下，每个阶段应具有大致相等的计算负载，以防止出现瓶颈。这通常涉及对不同复杂程度的层进行分组以实现平衡。内存使用优化是另一个关键因素。阶段的设计旨在适应各个设备的内存限制，同时最大限度地提高利用率。通信开销最小化也很重要。分区的目的是减少阶段之间传输的数据量，因为设备间通信可能是一个重要的性能瓶颈。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;例如，如果您要在 4 GPU 实例上部署具有 32 层的 LLaMA3-8B 模型，则可以在每个 GPU 上拆分和分布 8 层模型。请求的处理以顺序方式进行，计算从一个 GPU 开始，并通过点对点通信继续到下一个 GPU。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;同样，由于涉及多个 GPU 实例，如果 GPU 之间没有高速网络通信，网络可能会成为一个巨大的瓶颈。这种并行性可以提高 GPU 吞吐量，因为每个请求将需要每个 GPU 更少的资源并且应该很容易获得，但是随着请求的进行，它最终会增加整体延迟将按顺序处理，任何 GPU 计算或网络组件的延迟都会导致整体延迟激增。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://www.infracloud.io/assets/img/Blog/inference-parallelism/model-parallelism-layer-wise。 webp&#34; alt=&#34;模型并行层明智&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;a href=&#34;https://xzt102.github.io/publications/2021_WWW.pdf&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;（图片来源）&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;expert-parallelism&#34;&gt;专家并行&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;专家并行通常作为专家混合 (MoE) 实现，是一种允许在推理过程中高效使用大型模型的技术。它并没有解决将模型拟合到 GPU 内存中的问题，而是提供了一种选项，可以让广泛的功能模型根据请求上下文来服务请求。在该技术中，模型被划分为多个专家子网络。每个专家通常都是一个经过训练的神经网络，可以处理更广泛问题领域内的特定类型的输入或子任务。门控网络决定每个输入使用哪个专家。对于任何给定的输入，仅激活一部分专家。不同的专家可以分布在不同的 GPU 上。路由器/门控网络和主动专家可以并行操作。不活跃的专家不消耗计算资源。这大大减少了每个请求必须交互的参数数量，因为跳过了一些专家。但与张量和管道并行性一样，整体请求延迟在很大程度上依赖于 GPU 到 GPU 的通信网络。经过专家处理后，请求必须重新构造回其原始 GPU，从而通过 GPU 到 GPU 互连结构生成高网络通信。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;与张量并行相比，这种方法可以更好地利用硬件，因为您不必将操作分割成更小的块。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://www.infracloud.io/assets/img/Blog/inference-parallelism/moe-llm.webp&#34; alt= “教育部法学硕士”referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;a href=&#34;https://www.datasciencecentral.com/why-the-newest-llms-use-a-moe-mixture-of-experts-architecture/?ref=dailydev&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener &#34;&gt;（图片来源）&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;以下是我们讨论的方法的总结和比较。您可以在计划为您的用例选择一个时将其用作参考。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;方面&lt;/th&gt;&lt;th&gt;数据并行&lt;/th&gt;&lt;th&gt;张量并行&lt;/th&gt;&lt;th&gt;管道并行&lt;/th&gt;&lt;th&gt;专家并行&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;基本概念&lt;/td&gt;&lt;td&gt;跨多个设备分割输入数据&lt;/td&gt;&lt;td&gt;跨设备分割单个张量/层&lt;/td&gt;&lt;td&gt;分割模型跨设备划分为连续阶段&lt;/td&gt;&lt;td&gt;将模型拆分为多个专家子网络&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;工作原理&lt;/td&gt;&lt;td&gt;相同的模型被复制每个设备，处理不同的数据块&lt;/td&gt;&lt;td&gt;分布在多个设备上的单层/操作&lt;/td&gt;&lt;td&gt;不同设备上模型管道的不同部分&lt;/td&gt;&lt;td&gt;路由器选择特定的专家为了每个输入&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;并行化单元&lt;/td&gt;&lt;td&gt;批量输入&lt;/td&gt;&lt;td&gt;单个张量/层&lt;/td&gt;&lt;td&gt;模型阶段&lt;/td&gt;&lt;td&gt; td&gt;&lt;td&gt;专家（子网络）&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;可扩展性&lt;/td&gt;&lt;td&gt;随批量大小良好扩展&lt;/td&gt;&lt;td&gt;扩展对于非常大的模型来说很好&lt;/td&gt;&lt;td&gt;对于深度模型来说可以很好地扩展&lt;/td&gt;&lt;td&gt;对于宽模型来说可以很好地扩展&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;内存效率&lt;/td&gt;&lt; td&gt;低（每个设备上的完整模型）&lt;/td&gt;&lt;td&gt;高（每个设备上仅每个层的部分）&lt;/td&gt;&lt;td&gt;高（每个设备上仅每个层的部分模型）设备）&lt;/td&gt;&lt;td&gt;中到高（跨设备分布的专家）&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;通信开销&lt;/td&gt;&lt;td&gt;低&lt;/td&gt;&lt;td&gt;中至高&lt;/td&gt;&lt;td&gt;低（仅在相邻阶段之间）&lt;/td&gt;&lt;td&gt;中（路由器通信和专家选择）&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;负载平衡&lt;/td&gt;&lt;td&gt;如果数据均匀分布，则通常是平衡的&lt;/td&gt;&lt;td&gt;操作内平衡&lt;/td&gt;&lt;td&gt;可能具有挑战性，需要仔细的阶段设计&lt;/td&gt;&lt;td&gt;可以具有挑战性，并且需要有效的路由&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;延迟&lt;/td&gt;&lt;td&gt;大批量时较低&lt;/td&gt;&lt;td&gt;小批量时可以增加批次&lt;/td&gt;&lt;td&gt;由于管道深度而较高&lt;/td&gt;&lt;td&gt;如果路由高效，则可能较低&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;吞吐量&lt;/td&gt;&lt;td&gt;高对于大批量&lt;/td&gt;&lt;td&gt;对于大型模型可以很高&lt;/td&gt;&lt;td&gt;高，特别是对于深度模型&lt;/td&gt;&lt;td&gt;对于多样化可以非常高输入&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;典型用例&lt;/td&gt;&lt;td&gt;大批量推理，令人尴尬的并行任务&lt;/td&gt;&lt;td&gt;非常大的模型，不适合单个设备&lt;/td&gt;&lt;td&gt;具有顺序依赖关系的深度模型&lt;/td&gt;&lt;td&gt;具有不同子任务或专业&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;挑战&lt;/td&gt;&lt;td&gt;受批量大小限制，内存使用率高&lt;/td&gt;&lt;td&gt;实现复杂，潜在的通信瓶颈&lt;/td&gt;&lt;td &gt;管道气泡、最优阶段划分困难&lt;/td&gt;&lt;td&gt;负载均衡、路由开销、训练不稳定&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;对输入的适应性大小&lt;/td&gt;&lt;td&gt;适应性强&lt;/td&gt;&lt;td&gt;适应性较差，固定张量划分&lt;/td&gt;&lt;td&gt;适应性较差，固定管道&lt;/td&gt;&lt;td&gt;适应性强，不同的专家针对不同的输入&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;合适的模型类型&lt;/td&gt;&lt;td&gt;大多数模型类型&lt;/td&gt;&lt;td&gt;基于 Transformer 的模型，非常大的神经网络网络&lt;/td&gt;&lt;td&gt;深度序列模型&lt;/td&gt;&lt;td&gt;多任务模型、具有多种知识的语言模型&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;支持的推理后端&lt;/td&gt;&lt; td&gt;TensonRT-LLM，vLLM，TGI&lt;/td&gt;&lt;td&gt;TensonRT-LLM，vLLM，TGI&lt;/td&gt;&lt;td&gt;TensonRT-LLM， vLLM，TGI&lt;/td&gt;&lt;td&gt;TensonRT-LLM，vLLM，TGI&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;parallelism-techniques-combined&#34;&gt;并行技术组合&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;现在，您可能已经在想，如果使用上述并行方法意味着我们正在减少 GPU 的总体消耗或利用率，那么我们是否可以不组合或复制这些方法来提高 GPU 的总体吞吐量？结合推理并行方法可以带来更高效和可扩展的系统，特别是对于大型和复杂的模型。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在下表中，您可以看到 4 个可能的选项，但在实际场景中，根据您拥有的 GPU 数量，此组合可能会增长到非常大的数量。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;数据并行性 (DP)&lt;br&gt;+&lt;br&gt;管道并行性 (PP) &lt;/th&gt;&lt;th&gt;张量并行性 (TP)&lt;br&gt;+&lt;br&gt;管道并行性 (PP)&lt;/th&gt;&lt;th&gt;专家并行性 (EP)&lt;br&gt;+&lt;br&gt;数据并行性 (DP)&lt;/th&gt;&lt;th&gt;张量并行性 (TP)&lt;br&gt;+&lt;br&gt;专家并行性 (EP)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;分割将模型划分为阶段（管道并行）&lt;/td&gt;&lt;td&gt;将模型划分为阶段（管道并行）&lt;/td&gt;&lt;td&gt;跨设备分配专家（专家并行）&lt;/td&gt;&lt;td&gt;跨设备拆分大型专家模型（张量并行）&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;跨多个设备复制每个阶段（数据并行）&lt;/td&gt;&lt;td&gt;跨设备拆分每个阶段中的大张量（张量并行性）&lt;/td&gt;&lt;td&gt;并行处理多个输入（数据并行性）&lt;/td&gt;&lt;td&gt;跨设备拆分大型专家模型（张量并行性）&lt;/td&gt;&lt;td&gt;跨设备拆分大型专家模型（张量并行性）并行性）&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;例如，假设您有 64 个可用 GPU，并且您计划在其上部署 llama3-8b 或 Mistral 8*7B 模型。以下是并行方法的一些可能的组合。这些只是理解并行组合策略的示例，对于实际用例，您还需要考虑和基准测试其他选项。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;LLaMA 3-8B（64 个 GPU）&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;策略&lt;/th&gt;&lt;th&gt;GPU分配&lt;/th&gt;&lt;th&gt;优点&lt;/th&gt;&lt;th&gt;缺点&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;PP8TP8&lt;/td&gt;&lt;td&gt;64 = 8（管道）× 8 （张量）&lt;/td&gt;&lt;td&gt;– 平衡分布&lt;br&gt;– 减少通信&lt;/td&gt;&lt;td&gt;– 管道气泡&lt;br&gt;– 延迟增加&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PP4TP4DP4 &lt;/td&gt;&lt;td&gt;64 = 4（管道）× 4（张量）× 4（数据）&lt;/td&gt;&lt;td&gt; – 更高的吞吐量&lt;br&gt; – 批量灵活大小&lt;/td&gt;&lt;td&gt; – 复杂集成&lt;br&gt; – 需要大批量&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DP8TP8&lt;/td&gt;&lt;td&gt;64 = 8（数据）× 8（张量)&lt;/td&gt;&lt;td&gt;– 更高的吞吐量&lt;br&gt;– 无管道气泡&lt;/td&gt;&lt;td&gt;– 需要大批量&lt;br&gt;– 每块内存较高GPU&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Mistral 8*7B（64 个 GPU）&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;策略&lt;/th&gt;&lt;th&gt;GPU分配&lt;/th&gt;&lt;th&gt;优点&lt;/th&gt;&lt;th&gt;缺点&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;EP8TP8&lt;/td&gt;&lt;td&gt;64 = 8（专家）× 8（每张量专家）&lt;/td&gt;&lt;td&gt;– 平衡ed 内存分配&lt;br&gt;– 高效的内存使用&lt;/td&gt;&lt;td&gt;– 复杂的负载平衡&lt;br&gt;– 潜在的计算利用率不足&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;EP8TP4DP2&lt;/td&gt;&lt;td&gt; 64 = 8（专家）× 4（张量）× 2（数据）&lt;/td&gt;&lt;td&gt;– 更高的吞吐量&lt;br&gt;– 平衡利用率&lt;/td&gt;&lt;td&gt;– 需要小心负载均衡&lt;br&gt;-需要大批量&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;EP8PP2TP4&lt;/td&gt;&lt;td&gt;64 = 8（专家）×2（管道）×4（张量）&lt;/ td&gt;&lt;td&gt;– 支持更深层次的专家&lt;br&gt;– 灵活扩展&lt;/td&gt;&lt;td&gt;– 增加延迟&lt;br&gt;– 复杂同步&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure &gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;how-to-choose-one&#34;&gt;如何选择一个？&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;现在我们已经介绍了 4 种推理并行方法，然后我们有这些方法的多种组合，因此您可能遇到的一个常见问题是如何选择或确定要使用哪种方法。因此，推理并行方法的选择大致取决于以下因素：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;模型架构&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;用例要求（延迟与吞吐量）&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;硬件配置&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;model-architecture&#34;&gt;模型架构&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;模型架构在确定最有效的推理并行策略方面起着至关重要的作用。您需要确定您的模型架构，然后选择适合的并行方法或它们的组合。不同的模型结构适合不同的并行化技术：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol类=“wp-block-list”&gt;&#xA;&lt;li&gt;大型、深度模型（例如 GPT-4、PaLM 2）：&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;从管道并行性中获益&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;可以跨多个 GPU 分为多个阶段&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;适合具有多个连续层的模型&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;具有大层尺寸的宽模型（LLaMA 2、BLOOM）：&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;张量并行的理想选择&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;允许跨 GPU 拆分各个层&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;对于具有非常大矩阵运算的模型有效&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;集成模型或专家组合（Mixtral 8x7B、开关变压器）：&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;非常适合专家并行&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;不同的专家可以分布在 GPU 上&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;对于使用不同子网络执行各种任务的模型非常有用&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;具有小型计算图的模型（GPT-3.5-turbo、Falcon-7B）：&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;通常适用于简单的数据并行性&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;可以跨 GPU 复制整个模型&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;当模型完全适合 GPU 内存时有效&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;基于注意力的模型（例如 Transformer）：&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;可以从注意力切片或多头并行性中受益&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;允许跨 GPU 分配注意力计算&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;use-case-requirements-latency-vs-throughput-tradeoff&#34;&gt;用例要求（延迟与阈值）吞吐量权衡）&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;您需要熟悉您的业务需求或用例，即什么对业务更重要、用户请求的延迟或 GPU 的利用率。或者您能否确定应用程序的模式，即，它是实时应用程序，其中对用户请求的响应时间是用户体验的主要驱动因素，还是离线系统，其中对用户请求的响应时间不是用户体验的主要驱动因素？主要关心的问题&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们需要意识到这一点的原因是延迟和 GPU 吞吐量之间的权衡。如果您想减少用户请求的延迟，您需要为每个请求分配更多的 GPU 资源，并且您选择的并行方法将取决于此。您可以进行最佳批处理，这样请求就不会努力获取 GPU 资源，从而增加整体延迟。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;但是，如果不考虑延迟，那么您的目标应该是通过选择正确的并行方法和适当的批处理大小来实现 GPU 上的最大吞吐量，从而利用 GPU 资源达到最大吞吐量。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;用例&lt;/th&gt;&lt;th&gt;首选并行度&lt;/th&gt;&lt;th &gt;延迟与吞吐量考虑&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;实时聊天机器人&lt;/td&gt;&lt;td&gt;数据并行性或张量并行性&lt;/td&gt;&lt;td&gt;低延迟优先；中等吞吐量&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;批量文本处理&lt;/td&gt;&lt;td&gt;管道并行&lt;/td&gt;&lt;td&gt;高吞吐量优先；延迟不太重要&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;内容推荐&lt;/td&gt;&lt;td&gt;专家并行性&lt;/td&gt;&lt;td&gt;不同输入的高吞吐量；中等延迟&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;情感分析&lt;/td&gt;&lt;td&gt;数据并行性&lt;/td&gt;&lt;td&gt;高吞吐量；延迟对于批量处理不太重要&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;语音助手&lt;/td&gt;&lt;td&gt;张量并行或管道并行&lt;/td&gt;&lt;td&gt;非常低的延迟优先级；中等吞吐量&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;hardware-configuration&#34;&gt;硬件配置&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;硬件配置通常决定了可行的并行策略，并且应针对特定的推理工作负载和模型架构来优化选择。以下是一些影响整体并行性选择的硬件组件选择。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;has-fixed-layout&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;硬件组件&lt;/th&gt;&lt;th&gt;对并行性选择的影响&lt;/th&gt; &lt;th&gt;示例&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;GPU显存容量&lt;/td&gt;&lt;td&gt;决定数据并行的可行性并影响模型的程度分片&lt;/td&gt;&lt;td&gt;NVIDIA A100 (80GB) 允许比 A100 (40GB) 更大的模型块&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GPU 数量&lt;/td&gt;&lt;td&gt;影响分片程度所有策略均可实现并行性&lt;/td&gt;&lt;td&gt;8 个 GPU 比 4 个 GPU 实现更多并行性&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GPU 互连带宽&lt;/td&gt;&lt;td&gt;影响张量和管道并行性的效率&lt;/td&gt;&lt;td&gt;NVLink提供更高的禁令宽度高于 PCIe，有利于张量并行&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CPU 能力&lt;/td&gt;&lt;td&gt;影响并行策略中的数据预处理和后处理&lt;/td&gt;&lt;td&gt;高核数 CPU可以更好地处理数据并行开销&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;系统内存&lt;/td&gt;&lt;td&gt;影响保存大型数据集的能力并行性&lt;/td&gt;&lt;td&gt;1TB系统RAM允许大于256GB的批量大小&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;存储速度&lt;/td&gt;&lt;td&gt;影响数据并行性中的数据加载速度&lt;/td &gt;&lt;td&gt;NVMe SSD 提供比 SATA SSD 更快的数据加载&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;网络带宽&lt;/td&gt;&lt;td&gt;对于跨多个设备的分布式推理至关重要节点&lt;/td&gt;&lt;td&gt;基于 Infiniband、RoCE 的网络比 GPU 结构的传统网络更快&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;专用硬件&lt;/td&gt;&lt;td&gt;实现特定优化&lt;/td&gt;&lt;td&gt; td&gt;&lt;td&gt;Google TPU 针对张量运算进行了优化&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;结论&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;人工智能推理并行性是高效运行大型人工智能模型的游戏规则改变者。我们研究了划分工作的不同方法，例如数据并行、张量并行、管道并行和专家并行。每种方法都有其优点和缺点，选择正确的方法取决于您的具体需求和设置。看到像 &lt;a href=&#34;https://nvidia.github.io/TensorRT-LLM/advanced/expert-parallelism.html#how-to-enable&#34; rel=&#34;noreferrer noopener&#34; target=&#34;_blank&#34; 这样的工具真是令人兴奋&gt;TensorRT-LLM&lt;/a&gt;，&lt;a href=&#34;https://docs.vllm.ai/en/latest/serving/distributed_serving.html&#34; rel=&#34;noreferrer noopener&#34; target=&#34;_blank&#34;&gt;vLLM&lt;/a&gt; 和 Hugging Face 的 &lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many#tensor-parallelism&#34; rel= &#34;noreferrer noopener&#34; target=&#34;_blank&#34;&gt;文本生成推理&lt;/a&gt;让这些先进技术更容易被更多人使用。随着人工智能模型变得越来越大、越来越复杂，了解如何使用这些并行技术将变得非常重要。它们不仅仅是处理更大的模型，而是更智能、更高效地运行人工智能。通过充分利用这些方法，我们可以利用人工智能做出惊人的事情，使其更快、更便宜、更强大，适合各种用途。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;人工智能的未来不仅仅在于更大的模型；还在于更大的模型。这是关于找到在现实世界中使用它们的聪明方法。借助这些并行技术，我们为曾经被认为不可能的人工智能应用打开了大门。如果您正在寻找可以帮助您扩展或构建 AI 基础设施的专家，请联系我们的 &lt;a href=&#34;https://www.infracloud.io/build-ai-cloud/&#34; target=&#34;_blank&#34; rel =&#34;noreferrer noopener&#34;&gt;AI 和 GPU 云专家&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果您发现这篇文章有价值且信息丰富，请订阅我们的每周时事通讯以获取更多此类帖子。我很想听听您对这篇文章的看法，因此请在 &lt;a href=&#34;https://www.linkedin.com/in/aman-juneja-29158 上开始对话8b9/&#34; rel=&#34;noreferrer noopener&#34; target=&#34;_blank&#34;&gt;LinkedIn&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;references&#34;&gt;参考&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul class=&#34;wp-block-list&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/#:~:text=NVIDIA%20Blackwell% 3A%20A%20new%20平台，如%20GPT%201.8T%20MoE。” target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;揭秘万亿参数大型语言模型的人工智能推理部署&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;张量并行概述&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://xzt102.github.io/publications/2021_WWW.pdf&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;GPU 上的并行 DNN 训练：挑战和机遇&lt;/a&gt;&lt; /里&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.datasciencecentral.com/why-the-newest-llms-use-a-moe-mixture-of-experts-architecture/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;为什么最新的法学硕士使用 MoE（专家混合）架构&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Tue, 17 Dec 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Navigating Platform Engineering pitfalls with WebAssembly components】使用 WebAssembly 组件克服平台工程陷阱</title>
      <link>https://www.cncf.io/blog/2024/12/23/navigating-platform-engineering-pitfalls-with-webassembly-components/</link>
      <description>【&lt;p&gt;&lt;em&gt;Ambassador post by &lt;a href=&#34;https://www.linkedin.com/in/hectaman&#34;&gt;Liam Randall&lt;/a&gt;, CNCF Ambassador and CEO, Cosmonic&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We recently had the opportunity to &lt;a href=&#34;https://wasmcloud.com/blog/2024-09-24-wasmcloud-innovation-day-2024-in-review&#34;&gt;reflect&lt;/a&gt; on the state of platform engineering within large companies—and the role WebAssembly has to play in the discipline’s future. In this post, we’ll take a look at the cost and complexity challenges faced by teams building and running applications at scale. Then, we’ll consider the role of WebAssembly—and platforms like incubating Cloud Native Foundation (CNCF) wasmCloud—in overcoming these obstacles.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;The pitfalls of modern platform engineering&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Platform engineering as a practice really began during the “container era” of compute. We split archaic systems into manageable microservices, packaged those microservices into containers, then orchestrated those containers with Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This brought infrastructure and platform teams closer together. Infrastructure teams became part of the same team that provisioned common application services for developers to build with—everything from Secrets management to authentication and authorization. Without a doubt, containers have improved engineering for the better, but they’ve also brought along some big problems.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;💡 For a more in-depth analysis of the challenges facing platform engineering teams Bailey Hayes’s &lt;a href=&#34;https://youtu.be/PexRsU8sVIs&#34;&gt;recent Innovation Day keynote&lt;/a&gt; is unmissable.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;2235&#34; height=&#34;914&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform.jpg&#34; alt=&#34;Fig 1. Eras of compute and the evolution of the platform.&#34; class=&#34;wp-image-122098&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform.jpg 2235w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-300x123.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-1024x419.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-768x314.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-900x368.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-1800x736.jpg 1800w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-489x200.jpg 489w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-978x400.jpg 978w&#34; sizes=&#34;auto, (max-width: 2235px) 100vw, 2235px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Fig 1. Eras of compute and the evolution of the platform.&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;The high price of application build-run-maintain&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Today, it is simply too expensive to build, run, and maintain applications. This starts with the choice of language, and the “golden template” we use for all our different enterprise applications. We are tied into specific clients and frameworks that are built for a specific language. All of this means we have to build and maintain applications on an app-by-app basis.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When platform engineers need to update a given platform, they need their users—application developers—to update dependencies for every application. Because most applications are composed largely of open source dependencies, they inherit the cost of maintaining these to prevent vulnerabilities. Engineers churn on a hamster wheel of application maintenance which is expensive and unproductive.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Containers can’t solve the hamster wheel. Moreover, containerization runs up against some significant limits in portability. We have to build differentiated containers for diverse architectures. Even at their leanest, Kubernetes and containers are too unwieldy to run on sensors, on IoT devices, and in many other edge environments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So how can we make applications that are truly portable? While we’re at it, can we &lt;strong&gt;solve the hamster wheel&lt;/strong&gt; and language silo problems?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The answer lies in portable, interoperable &lt;strong&gt;WebAssembly components&lt;/strong&gt;. We’re entering a new era of &lt;em&gt;componentization&lt;/em&gt; where our distributed units of compute are WebAssembly components—orchestrated by wasmCloud.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Wasm vs Containers&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It’s important to mention, wasmCloud loves Kubernetes—Wasm runs really well &lt;a href=&#34;https://wasmcloud.com/blog/how-to-run-webassembly-components-on-kubernetes-with-wasmcloud&#34;&gt;alongside containers&lt;/a&gt;. The &lt;a href=&#34;https://github.com/wasmCloud/wasmcloud-operator&#34;&gt;wasmcloud-operator&lt;/a&gt; and Helm charts make deploying Wasm on Kubernetes pretty simple.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;💡 For an example of a large organization bringing Wasm to its Kubernetes estate, &lt;a href=&#34;https://www.cncf.io/blog/2022/11/17/better-together-a-kubernetes-and-wasm-case-study/&#34;&gt;Adobe’s use case&lt;/a&gt; is a great starting point.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Having said that, Kubernetes is designed for &lt;em&gt;infrastructure&lt;/em&gt; orchestration, not application management. wasmCloud is designed for application orchestration at-scale, and it can handle that job either standalone or on a Kubernetes cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Think of WebAssembly as a tiny VM. A Wasm application is compiled with components into a highly efficient binary format which is smaller and more lightweight than traditional container images. This reduced size minimizes memory and storage requirements, allowing more workloads to fit into the same amount of space. Containers are much larger—ranging from MBs to GBs—where a componentized WebAssembly application ranges from &lt;strong&gt;KBs to MBs&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Wasm components themselves are sandboxed, stateless, and &lt;strong&gt;secure by-default&lt;/strong&gt; because they are incapable of interacting with an operating system on their own. With containers, there are a host of processes, best practices, and tools required to achieve the right level of security and isolation. Containers are an intrinsically safe way to sandbox and isolate arbitrary user code.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While containers enhance the portability of applications, those images are still bound to specific CPU architectures. With WebAssembly components, &lt;strong&gt;the .wasm binary is the same for any environment&lt;/strong&gt;, as long as that environment has a WebAssembly runtime. Wasm’s portability allows workloads to run across heterogeneous environments, such as cloud, edge, or even resource-constrained devices.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Container cold-starts take seconds, whereas &lt;strong&gt;WebAssembly components start in less than a millisecond&lt;/strong&gt;. The latency of a user’s request is typically about 200ms, which means containers must be running at all times, ready to service requests even when they’re not needed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;2158&#34; height=&#34;943&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly.jpg&#34; alt=&#34;Fig 2. Container cold start time compared to that of WebAssembly.&#34; class=&#34;wp-image-122099&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly.jpg 2158w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-300x131.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-1024x447.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-768x336.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-900x393.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-1800x787.jpg 1800w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-458x200.jpg 458w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-915x400.jpg 915w&#34; sizes=&#34;auto, (max-width: 2158px) 100vw, 2158px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Fig 2. Container cold start time compared to that of WebAssembly.&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We’ll soon see new types of architectures evolve that capitalize on the size and speed of components. Why make a network request to somewhere else when you run that compute locally, on a user’s device?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This all sounds great. But what if you’ve spent years putting Kubernetes at the center of your universe—if containers work well enough, does adopting Wasm equal another heavy lift into the unknown? To answer that, we need to explore how components work.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Components…everywhere!&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We often pack the whole world into containers, with Wasm components we get down to the application’s core logic. Packaged as &lt;a href=&#34;https://tag-runtime.cncf.io/wgs/wasm/deliverables/wasm-oci-artifact/&#34;&gt;OCI artifacts&lt;/a&gt;, components radically simplify the way that developers build applications. We compose applications with standard, &lt;strong&gt;reusable&lt;/strong&gt; building blocks (components) that only contain the logic we need and that are compiled at runtime.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;That’s huge. We can reuse our components—rather than rewriting them—and they only contain the code required. By eliminating boilerplate code (the stuff we import in libraries) vulnerabilities are completely eliminated. When thinking about the time spent identifying, reporting, and remediating vulnerabilities, this is transformational.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Components build on top of each other, &lt;strong&gt;just like Lego blocks&lt;/strong&gt;. They run anywhere components are supported and, of course, because they’re tiny, we can run componentized applications in resource-constrained environments at network edges. As we’ll see later manufacturing and IoT is emerging as a clear use case for wasmCloud.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;💡 The &lt;a href=&#34;https://wasmcloud.com/docs/concepts/components&#34;&gt;Components Starter Guide&lt;/a&gt; is a fantastic resource for those at the start of their journey with components.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1539&#34; height=&#34;812&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries.jpg&#34; alt=&#34;Fig 3. Components are language interoperable and API-driven design defines operational boundaries.&#34; class=&#34;wp-image-122100&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries.jpg 1539w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-300x158.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-1024x540.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-768x405.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-194x102.jpg 194w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-388x204.jpg 388w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-776x408.jpg 776w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-900x475.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-379x200.jpg 379w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-758x400.jpg 758w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-590x310.jpg 590w&#34; sizes=&#34;auto, (max-width: 1539px) 100vw, 1539px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Fig 3. Components are language interoperable and API-driven design defines operational boundaries.&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Most importantly, components are &lt;strong&gt;interoperable and polyglot&lt;/strong&gt;. First principles dictate that we use applications written in many different languages, all of which must be supported. Components communicate over a common set of shared, standardized interfaces—WASI—bound together with Wasm Interface Types (WIT). One might be written in Go, another in Rust, and another in JavaScript, but they can all communicate with one another over these shared APIs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;💡 Check out Alex Creighton’s &lt;a href=&#34;https://github.com/WebAssembly/component-model/blob/main/design/mvp/WIT.md&#34;&gt;documentation&lt;/a&gt; which perfectly explains the ways in which the WIT IDL supports the WebAssembly Component Model.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Teams use their favorite libraries in their favorite languages, and once they compile the code to a Wasm component, other components can make use of the functions they expose—regardless of the language and libraries used to write those components.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;💡 The Bytecode Alliance also has &lt;a href=&#34;https://component-model.bytecodealliance.org/introduction.html&#34;&gt;extensive resources&lt;/a&gt; on the WebAssembly Component Model. And check out the &lt;a href=&#34;https://cosmonic.com/downloads/WIT-Cheatsheet-v1.2.pdf&#34;&gt;WIT cheat sheet&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Composing in wasmCloud&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;An incubating, open source &lt;a href=&#34;https://www.cncf.io/projects/wasmcloud/&#34;&gt;CNCF project&lt;/a&gt;, wasmCloud is designed to bring the WebAssembly Component Model to life in real-world use cases. Components are the portable unit of code, and wasmCloud orchestrates deployments across distributed environments. Like Kubernetes is the chosen orchestration platform for containers, wasmCloud is cloud-native application orchestration, at-scale.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly &lt;strong&gt;eliminates the rinse-and-repeat,&lt;/strong&gt; app-by-app maintenance cycle common in most organizations. wasmCloud allows teams to create one lean set of core applications, built on common standards, and portable to &lt;em&gt;any&lt;/em&gt; location.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The architecture of wasmCloud makes it naturally suited to hybrid clouds and multi-tenant environments, with automatic failover and load balancing built in. wasmCloud can stand alone or run on Kubernetes with the &lt;a href=&#34;https://github.com/wasmCloud/wasmcloud-operator&#34;&gt;wasmcloud-operator.&lt;/a&gt;. Moreover, wasmCloud is designed to integrate with existing cloud native tooling and standards such as OpenTelementry and Open Policy Agent.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;2078&#34; height=&#34;943&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges.jpg&#34; alt=&#34;Fig 4: wasmCloud is built for orchestrating applications in the cloud and on diverse edges.&#34; class=&#34;wp-image-122101&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges.jpg 2078w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges-300x136.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges-1024x465.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges-768x349.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges-900x408.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges-1800x817.jpg 1800w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges-441x200.jpg 441w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges-881x400.jpg 881w&#34; sizes=&#34;auto, (max-width: 2078px) 100vw, 2078px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Fig 4: wasmCloud is built for orchestrating applications in the cloud and on diverse edges.&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In wasmCloud, Wasm applications use open standards and common interfaces for platform capabilities. These can be swapped runtime, allowing us to switch vendors or do maintenance upgrades on-the-fly. Because these are high-level APIs, they’re vendor-agnostic which means teams can use their own tooling.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;💡 Take a look at the growing set of &lt;a href=&#34;https://wasmcloud.com/docs/capabilities/&#34;&gt;capability providers&lt;/a&gt; to see how wasmCloud integrates with common cloud native tools.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;wasmCloud also allows engineers to write their own custom capabilities and interfaces—completely essential given every organization’s use cases and infrastructure are different.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Industrial adoption&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We’re seeing WebAssembly being adopted in a wide variety of use cases in banking, manufacturing, telecommunications, digital services, gaming, and more. In every case, this idea of being able to build one set of applications, capable of running everywhere is what engineers are asking for.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Manufacturing and IoT use cases are proliferating because orchestrators like wasmCloud make it possible to put compute power on devices and in production lines. A great example is industrial analytics company &lt;a href=&#34;https://www.cncf.io/blog/2024/08/23/wasmcloud-on-the-factory-floor-efficient-and-secure-processing-of-high-velocity-machine-data/&#34;&gt;MachineMetrics&lt;/a&gt;: deploying wasmCloud on factory machinery to put real-time analytics on expensive machinery and improve performance and longevity.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Adobe has been a long-time user of wasmCloud. They first proved the value of bringing wasmCloud to its Kubernetes infrastructure to &lt;a href=&#34;https://wasmcloud.com/blog/better-together-building-efficient-microservices-in-kubernetes-with-webassembly&#34;&gt;improve the efficiency of microservices&lt;/a&gt; running in Kubernetes. Lean and lightweight, wasmCloud can be almost instantly scaled as traffic scales up, giving more scheduling flexibility than a coarse-grained container. Now, the use cases for Wasm are growing at Adobe.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Adoption in telecoms is growing. &lt;a href=&#34;https://www.youtube.com/watch?v=1sWQqgK-79c&#34;&gt;Orange&lt;/a&gt; is working with wasmCloud to improve service delivery on customer handsets. Orange is also part of a wider group which includes Vodafone, Etisalat and nbnco who have &lt;a href=&#34;https://www.cncf.io/blog/2024/01/05/bringing-webassembly-to-telecoms-with-cncf-wasmcloud/&#34;&gt;proved the potential value&lt;/a&gt; of &lt;em&gt;replacing&lt;/em&gt; Kubernetes with Wasm in managing the TM Forum’s estate of open APIs. The WebAssembly Canvas project is now in Phase II which will look at the value of bringing wasmCloud &lt;em&gt;together&lt;/em&gt; with Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Akamai’s &lt;a href=&#34;https://youtu.be/vEHbZCBmPok&#34;&gt;recent presentation&lt;/a&gt; at wasmCloud Innovation Day gave us an insight into how platforms like wasmCloud can extend compute power to the edge in even more advanced use cases. And our celestial-terrestrial mesh demo shows how space agencies could put Wasm to use in super-distributed, multi-tenant use cases.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Finally, it was incredible to see engineers from global financial services company American Express, &lt;a href=&#34;https://www.youtube.com/watch?v=KrugZVTKDKk&amp;amp;list=PLbzoR-pLrL6o0UD4PoO0H_RnoToEiWBIS&amp;amp;index=11&#34;&gt;on stage at WasmCon&lt;/a&gt;, showing how they’re bringing Wasm, and wasmCloud, to their architecture to elevate serverless platforms with Wasm components.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1920&#34; height=&#34;1243&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrestrial-and-celestial-compute-mesh-for-highly-distributed-use-cases.png&#34; alt=&#34;Fig 5. An example of wasmCloud as a terrestrial and celestial compute mesh for highly distributed use cases.&#34; class=&#34;wp-image-122102&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrestrial-and-celestial-compute-mesh-for-highly-distributed-use-cases.png 1920w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrestrial-and-celestial-compute-mesh-for-highly-distributed-use-cases-300x194.png 300w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrestrial-and-celestial-compute-mesh-for-highly-distributed-use-cases-1024x663.png 1024w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrestrial-and-celestial-compute-mesh-for-highly-distributed-use-cases-768x497.png 768w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrestrial-and-celestial-compute-mesh-for-highly-distributed-use-cases-900x583.png 900w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrestrial-and-celestial-compute-mesh-for-highly-distributed-use-cases-1800x1165.png 1800w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrestrial-and-celestial-compute-mesh-for-highly-distributed-use-cases-309x200.png 309w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrestrial-and-celestial-compute-mesh-for-highly-distributed-use-cases-618x400.png 618w&#34; sizes=&#34;auto, (max-width: 1920px) 100vw, 1920px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Fig 5. An example of wasmCloud as a terrestrial and celestial compute mesh for highly distributed use cases.&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Will Wasm replace containers?&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Like in past epochs of computing, VM’s did not disappear with the advent of containers, just as containers will not be completely replaced by Wasm. Stateful, long-running compute and appliances like databases are unlikely to be replaced with Wasm. This evolution in technology will not occur overnight and so Wasm apps will continue to run alongside the thousands of apps already running in containers and on Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Take a look at our &lt;a href=&#34;https://wasmcloud.com/docs/intro&#34;&gt;documentation&lt;/a&gt; and click on &lt;a href=&#34;https://wasmcloud.com/docs/tour/hello-world&#34;&gt;Quickstart&lt;/a&gt; for a tour of wasmCloud, its features and functionality.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Don’t forget, the wasmCloud community meets every week on Wednesday at 1pm EST. You can add the meeting to your calendar, or join us on &lt;a href=&#34;https://www.youtube.com/@wasmCloud/streams&#34;&gt;YouTube&lt;/a&gt;. Oh, and come join the discussion on &lt;a href=&#34;https://wasmcloud.slack.com/&#34;&gt;Slack&lt;/a&gt;!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;大使帖子，作者：CNCF 大使兼 Cosmonic 首席执行官 &lt;a href=&#34;https://www.linkedin.com/in/hectaman&#34;&gt;Liam Randall&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们最近有机会&lt;a href=&#34;https://wasmcloud.com/blog/2024-09-24-wasmcloud-innovation-day-2024-in-review&#34;&gt;反思&lt;/a&gt;大公司内平台工程的现状以及 WebAssembly 在该学科的未来中扮演的角色。在这篇文章中，我们将研究大规模构建和运行应用程序的团队所面临的成本和复杂性挑战。然后，我们将考虑 WebAssembly 以及孵化 Cloud Native Foundation (CNCF) wasmCloud 等平台在克服这些障碍方面的作用。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;现代平台工程的陷阱&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;平台工程作为一种实践真正开始于计算的“容器时代”。我们将过时的系统拆分为可管理的微服务，将这些微服务打包到容器中，然后使用 Kubernetes 编排这些容器。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;这使基础设施和平台团队更加紧密地联系在一起。基础设施团队成为为开发人员提供通用应用程序服务的同一团队的一部分，以供开发人员构建从秘密管理到身份验证和授权的一切服务。毫无疑问，容器使工程技术变得更好，但它们也带来了一些大问题。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;​​💡 要更深入地分析平台工程团队所面临的挑战，Bailey Hayes 的 &lt;a href=&#34;https://youtu.be/PexRsU8sVIs &#34;&gt;最近的创新日主题演讲&lt;/a&gt;不容错过。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“2235”高度=“914”src=“https://www.cncf.io/ wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform.jpg&#34; alt=&#34;图 1. 计算时代和计算机的演变 平台。” class =“wp-image-122098”srcset =“https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-平台.jpg 2235w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-300x123.jpg 300w， https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-1024x419.jpg 1024w， https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-768x314.jpg 768w， https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-900x368.jpg 900w， https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-1800x736.jpg 1800w， https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-489x200.jpg 489w， https://www.cncf.io/wp-content/uploads/2024/12/Fig-1-Eras-of-compute-and-the-evolution-of-the-platform-978x400.jpg 978w“尺寸=”自动，（最大宽度：2235px) 100vw, 2235px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 1. 计算时代和平台的演变。&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;应用构建-运行-维护的高昂成本&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如今，构建、运行和维护应用程序的成本实在太高了。这从语言的选择以及我们用于所有不同企业应用程序的“黄金模板”开始。我们与为特定语言构建的特定客户端和框架联系在一起。所有这些意味着我们必须逐个应用地构建和维护应用程序。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;当平台工程师需要更新给定平台时，他们需要用户（应用程序开发人员）更新每个应用程序的依赖项。由于大多数应用程序主要由开源依赖项组成，因此它们继承了维护这些依赖项以防止漏洞的成本。工程师们忙于进行应用程序维护，这是昂贵且低效的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;容器解决不了仓鼠轮子的问题。此外，容器化在可移植性方面遇到了一些重大限制。我们要针对不同的架构打造差异化的容器。即使在最精简的情况下，Kubernetes 和容器也太笨重，无法在传感器、物联网设备和许多其他边缘环境中运行。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;那么我们怎样才能制作出真正可移植的应用程序呢？当我们这样做时，我们可以&lt;strong&gt;解决仓鼠轮&lt;/strong&gt;和语言孤岛问题吗？&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;答案在于可移植、可互操作的&lt;strong&gt;WebAssembly 组件&lt;/strong&gt;。我们正在进入一个&lt;em&gt;组件化&lt;/em&gt;的新时代，我们的分布式计算单元是由 wasmCloud 编排的 WebAssembly 组件。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Wasm 与容器&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;值得一提的是，wasmCloud 热爱 Kubernetes——Wasm 运行得非常好 &lt;a href=&#34;https://wasmcloud.com/blog/how-to-run-web assembly-components-on-kubernetes-with-wasmcloud&#34;&gt;与容器一起&lt;/a&gt;。 &lt;a href=&#34;https://github.com/wasmCloud/wasmcloud-operator&#34;&gt;wasmcloud-operator&lt;/a&gt; 和 Helm 图表使在 Kubernetes 上部署 Wasm 变得非常简单。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;​​💡 作为将 Wasm 引入其 Kubernetes 产业的大型组织的示例，&lt;a href=&#34;https://www.cncf.io/blog /2022/11/17/better-together-a-kubernetes-and-wasm-case-study/&#34;&gt;Adobe 的用例&lt;/a&gt;是一个很好的起点。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;话虽如此，Kubernetes 是为&lt;em&gt;基础设施&lt;/em&gt;编排而设计的，而不是应用程序管理。 wasmCloud 专为大规模应用程序编排而设计，它可以独立处理该作业，也可以在 Kubernetes 集群上处理该作业。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;将 WebAssembly 视为一个微型虚拟机。 Wasm 应用程序与组件一起编译成高效的二进制格式，比传统容器更小、更轻量图像。这种减小的尺寸最大限度地减少了内存和存储需求，从而允许在相同的空间内容纳更多的工作负载。容器要大得多（从 MB 到 GB），而组件化 WebAssembly 应用程序的范围从 &lt;strong&gt;KB 到 MB&lt;/strong&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Wasm 组件本身是沙盒的、无状态的，并且&lt;strong&gt;默认情况下是安全的&lt;/strong&gt;，因为它们无法自行与操作系统交互。对于容器，需要大量流程、最佳实践和工具来实现适当的安全性和隔离级别。容器是一种本质安全的沙箱和隔离任意用户代码的方式。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;虽然容器增强了应用程序的可移植性，但这些镜像仍然绑定到特定的 CPU 架构。对于 WebAssembly 组件，&lt;strong&gt;.wasm 二进制文件对于任何环境都是相同的&lt;/strong&gt;，只要该环境具有 WebAssembly 运行时即可。 Wasm 的可移植性允许工作负载跨异构环境运行，例如云、边缘，甚至资源受限的设备。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;容器冷启动需要几秒钟，而&lt;strong&gt;WebAssembly 组件启动时间不到一毫秒&lt;/strong&gt;。用户请求的延迟通常约为 200 毫秒，这意味着容器必须始终运行，即使在不需要时也准备好服务请求。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“2158”高度=“943”src=“https://www.cncf.io/ wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly.jpg&#34; alt=&#34;图 2. 与此相比的容器冷启动时间WebAssembly 的。” class =“wp-image-122099”srcset =“https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that- of-WebAssembly.jpg 2158w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-300x131.jpg 300w， https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-1024x447.jpg 1024w， https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-768x336.jpg 768w， https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-900x393.jpg 900w， https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-1800x787.jpg 1800w， https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-458x200.jpg 458w， https://www.cncf.io/wp-content/uploads/2024/12/Fig-2-Container-cold-start-time-compared-to-that-of-WebAssembly-915x400.jpg 915w“尺寸=”自动，（最大宽度：2158px）100vw，2158px“referrerpolicy =“no-referrer”&gt; &lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 2. 容器冷启动时间与 WebAssembly 的比较。&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们很快就会看到新类型架构的发展充分利用了组件的大小和速度。当您在用户设备上本地运行该计算时，为什么要向其他地方发出网络请求？&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;这一切听起来都很棒。但是，如果您花了数年时间将 Kubernetes 置于您宇宙的中心，如果容器运行得足够好，采用 Wasm 是否等于又一次进入未知世界的沉重负担？为了回答这个问题，我们需要探索组件是如何工作的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;组件……无处不在！&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们经常将整个世界打包到容器中，通过 Wasm 组件我们可以深入了解应用程序的核心逻辑。组件打包为 &lt;a href=&#34;https://tag-runtime.cncf.io/wgs/wasm/deliverables/wasm-oci-artifact/&#34;&gt;OCI 工件&lt;/a&gt;，从根本上简化了开发人员构建应用程序的方式。我们使用标准的、&lt;strong&gt;可重用&lt;/strong&gt;构建块（组件）来编写应用程序，这些构建块（组件）仅包含我们需要的逻辑并在运行时编译。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;这是巨大的。我们可以重用我们的组件，而不是重写它们，并且它们只包含所需的代码。通过消除样板代码（我们在库中导入的东西），漏洞就被完全消除了。考虑到识别、报告和修复漏洞所花费的时间，这是变革性的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;组件相互构建，&lt;strong&gt;就像乐高积木&lt;/strong&gt;。它们可以在支持组件的任何地方运行，当然，因为它们很小，所以我们可以在网络边缘的资源受限环境中运行组件化应用程序。正如我们稍后将看到的，制造业和物联网正在成为 wasmCloud 的一个明显用例。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;​​💡&lt;a href=&#34;https://wasmcloud.com/docs/concepts/components&#34;&gt;组件入门指南&lt;/a&gt;是对于那些刚开始使用组件的人来说是很棒的资源。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“1539”高度=“812”src=“https://www.cncf.io/ wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries.jpg&#34; alt=&#34;图 3. 组件是语言可互操作和 API 驱动的设计定义了操作边界。” class =“wp-image-122100”srcset =“https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-设计定义操作边界.jpg 1539w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-300x158.jpg 300w , https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-1024x540.jpg 1024w , https://www.cncf.io/wp-content/uploads/2024/12/Fig-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-768x405.jpg 768w ，https://www.cncf.io/wp-content/uploads/2024/12/Figure-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-194x102.jpg 194w， https://www.cncf.io/wp-content/uploads/2024/12/Figure-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-388x204.jpg 388w , https://www.cncf.io/wp-content/uploads/2024/12/Figure-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-776x408.jpg 776w , https://www.cncf.io/wp-content/uploads/2024/12/Figure-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-900x475.jpg 900w , https://www.cncf.io/wp-content/uploads/2024/12/Figure-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-379x200.jpg 379w , https://www.cncf.io/wp-content/uploads/2024/12/Figure-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-758x400.jpg 758w , https://www.cncf.io/wp-content/uploads/2024/12/Figure-3-Components-are-language-interoperable-and-API-driven-design-defines-operational-boundaries-590x310.jpg 590w “尺寸=“自动，（最大宽度：1539px）100vw，1539px” referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 3. 组件是语言可互操作的，API 驱动的设计定义了操作边界。&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;最重要的是，组件&lt;strong&gt;可互操作且支持多种语言&lt;/strong&gt;。第一原则规定我们使用以多种不同语言编写的应用程序，所有这些语言都必须得到支持。组件通过一组通用的共享标准化接口（WASI）进行通信，该接口与 Wasm 接口类型 (WIT) 绑定在一起。一个可能是用 Go 编写的，另一个是用 Rust 编写的，另一个是用 JavaScript 编写的，但它们都可以通过这些共享 API 相互通信。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;​​💡 查看 Alex Creighton 的 &lt;a href=&#34;https://github.com/WebAssembly/component-model/blob/main/design/mvp /WIT.md&#34;&gt;文档&lt;/a&gt;完美地解释了 WIT IDL 支持 WebAssembly 组件模型的方式。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;团队以他们最喜欢的语言使用他们最喜欢的库，一旦他们将代码编译为 Wasm 组件，其他组件就可以使用他们公开的功能 - 无论用于编写这些组件的语言和库是什么。&lt;/p &gt;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;​​💡字节码联盟还拥有&lt;a href=&#34;https://component-model.bytecodealliance.org/introduction.html&#34;&gt;广泛的资源&lt; /a&gt; 在 WebAssembly 组件模型上。并查看&lt;a href=&#34;https://cosmonic.com/downloads/WIT-Cheatsheet-v1.2.pdf&#34;&gt;WIT 备忘单&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;在 wasmCloud 中创作&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;wasmCloud 是一个正在孵化的开源&lt;a href=&#34;https://www.cncf.io/projects/wasmcloud/&#34;&gt;CNCF 项目&lt;/a&gt;，旨在将 WebAssembly 组件模型变为现实。世界使用案例。组件是可移植的代码单元，wasmCloud 跨分布式环境协调部署。就像 Kubernetes 是容器选择的编排平台一样，wasmCloud 是大规模的云原生应用程序编排。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly &lt;strong&gt;消除了大多数组织中常见的逐个应用程序的冲洗和重复维护周期。 wasmCloud 允许团队创建一组精简的核心应用程序，这些应用程序基于通用标准构建，并且可移植到&lt;em&gt;任何&lt;/em&gt;位置。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;wasmCloud 的架构使其天生适合混合云和多租户环境，并内置自动故障转移和负载平衡。wasmCloud 可以独立运行，也可以通过 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/k8s&#34; 运行在 Kubernetes 上。 com/wasmCloud/wasmcloud-operator&#34;&gt;wasmcloud-operator。&lt;/a&gt;。此外，wasmCloud 旨在与现有的云原生工具和标准集成，例如 OpenTelementry 和 Open Policy Agent。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“2078”高度=“943”src=“https://www.cncf.io/ wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges.jpg&#34; alt=&#34;Fig 4：wasmCloud 专为在云端和不同边缘编排应用程序而构建。” class =“wp-image-122101”srcset =“https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in- the-cloud-and-on-diverse-edges.jpg 2078w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges- 300x136.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges- 1024x465.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges- 768x349.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges- 900x408.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges- 1800x817.jpg 1800w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges- 441x200.jpg 441w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-4-wasmCloud-is-built-for-orchestrating-applications-in-the-cloud-and-on-diverse-edges- 881x400.jpg 881w&#34; 尺寸 = &#34;自动, (最大宽度: 2078 像素) 100vw, 2078 像素&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 4：wasmCloud 专为在云端和不同边缘编排应用程序而构建。&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在 wasmCloud 中，Wasm 应用程序使用开放标准和通用接口来实现平台功能。这些可以在运行时交换，允许我们更换供应商或进行维护nce 即时升级。因为这些是高级 API，所以它们与供应商无关，这意味着团队可以使用自己的工具。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-gray-300-background-color has-background&#34;&gt;​​💡 看看不断增长的&lt;a href=&#34;https://wasmcloud.com/docs/capability/&#34;&gt;能力提供者&lt; /a&gt; 查看 wasmCloud 如何与常见的云原生工具集成。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;wasmCloud 还允许工程师编写自己的自定义功能和接口 - 鉴于每个组织的用例和基础设施都不同，这是完全必要的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;工业采用&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们看到 WebAssembly 被广泛应用于银行、制造、电信、数字服务、游戏等领域的各种用例中。在每种情况下，工程师所要求的就是能够构建一组能够在任何地方运行的应用程序。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;制造和物联网用例正在激增，因为像 wasmCloud 这样的编排器可以将计算能力放在设备和生产线上。一个很好的例子是工业分析公司 &lt;a href=&#34;https://www.cncf.io/blog/2024/08/23/wasmcloud-on-the-factory-floor-efficient-and-secure-processing-of- high-velocity-machine-data/&#34;&gt;MachineMetrics&lt;/a&gt;：在工厂机械上部署 wasmCloud，对昂贵的机械进行实时分析并提高性能和使用寿命。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Adobe 一直是 wasmCloud 的长期用户。他们首先证明了将 wasmCloud 引入其 Kubernetes 基础设施对于提高效率的价值在 Kubernetes 中运行的微服务&lt;/a&gt;。 wasmCloud 精益且轻量，几乎可以随着流量的增加而立即扩展，从而比粗粒度容器提供更多的调度灵活性。现在，Adobe 的 Wasm 用例正在不断增长。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;电信领域的采用正在不断增长。 &lt;a href=&#34;https://www.youtube.com/watch?v=1sWQqgK-79c&#34;&gt;Orange&lt;/a&gt; 正在与 wasmCloud 合作，改善客户手机上的服务交付。 Orange 也是包括沃达丰、Etisalat 和 nbnco 在内的更广泛集团的一部分，这些集团拥有&lt;a href=&#34;https://www.cncf.io/blog/2024/01/05/bringing-web assembly-to-telecoms-with- cncf-wasmcloud/&#34;&gt;证明了用 Wasm 取代 Kubernetes 在管理 TM 论坛的开放 API 资产方面的潜在价值&lt;/a&gt;。 WebAssembly Canvas 项目目前处于第二阶段，该项目将研究将 wasmCloud 与 Kubernetes 结合在一起的价值。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Akamai 在 wasmCloud 创新日上&lt;a href=&#34;https://youtu.be/vEHbZCBmPok&#34;&gt;最近的演示&lt;/a&gt;让我们深入了解了 wasmCloud 等平台如何以更先进的方式将计算能力扩展到边缘用例。我们的天地网格演示展示了航天机构如何将 Wasm 用于超级分布式、多租户用例。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;最后，看到来自全球金融服务公司美国运通的工程师&lt;a href=&#34;https://www.youtube.com/watch?v=KrugZVTKDKk&amp;list=PLbzoR-pLrL6o0UD4PoO0H_RnoToEiWBIS&amp;index=11&#34;&gt;在 WasmCon 上登台&lt;/a&gt;真是令人难以置信，展示了他们如何将 Wasm 和 wasmCloud 引入其架构，以使用 Wasm 组件提升无服务器平台。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“1920”高度=“1243” src =“https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrescial-and-celestial-compute-mesh-for -highly-distributed-use-cases.png&#34; alt=&#34;图 5. wasmCloud 作为高度分布式用例的陆地和天体计算网格的示例。&#34; class =“wp-image-122102”srcset =“https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrescial- and-celestial-compute-mesh-for-highly-distributed-use-cases.png 1920w， https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrescial-and-celestial-compute-mesh-for-highly-分布式用例-300x194.png 300w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrescial-and-celestial-compute-mesh-for-highly-分布式用例-1024x663.png 1024w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrescial-and-celestial-compute-mesh-for-highly-分布式用例-768x497.png 768w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrescial-and-celestial-compute-mesh-for-highly-分布式用例-900x583.png 900w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrescial-and-celestial-compute-mesh-for-highly-分布式用例-1800x1165.png 1800w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrescial-and-celestial-compute-mesh-for-highly-分布式用例-309x200.png 309w, https://www.cncf.io/wp-content/uploads/2024/12/Fig-5-An-example-of-wasmCloud-as-a-terrescial-and-celestial-compute-mesh-for-highly-分布式使用案例-618x400.png 618w“尺寸=”自动，（最大宽度：1920px）100vw， 1920px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 5. wasmCloud 作为高度分布式用例的陆地和天体计算网格的示例。&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Wasm 会取代容器吗？&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;就像过去的计算时代一样，VM 并没有随着容器的出现而消失，就像容器不会被 Wasm 完全取代一样。有状态、长时间运行的计算和数据库等设备不太可能被 Wasm 取代。这种技术的发展不会在一夜之间发生，因此 Wasm 应用程序将继续与容器和 Kubernetes 上已经运行的数千个应用程序一起运行。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;查看我们的&lt;a href=&#34;https://wasmcloud.com/docs/intro&#34;&gt;文档&lt;/a&gt; 并单击&lt;a href=&#34;https://wasmcloud.com/docs/tour/hello-world&#34;&gt;快速入门&lt;/a&gt;，了解 wasmCloud 及其特性和功能。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;不要忘记，wasmCloud 社区每周三下午 1 点（美国东部时间）举行会议。您可以将会议添加到日历中，或通过 &lt;a href=&#34;https://www.youtube.com/@wasmCloud/streams&#34;&gt;YouTube&lt;/a&gt; 加入我们。哦，来参加 &lt;a href=&#34;https://wasmcloud.slack.com/&#34;&gt;Slack&lt;/a&gt; 上的讨论吧！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Sun, 22 Dec 2024 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>