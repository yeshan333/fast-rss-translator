<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>CNCF - Blog</title>
    <link>http://rsshub.rssforever.com/cncf</link>
    <description>CNCF - Blog - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)</description>
    <managingEditor>i@diygod.me (DIYgod)</managingEditor>
    <item>
      <title>【Unlocking the power of ephemeral environments with Devtron】使用 Devtron 释放短暂环境的力量</title>
      <link>https://www.cncf.io/blog/2024/07/12/unlocking-the-power-of-ephemeral-environments-with-devtron/</link>
      <description>【&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://devtron.ai/blog/unlocking-the-power-of-ephemeral-environments-with-devtron/&#34;&gt;Devtron’s blog&lt;/a&gt; by Abhinav Dubey&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;TL;DR: The blog talks about how ephemeral environments with Devtron become much easier, reducing the complexities, automating the process, and optimizing infra cost.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the world of software development,&amp;nbsp;&lt;strong&gt;ephemeral environments&lt;/strong&gt;&amp;nbsp;are temporary setups that serve specific purposes, such as testing or staging new features. These environments are short-lived, designed to exist only for the duration of their use case—like testing a feature branch—before being dismantled.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments contrast with traditional static environments, which are permanent and can lead to inefficiencies, especially when underutilized. They offer a dynamic approach, allowing developers to create an isolated environment on demand without affecting the main codebase or other ongoing development activities.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-technical-and-business-value-of-ephemeral-environments&#34;&gt;The Technical and Business Value of Ephemeral Environments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments provide significant advantages in different sectors as mentioned below:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cost Efficiency&lt;/strong&gt;: By creating environments only when needed and tearing them down afterward, organizations avoid the cost of maintaining idle resources. This is particularly beneficial for companies where lower-end environments such as&amp;nbsp;&lt;code&gt;dev-env&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;non-prod&lt;/code&gt;&amp;nbsp;can cost up to five times more than production environments.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Agility and Speed&lt;/strong&gt;: Developers can quickly spin up environments to test new features or bug fixes without waiting for access to a shared environment. This agility accelerates development cycles and time-to-market.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Risk Reduction&lt;/strong&gt;: Testing in isolated environments ensures that unstable code does not affect the rest of the system, reducing the risk of bugs in production.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;is-an-ephemeral-environment-right-for-you&#34;&gt;Is an Ephemeral Environment Right for You?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Deciding whether ephemeral environments are suitable for your organization involves considering your development needs and organizational goals. Key questions include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Do your development teams frequently need isolated environments for testing?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Are you looking to optimize costs associated with non-production environments?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Is there a need to increase deployment speed and reduce risk in production?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you answered “yes” to any of these, ephemeral environments could be highly beneficial for you.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;traditional-approach-to-ephemeral-environment&#34;&gt;Traditional Approach to Ephemeral Environment&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environment as mentioned above are the short lived environments, created and destroyed once the task is completed. We can create our scripts maybe in Terraform or Ansible or in python/shell to spin up a complete new environment that can be your VM Machines or Kubernetes clusters. Even though the automation can be achieved, there are few disadvantages associated with this approach, that include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Delay in Releases:&lt;/strong&gt;&amp;nbsp;The time taken to bring up the entire infrastructure can lead to delays in testing features or conducting sanity checks for bug fixes, resulting in a longer time to market.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;High Complexity:&amp;nbsp;&lt;/strong&gt;Creating and maintaining scripts to standardize environments across different stages can be complex and error-prone&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Manual Interventions:&amp;nbsp;&lt;/strong&gt;Even with automation scripts, manual interventions are often required to configure and install dependencies based on the application’s specific requirements, adding to the setup time.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;DevOps Dependencies:&lt;/strong&gt;&amp;nbsp;Developers typically lack expertise in tools like Terraform or Ansible, making them dependent on DevOps or SRE teams to make changes and install dependencies for their applications, which can slow down the development process.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Resource Management:&amp;nbsp;&lt;/strong&gt;Managing the lifecycle of ephemeral environments can be challenging. These environments need to be deleted once tasks are completed; otherwise, they lead to resource wastage and increased costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;High Infra Cost:&amp;nbsp;&lt;/strong&gt;The costs associated with spinning up and maintaining ephemeral environments, particularly in cloud-based setups, can add significantly to the overall infrastructure expenses.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;rethinking-ephemeral-environments&#34;&gt;Rethinking Ephemeral Environments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When we talk in terms of Kubernetes, setting up Ephemeral Environments becomes a lot easier than the traditional approach. Kubernetes has a beautiful thing called namespaces, a logical separation of group of resources, providing isolation of workloads within the same cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;By leveraging namespaces and some advanced autoscaling methods, it becomes much more easier to create a ephemeral environment that is cost-effective, less complex and helps you dynamically bring up the resources and hibernate when not in use. &amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;how-to-set-up-an-ephemeral-environment-in-k8s-manually&#34;&gt;How to Set Up an Ephemeral Environment in K8s Manually?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Setting up an ephemeral environment, especially within a Kubernetes ecosystem, involves several key steps that ensure agility, efficiency, and cost-effectiveness. Below, we detail a straightforward approach to creating and managing these temporary environments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 1: Define Your Infrastructure Requirements&lt;/strong&gt;&lt;br&gt;Before you create an ephemeral environment, it’s essential to understand the specific requirements of the application or feature being tested. This includes the necessary computing resources, the required services, and any dependencies that need to be replicated from the production environment.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 2: Automate the Environment Setup&lt;/strong&gt;&lt;br&gt;Automation is crucial in managing ephemeral environments to ensure they can be spun up and torn down efficiently. Tools like Terraform or Ansible can be used to script the creation of your infrastructure. In Kubernetes, you might automate setting up namespaces, deploying container images, and configuring network policies through CI/CD pipelines.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Using Kubernetes Namespaces&lt;/strong&gt;&lt;br&gt;In Kubernetes, namespaces provide a way to divide cluster resources between multiple users. Each ephemeral environment can be created in its namespace, isolating its running processes and resources from other environments&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl create namespace your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 3: Deploy Your Application&lt;/strong&gt;&lt;br&gt;Once the namespace is ready, deploy your application using Kubernetes manifests or Helm charts. This step often involves setting up the necessary config maps and secrets to configure the application according to the environment&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl apply -f your-application-deployment.yaml -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Or using Helm&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;helm install your-application-release your-helm-chart/ -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 4: Configure Autoscaling and Monitoring&lt;/strong&gt;&lt;br&gt;To optimize costs and resource usage, configure autoscaling for your application workloads. Kubernetes Horizontal Pod Autoscaler (HPA) or a more advanced tool like KEDA can be used to automatically adjust the number of pods based on traffic or other metrics&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;apiVersion: autoscaling/v2beta2&#xA;kind: HorizontalPodAutoscaler&#xA;metadata:&#xA;  name: your-application-hpa&#xA;  namespace: your-environment-name&#xA;spec:&#xA;  scaleTargetRef:&#xA;    apiVersion: apps/v1&#xA;    kind: Deployment&#xA;    name: your-application-deployment&#xA;  minReplicas: 1&#xA;  maxReplicas: 10&#xA;  metrics:&#xA;  - type: Resource&#xA;    resource:&#xA;      name: cpu&#xA;      target:&#xA;        type: Utilization&#xA;        averageUtilization: 50&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Monitoring is also essential to track the performance and health of your temporary environment. Tools like Prometheus for monitoring and Grafana for visualization can be integrated to monitor the environment’s metrics.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 5: Implement Cleanup Procedures&lt;/strong&gt;&lt;br&gt;To ensure that resources are not wasted, set up automatic cleanup procedures to tear down the environment after use. This can be scheduled using cron jobs or integrated into your CI/CD pipeline to destroy the environment once the testing is complete:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl delete namespace your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Or a more controlled cleanup with Helm&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;helm uninstall your-application-release -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 6: Documentation and Training&lt;/strong&gt;&lt;br&gt;Finally, document the entire process and provide training for your teams. This ensures that everyone understands how to efficiently use ephemeral environments, which helps in maximizing the benefits while minimizing potential disruptions or misuse.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Manually creating and deleting the namespaces, and integrating it within pipelines can be something a big pain when it comes to developer productivity. Integrating different tools such as Grafana, Prometheus, Jenkins, ArgoCD, KEDA, etc can be a tedius task for DevOps / SRE engineers as well. With the the involvement of custom scripting, again the complexities increases, high rish to human errors. With Devtron’s simplified workflow, it becomes a lot more easier to automate the process, and improve the developer productivity while reducing its high dependencies from DevOps/ SRE teams.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;how-devtron-simplifies-ephemeral-environments&#34;&gt;How Devtron Simplifies Ephemeral Environments?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Devtron enhances the management of ephemeral environments through its modern dashboard, simplified workflows, automation and effective cost-management strategies. Here are key features:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Namespace Utilization&lt;/strong&gt;: In Kubernetes, namespaces provide logical separation, allowing multiple ephemeral environments within the same cluster without additional cost. Devtron leverages this to minimize the overhead associated with setting up and tearing down environments.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Cost Management&lt;/strong&gt;: Devtron implements strategies such as leveraging spot instances and right-sizing resources, ensuring that the infrastructure costs are kept to a minimum. For example, by using spot instances, organizations can save up to 70-90% compared to standard costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Automated Scaling&lt;/strong&gt;: Devtron employs tools like KEDA for event-driven autoscaling, ensuring resources are used efficiently. Environments can scale down automatically during inactivity and scale up when needed, further optimizing costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Simplified Workflow:&amp;nbsp;&lt;/strong&gt;Devtron provides an intuitive dashboard for all operating on Kubernetes, providing Kubernetes-native CI/CD pipelines, simplifying the heavy scripting and stitching up of different tools to complete an end-to-end workflow.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Along with that, there are many other factors which makes the entire process much more seamless, such as visibility of workloads, application metrics, configurations management, etc.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;setting-up-ephemeral-environments-with-devtron&#34;&gt;Setting-up Ephemeral Environments With Devtron&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Devtron is a Software Distribution Platform designed for Kubernetes. On its mission to democratize Kubernetes, ephemeral environments are one among the many other features, that make life easier. With Devtron’s intuitive dashboard, operations on Kubernetes become flawless, and it goes with ephemeral environments as well. To get started with ephemeral environment, follow the below steps.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 1: Install the keda-add-on-http from the chart’s marketplace. Navigate to the&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/deploy-chart/overview-of-charts?ref=devtron.ai&#34;&gt;charts store&lt;/a&gt;, search for Keda, and as you can see in the below image, you can see all charts related to Keda. Select the appropriate helm chart and deploy it. To add any helm chart which is not listed on the charts store,&amp;nbsp;&lt;a href=&#34;https://devtron.ai/blog/helm-chart-deployment/&#34;&gt;free feel to check out this blog&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-11-at-3.35.11-PM.png&#34; alt=&#34;keda-http-add-on&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 1] KEDA HTTP Add-on Controller&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 2: Once the controller has been successfully installed, you can see a consolidated view of the deployed helm chart, along with its resources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-11-at-3.40.27-PM.png&#34; alt=&#34;resources-grouped-view&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 2] Controller Successfully Deployed&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 3: Now, let’s move and configure the ephemeral environment for my microservice called,&amp;nbsp;&lt;code&gt;payment-svc&lt;/code&gt;. To configure the ephemeral environment for any application, the process remains the same and you should be able to configure/ clone the workflows for different applications. Navigate to&amp;nbsp;&lt;code&gt;Workflow Editor&lt;/code&gt;, add a workflow for the respective environment where you want to deploy your applications, in our case, its&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment as you can see in the below image. To understand more about workflows in Devtron, feel free to refer the&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/applications/creating-application/workflow/ci-pipeline?ref=devtron.ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-4.45.56-PM.png&#34; alt=&#34;workflow-editor&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 3] Adding Deployment Pipeline&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 4: Once the workflow has been created, Devtron automatically creates&amp;nbsp;&lt;code&gt;environment-overrides&lt;/code&gt;&amp;nbsp;for the deployment environment.&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/applications/creating-application/environment-overrides?ref=devtron.ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Environment overrides&lt;/a&gt;&amp;nbsp;help you manage your Kubernetes configuration for the specific environment in a more efficient way. Under the&amp;nbsp;&lt;code&gt;environment override&lt;/code&gt;&amp;nbsp;&amp;gt;&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment, we can add the relevant configurations required in the deployment template which would create the HttpScaledObject, responsible for bringing the environment up and running dynamically as it receives any HTTP request.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-4.46.31-PM.png&#34; alt=&#34;deployment-template&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 4] Configuring HTTPScaledObject&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 5: After providing the relevant configuration, navigate to&amp;nbsp;&lt;code&gt;Build &amp;amp; Deploy&lt;/code&gt;&amp;nbsp;section, select the relevant image, and deploy it in the&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment. Upon successful deployment, you can see the application status as Healthy, and all details about the deployment are as you can see in the below image.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.14.09-PM.png&#34; alt=&#34;application-details&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 5] Application Details&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;You can also see all the resources deployed along with the deployment in a resourced grouped view, and perform operations such as checking the logs, events, manifests, or exec into the terminals. You can notice in the below image that, we have a Deployment object but there isn’t any pod running as of now. This is because it automatically scaled down the workload since there is no HTTP request hitting the given hostname/ service.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.28.14-PM.png&#34; alt=&#34;deployment-replicaset&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 6] Deployment &amp;amp; ReplicaSet&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 6: In the Devtron dashboard, it automatically picks up the ingress host and shows it in&amp;nbsp;&lt;code&gt;URLs&lt;/code&gt;&amp;nbsp;section at the top right of the dashboard as you can see in Fig. 5, and if any request has been made into the hostname, it will automatically scale up the pod and it can serve the traffic as you can see in the below image.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.51.33-PM.png&#34; alt=&#34;scaled-up-pod&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 7] Dynamically Scaled-up Pod&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments offer a flexible, cost-effective solution for managing development stages, particularly in a dynamic and fast-paced software development landscape. Devtron’s approach not only simplifies the management of these environments but also enhances cost efficiency and deployment agility.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Organizations looking to streamline their development processes and reduce costs should consider implementing ephemeral environments, especially those already using Kubernetes. With Devtron, the transition is smoother, allowing teams to focus more on innovation and less on infrastructure management.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Feel free to join our&amp;nbsp;&lt;a href=&#34;https://discord.devtron.ai/?ref=devtron.ai&#34;&gt;Discord Community&lt;/a&gt;&amp;nbsp;if you have any questions. Would love to address any queries or questions. If you liked Devtron, do give it a&amp;nbsp;&lt;a href=&#34;https://github.com/devtron-labs/devtron?ref=devtron.ai&#34;&gt;Star ⭐️ on GitHub&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://devtron.ai/blog/unlocking-the-power-of-ephemeral-environments-with-devtron/&#34;&gt;Devtron’s blog&lt;/a&gt; by Abhinav Dubey&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;TL;DR: The blog talks about how ephemeral environments with Devtron become much easier, reducing the complexities, automating the process, and optimizing infra cost.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the world of software development,&amp;nbsp;&lt;strong&gt;ephemeral environments&lt;/strong&gt;&amp;nbsp;are temporary setups that serve specific purposes, such as testing or staging new features. These environments are short-lived, designed to exist only for the duration of their use case—like testing a feature branch—before being dismantled.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments contrast with traditional static environments, which are permanent and can lead to inefficiencies, especially when underutilized. They offer a dynamic approach, allowing developers to create an isolated environment on demand without affecting the main codebase or other ongoing development activities.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-technical-and-business-value-of-ephemeral-environments&#34;&gt;The Technical and Business Value of Ephemeral Environments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments provide significant advantages in different sectors as mentioned below:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cost Efficiency&lt;/strong&gt;: By creating environments only when needed and tearing them down afterward, organizations avoid the cost of maintaining idle resources. This is particularly beneficial for companies where lower-end environments such as&amp;nbsp;&lt;code&gt;dev-env&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;non-prod&lt;/code&gt;&amp;nbsp;can cost up to five times more than production environments.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Agility and Speed&lt;/strong&gt;: Developers can quickly spin up environments to test new features or bug fixes without waiting for access to a shared environment. This agility accelerates development cycles and time-to-market.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Risk Reduction&lt;/strong&gt;: Testing in isolated environments ensures that unstable code does not affect the rest of the system, reducing the risk of bugs in production.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;is-an-ephemeral-environment-right-for-you&#34;&gt;Is an Ephemeral Environment Right for You?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Deciding whether ephemeral environments are suitable for your organization involves considering your development needs and organizational goals. Key questions include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Do your development teams frequently need isolated environments for testing?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Are you looking to optimize costs associated with non-production environments?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Is there a need to increase deployment speed and reduce risk in production?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you answered “yes” to any of these, ephemeral environments could be highly beneficial for you.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;traditional-approach-to-ephemeral-environment&#34;&gt;Traditional Approach to Ephemeral Environment&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environment as mentioned above are the short lived environments, created and destroyed once the task is completed. We can create our scripts maybe in Terraform or Ansible or in python/shell to spin up a complete new environment that can be your VM Machines or Kubernetes clusters. Even though the automation can be achieved, there are few disadvantages associated with this approach, that include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Delay in Releases:&lt;/strong&gt;&amp;nbsp;The time taken to bring up the entire infrastructure can lead to delays in testing features or conducting sanity checks for bug fixes, resulting in a longer time to market.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;High Complexity:&amp;nbsp;&lt;/strong&gt;Creating and maintaining scripts to standardize environments across different stages can be complex and error-prone&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Manual Interventions:&amp;nbsp;&lt;/strong&gt;Even with automation scripts, manual interventions are often required to configure and install dependencies based on the application’s specific requirements, adding to the setup time.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;DevOps Dependencies:&lt;/strong&gt;&amp;nbsp;Developers typically lack expertise in tools like Terraform or Ansible, making them dependent on DevOps or SRE teams to make changes and install dependencies for their applications, which can slow down the development process.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Resource Management:&amp;nbsp;&lt;/strong&gt;Managing the lifecycle of ephemeral environments can be challenging. These environments need to be deleted once tasks are completed; otherwise, they lead to resource wastage and increased costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;High Infra Cost:&amp;nbsp;&lt;/strong&gt;The costs associated with spinning up and maintaining ephemeral environments, particularly in cloud-based setups, can add significantly to the overall infrastructure expenses.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;rethinking-ephemeral-environments&#34;&gt;Rethinking Ephemeral Environments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When we talk in terms of Kubernetes, setting up Ephemeral Environments becomes a lot easier than the traditional approach. Kubernetes has a beautiful thing called namespaces, a logical separation of group of resources, providing isolation of workloads within the same cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;By leveraging namespaces and some advanced autoscaling methods, it becomes much more easier to create a ephemeral environment that is cost-effective, less complex and helps you dynamically bring up the resources and hibernate when not in use. &amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;how-to-set-up-an-ephemeral-environment-in-k8s-manually&#34;&gt;How to Set Up an Ephemeral Environment in K8s Manually?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Setting up an ephemeral environment, especially within a Kubernetes ecosystem, involves several key steps that ensure agility, efficiency, and cost-effectiveness. Below, we detail a straightforward approach to creating and managing these temporary environments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 1: Define Your Infrastructure Requirements&lt;/strong&gt;&lt;br&gt;Before you create an ephemeral environment, it’s essential to understand the specific requirements of the application or feature being tested. This includes the necessary computing resources, the required services, and any dependencies that need to be replicated from the production environment.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 2: Automate the Environment Setup&lt;/strong&gt;&lt;br&gt;Automation is crucial in managing ephemeral environments to ensure they can be spun up and torn down efficiently. Tools like Terraform or Ansible can be used to script the creation of your infrastructure. In Kubernetes, you might automate setting up namespaces, deploying container images, and configuring network policies through CI/CD pipelines.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Using Kubernetes Namespaces&lt;/strong&gt;&lt;br&gt;In Kubernetes, namespaces provide a way to divide cluster resources between multiple users. Each ephemeral environment can be created in its namespace, isolating its running processes and resources from other environments&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl create namespace your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 3: Deploy Your Application&lt;/strong&gt;&lt;br&gt;Once the namespace is ready, deploy your application using Kubernetes manifests or Helm charts. This step often involves setting up the necessary config maps and secrets to configure the application according to the environment&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl apply -f your-application-deployment.yaml -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Or using Helm&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;helm install your-application-release your-helm-chart/ -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 4: Configure Autoscaling and Monitoring&lt;/strong&gt;&lt;br&gt;To optimize costs and resource usage, configure autoscaling for your application workloads. Kubernetes Horizontal Pod Autoscaler (HPA) or a more advanced tool like KEDA can be used to automatically adjust the number of pods based on traffic or other metrics&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;apiVersion: autoscaling/v2beta2&#xA;kind: HorizontalPodAutoscaler&#xA;metadata:&#xA;  name: your-application-hpa&#xA;  namespace: your-environment-name&#xA;spec:&#xA;  scaleTargetRef:&#xA;    apiVersion: apps/v1&#xA;    kind: Deployment&#xA;    name: your-application-deployment&#xA;  minReplicas: 1&#xA;  maxReplicas: 10&#xA;  metrics:&#xA;  - type: Resource&#xA;    resource:&#xA;      name: cpu&#xA;      target:&#xA;        type: Utilization&#xA;        averageUtilization: 50&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Monitoring is also essential to track the performance and health of your temporary environment. Tools like Prometheus for monitoring and Grafana for visualization can be integrated to monitor the environment’s metrics.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 5: Implement Cleanup Procedures&lt;/strong&gt;&lt;br&gt;To ensure that resources are not wasted, set up automatic cleanup procedures to tear down the environment after use. This can be scheduled using cron jobs or integrated into your CI/CD pipeline to destroy the environment once the testing is complete:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl delete namespace your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Or a more controlled cleanup with Helm&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;helm uninstall your-application-release -n your-environment-name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step 6: Documentation and Training&lt;/strong&gt;&lt;br&gt;Finally, document the entire process and provide training for your teams. This ensures that everyone understands how to efficiently use ephemeral environments, which helps in maximizing the benefits while minimizing potential disruptions or misuse.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Manually creating and deleting the namespaces, and integrating it within pipelines can be something a big pain when it comes to developer productivity. Integrating different tools such as Grafana, Prometheus, Jenkins, ArgoCD, KEDA, etc can be a tedius task for DevOps / SRE engineers as well. With the the involvement of custom scripting, again the complexities increases, high rish to human errors. With Devtron’s simplified workflow, it becomes a lot more easier to automate the process, and improve the developer productivity while reducing its high dependencies from DevOps/ SRE teams.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;how-devtron-simplifies-ephemeral-environments&#34;&gt;How Devtron Simplifies Ephemeral Environments?&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Devtron enhances the management of ephemeral environments through its modern dashboard, simplified workflows, automation and effective cost-management strategies. Here are key features:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Namespace Utilization&lt;/strong&gt;: In Kubernetes, namespaces provide logical separation, allowing multiple ephemeral environments within the same cluster without additional cost. Devtron leverages this to minimize the overhead associated with setting up and tearing down environments.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Cost Management&lt;/strong&gt;: Devtron implements strategies such as leveraging spot instances and right-sizing resources, ensuring that the infrastructure costs are kept to a minimum. For example, by using spot instances, organizations can save up to 70-90% compared to standard costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Automated Scaling&lt;/strong&gt;: Devtron employs tools like KEDA for event-driven autoscaling, ensuring resources are used efficiently. Environments can scale down automatically during inactivity and scale up when needed, further optimizing costs.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Simplified Workflow:&amp;nbsp;&lt;/strong&gt;Devtron provides an intuitive dashboard for all operating on Kubernetes, providing Kubernetes-native CI/CD pipelines, simplifying the heavy scripting and stitching up of different tools to complete an end-to-end workflow.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Along with that, there are many other factors which makes the entire process much more seamless, such as visibility of workloads, application metrics, configurations management, etc.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;setting-up-ephemeral-environments-with-devtron&#34;&gt;Setting-up Ephemeral Environments With Devtron&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Devtron is a Software Distribution Platform designed for Kubernetes. On its mission to democratize Kubernetes, ephemeral environments are one among the many other features, that make life easier. With Devtron’s intuitive dashboard, operations on Kubernetes become flawless, and it goes with ephemeral environments as well. To get started with ephemeral environment, follow the below steps.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 1: Install the keda-add-on-http from the chart’s marketplace. Navigate to the&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/deploy-chart/overview-of-charts?ref=devtron.ai&#34;&gt;charts store&lt;/a&gt;, search for Keda, and as you can see in the below image, you can see all charts related to Keda. Select the appropriate helm chart and deploy it. To add any helm chart which is not listed on the charts store,&amp;nbsp;&lt;a href=&#34;https://devtron.ai/blog/helm-chart-deployment/&#34;&gt;free feel to check out this blog&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-11-at-3.35.11-PM.png&#34; alt=&#34;keda-http-add-on&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 1] KEDA HTTP Add-on Controller&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 2: Once the controller has been successfully installed, you can see a consolidated view of the deployed helm chart, along with its resources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-11-at-3.40.27-PM.png&#34; alt=&#34;resources-grouped-view&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 2] Controller Successfully Deployed&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 3: Now, let’s move and configure the ephemeral environment for my microservice called,&amp;nbsp;&lt;code&gt;payment-svc&lt;/code&gt;. To configure the ephemeral environment for any application, the process remains the same and you should be able to configure/ clone the workflows for different applications. Navigate to&amp;nbsp;&lt;code&gt;Workflow Editor&lt;/code&gt;, add a workflow for the respective environment where you want to deploy your applications, in our case, its&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment as you can see in the below image. To understand more about workflows in Devtron, feel free to refer the&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/applications/creating-application/workflow/ci-pipeline?ref=devtron.ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-4.45.56-PM.png&#34; alt=&#34;workflow-editor&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 3] Adding Deployment Pipeline&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 4: Once the workflow has been created, Devtron automatically creates&amp;nbsp;&lt;code&gt;environment-overrides&lt;/code&gt;&amp;nbsp;for the deployment environment.&amp;nbsp;&lt;a href=&#34;https://docs.devtron.ai/usage/applications/creating-application/environment-overrides?ref=devtron.ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Environment overrides&lt;/a&gt;&amp;nbsp;help you manage your Kubernetes configuration for the specific environment in a more efficient way. Under the&amp;nbsp;&lt;code&gt;environment override&lt;/code&gt;&amp;nbsp;&amp;gt;&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment, we can add the relevant configurations required in the deployment template which would create the HttpScaledObject, responsible for bringing the environment up and running dynamically as it receives any HTTP request.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-4.46.31-PM.png&#34; alt=&#34;deployment-template&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 4] Configuring HTTPScaledObject&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 5: After providing the relevant configuration, navigate to&amp;nbsp;&lt;code&gt;Build &amp;amp; Deploy&lt;/code&gt;&amp;nbsp;section, select the relevant image, and deploy it in the&amp;nbsp;&lt;code&gt;dev-testing&lt;/code&gt;&amp;nbsp;environment. Upon successful deployment, you can see the application status as Healthy, and all details about the deployment are as you can see in the below image.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.14.09-PM.png&#34; alt=&#34;application-details&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 5] Application Details&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;You can also see all the resources deployed along with the deployment in a resourced grouped view, and perform operations such as checking the logs, events, manifests, or exec into the terminals. You can notice in the below image that, we have a Deployment object but there isn’t any pod running as of now. This is because it automatically scaled down the workload since there is no HTTP request hitting the given hostname/ service.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.28.14-PM.png&#34; alt=&#34;deployment-replicaset&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 6] Deployment &amp;amp; ReplicaSet&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Step 6: In the Devtron dashboard, it automatically picks up the ingress host and shows it in&amp;nbsp;&lt;code&gt;URLs&lt;/code&gt;&amp;nbsp;section at the top right of the dashboard as you can see in Fig. 5, and if any request has been made into the hostname, it will automatically scale up the pod and it can serve the traffic as you can see in the below image.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://devtron.ai/blog/content/images/2024/06/Screenshot-2024-06-15-at-6.51.33-PM.png&#34; alt=&#34;scaled-up-pod&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;[Fig. 7] Dynamically Scaled-up Pod&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Ephemeral environments offer a flexible, cost-effective solution for managing development stages, particularly in a dynamic and fast-paced software development landscape. Devtron’s approach not only simplifies the management of these environments but also enhances cost efficiency and deployment agility.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Organizations looking to streamline their development processes and reduce costs should consider implementing ephemeral environments, especially those already using Kubernetes. With Devtron, the transition is smoother, allowing teams to focus more on innovation and less on infrastructure management.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Feel free to join our&amp;nbsp;&lt;a href=&#34;https://discord.devtron.ai/?ref=devtron.ai&#34;&gt;Discord Community&lt;/a&gt;&amp;nbsp;if you have any questions. Would love to address any queries or questions. If you liked Devtron, do give it a&amp;nbsp;&lt;a href=&#34;https://github.com/devtron-labs/devtron?ref=devtron.ai&#34;&gt;Star ⭐️ on GitHub&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Thu, 11 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【SQL simplifies TSDB – how to migrate from InfluxQL to SQL】SQL 简化 TSDB – 如何从 InfluxQL 迁移到 SQL</title>
      <link>https://www.cncf.io/blog/2024/07/10/sql-simplifies-tsdb-how-to-migrate-from-influxql-to-sql/</link>
      <description>【&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql&#34;&gt;Greptime’s blog&lt;/a&gt; by tison&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. SQL is a more common and general language for querying time series data, making migrating from InfluxQL to SQL a growing trend.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/coverimage1.png&#34; alt=&#34;Image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB uses SQL as its primary query language. Once users ingest data into GreptimeDB via the InfluxDB line protocol or other APIs, a common question arises: how can I analyze the data ingested? Specifically, how can existing InfluxQL queries be migrated to SQL queries?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To address the question above, this article outlines the differences between the query languages of InfluxDB (InfluxQL or Flux) and SQL, as well as a cheat sheet for migrating from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;overview-of-query-languages&#34;&gt;Overview of Query Languages&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#overview-of-query-languages&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/&#34;&gt;InfluxQL&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V1. It’s a SQL-like query language but not a SQL dialect. Below are some examples of InfluxQL queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT * FROM h2o_feet;&#xA;SELECT * FROM h2o_feet LIMIT 5;&#xA;SELECT COUNT(&#34;water_level&#34;) FROM h2o_feet;&#xA;SELECT &#34;level description&#34;, &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;SELECT *::field FROM &#34;h2o_feet&#34;;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When InfluxDB was designed and developed, there weren’t as many database developers as today. Consequently, despite InfluxQL’s efforts to closely resemble SQL syntax, implementing basic SQL capabilities supported by relational algebra and adding time series query extensions was quite challenging.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL instead implemented functions and syntax specifically designed for time series data analysis. For instance, all InfluxQL queries default to returning the timestamp column in ascending order, and all queries must include field columns to return results.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, special query syntax is designed for querying over time series rather than rows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Essentially, InfluxQL was developed from the raw need for time series data analysis focused on numerical metrics. As InfluxDB evolved, InfluxQL also supported continuous queries and retention policies to solve some requirements of real-time data processing.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL can still be used in InfluxDB V2, it faces&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v2/query-data/influxql/&#34;&gt;a series of challenges due to model mismatches&lt;/a&gt;, as InfluxDB V2 mainly promotes the Flux query language.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/&#34;&gt;Flux&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V2. Unlike InfluxQL, which has a SQL-like syntax, Flux uses a DataFrame style syntax. Developers who have written programs in Elixir will find the syntax familiar. Here are some examples of Flux queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;erlang&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;from(bucket: &#34;example-bucket&#34;)&#xA;    |&amp;gt; range(start: -1d)&#xA;    |&amp;gt; filter(fn: (r) =&amp;gt; r._measurement == &#34;example-measurement&#34;)&#xA;    |&amp;gt; mean()&#xA;    |&amp;gt; yield(name: &#34;_result&#34;)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Designed to support joint analysis of time series data across various data sources, Flux allows users to fetch data from time series databases (InfluxDB), relational databases (PostgreSQL or MySQL), and CSV files for analysis. For example,&amp;nbsp;&lt;code&gt;sql.from&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;csv.from&lt;/code&gt;&amp;nbsp;can replace&amp;nbsp;&lt;code&gt;from(bucket)&lt;/code&gt;&amp;nbsp;in the example above, allowing fetching data from other sources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Flux can only be used in InfluxDB V2; it is not implemented in V1 and has been&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/future-of-flux/&#34;&gt;abandoned in V3&lt;/a&gt;. The reason is apparent: the learning curve is too steep. Without professional language developers, expanding syntax while fixing various design and implementation issues is almost impossible, resulting in unsustainable engineering costs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, the Structured Query Language, is familiar to data analysts and is based on relational algebra.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Unlike DSLs tailored for specific business scenarios, SQL has a solid theoretical foundation. Since E. F. Codd published the seminal paper “&lt;a href=&#34;https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf&#34;&gt;A Relational Model of Data for Large Shared Data Banks&lt;/a&gt;,” research on relational databases has flourished for over fifty years.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Despite unique extensions in various SQL databases that sometimes confuse users, the basic query and analysis capabilities are consistently implemented across all SQL databases, supported by relational algebra. One or two decades ago, there might have been debates about SQL’s relevance. However, SQL has undoubtedly reasserted itself as the default choice for data analysis today. Over the years, SQL has been continuously improved and expanded, and it is widely adopted globally through a series of proven implementations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL is the primary query language for InfluxDB V3 and GreptimeDB. Both now recommend users analyze time series data using SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In GreptimeDB,&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql&#34;&gt;you can use standard SQL to query your data&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT ts, idc, AVG(memory_util)&#xA;FROM system_metrics&#xA;GROUP BY idc&#xA;ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The solid theoretical foundation of SQL helps emerging time series databases reliably implement complex query logic and data management tasks. Also, the broader SQL ecosystem enables emerging time series databases to quickly integrate into the data analysis tech stack. For example, in the previous&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-03-19-keyboard-monitoring&#34;&gt;input behavior analysis demos&lt;/a&gt;, we showcase an integration between GreptimeDB and Streamlit for visualizing time series by leveraging GreptimeDB’s MySQL protocol support.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;challenges-in-time-series-analysis&#34;&gt;Challenges in Time Series Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#challenges-in-time-series-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql-1&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While SQL has a solid theoretical foundation and a broader analytical ecosystem, traditional SQL databases suffer when handling time series data, primarily due to their large size.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The value provided from a single data point of a time series is often very low. Most metrics uploaded by devices aren’t explicitly handled, and the healthy status reported doesn’t require special attention. Thus, the cost-efficiency of storing time series data is crucial. How to leverage modern cloud commodity storage to reduce costs and use cutting-edge compression for time series data are key points for time series databases.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Furthermore, extracting essential information efficiently from vast amounts of time series data often requires specific query extensions for optimization. GreptimeDB’s support for&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY&lt;/a&gt;&amp;nbsp;to help users analyze data aggregation within specific time windows is one such example.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux-1&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The learning curve itself essentially doomed this dialect. As mentioned above, being a DSL solely supported by a single provider, Flux faced significant challenges in language robustness, performance optimization, and ecosystem development. The sole provider has since abandoned further development of Flux, making it a language of the past.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql-1&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL syntax resembles SQL, subtle differences can be frustrating. Despite efforts to mimic SQL syntax, InfluxQL fundamentally remains a DSL tailored to time series analysis needs focusing on metrics. Its challenges in development and maintenance costs are similar to those faced by Flux.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For example, InfluxQL does not support&amp;nbsp;&lt;code&gt;JOIN&lt;/code&gt;&amp;nbsp;queries. Although one can write queries like&amp;nbsp;&lt;code&gt;SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&lt;/code&gt;, it simply reads data from both measurements separately:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&amp;gt; SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&#xA;&#xA;name: h2o_feet&#xA;--------------&#xA;time                   level description      location       pH   water_level&#xA;2015-08-18T00:00:00Z   below 3 feet           santa_monica        2.064&#xA;2015-08-18T00:00:00Z   between 6 and 9 feet   coyote_creek        8.12&#xA;[...]&#xA;2015-09-18T21:36:00Z   between 3 and 6 feet   santa_monica        5.066&#xA;2015-09-18T21:42:00Z   between 3 and 6 feet   santa_monica        4.938&#xA;&#xA;name: h2o_pH&#xA;------------&#xA;time                   level description   location       pH   water_level&#xA;2015-08-18T00:00:00Z                       santa_monica   6&#xA;2015-08-18T00:00:00Z                       coyote_creek   7&#xA;[...]&#xA;2015-09-18T21:36:00Z                       santa_monica   8&#xA;2015-09-18T21:42:00Z                       santa_monica   7&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Moreover, despite InfluxDB V3 supporting InfluxQL due to strong user demand to facilitate migration, InfluxDB V3 primarily promotes SQL-based queries. Thus, it’s fair to say that InfluxQL is also fading away.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;migrating-to-sql-analysis&#34;&gt;Migrating to SQL Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#migrating-to-sql-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Today, many existing time series data analysis logics are written in InfluxQL. This section outlines the core differences between InfluxQL and SQL and illustrates how to migrate from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;timestamp-column&#34;&gt;Timestamp Column&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#timestamp-column&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;A key difference in application logic migration is that&amp;nbsp;&lt;strong&gt;SQL does not treat the time column especially, while InfluxQL returns the time column by default and sorts results in ascending order by timestamp&lt;/strong&gt;. SQL queries need to explicitly specify the time column to include timestamps in the result set and manually specify sorting logic.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;SELECT &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;SELECT ts, location, water_level FROM h2o_feet ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When writing data, InfluxQL automatically populates the time column with the current time, whereas SQL requires manual specification of the time column value. If using the current time, it must be explicitly written:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;INSERT INTO &#34;measurement&#34; (tag, value) VALUES (&#39;my_tag&#39;, 42);&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support inserting multiple rows in one INSERT statement, whereas SQL databases typically support this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag_0&#39;, 42), (NOW(), &#39;my_tag_1&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, InfluxQL uses the&amp;nbsp;&lt;code&gt;tz()&lt;/code&gt;&amp;nbsp;function to specify the query timezone, while SQL typically has other ways to set the timezone. GreptimeDB supports&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/mysql#time-zone&#34;&gt;MySQL&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/postgresql#time-zone&#34;&gt;PostgreSQL&lt;/a&gt;&amp;nbsp;syntax for setting the timezone.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;time-series&#34;&gt;Time Series&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#time-series&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL implements time series granularity query syntax, such as SLIMIT and&amp;nbsp;&lt;code&gt;SOFFSET&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;SLIMIT&lt;/code&gt;&amp;nbsp;limits the number of data points returned for each time series in the result set. For example,&amp;nbsp;&lt;code&gt;SLIMIT 1&lt;/code&gt;&amp;nbsp;means, at most, one result per time series that meets the filter condition.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, not specifically designed for time series data analysis, requires some workarounds:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT DISTINCT ON (host) * FROM monitor ORDER BY host, ts DESC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This query returns one result per time series, distinguished by the host tag:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;+-----------+---------------------+------+--------+&#xA;| host      | ts                  | cpu  | memory |&#xA;+-----------+---------------------+------+--------+&#xA;| 127.0.0.1 | 2022-11-03 03:39:58 |  0.5 |    0.2 |&#xA;| 127.0.0.2 | 2022-11-03 03:39:58 |  0.2 |    0.3 |&#xA;+-----------+---------------------+------+--------+&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;interval-literals&#34;&gt;Interval Literals&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#interval-literals&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL’s interval syntax resembles&amp;nbsp;&lt;code&gt;1d&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;12m&lt;/code&gt;, while SQL has standard syntax for time intervals:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INTERVAL &#39;1 DAY&#39;&#xA;INTERVAL &#39;1 YEAR 3 HOURS 20 MINUTES&#39;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;data-columns-and-tag-columns&#34;&gt;Data Columns and Tag Columns&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#data-columns-and-tag-columns&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL distinguishes between data columns and tag columns at the model level; queries that only SELECT tag columns will not return data. InfluxQL also supports the&amp;nbsp;&lt;code&gt;::field&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;::tag&lt;/code&gt;&amp;nbsp;suffixes to specify data columns or tag columns, allowing for columns with the same name.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL standards do not differentiate between data columns and tag columns, treating them all as regular columns. However, specific implementations may map these concepts differently. For example, GreptimeDB’s&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/concepts/data-model&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;data model&lt;/a&gt;&amp;nbsp;distinguishes between timestamp columns, tag columns, and data columns and has corresponding mapping rules.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image1.png&#34; alt=&#34;The Data Structure of GreptimeDB&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;The Data Structure of GreptimeDB&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;function-names&#34;&gt;Function Names&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#function-names&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Some function names differ between InfluxQL and SQL. For instance, the&amp;nbsp;&lt;code&gt;MEAN&lt;/code&gt;&amp;nbsp;function in InfluxQL corresponds to the&amp;nbsp;&lt;code&gt;AVG&lt;/code&gt;&amp;nbsp;function in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, many other functions, such as&amp;nbsp;&lt;code&gt;COUNT&lt;/code&gt;,&amp;nbsp;&lt;code&gt;SUM&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;MIN,&lt;/code&gt;&amp;nbsp;remain the same in both languages.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;identifiers&#34;&gt;Identifiers&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#identifiers&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In InfluxQL, identifiers are always double-quoted, while SQL supports unquoted identifiers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It is worth noting that SQL identifiers are case-insensitive by default. If case sensitivity is needed, the identifiers should be enclosed in the appropriate quotes. In GreptimeDB, double quotes are used by default. However, when connecting via MySQL or PostgreSQL clients, the corresponding dialect’s syntax is respected.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Examples of identifier usage differences between InfluxQL and SQL are as follows:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image2.png&#34; alt=&#34;The Usage Differences between InfluxQL and SQL&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;join&#34;&gt;JOIN&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#join&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support JOIN queries, while one of the fundamental capabilities of SQL databases is support for JOIN queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches&lt;/em&gt;&#xA;SELECT a.* FROM system_metrics a JOIN idc_info b ON a.idc = b.idc_id;&#xA;&#xA;&lt;em&gt;-- Select all rows from the idc_info table and system_metrics table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT a.* FROM idc_info a LEFT JOIN system_metrics b ON a.idc_id = b.idc;&#xA;&#xA;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT b.* FROM system_metrics a RIGHT JOIN idc_info b ON a.idc = b.idc_id;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These are examples of&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/reference/sql/join&#34;&gt;JOIN queries in GreptimeDB&lt;/a&gt;, which supports:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;INNER JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;LEFT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;RIGHT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;FULL OUTER JOIN&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;queries-over-time-windows&#34;&gt;Queries over Time Windows&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#queries-over-time-windows&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL’s&amp;nbsp;&lt;code&gt;GROUP BY&lt;/code&gt;&amp;nbsp;statement supports passing a time window to aggregate data within a specific length of time windows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL does not have such specific query capabilities; the closest equivalent is the&amp;nbsp;&lt;code&gt;OVER ... PARTITION BY&lt;/code&gt;&amp;nbsp;syntax, which can be quite complex to understand.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB implements its own&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY extension syntax&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT &#xA;    ts, &#xA;    host, &#xA;    avg(cpu) RANGE &#39;10s&#39; FILL LINEAR&#xA;FROM monitor&#xA;ALIGN &#39;5s&#39; TO &#39;2023-12-01T00:00:00&#39; BY (host) ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;continuous-aggregation&#34;&gt;Continuous Aggregation&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#continuous-aggregation&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL supports&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/continuous_queries/&#34;&gt;continuous aggregation&lt;/a&gt;, which corresponds to the standard concept of materialized views in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, the implementation of materialized views in most SQL databases is still fragile and remains an area for further exploration.&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-06-04-flow-engine&#34;&gt;GreptimeDB supports continuous aggregation&lt;/a&gt;&amp;nbsp;to meet these needs based on its flow engine.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#conclusion&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. While InfluxQL and Flux are used by InfluxDB and specifically created for handling time series data, SQL is a widely used query language in relational databases. Its robust theoretical foundation and rich ecosystem allow data analysts to quickly get started and use effective tools for time series data analysis.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB natively supports SQL queries. Visit our&amp;nbsp;&lt;a href=&#34;https://greptime.com/&#34;&gt;homepage&lt;/a&gt;&amp;nbsp;for more information or&amp;nbsp;&lt;a href=&#34;https://console.greptime.cloud/signup&#34;&gt;create a free cloud service&lt;/a&gt;&amp;nbsp;instance to start your trial today.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;about-greptime&#34;&gt;About Greptime&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#about-greptime&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We help industries that generate large amounts of time-series data, such as Connected Vehicles (CV), IoT, and Observability, to efficiently uncover the hidden value of data in real-time.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Visit the&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;latest version&lt;/a&gt;&amp;nbsp;from any device to get started and get the most out of your data.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GreptimeDB&lt;/a&gt;, written in Rust, is a distributed, open-source, time-series database designed for scalability, efficiency, and powerful analytics.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://greptime.com/product/carcloud&#34;&gt;Edge-Cloud Integrated TSDB&lt;/a&gt;&amp;nbsp;is designed for the unique demands of edge storage and compute in IoT. It tackles the exponential growth of edge data by integrating a multimodal edge-side database with cloud-based GreptimeDB Enterprise. This combination reduces traffic, computing, and storage costs while enhancing data timeliness and business insights.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.greptime.com/product/cloud&#34;&gt;GreptimeCloud&lt;/a&gt;&amp;nbsp;is a fully-managed cloud database-as-a-service (DBaaS) solution built on GreptimeDB. It efficiently supports applications in fields such as observability, IoT, and finance.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Star us on&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GitHub&lt;/a&gt;&amp;nbsp;or join GreptimeDB Community on&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/slack&#34;&gt;Slack&lt;/a&gt;&amp;nbsp;to get connected. Also, you can go to our&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb/contribute&#34;&gt;contribution page&lt;/a&gt;&amp;nbsp;to find some interesting issues to start with.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql&#34;&gt;Greptime’s blog&lt;/a&gt; by tison&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. SQL is a more common and general language for querying time series data, making migrating from InfluxQL to SQL a growing trend.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/coverimage1.png&#34; alt=&#34;Image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB uses SQL as its primary query language. Once users ingest data into GreptimeDB via the InfluxDB line protocol or other APIs, a common question arises: how can I analyze the data ingested? Specifically, how can existing InfluxQL queries be migrated to SQL queries?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To address the question above, this article outlines the differences between the query languages of InfluxDB (InfluxQL or Flux) and SQL, as well as a cheat sheet for migrating from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;overview-of-query-languages&#34;&gt;Overview of Query Languages&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#overview-of-query-languages&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/&#34;&gt;InfluxQL&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V1. It’s a SQL-like query language but not a SQL dialect. Below are some examples of InfluxQL queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT * FROM h2o_feet;&#xA;SELECT * FROM h2o_feet LIMIT 5;&#xA;SELECT COUNT(&#34;water_level&#34;) FROM h2o_feet;&#xA;SELECT &#34;level description&#34;, &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;SELECT *::field FROM &#34;h2o_feet&#34;;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When InfluxDB was designed and developed, there weren’t as many database developers as today. Consequently, despite InfluxQL’s efforts to closely resemble SQL syntax, implementing basic SQL capabilities supported by relational algebra and adding time series query extensions was quite challenging.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL instead implemented functions and syntax specifically designed for time series data analysis. For instance, all InfluxQL queries default to returning the timestamp column in ascending order, and all queries must include field columns to return results.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, special query syntax is designed for querying over time series rather than rows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Essentially, InfluxQL was developed from the raw need for time series data analysis focused on numerical metrics. As InfluxDB evolved, InfluxQL also supported continuous queries and retention policies to solve some requirements of real-time data processing.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL can still be used in InfluxDB V2, it faces&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v2/query-data/influxql/&#34;&gt;a series of challenges due to model mismatches&lt;/a&gt;, as InfluxDB V2 mainly promotes the Flux query language.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/&#34;&gt;Flux&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V2. Unlike InfluxQL, which has a SQL-like syntax, Flux uses a DataFrame style syntax. Developers who have written programs in Elixir will find the syntax familiar. Here are some examples of Flux queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;erlang&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;from(bucket: &#34;example-bucket&#34;)&#xA;    |&amp;gt; range(start: -1d)&#xA;    |&amp;gt; filter(fn: (r) =&amp;gt; r._measurement == &#34;example-measurement&#34;)&#xA;    |&amp;gt; mean()&#xA;    |&amp;gt; yield(name: &#34;_result&#34;)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Designed to support joint analysis of time series data across various data sources, Flux allows users to fetch data from time series databases (InfluxDB), relational databases (PostgreSQL or MySQL), and CSV files for analysis. For example,&amp;nbsp;&lt;code&gt;sql.from&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;csv.from&lt;/code&gt;&amp;nbsp;can replace&amp;nbsp;&lt;code&gt;from(bucket)&lt;/code&gt;&amp;nbsp;in the example above, allowing fetching data from other sources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Flux can only be used in InfluxDB V2; it is not implemented in V1 and has been&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/future-of-flux/&#34;&gt;abandoned in V3&lt;/a&gt;. The reason is apparent: the learning curve is too steep. Without professional language developers, expanding syntax while fixing various design and implementation issues is almost impossible, resulting in unsustainable engineering costs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, the Structured Query Language, is familiar to data analysts and is based on relational algebra.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Unlike DSLs tailored for specific business scenarios, SQL has a solid theoretical foundation. Since E. F. Codd published the seminal paper “&lt;a href=&#34;https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf&#34;&gt;A Relational Model of Data for Large Shared Data Banks&lt;/a&gt;,” research on relational databases has flourished for over fifty years.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Despite unique extensions in various SQL databases that sometimes confuse users, the basic query and analysis capabilities are consistently implemented across all SQL databases, supported by relational algebra. One or two decades ago, there might have been debates about SQL’s relevance. However, SQL has undoubtedly reasserted itself as the default choice for data analysis today. Over the years, SQL has been continuously improved and expanded, and it is widely adopted globally through a series of proven implementations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL is the primary query language for InfluxDB V3 and GreptimeDB. Both now recommend users analyze time series data using SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In GreptimeDB,&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql&#34;&gt;you can use standard SQL to query your data&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT ts, idc, AVG(memory_util)&#xA;FROM system_metrics&#xA;GROUP BY idc&#xA;ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The solid theoretical foundation of SQL helps emerging time series databases reliably implement complex query logic and data management tasks. Also, the broader SQL ecosystem enables emerging time series databases to quickly integrate into the data analysis tech stack. For example, in the previous&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-03-19-keyboard-monitoring&#34;&gt;input behavior analysis demos&lt;/a&gt;, we showcase an integration between GreptimeDB and Streamlit for visualizing time series by leveraging GreptimeDB’s MySQL protocol support.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;challenges-in-time-series-analysis&#34;&gt;Challenges in Time Series Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#challenges-in-time-series-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql-1&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While SQL has a solid theoretical foundation and a broader analytical ecosystem, traditional SQL databases suffer when handling time series data, primarily due to their large size.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The value provided from a single data point of a time series is often very low. Most metrics uploaded by devices aren’t explicitly handled, and the healthy status reported doesn’t require special attention. Thus, the cost-efficiency of storing time series data is crucial. How to leverage modern cloud commodity storage to reduce costs and use cutting-edge compression for time series data are key points for time series databases.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Furthermore, extracting essential information efficiently from vast amounts of time series data often requires specific query extensions for optimization. GreptimeDB’s support for&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY&lt;/a&gt;&amp;nbsp;to help users analyze data aggregation within specific time windows is one such example.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux-1&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The learning curve itself essentially doomed this dialect. As mentioned above, being a DSL solely supported by a single provider, Flux faced significant challenges in language robustness, performance optimization, and ecosystem development. The sole provider has since abandoned further development of Flux, making it a language of the past.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql-1&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL syntax resembles SQL, subtle differences can be frustrating. Despite efforts to mimic SQL syntax, InfluxQL fundamentally remains a DSL tailored to time series analysis needs focusing on metrics. Its challenges in development and maintenance costs are similar to those faced by Flux.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For example, InfluxQL does not support&amp;nbsp;&lt;code&gt;JOIN&lt;/code&gt;&amp;nbsp;queries. Although one can write queries like&amp;nbsp;&lt;code&gt;SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&lt;/code&gt;, it simply reads data from both measurements separately:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&amp;gt; SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&#xA;&#xA;name: h2o_feet&#xA;--------------&#xA;time                   level description      location       pH   water_level&#xA;2015-08-18T00:00:00Z   below 3 feet           santa_monica        2.064&#xA;2015-08-18T00:00:00Z   between 6 and 9 feet   coyote_creek        8.12&#xA;[...]&#xA;2015-09-18T21:36:00Z   between 3 and 6 feet   santa_monica        5.066&#xA;2015-09-18T21:42:00Z   between 3 and 6 feet   santa_monica        4.938&#xA;&#xA;name: h2o_pH&#xA;------------&#xA;time                   level description   location       pH   water_level&#xA;2015-08-18T00:00:00Z                       santa_monica   6&#xA;2015-08-18T00:00:00Z                       coyote_creek   7&#xA;[...]&#xA;2015-09-18T21:36:00Z                       santa_monica   8&#xA;2015-09-18T21:42:00Z                       santa_monica   7&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Moreover, despite InfluxDB V3 supporting InfluxQL due to strong user demand to facilitate migration, InfluxDB V3 primarily promotes SQL-based queries. Thus, it’s fair to say that InfluxQL is also fading away.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;migrating-to-sql-analysis&#34;&gt;Migrating to SQL Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#migrating-to-sql-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Today, many existing time series data analysis logics are written in InfluxQL. This section outlines the core differences between InfluxQL and SQL and illustrates how to migrate from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;timestamp-column&#34;&gt;Timestamp Column&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#timestamp-column&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;A key difference in application logic migration is that&amp;nbsp;&lt;strong&gt;SQL does not treat the time column especially, while InfluxQL returns the time column by default and sorts results in ascending order by timestamp&lt;/strong&gt;. SQL queries need to explicitly specify the time column to include timestamps in the result set and manually specify sorting logic.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;SELECT &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;SELECT ts, location, water_level FROM h2o_feet ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When writing data, InfluxQL automatically populates the time column with the current time, whereas SQL requires manual specification of the time column value. If using the current time, it must be explicitly written:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;INSERT INTO &#34;measurement&#34; (tag, value) VALUES (&#39;my_tag&#39;, 42);&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support inserting multiple rows in one INSERT statement, whereas SQL databases typically support this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag_0&#39;, 42), (NOW(), &#39;my_tag_1&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, InfluxQL uses the&amp;nbsp;&lt;code&gt;tz()&lt;/code&gt;&amp;nbsp;function to specify the query timezone, while SQL typically has other ways to set the timezone. GreptimeDB supports&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/mysql#time-zone&#34;&gt;MySQL&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/postgresql#time-zone&#34;&gt;PostgreSQL&lt;/a&gt;&amp;nbsp;syntax for setting the timezone.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;time-series&#34;&gt;Time Series&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#time-series&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL implements time series granularity query syntax, such as SLIMIT and&amp;nbsp;&lt;code&gt;SOFFSET&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;SLIMIT&lt;/code&gt;&amp;nbsp;limits the number of data points returned for each time series in the result set. For example,&amp;nbsp;&lt;code&gt;SLIMIT 1&lt;/code&gt;&amp;nbsp;means, at most, one result per time series that meets the filter condition.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, not specifically designed for time series data analysis, requires some workarounds:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT DISTINCT ON (host) * FROM monitor ORDER BY host, ts DESC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This query returns one result per time series, distinguished by the host tag:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;+-----------+---------------------+------+--------+&#xA;| host      | ts                  | cpu  | memory |&#xA;+-----------+---------------------+------+--------+&#xA;| 127.0.0.1 | 2022-11-03 03:39:58 |  0.5 |    0.2 |&#xA;| 127.0.0.2 | 2022-11-03 03:39:58 |  0.2 |    0.3 |&#xA;+-----------+---------------------+------+--------+&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;interval-literals&#34;&gt;Interval Literals&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#interval-literals&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL’s interval syntax resembles&amp;nbsp;&lt;code&gt;1d&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;12m&lt;/code&gt;, while SQL has standard syntax for time intervals:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INTERVAL &#39;1 DAY&#39;&#xA;INTERVAL &#39;1 YEAR 3 HOURS 20 MINUTES&#39;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;data-columns-and-tag-columns&#34;&gt;Data Columns and Tag Columns&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#data-columns-and-tag-columns&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL distinguishes between data columns and tag columns at the model level; queries that only SELECT tag columns will not return data. InfluxQL also supports the&amp;nbsp;&lt;code&gt;::field&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;::tag&lt;/code&gt;&amp;nbsp;suffixes to specify data columns or tag columns, allowing for columns with the same name.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL standards do not differentiate between data columns and tag columns, treating them all as regular columns. However, specific implementations may map these concepts differently. For example, GreptimeDB’s&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/concepts/data-model&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;data model&lt;/a&gt;&amp;nbsp;distinguishes between timestamp columns, tag columns, and data columns and has corresponding mapping rules.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image1.png&#34; alt=&#34;The Data Structure of GreptimeDB&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;The Data Structure of GreptimeDB&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;function-names&#34;&gt;Function Names&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#function-names&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Some function names differ between InfluxQL and SQL. For instance, the&amp;nbsp;&lt;code&gt;MEAN&lt;/code&gt;&amp;nbsp;function in InfluxQL corresponds to the&amp;nbsp;&lt;code&gt;AVG&lt;/code&gt;&amp;nbsp;function in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, many other functions, such as&amp;nbsp;&lt;code&gt;COUNT&lt;/code&gt;,&amp;nbsp;&lt;code&gt;SUM&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;MIN,&lt;/code&gt;&amp;nbsp;remain the same in both languages.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;identifiers&#34;&gt;Identifiers&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#identifiers&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In InfluxQL, identifiers are always double-quoted, while SQL supports unquoted identifiers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It is worth noting that SQL identifiers are case-insensitive by default. If case sensitivity is needed, the identifiers should be enclosed in the appropriate quotes. In GreptimeDB, double quotes are used by default. However, when connecting via MySQL or PostgreSQL clients, the corresponding dialect’s syntax is respected.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Examples of identifier usage differences between InfluxQL and SQL are as follows:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image2.png&#34; alt=&#34;The Usage Differences between InfluxQL and SQL&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;join&#34;&gt;JOIN&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#join&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support JOIN queries, while one of the fundamental capabilities of SQL databases is support for JOIN queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches&lt;/em&gt;&#xA;SELECT a.* FROM system_metrics a JOIN idc_info b ON a.idc = b.idc_id;&#xA;&#xA;&lt;em&gt;-- Select all rows from the idc_info table and system_metrics table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT a.* FROM idc_info a LEFT JOIN system_metrics b ON a.idc_id = b.idc;&#xA;&#xA;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT b.* FROM system_metrics a RIGHT JOIN idc_info b ON a.idc = b.idc_id;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These are examples of&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/reference/sql/join&#34;&gt;JOIN queries in GreptimeDB&lt;/a&gt;, which supports:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;INNER JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;LEFT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;RIGHT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;FULL OUTER JOIN&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;queries-over-time-windows&#34;&gt;Queries over Time Windows&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#queries-over-time-windows&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL’s&amp;nbsp;&lt;code&gt;GROUP BY&lt;/code&gt;&amp;nbsp;statement supports passing a time window to aggregate data within a specific length of time windows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL does not have such specific query capabilities; the closest equivalent is the&amp;nbsp;&lt;code&gt;OVER ... PARTITION BY&lt;/code&gt;&amp;nbsp;syntax, which can be quite complex to understand.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB implements its own&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY extension syntax&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT &#xA;    ts, &#xA;    host, &#xA;    avg(cpu) RANGE &#39;10s&#39; FILL LINEAR&#xA;FROM monitor&#xA;ALIGN &#39;5s&#39; TO &#39;2023-12-01T00:00:00&#39; BY (host) ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;continuous-aggregation&#34;&gt;Continuous Aggregation&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#continuous-aggregation&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL supports&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/continuous_queries/&#34;&gt;continuous aggregation&lt;/a&gt;, which corresponds to the standard concept of materialized views in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, the implementation of materialized views in most SQL databases is still fragile and remains an area for further exploration.&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-06-04-flow-engine&#34;&gt;GreptimeDB supports continuous aggregation&lt;/a&gt;&amp;nbsp;to meet these needs based on its flow engine.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#conclusion&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. While InfluxQL and Flux are used by InfluxDB and specifically created for handling time series data, SQL is a widely used query language in relational databases. Its robust theoretical foundation and rich ecosystem allow data analysts to quickly get started and use effective tools for time series data analysis.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB natively supports SQL queries. Visit our&amp;nbsp;&lt;a href=&#34;https://greptime.com/&#34;&gt;homepage&lt;/a&gt;&amp;nbsp;for more information or&amp;nbsp;&lt;a href=&#34;https://console.greptime.cloud/signup&#34;&gt;create a free cloud service&lt;/a&gt;&amp;nbsp;instance to start your trial today.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;about-greptime&#34;&gt;About Greptime&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#about-greptime&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We help industries that generate large amounts of time-series data, such as Connected Vehicles (CV), IoT, and Observability, to efficiently uncover the hidden value of data in real-time.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Visit the&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;latest version&lt;/a&gt;&amp;nbsp;from any device to get started and get the most out of your data.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GreptimeDB&lt;/a&gt;, written in Rust, is a distributed, open-source, time-series database designed for scalability, efficiency, and powerful analytics.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://greptime.com/product/carcloud&#34;&gt;Edge-Cloud Integrated TSDB&lt;/a&gt;&amp;nbsp;is designed for the unique demands of edge storage and compute in IoT. It tackles the exponential growth of edge data by integrating a multimodal edge-side database with cloud-based GreptimeDB Enterprise. This combination reduces traffic, computing, and storage costs while enhancing data timeliness and business insights.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.greptime.com/product/cloud&#34;&gt;GreptimeCloud&lt;/a&gt;&amp;nbsp;is a fully-managed cloud database-as-a-service (DBaaS) solution built on GreptimeDB. It efficiently supports applications in fields such as observability, IoT, and finance.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Star us on&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GitHub&lt;/a&gt;&amp;nbsp;or join GreptimeDB Community on&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/slack&#34;&gt;Slack&lt;/a&gt;&amp;nbsp;to get connected. Also, you can go to our&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb/contribute&#34;&gt;contribution page&lt;/a&gt;&amp;nbsp;to find some interesting issues to start with.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Tue, 09 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Organising the first KCD Hyderabad – my amazing experience】组织第一届 KCD 海得拉巴——我的奇妙经历</title>
      <link>https://www.cncf.io/blog/2024/07/15/organising-the-first-kcd-hyderabad-my-amazing-experience/</link>
      <description>【&lt;p&gt;&lt;em&gt;KCD post originally published on &lt;a href=&#34;https://socialmaharaj.com/2024/06/29/first-kcd-hyderabad-experience/&#34;&gt;Social Maharaj &lt;/a&gt;by Atulpriya Sharma&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Being a food and travel blogger, I often attend a lot of food meet-ups where I get to experience different dishes and meet new people as well.&amp;nbsp;&lt;strong&gt;But did you know there are a lot of tech events too?&lt;/strong&gt;&amp;nbsp;Yes, if you asked me a couple of years ago, I would have stared at you like a wall. But today, things are different.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Looking at the cloud native landscape, there are a lot of local, regional, national and international conferences and meet-ups that are organised. Locally, we have&amp;nbsp;&lt;a href=&#34;https://community.cncf.io/hyderabad/&#34;&gt;CNCF Hyderabad&lt;/a&gt;&amp;nbsp;meet-ups that I organise monthly, regionally there are bigger conferences like&amp;nbsp;&lt;strong&gt;Kubernetes Community Days (KCDs)&amp;nbsp;&lt;/strong&gt;that happen at the city level KCD Bengaluru, KCD Chennai, KCD Hyderabad etc., nationally there are&amp;nbsp;&lt;strong&gt;KubeDay&lt;/strong&gt;&amp;nbsp;events&amp;nbsp;&lt;em&gt;(the last one in India was in Bengaluru last year)&amp;nbsp;&lt;/em&gt;and international ones are KubeCon. Fortunately, I got to attend&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2023/11/21/my-kubecon-chicago-2023-experience/&#34;&gt;KubeCon Chicago&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2024/04/06/kubecon-paris-2024-experience/&#34;&gt;KubeCon Paris&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the last couple of years that I’ve been in the cloud native space, I’ve attended and spoken at a lot of meet-ups and conferences be it KubeCon or KCDs. But I was never on the organising side of things. However, over the last 6 months or so I have been busy with a couple of other folks to organise Hyderabad’s First KCD event.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1024&#34; height=&#34;576&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-3.jpg&#34; alt=&#34;Atulpriya giving a talk&#34; class=&#34;wp-image-114131&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-3.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-300x169.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-768x432.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-900x506.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-356x200.jpg 356w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-711x400.jpg 711w&#34; sizes=&#34;(max-width: 1024px) 100vw, 1024px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We concluded that on June 22nd, so you can get a glimpse of what happened by browsing through the&amp;nbsp;&lt;a href=&#34;https://twitter.com/hashtag/KCDHyderabad?src=hashtag_click&#34;&gt;#KCDHyderabad&lt;/a&gt;&amp;nbsp;hashtag.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this blog post, I’ll talk about my experience of organising the first KCD Hyderabad, things that I learnt, processes and mistakes that I made.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Preparing for KCD Hyderabad&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Every year,&amp;nbsp;&lt;strong&gt;we can host only 3 KCDs in India&lt;/strong&gt;, so the first one was in Kochi followed by Pune and finally ending the year with KCD Hyderabad. I was a speaker at KCD Kerala and KCD Pune and one of the organisers for KCD Hyderabad.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It all started way back in May 2023 when some of our community members reached out asking if we would ever have a KCD Hyderabad and Hyderabad becoming one of the tech hubs in the country, we thought we must plan and have one. So a GitHub issue was raised in May 2023 and what followed was weekly calls with other interested organisers to prepare, plan and execute.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Finalising the Date&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;June 22, was the date that was reserved for KCD Hyderabad. Internally we spent a week or so to figure out the perfect date considering the holidays, schools, festivals etc. to ensure we reach the maximum people.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Further, the next question was whether we wanted to do it as a multiple-day conference with multiple rooms or a single-day event. Since this was the first time we were doing it, we tried to keep it to one day only.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Venue&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Another big ticket item is the venue. Most other KCDs in India took place at star hotels and convention centres. Being part of the F&amp;amp;B industry, I had connections at&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2021/08/04/staycation-in-hyderabad-novotel-hicc/&#34;&gt;Novotel HICC&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2021/10/16/turquoise-le-meridien-hyderabad-review/&#34;&gt;Le Meridien&lt;/a&gt;’s and other similar hotels. However, even with the connections and discounts, the amount that was quoted was way too much. So we decided to not go for such properties and instead look for smaller venues that weren’t too costly but also could accommodate a decent crowd.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://pbs.twimg.com/media/GQpM2j9XYAE-3z8?format=jpg&amp;amp;name=large&#34; alt=&#34;T-Hub - venue for KCD Hyderabad&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Eventually after scouting for places, we settled for&amp;nbsp;&lt;strong&gt;T-Hub&lt;/strong&gt;&amp;nbsp;opposite Ikea. It’s among the largest incubators in the world and is home to a lot of start-ups. They have multiple spaces that they rent out for events. I had attended meet-ups at T-Hub in the past, so was aware of the space. We eventually decided to go with the two rooms in T-Hub that would accommodate 400-430 people.&amp;nbsp; With this, we were now clear on the number of people we could allow.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Sponsors&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Further, we had to look for sponsors who would sponsor the conference. This is a totally new experience for me. We were reaching out to folks on LinkedIn, Twitter, Email and all other mediums to reach out to folks who might be interested in sponsoring the event.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Luckily we found quite a few folks who were interested in sponsoring the event. By the time we were close to the event day, we had to stop taking sponsorships – not because we didn’t need the money but because we couldn’t fulfil the requirements due to logistical constraints.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Speakers and Agenda&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The most important parts of a tech conference are the speakers and the agenda. People attend conferences to learn and network and that meant we had to carefully choose the speakers and experts that were going to attend KCD Hyderabad.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We opened the&amp;nbsp;&lt;strong&gt;Call For Proposals (CFPs)&lt;/strong&gt;&amp;nbsp;a few months before the event and to our surprise, we had 180 topics that were submitted by over 100 unique speakers. To give you a context, for a single-day event, considering 20 mins for each talk and multiple breaks, we can at max accommodate 10-12 talks.&amp;nbsp;Further, we also had&amp;nbsp;&lt;strong&gt;Mr. Jayesh Ranjan, IAS&lt;/strong&gt;&amp;nbsp;who was the chief guest for the event.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://pbs.twimg.com/media/GQqsxcEXAAALSG3?format=jpg&amp;amp;name=medium&#34; alt=&#34;Jayesh Ranjan, IAS - Chief Guest at KCD Hyderabad&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;With 180 talks, it was a herculean task to review all of them. So I reached out to a few experts in the community to help us review the CPFs and they also took a week to finalise the top talks. With the talks finalised, we had to publish the agenda and inform all the speakers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But guess what? Not all selected speakers respond or can join the event, so we need to have backup speakers as well. Quite a task this was.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Swags&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Another crucial part of any conference is swags and food. Swags are things that the attendees get when they attend a conference. Since KCD Hyderabad like others was a paid event, we had to give them something. So we spent a few weeks figuring out what we could give them.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://media.licdn.com/dms/image/D5622AQGpCw4llMs3Eg/feedshare-shrink_2048_1536/0/1719062626637?e=1722470400&amp;amp;v=beta&amp;amp;t=Jmv_egcrrhfuzMcKNtMDZ171IwE-0xwGrrtKlFHHBf4&#34; alt=&#34;KCD Hyderabad Swags&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Eventually, we settled for a&amp;nbsp;&lt;strong&gt;customised T-shirt, an umbrella, a tote bag&amp;nbsp;&lt;/strong&gt;and&lt;strong&gt;&amp;nbsp;laptop stickers&lt;/strong&gt;. To add to this, we additionally gave a&amp;nbsp;&lt;strong&gt;&lt;a href=&#34;https://amzn.to/3XGbBbV&#34;&gt;Logitech K480 keyboard&lt;/a&gt;&lt;/strong&gt;&amp;nbsp;to all our speakers and a&amp;nbsp;&lt;strong&gt;Portronics Freedom Wireless Charger&lt;/strong&gt;&amp;nbsp;to all the volunteers. Finalising the items was easy, but customising them was a task. I remember visiting local vendors to check the quality of T-shirts, bags and everything else.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Food&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Food is another critical thing. Since we were expecting around 400 people with a lot of people coming from outside of Hyderabad, I was adamant about having a&amp;nbsp;&lt;strong&gt;good Biryani for lunch&lt;/strong&gt;. Further, being the Maharaj, I had to ensure we gave the attendees a taste of Hyderabad. Again, I hunted for eateries and caterers who could give me that experience without burning a hole in my pocket.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I spent a couple of weekends, just visiting restaurants and caterers to finalise them. Eventually, we stuck with&amp;nbsp;&lt;strong&gt;Deccan Kitchen&lt;/strong&gt;, a popular restaurant and caterer in Hyderabad and I guess we did a good job when it came to food. People loved the food and Biryani especially, so I was relieved.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So these were some of the major elements that we had to look at while organising the first KCD in Hyderabad. Over the last 6 months or so, we split our responsibilities to focus on things that we’re good at and started working towards it. Hundreds of Google meets and calls, in-person meets and eventually, KCD Hyderabad was a success! With over&amp;nbsp;&lt;strong&gt;400 people in attendance from across the globe&lt;/strong&gt;, it was quite an event.&amp;nbsp; You can get a glimpse of what happened by browsing through the&amp;nbsp;&lt;a href=&#34;https://twitter.com/hashtag/KCDHyderabad?src=hashtag_click&#34;&gt;#KCDHyderabad&amp;nbsp;&lt;/a&gt;hashtag.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But as they say, there’s learning in everything. Organising an event of a scale of KCD isn’t easy. There are so many things and people involved that you need to ensure that everything is well-planned and executed smoothly. Here are my 5 key learnings after organizing KCD Hyderabad.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;5 Key Learnings&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Planning and preparation are crucial&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I learned that thorough planning is essential. We started preparing months in advance, from choosing the date to finding the right venue. We had to consider holidays, school schedules, and festivals to maximize attendance. Every detail, from deciding on a single-day event to selecting the venue, required careful thought and discussion among organizers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Further,&amp;nbsp;&lt;strong&gt;things don’t work as planned&lt;/strong&gt;. From spending the entire night at the venue to ensure the event setup was done, to only realising that even on the day of the event we were still not complete. So, always be prepared for eventualities, they will come from nowhere.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Budget management is a balancing act&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Managing the budget was challenging. We had to make smart choices to keep costs down without compromising on quality. For example, instead of expensive hotels, we chose T-Hub, which was more affordable but still accommodated our needs. We also had to be creative with sponsorships, reaching out through various channels to secure funding while&amp;nbsp;&lt;strong&gt;being careful not to over-commit&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Content curation is both exciting and challenging&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Selecting speakers and creating the agenda was one of the most crucial and difficult tasks. We received 180 talk proposals for only 10-12 available slots. It took a team of experts to review and select the best ones.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I learned that it’s important to have backup speakers too, as not all selected speakers can always make it. Further, with most sponsors wanting a slot to speak, it’s difficult to maintain the balance of community vs sponsored talks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Attention to attendee experience pays off&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I realized how important it is to focus on the attendee experience. This included carefully choosing swag items like customized T-shirts, umbrellas, and laptop stickers. We also put a lot of effort into selecting good food, especially ensuring we had great biryani to showcase Hyderabad’s cuisine.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The positive feedback on these aspects was really rewarding. Sure there were a few niggles where some people told the queues were too long and things like that, but overall people were quite satisfied.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Teamwork and delegation are key&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Organizing an event of this scale taught me the importance of teamwork and delegation. Over six months, we divided responsibilities based on our strengths. It involved countless meetings, calls, and in-person discussions. I learned that success depends on everyone working together and focusing on their specific tasks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We also looked out for volunteers who could help. A handful of them did turn up eventually and took care of a lot of things on D-day. I felt, we could have leveraged their support a little earlier in the preparation cycle as well.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;When Is The Next KCD Hyderabad?&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Reflecting on our successful first KCD Hyderabad, we’ve learned valuable lessons about thorough planning, budget management, content curation, attendee experience, and teamwork. These insights will be crucial for future organizers.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As for the next KCD Hyderabad, while I’d love to announce a date, it’s not that simple. With only 3 KCDs allowed annually in India, we rotate between cities to reach different communities. Having said that, I’m optimistic about its return given the fantastic response.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the meantime, stay connected with the&amp;nbsp;&lt;a href=&#34;https://community.cncf.io/hyderabad/&#34;&gt;CNCF Hyderabad community&lt;/a&gt;&amp;nbsp;and watch for KCDs in other Indian cities. Thank you to everyone who made our inaugural event special. When KCD Hyderabad returns, we’ll apply these learnings to make it even better.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;That’s it for this blog post. I hope you found this helpful to plan an event like this one in your city. Feel free to reach out to me for any assistance. Drop your thoughts in the comments below, tweet to me at&amp;nbsp;&lt;a href=&#34;https://twitter.com/Atulmaharaj&#34;&gt;@Atulmaharaj&lt;/a&gt;, DM on&amp;nbsp;&lt;a href=&#34;https://instagram.com/Atulmaharaj&#34;&gt;Atulmaharaj on Instagram&lt;/a&gt;, or&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/get-in-touch/&#34;&gt;Get In Touch&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;KCD post originally published on &lt;a href=&#34;https://socialmaharaj.com/2024/06/29/first-kcd-hyderabad-experience/&#34;&gt;Social Maharaj &lt;/a&gt;by Atulpriya Sharma&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Being a food and travel blogger, I often attend a lot of food meet-ups where I get to experience different dishes and meet new people as well.&amp;nbsp;&lt;strong&gt;But did you know there are a lot of tech events too?&lt;/strong&gt;&amp;nbsp;Yes, if you asked me a couple of years ago, I would have stared at you like a wall. But today, things are different.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Looking at the cloud native landscape, there are a lot of local, regional, national and international conferences and meet-ups that are organised. Locally, we have&amp;nbsp;&lt;a href=&#34;https://community.cncf.io/hyderabad/&#34;&gt;CNCF Hyderabad&lt;/a&gt;&amp;nbsp;meet-ups that I organise monthly, regionally there are bigger conferences like&amp;nbsp;&lt;strong&gt;Kubernetes Community Days (KCDs)&amp;nbsp;&lt;/strong&gt;that happen at the city level KCD Bengaluru, KCD Chennai, KCD Hyderabad etc., nationally there are&amp;nbsp;&lt;strong&gt;KubeDay&lt;/strong&gt;&amp;nbsp;events&amp;nbsp;&lt;em&gt;(the last one in India was in Bengaluru last year)&amp;nbsp;&lt;/em&gt;and international ones are KubeCon. Fortunately, I got to attend&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2023/11/21/my-kubecon-chicago-2023-experience/&#34;&gt;KubeCon Chicago&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2024/04/06/kubecon-paris-2024-experience/&#34;&gt;KubeCon Paris&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the last couple of years that I’ve been in the cloud native space, I’ve attended and spoken at a lot of meet-ups and conferences be it KubeCon or KCDs. But I was never on the organising side of things. However, over the last 6 months or so I have been busy with a couple of other folks to organise Hyderabad’s First KCD event.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1024&#34; height=&#34;576&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-3.jpg&#34; alt=&#34;Atulpriya giving a talk&#34; class=&#34;wp-image-114131&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-3.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-300x169.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-768x432.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-900x506.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-356x200.jpg 356w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-3-711x400.jpg 711w&#34; sizes=&#34;(max-width: 1024px) 100vw, 1024px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We concluded that on June 22nd, so you can get a glimpse of what happened by browsing through the&amp;nbsp;&lt;a href=&#34;https://twitter.com/hashtag/KCDHyderabad?src=hashtag_click&#34;&gt;#KCDHyderabad&lt;/a&gt;&amp;nbsp;hashtag.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this blog post, I’ll talk about my experience of organising the first KCD Hyderabad, things that I learnt, processes and mistakes that I made.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Preparing for KCD Hyderabad&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Every year,&amp;nbsp;&lt;strong&gt;we can host only 3 KCDs in India&lt;/strong&gt;, so the first one was in Kochi followed by Pune and finally ending the year with KCD Hyderabad. I was a speaker at KCD Kerala and KCD Pune and one of the organisers for KCD Hyderabad.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It all started way back in May 2023 when some of our community members reached out asking if we would ever have a KCD Hyderabad and Hyderabad becoming one of the tech hubs in the country, we thought we must plan and have one. So a GitHub issue was raised in May 2023 and what followed was weekly calls with other interested organisers to prepare, plan and execute.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Finalising the Date&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;June 22, was the date that was reserved for KCD Hyderabad. Internally we spent a week or so to figure out the perfect date considering the holidays, schools, festivals etc. to ensure we reach the maximum people.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Further, the next question was whether we wanted to do it as a multiple-day conference with multiple rooms or a single-day event. Since this was the first time we were doing it, we tried to keep it to one day only.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Venue&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Another big ticket item is the venue. Most other KCDs in India took place at star hotels and convention centres. Being part of the F&amp;amp;B industry, I had connections at&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2021/08/04/staycation-in-hyderabad-novotel-hicc/&#34;&gt;Novotel HICC&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/2021/10/16/turquoise-le-meridien-hyderabad-review/&#34;&gt;Le Meridien&lt;/a&gt;’s and other similar hotels. However, even with the connections and discounts, the amount that was quoted was way too much. So we decided to not go for such properties and instead look for smaller venues that weren’t too costly but also could accommodate a decent crowd.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://pbs.twimg.com/media/GQpM2j9XYAE-3z8?format=jpg&amp;amp;name=large&#34; alt=&#34;T-Hub - venue for KCD Hyderabad&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Eventually after scouting for places, we settled for&amp;nbsp;&lt;strong&gt;T-Hub&lt;/strong&gt;&amp;nbsp;opposite Ikea. It’s among the largest incubators in the world and is home to a lot of start-ups. They have multiple spaces that they rent out for events. I had attended meet-ups at T-Hub in the past, so was aware of the space. We eventually decided to go with the two rooms in T-Hub that would accommodate 400-430 people.&amp;nbsp; With this, we were now clear on the number of people we could allow.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Sponsors&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Further, we had to look for sponsors who would sponsor the conference. This is a totally new experience for me. We were reaching out to folks on LinkedIn, Twitter, Email and all other mediums to reach out to folks who might be interested in sponsoring the event.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Luckily we found quite a few folks who were interested in sponsoring the event. By the time we were close to the event day, we had to stop taking sponsorships – not because we didn’t need the money but because we couldn’t fulfil the requirements due to logistical constraints.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Speakers and Agenda&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The most important parts of a tech conference are the speakers and the agenda. People attend conferences to learn and network and that meant we had to carefully choose the speakers and experts that were going to attend KCD Hyderabad.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We opened the&amp;nbsp;&lt;strong&gt;Call For Proposals (CFPs)&lt;/strong&gt;&amp;nbsp;a few months before the event and to our surprise, we had 180 topics that were submitted by over 100 unique speakers. To give you a context, for a single-day event, considering 20 mins for each talk and multiple breaks, we can at max accommodate 10-12 talks.&amp;nbsp;Further, we also had&amp;nbsp;&lt;strong&gt;Mr. Jayesh Ranjan, IAS&lt;/strong&gt;&amp;nbsp;who was the chief guest for the event.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://pbs.twimg.com/media/GQqsxcEXAAALSG3?format=jpg&amp;amp;name=medium&#34; alt=&#34;Jayesh Ranjan, IAS - Chief Guest at KCD Hyderabad&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;With 180 talks, it was a herculean task to review all of them. So I reached out to a few experts in the community to help us review the CPFs and they also took a week to finalise the top talks. With the talks finalised, we had to publish the agenda and inform all the speakers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But guess what? Not all selected speakers respond or can join the event, so we need to have backup speakers as well. Quite a task this was.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Swags&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Another crucial part of any conference is swags and food. Swags are things that the attendees get when they attend a conference. Since KCD Hyderabad like others was a paid event, we had to give them something. So we spent a few weeks figuring out what we could give them.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://media.licdn.com/dms/image/D5622AQGpCw4llMs3Eg/feedshare-shrink_2048_1536/0/1719062626637?e=1722470400&amp;amp;v=beta&amp;amp;t=Jmv_egcrrhfuzMcKNtMDZ171IwE-0xwGrrtKlFHHBf4&#34; alt=&#34;KCD Hyderabad Swags&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Eventually, we settled for a&amp;nbsp;&lt;strong&gt;customised T-shirt, an umbrella, a tote bag&amp;nbsp;&lt;/strong&gt;and&lt;strong&gt;&amp;nbsp;laptop stickers&lt;/strong&gt;. To add to this, we additionally gave a&amp;nbsp;&lt;strong&gt;&lt;a href=&#34;https://amzn.to/3XGbBbV&#34;&gt;Logitech K480 keyboard&lt;/a&gt;&lt;/strong&gt;&amp;nbsp;to all our speakers and a&amp;nbsp;&lt;strong&gt;Portronics Freedom Wireless Charger&lt;/strong&gt;&amp;nbsp;to all the volunteers. Finalising the items was easy, but customising them was a task. I remember visiting local vendors to check the quality of T-shirts, bags and everything else.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Food&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Food is another critical thing. Since we were expecting around 400 people with a lot of people coming from outside of Hyderabad, I was adamant about having a&amp;nbsp;&lt;strong&gt;good Biryani for lunch&lt;/strong&gt;. Further, being the Maharaj, I had to ensure we gave the attendees a taste of Hyderabad. Again, I hunted for eateries and caterers who could give me that experience without burning a hole in my pocket.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I spent a couple of weekends, just visiting restaurants and caterers to finalise them. Eventually, we stuck with&amp;nbsp;&lt;strong&gt;Deccan Kitchen&lt;/strong&gt;, a popular restaurant and caterer in Hyderabad and I guess we did a good job when it came to food. People loved the food and Biryani especially, so I was relieved.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So these were some of the major elements that we had to look at while organising the first KCD in Hyderabad. Over the last 6 months or so, we split our responsibilities to focus on things that we’re good at and started working towards it. Hundreds of Google meets and calls, in-person meets and eventually, KCD Hyderabad was a success! With over&amp;nbsp;&lt;strong&gt;400 people in attendance from across the globe&lt;/strong&gt;, it was quite an event.&amp;nbsp; You can get a glimpse of what happened by browsing through the&amp;nbsp;&lt;a href=&#34;https://twitter.com/hashtag/KCDHyderabad?src=hashtag_click&#34;&gt;#KCDHyderabad&amp;nbsp;&lt;/a&gt;hashtag.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But as they say, there’s learning in everything. Organising an event of a scale of KCD isn’t easy. There are so many things and people involved that you need to ensure that everything is well-planned and executed smoothly. Here are my 5 key learnings after organizing KCD Hyderabad.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;5 Key Learnings&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Planning and preparation are crucial&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I learned that thorough planning is essential. We started preparing months in advance, from choosing the date to finding the right venue. We had to consider holidays, school schedules, and festivals to maximize attendance. Every detail, from deciding on a single-day event to selecting the venue, required careful thought and discussion among organizers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Further,&amp;nbsp;&lt;strong&gt;things don’t work as planned&lt;/strong&gt;. From spending the entire night at the venue to ensure the event setup was done, to only realising that even on the day of the event we were still not complete. So, always be prepared for eventualities, they will come from nowhere.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Budget management is a balancing act&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Managing the budget was challenging. We had to make smart choices to keep costs down without compromising on quality. For example, instead of expensive hotels, we chose T-Hub, which was more affordable but still accommodated our needs. We also had to be creative with sponsorships, reaching out through various channels to secure funding while&amp;nbsp;&lt;strong&gt;being careful not to over-commit&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Content curation is both exciting and challenging&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Selecting speakers and creating the agenda was one of the most crucial and difficult tasks. We received 180 talk proposals for only 10-12 available slots. It took a team of experts to review and select the best ones.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I learned that it’s important to have backup speakers too, as not all selected speakers can always make it. Further, with most sponsors wanting a slot to speak, it’s difficult to maintain the balance of community vs sponsored talks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Attention to attendee experience pays off&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I realized how important it is to focus on the attendee experience. This included carefully choosing swag items like customized T-shirts, umbrellas, and laptop stickers. We also put a lot of effort into selecting good food, especially ensuring we had great biryani to showcase Hyderabad’s cuisine.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The positive feedback on these aspects was really rewarding. Sure there were a few niggles where some people told the queues were too long and things like that, but overall people were quite satisfied.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;Teamwork and delegation are key&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Organizing an event of this scale taught me the importance of teamwork and delegation. Over six months, we divided responsibilities based on our strengths. It involved countless meetings, calls, and in-person discussions. I learned that success depends on everyone working together and focusing on their specific tasks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We also looked out for volunteers who could help. A handful of them did turn up eventually and took care of a lot of things on D-day. I felt, we could have leveraged their support a little earlier in the preparation cycle as well.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;When Is The Next KCD Hyderabad?&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Reflecting on our successful first KCD Hyderabad, we’ve learned valuable lessons about thorough planning, budget management, content curation, attendee experience, and teamwork. These insights will be crucial for future organizers.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As for the next KCD Hyderabad, while I’d love to announce a date, it’s not that simple. With only 3 KCDs allowed annually in India, we rotate between cities to reach different communities. Having said that, I’m optimistic about its return given the fantastic response.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the meantime, stay connected with the&amp;nbsp;&lt;a href=&#34;https://community.cncf.io/hyderabad/&#34;&gt;CNCF Hyderabad community&lt;/a&gt;&amp;nbsp;and watch for KCDs in other Indian cities. Thank you to everyone who made our inaugural event special. When KCD Hyderabad returns, we’ll apply these learnings to make it even better.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;That’s it for this blog post. I hope you found this helpful to plan an event like this one in your city. Feel free to reach out to me for any assistance. Drop your thoughts in the comments below, tweet to me at&amp;nbsp;&lt;a href=&#34;https://twitter.com/Atulmaharaj&#34;&gt;@Atulmaharaj&lt;/a&gt;, DM on&amp;nbsp;&lt;a href=&#34;https://instagram.com/Atulmaharaj&#34;&gt;Atulmaharaj on Instagram&lt;/a&gt;, or&amp;nbsp;&lt;a href=&#34;https://socialmaharaj.com/get-in-touch/&#34;&gt;Get In Touch&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Sun, 14 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Slurm: An HPC workload manager】Slurm：HPC 工作负载管理器</title>
      <link>https://www.cncf.io/blog/2024/07/08/slurm-an-hpc-workload-manager/</link>
      <description>【&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/&#34;&gt;SuperOrbital’s blog&lt;/a&gt; by Sean Kane&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/cover.jpg&#34; alt=&#34;datacenter servers&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this article, we are going to explore&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm&#34;&gt;Slurm&lt;/a&gt;, a popular open-source high-performance computing (HPC&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:hpc&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;) workload manager, and discover what it is, why people use it, and how it differs from Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For context, when we talk about high-performance computing, we are primarily talking about computational tasks that require a lot of time on very powerful computer systems and are often designed, so that they can be&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Parallel_computing&#34;&gt;highly parallelized&lt;/a&gt;, across resources within a super-computer or large cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These sorts of jobs are very common in scientific research,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_simulation&#34;&gt;computer simulation&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_intelligence&#34;&gt;artificial intelligence&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&gt;machine learning&lt;/a&gt;&amp;nbsp;tasks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;what-is-slurm&#34;&gt;What is Slurm?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To almost directly quote the Slurm repository&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm/blob/master/README.rst#slurm-workload-manager&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;Slurm is an open-source cluster resource management and job scheduling system for Linux (&lt;em&gt;primarily&lt;/em&gt;) that strives to be simple, scalable, portable, fault-tolerant, and interconnect agnostic. As a cluster resource manager, Slurm provides three key functions:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Second, it provides a framework for starting, executing, and monitoring work (&lt;em&gt;normally a parallel job&lt;/em&gt;) on the set of allocated nodes.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;And finally, it arbitrates conflicting requests for resources by managing a queue of pending work.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In short, Slurm is a system that is primarily designed to manage&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Batch_processing&#34;&gt;batch processing&lt;/a&gt;&amp;nbsp;workloads running in a shared&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_cluster&#34;&gt;computing cluster&lt;/a&gt;. Slurm can be configured to run any type of batch job but is often utilized on systems that support multiple users with parallel computing tasks to run.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;According to&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;, the primary support and development company for the Slurm community, Slurm is being used on approximately 65% of the&amp;nbsp;&lt;a href=&#34;https://top500.org/lists/top500/2024/06/&#34;&gt;TOP500 supercomputers in the world&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;a-bit-of-history&#34;&gt;A bit of history&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the mid-1990s, as Linux became more popular, there was an increased interest in making large-scale computer processing more accessible through the use of commodity hardware and projects like&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Beowulf_cluster&#34;&gt;Beowulf&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In 2001, engineers at the&amp;nbsp;&lt;a href=&#34;https://www.llnl.gov/&#34;&gt;Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;(LLNL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:llnl&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;) in the United States, started looking into how they might migrate from proprietary HPC systems to open-source solutions. They determined that there weren’t any good resource managers available at the time, and this directly led to the creation of Slurm, which was initially released in 2002.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;a href=&#34;https://slurm.schedmd.com/slurm_design.pdf&#34;&gt;The name Slurm is a reference&lt;/a&gt;&amp;nbsp;to the&amp;nbsp;&lt;a href=&#34;https://futurama.fandom.com/wiki/Slurm&#34;&gt;soda&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://www.imdb.com/title/tt0149460/&#34;&gt;Futurama&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the years, Slurm has evolved to support a wide variety of hardware configurations and in 2010, some initial scheduling logic was added to Slurm, enabling it to function as both a stand-alone resource manager and a scheduler&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:history&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-architecture&#34;&gt;The Architecture&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Like many similar systems, a Slurm setup consists of at least one controller node, and at least one compute node, which can be on the same physical systems.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/slurm-layout.gif&#34; alt=&#34;Diagram of the basic Slurm architecture&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;the-controller&#34;&gt;The Controller&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Two daemons are responsible for managing the cluster. These two processes can be run on a single node or separated, depending on the performance and resiliency required by the administrator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmctld&#34;&gt;slurmctld&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmctld&lt;/code&gt;&amp;nbsp;is the primary management daemon for Slurm. It is responsible for all the daemons, resources, and jobs, within the cluster. Because it is a critical component in the cluster, it is often configured with a passive backup that can take over management if the primary daemon ever fails.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmdbd&#34;&gt;slurmdbd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmdbd&lt;/code&gt;&amp;nbsp;provides an interface for the&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/MySQL&#34;&gt;MySQL database&lt;/a&gt;&amp;nbsp;that Slurm uses for stateful data, like job accounting records.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;compute-nodes&#34;&gt;Compute Nodes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These represent the nodes whose resources are available to any jobs that might need to be scheduled in the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmd&#34;&gt;slurmd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmd&lt;/code&gt;&amp;nbsp;is the daemon that runs on each compute node and is responsible for accepting work, launching tasks, and generally managing the workloads on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SchedMD maintains the core documentation for Slurm and the&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/quickstart_admin.html#quick_start&#34;&gt;current installation documentation&lt;/a&gt;&amp;nbsp;can be found on their website.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Getting Slurm working is not particularly difficult, but fine-tuning it for your requirements can be a time-consuming task. Since performance is often a primary focus with these clusters and Slurm is designed to work with a very wide set of hardware and software libraries, it can take a lot of effort to fine-tune the installation, so that it utilizes the hardware to the best of its ability, and all the proper software is available to streamline the types of jobs that will be run on the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;At a minimum clusters will require node name resolution, a MySQL instance, shared block storage, a synchronized time source, synchronized users, the&amp;nbsp;&lt;a href=&#34;https://dun.github.io/munge/&#34;&gt;Munge&lt;/a&gt;&amp;nbsp;daemon for user authentication, a full set of custom-built Slurm executables, and various configuration files.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In addition to this, the nodes will need to contain the various hardware, programming languages, libraries, and other tools that the majority of jobs will require to run. This could include things like GPUs&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:gpu&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;and their related toolchains (CUDA&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cuda&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;/cuDNN&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cudnn&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;), MPI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:mpi&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;libraries, OCI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:oci&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;runtimes for&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/containers.html&#34;&gt;container support&lt;/a&gt;, and much more.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm’s greatest asset and liability is how flexible it is. This makes it something that can be highly optimized for the requirements of the organization, while also making it susceptible to misconfiguration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;workloads&#34;&gt;Workloads&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To provide some context, let’s take a look at an example user job and workflow that utilizes some of the features of Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the&amp;nbsp;&lt;code&gt;mpi-ping-pong.py&lt;/code&gt;&amp;nbsp;code below, you will find a&amp;nbsp;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt;&amp;nbsp;script that simulates a pseudo-game of&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Table_tennis&#34;&gt;Ping Pong/Table Tennis&lt;/a&gt;&amp;nbsp;by making use of MPI to pass a message (&lt;em&gt;“ball”&lt;/em&gt;) between as many processes as we have configured to participate in the parallel job.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;local-testing&#34;&gt;Local Testing&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run this code, you simply need to have&amp;nbsp;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/quickstart.html&#34;&gt;OpenMPI&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://mpi4py.readthedocs.io/en/stable/install.html&#34;&gt;mpi4py&lt;/a&gt;&amp;nbsp;installed on your system.mpi-ping-pong.py&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/mpi-ping-pong.py&#34;&gt;mpi-ping-pong.py&lt;/a&gt;&amp;nbsp;is what many would refer to as an&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Embarrassingly_parallel&#34;&gt;embarrassingly parallel&lt;/a&gt;&amp;nbsp;workload because it can be scaled with no real effort at all.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you run this Python application on your local system, you will see something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ python ./mpi-ping-pong.py&#xA;&#xA;Hello, World! I am 🏓 Player (Rank)  0 . Process  1  of  1  on  laptop.local&#xA;Player (Rank)  0 &#39;s receiving 🏓 neighbor is Player (Rank) 0 and my sending 🏓 neighbor is Player (Rank) 0.&#xA;Player 0 started round 1 by hitting the 🏓 ball towards the wall.&#xA;Player 0 received the 🏓 ball that bounced off the wall.&#xA;Player 0 started round 2 by hitting the 🏓 ball towards the wall.&#xA;Player 0 received the 🏓 ball that bounced off the wall.&#xA;…&#xA;Player 0 started round 299 by hitting the 🏓 ball towards the wall.&#xA;Player 0 received the 🏓 ball that bounced off the wall.&#xA;🎉 Player 0 won the game of 🏓 ping pong, since they were playing alone! 🎉&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As you can see the application runs perfectly fine, and uses a single rank&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:rank&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;/process (&lt;em&gt;CPU&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cpu&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;core, in this case&lt;/em&gt;). Since we only have a single process participating in this game of Ping Pong, the program simply sends and receives messages to and from from itself.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, assuming that we have at least two CPU cores, then we can run this application utilizing as many of them as we want, by leveraging the MPI command line tool&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run 12 instances of our application, which will all communicate with each other in parallel across 12 cores on our system, we can do the following on our local system:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: These messages will be somewhat out of order. This is the nature of parallel computing, buffers, etc. To deal with this you would likely need to send all the messages back to single process for ordering and printing.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ mpirun -np 12 ./mpi-ping-pong.py&#xA;&#xA;Player (Rank)  0 &#39;s receiving 🏓 neighbor is Player (Rank) 11 and my sending 🏓 neighbor is Player (Rank) 1.&#xA;Hello, World! I am 🏓 Player (Rank)  0 . Process  1  of  12  on  laptop.local&#xA;Player (Rank)  1 &#39;s receiving 🏓 neighbor is Player (Rank) 0 and my sending 🏓 neighbor is Player (Rank) 2.&#xA;Hello, World! I am 🏓 Player (Rank)  1 . Process  2  of  12  on  laptop.local&#xA;Player (Rank)  2 &#39;s receiving 🏓 neighbor is Player (Rank) 1 and my sending 🏓 neighbor is Player (Rank) 3.&#xA;Hello, World! I am 🏓 Player (Rank)  2 . Process  3  of  12  on  laptop.local&#xA;…&#xA;🛑 Player 0 is bored of 🏓 ping pong and is quitting! 🛑&#xA;🛑 Player 1 is bored of 🏓 ping pong and is quitting! 🛑&#xA;🛑 Player 2 is bored of 🏓 ping pong and is quitting! 🛑&#xA;…&#xA;🎉 Player 11 won the game of 🏓 ping pong, since everyone else quit! 🎉&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;massive-scale-with-slurm&#34;&gt;Massive Scale with Slurm&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Now, imagine that we want to run this across hundreds of CPUs. Slurm makes this trivial as long as you have enough systems in your cluster to support this work.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In Slurm, you can define a batch job with a shell script (&lt;em&gt;e.g. /shared_storage/mpi-ping-pong.slurm&lt;/em&gt;) that looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;#!/bin/bash&#xA;&#xA;#SBATCH -J mpi-ping-pong                         # Name the job&#xA;#SBATCH -o /shared_storage/mpi-ping-pong-%j.out  # The standard output file&#xA;#SBATCH -e /shared_storage/mpi-ping-pong-%j.err  # The standard error file&#xA;#SBATCH -t 0-2:00:00         # Run for a max time of 0d, 2h, 0m, 0s&#xA;#SBATCH --nodes=3            # Request 3 nodes&#xA;#SBATCH --ntasks-per-node=1  # Request 1 cores or task per node&#xA;&#xA;echo &#34;Date start                = $(date)&#34;&#xA;echo &#34;Initiating Host           = $(hostname)&#34;&#xA;echo &#34;Working Directory         = $(pwd)&#34;&#xA;echo &#34;&#34;&#xA;echo &#34;Number of Nodes Allocated = ${SLURM_JOB_NUM_NODES}&#34;&#xA;echo &#34;Number of Tasks Allocated = ${SLURM_NTASKS}&#34;&#xA;echo &#34;&#34;&#xA;&#xA;mpirun python /shared_storage/mpi-ping-pong.py&#xA;RETURN=${?}&#xA;&#xA;echo &#34;&#34;&#xA;echo &#34;Exit code                 = ${RETURN}&#34;&#xA;echo &#34;Date end                  = $(date)&#34;&#xA;echo &#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Each&amp;nbsp;&lt;code&gt;#SBATCH&lt;/code&gt;&amp;nbsp;line defines an&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/sbatch.html&#34;&gt;&lt;code&gt;sbatch&lt;/code&gt;&lt;/a&gt;&amp;nbsp;command line argument to be used for the job. In this case, we are going to request that the workload run on three nodes and that it will only use a single core on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that all the compute nodes in our cluster are configured correctly and our Python script exists on the shared storage at&amp;nbsp;&lt;em&gt;/shared_storage/mpi-ping-pong.py&lt;/em&gt;&amp;nbsp;then we can submit this job to the cluster, by running:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ sbatch /shared_storage/mpi-ping-pong.slurm&#xA;&#xA;Submitted batch job 87&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This job will run very fast, but if we needed to we could use commands like&amp;nbsp;&lt;code&gt;squeue&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;sinfo&lt;/code&gt;&amp;nbsp;to examine the state of the cluster and jobs while it was running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When the job is finished we can look at&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.err&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.out&lt;/code&gt;&amp;nbsp;to see all the output from our application. In this case, the error file is empty, but our output file looks like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;Date start                = Tue May 14 10:15:02 PM UTC 2024&#xA;Initiating Host           = worker-0&#xA;Working Directory         = /shared_storage&#xA;&#xA;Number of Nodes Allocated = 3&#xA;Number of Tasks Allocated = 3&#xA;&#xA;Hello, World! I am 🏓 Player (Rank)  1 . Process  2  of  3  on  worker-1&#xA;Player (Rank)  1 &#39;s receiving 🏓 neighbor is Player (Rank) 0 and my sending 🏓 neighbor is Player (Rank) 2.&#xA;Hello, World! I am 🏓 Player (Rank)  2 . Process  3  of  3  on  worker-2&#xA;Player (Rank)  2 &#39;s receiving 🏓 neighbor is Player (Rank) 1 and my sending 🏓 neighbor is Player (Rank) 0.&#xA;Hello, World! I am 🏓 Player (Rank)  0 . Process  1  of  3  on  worker-0&#xA;Player (Rank)  0 &#39;s receiving 🏓 neighbor is Player (Rank) 2 and my sending 🏓 neighbor is Player (Rank) 1.&#xA;Player 0 sent the 🏓 ball to Player 1.&#xA;Player 1 received the 🏓 ball from Player 0.&#xA;Player 1 sent the 🏓 ball to Player 2.&#xA;Player 2 received the 🏓 ball from Player 1.&#xA;Player 2 sent the 🏓 ball to Player 0.&#xA;Player 0 received the 🏓 ball from Player 2.&#xA;Player 0 started round 2 by sending the 🏓 ball to Player 1.&#xA;…&#xA;Player 0 started round 900 by sending the 🏓 ball to Player 1.&#xA;🛑 Player 0 is bored of 🏓 ping pong and is quitting! 🛑&#xA;Player 1 received the 🏓 ball from Player 0.&#xA;Player 1 sent the 🏓 ball to Player 2.&#xA;🛑 Player 1 is bored of 🏓 ping pong and is quitting! 🛑&#xA;Player 2 received the 🏓 ball from Player 1.&#xA;🎉 Player 2 won the game of 🏓 ping pong, since everyone else quit! 🎉&#xA;&#xA;Exit code                 = 0&#xA;Date end                  = Tue May 14 10:15:03 PM UTC 2024&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you look closely at the&amp;nbsp;&lt;code&gt;Hello, World!&lt;/code&gt;&amp;nbsp;lines for each rank/process you will notice that they are all running on completely different nodes (&lt;em&gt;worker-[0-2]&lt;/em&gt;), but the application still behaves exactly as we expect it to, and the messages are happily passed between each process running on a different core on different systems. Because of this, we could use Slurm to easily run our application across hundreds or even thousands of cores if that is what we needed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, could we do all of this with Kubernetes today? Well, yes and no.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The real power of Slurm is that you have so much control over how it is built, how you assign resources to your workloads, and how they are partitioned while running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;mpi---message-passing-interface&#34;&gt;MPI – Message Passing Interface&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although you could easily run this application inside a single pod on Kubernetes, to truly leverage the MPI capabilities that we are expecting we would need to install and utilize something like&amp;nbsp;&lt;a href=&#34;https://www.kubeflow.org/&#34;&gt;KubeFlow&lt;/a&gt;&amp;nbsp;along with&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/mpi-operator&#34;&gt;KubeFlow’s beta MPI Operator&lt;/a&gt;&amp;nbsp;to even get started. You might also want to consider something like the&amp;nbsp;&lt;a href=&#34;https://kueue.sigs.k8s.io/&#34;&gt;Kubernetes-native Job Queueing support&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;At the time of writing this&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/manifests/tree/v1.8.0?tab=readme-ov-file#prerequisites&#34;&gt;KubeFlow v.1.8.0 doesn’t support versions of Kubernetes greater than v1.26&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One obvious side effect of using Kubernetes is that we&amp;nbsp;&lt;strong&gt;MUST&lt;/strong&gt;&amp;nbsp;run our code inside one or more containers. This is possible with Slurm but is not a hard requirement. We will also need to ensure that Python, MPI, and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;are all installed and configured correctly in our container, as we will need them all to run our application and manage each of the MPI worker pods.&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is required because this is how&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;will connect to each of the pods to launch the parallel instances of our application. The manifest that we will need to apply to our Kubernetes cluster for use with&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;is simply named&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/manifests/mpi-ping-pong/mpijob.yaml&#34;&gt;mpijob.yaml&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;You can also find&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/Dockerfile&#34;&gt;the Dockerfile for the container that we are running&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground&#34;&gt;the GitHub repo&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;mpijob.yaml&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;em&gt;&lt;/em&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that Kubeflow and the mpi-operator are properly installed in our cluster, then applying the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;manifest should “just work”.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ git clone https://github.com/superorbital/mpi-playground.git&#xA;$ cd mpi-playground&#xA;$ kubectl apply -f ./manifests/mpi-ping-pong/mpijob.yaml&#xA;mpijob.kubeflow.org/mpi-ping-pong created&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If you want to try this out yourself, you will likely find that it is easiest to delete the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;and then re-apply it each time you want to run it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;code&gt;kubectl delete mpijobs.kubeflow.org mpi-ping-pong&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the animated GIF below you will see the manifest being launched, followed by the launcher and worker pods being spun up, once the workers are done, then the launcher will complete and the launcher logs will be displayed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/demo.gif&#34; alt=&#34;MPI Ping Pong in Kubernetes demo&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The sequence of events looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The required number of workers are started and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is run inside each worker container.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher is started and executes the desired&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;command.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;connects to each worker and starts the individual tasks.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The workers complete their tasks and exit.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher completes and exits.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The pod logs for the launcher will contain all of the output that we would normally expect to see from an&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;session.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, today it is technically possible to run at least some MPI-style jobs inside Kubernetes, but how practical is it?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Well, one of the big advantages of Kubernetes is its ability to scale. Unlike Slurm, which has no cluster scaling functionality, a Kubernetes cluster can be designed to scale up nodes for a very large workflow and then scale back down once that workflow is finished and those nodes are no longer required for any immediately upcoming workflows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But, I think the real answer here is that it depends a lot on what you are trying to do, how much control you have over the Kubernetes cluster setup, and how much effort you want to put into getting it all working.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;artificial-intelligence-and-machine-learning&#34;&gt;Artificial Intelligence and Machine Learning&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although the technical details are different, most of the basic observations in this article also apply to artificial intelligence (AI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ai&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;), machine learning (ML&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ml&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;), and deep learning (DL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:dl&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;) applications. In most instances, these workloads will make heavy use of systems equipped with advanced GPUs to significantly improve the performance of their computation tasks. Ensuring that each process has exclusive access to the required hardware resources is often critical for the success and performance of the task.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;slurm-on-kubernetes&#34;&gt;Slurm on Kubernetes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the last couple of years,&amp;nbsp;&lt;a href=&#34;https://www.linkedin.com/in/timwickberg/&#34;&gt;Tim Wickberg, SchedMD’s CTO&lt;/a&gt;, has given a talk entitled “&lt;a href=&#34;https://slurm.schedmd.com/SC23/Slurm-and-or-vs-Kubernetes.pdf&#34;&gt;Slurm and/or/vs Kubernetes&lt;/a&gt;”, which looks at Slurm and Kubernetes and explores how each of these tools could be extended to leverage their strengths and generally improve the current state of high-performance workload managers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One project that has come out of the effort to find synergy between these two tools and communities, is&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/blog/sunk-slurm-on-kubernetes-implementations&#34;&gt;SUNK&lt;/a&gt;, which stands for&amp;nbsp;&lt;em&gt;&lt;strong&gt;S&lt;/strong&gt;l&lt;strong&gt;U&lt;/strong&gt;rm o&lt;strong&gt;N&lt;/strong&gt;&amp;nbsp;&lt;strong&gt;K&lt;/strong&gt;ubernetes&lt;/em&gt;. It is a project in early development by&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/&#34;&gt;CoreWeave&lt;/a&gt;&amp;nbsp;to enable Slurm workloads in Kubernetes Clusters.&amp;nbsp;&lt;a href=&#34;https://www.youtube.com/watch?v=48ONt0UKiew&#34;&gt;The CoreWeave talk that officially introduced this project can viewed on YouTube&lt;/a&gt;&amp;nbsp;if you would like to hear more details about the work that is being done with this. You can also grab&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/SLUG23/CoreWeave-SLUG23.pdf&#34;&gt;a PDF of the slides&lt;/a&gt;&amp;nbsp;for the talk. Once the code is released it should be available at&amp;nbsp;&lt;a href=&#34;https://github.com/coreweave/sunk&#34;&gt;github.com/coreweave/sunk&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;kubernetes-device-management&#34;&gt;Kubernetes Device Management&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The&amp;nbsp;&lt;a href=&#34;https://community.arm.com/arm-research/b/articles/posts/a-smarter-device-manager-for-kubernetes-on-the-edge&#34;&gt;Smarter Device Project&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://www.arm.com/resources/research&#34;&gt;ARM research&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:arm&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;is also an interesting&amp;nbsp;&lt;a href=&#34;https://github.com/smarter-project/smarter-device-manager&#34;&gt;Kubernetes application&lt;/a&gt;&amp;nbsp;that might be useful for these sorts of workloads and makes it possible to expose, reserve, and manage any Linux device (&lt;em&gt;e.g. /dev/video*&lt;/em&gt;) within Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For GPUs,&amp;nbsp;&lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/k8s-device-plugin&#34;&gt;NVIDIA has a device plugin&lt;/a&gt;&amp;nbsp;to provide similar functionality, which is often deployed along with their&amp;nbsp;&lt;a href=&#34;https://github.com/NVIDIA/gpu-operator&#34;&gt;gpu-operator&lt;/a&gt;. This also enables features in Kubernetes like GPU time-slicing, something similar to the generic&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/gres.html#Sharding&#34;&gt;GPU sharding&lt;/a&gt;&amp;nbsp;available in Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;in-conclusion&#34;&gt;In Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm is a very useful tool for running parallel applications that require very specific resources inside a compute cluster. It was designed for a set of workloads that are outside the standard stateless microservice focus of Kubernetes, but even today Kubernetes can be made to work with some of these workloads without too much hassle, however, truly leveraging the advantages of Kubernetes for these more traditional high-performance computing workloads will require projects like SUNK and&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;to reach a reasonable level of maturity and then in the future, there may be newer projects that rethink the current approaches and build Kubernetes-first tools from the ground-up that leverage the strengths that are already available in the platform while extending its ability to handle the type of workloads that have been traditionally handled by tools like Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;further-reading&#34;&gt;Further Reading&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained&#34;&gt;Machine learning, explained&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.geeksforgeeks.org/machine-learning/&#34;&gt;Machine Learning Tutorial&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://carleton.ca/rcs/rcdc/introduction-to-mpi/&#34;&gt;Introduction to MPI&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://mpitutorial.com/&#34;&gt;A Comprehensive MPI Tutorial Resource&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Intro to Multi-Node Machine Learning&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-1-setting-up-an-hpc-cluster&#34;&gt;1: Setting up an HPC cluster&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-2-using-slurm&#34;&gt;2: Using Slurm&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;Unreleased as of 05/16/2024&lt;/em&gt;&amp;nbsp;–&amp;nbsp;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-3-multi-node-training&#34;&gt;3: Multi-Node Training&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://blog.kubeflow.org/integrations/operators/2020/03/16/mpi-operator.html&#34;&gt;Introduction to Kubeflow MPI Operator and Industry Adoption&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Cover image by&amp;nbsp;&lt;a href=&#34;https://pixabay.com/users/dlohner-4631193/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;dlohner&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://pixabay.com//?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;Pixabay&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Slurm architecture overview by SchedMD for their&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/network.html&#34;&gt;network overview&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/High-performance_computing&#34;&gt;HPC – high-performance computing&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:hpc&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Lawrence_Livermore_National_Laboratory&#34;&gt;LLNL – Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:llnl&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.schedmd.com/about-schedmd/slurm-history&#34;&gt;Slurm history&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:history&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Graphics_processing_unit&#34;&gt;GPU – graphics processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:gpu&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA® – Compute Unified Device Architecture&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cuda&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN – NVIDIA CUDA® Deep Neural Network library&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cudnn&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.open-mpi.org/&#34;&gt;MPI – Message Passing Interface&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:mpi&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Open_Container_Initiative&#34;&gt;OCI – Open Container Initiative&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:oci&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.uio.no/studier/emner/matnat/ifi/INF3380/v11/undervisningsmateriale/inf3380-week06.pdf&#34;&gt;Rank&lt;/a&gt;&amp;nbsp;– Each process has a unique rank, i.e. an integer identifier, within a communicator. The rank value is between zero and the number of processes minus one. The rank value is used to distinguish one process from another.&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:rank&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Central_processing_unit&#34;&gt;CPU – central processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cpu&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/artificial-intelligence&#34;&gt;AI – Artificial Intelligence&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ai&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/machine-learning&#34;&gt;ML – Machine Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ml&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/deep-learning&#34;&gt;DL – Deep Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:dl&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/ARM_architecture_family&#34;&gt;ARM – Advanced RISC Machine&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:arm&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/&#34;&gt;SuperOrbital’s blog&lt;/a&gt; by Sean Kane&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/cover.jpg&#34; alt=&#34;datacenter servers&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this article, we are going to explore&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm&#34;&gt;Slurm&lt;/a&gt;, a popular open-source high-performance computing (HPC&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:hpc&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;) workload manager, and discover what it is, why people use it, and how it differs from Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For context, when we talk about high-performance computing, we are primarily talking about computational tasks that require a lot of time on very powerful computer systems and are often designed, so that they can be&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Parallel_computing&#34;&gt;highly parallelized&lt;/a&gt;, across resources within a super-computer or large cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These sorts of jobs are very common in scientific research,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_simulation&#34;&gt;computer simulation&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_intelligence&#34;&gt;artificial intelligence&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&gt;machine learning&lt;/a&gt;&amp;nbsp;tasks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;what-is-slurm&#34;&gt;What is Slurm?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To almost directly quote the Slurm repository&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm/blob/master/README.rst#slurm-workload-manager&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;Slurm is an open-source cluster resource management and job scheduling system for Linux (&lt;em&gt;primarily&lt;/em&gt;) that strives to be simple, scalable, portable, fault-tolerant, and interconnect agnostic. As a cluster resource manager, Slurm provides three key functions:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Second, it provides a framework for starting, executing, and monitoring work (&lt;em&gt;normally a parallel job&lt;/em&gt;) on the set of allocated nodes.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;And finally, it arbitrates conflicting requests for resources by managing a queue of pending work.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In short, Slurm is a system that is primarily designed to manage&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Batch_processing&#34;&gt;batch processing&lt;/a&gt;&amp;nbsp;workloads running in a shared&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_cluster&#34;&gt;computing cluster&lt;/a&gt;. Slurm can be configured to run any type of batch job but is often utilized on systems that support multiple users with parallel computing tasks to run.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;According to&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;, the primary support and development company for the Slurm community, Slurm is being used on approximately 65% of the&amp;nbsp;&lt;a href=&#34;https://top500.org/lists/top500/2024/06/&#34;&gt;TOP500 supercomputers in the world&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;a-bit-of-history&#34;&gt;A bit of history&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the mid-1990s, as Linux became more popular, there was an increased interest in making large-scale computer processing more accessible through the use of commodity hardware and projects like&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Beowulf_cluster&#34;&gt;Beowulf&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In 2001, engineers at the&amp;nbsp;&lt;a href=&#34;https://www.llnl.gov/&#34;&gt;Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;(LLNL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:llnl&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;) in the United States, started looking into how they might migrate from proprietary HPC systems to open-source solutions. They determined that there weren’t any good resource managers available at the time, and this directly led to the creation of Slurm, which was initially released in 2002.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;a href=&#34;https://slurm.schedmd.com/slurm_design.pdf&#34;&gt;The name Slurm is a reference&lt;/a&gt;&amp;nbsp;to the&amp;nbsp;&lt;a href=&#34;https://futurama.fandom.com/wiki/Slurm&#34;&gt;soda&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://www.imdb.com/title/tt0149460/&#34;&gt;Futurama&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the years, Slurm has evolved to support a wide variety of hardware configurations and in 2010, some initial scheduling logic was added to Slurm, enabling it to function as both a stand-alone resource manager and a scheduler&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:history&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-architecture&#34;&gt;The Architecture&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Like many similar systems, a Slurm setup consists of at least one controller node, and at least one compute node, which can be on the same physical systems.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/slurm-layout.gif&#34; alt=&#34;Diagram of the basic Slurm architecture&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;the-controller&#34;&gt;The Controller&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Two daemons are responsible for managing the cluster. These two processes can be run on a single node or separated, depending on the performance and resiliency required by the administrator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmctld&#34;&gt;slurmctld&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmctld&lt;/code&gt;&amp;nbsp;is the primary management daemon for Slurm. It is responsible for all the daemons, resources, and jobs, within the cluster. Because it is a critical component in the cluster, it is often configured with a passive backup that can take over management if the primary daemon ever fails.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmdbd&#34;&gt;slurmdbd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmdbd&lt;/code&gt;&amp;nbsp;provides an interface for the&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/MySQL&#34;&gt;MySQL database&lt;/a&gt;&amp;nbsp;that Slurm uses for stateful data, like job accounting records.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;compute-nodes&#34;&gt;Compute Nodes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These represent the nodes whose resources are available to any jobs that might need to be scheduled in the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmd&#34;&gt;slurmd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmd&lt;/code&gt;&amp;nbsp;is the daemon that runs on each compute node and is responsible for accepting work, launching tasks, and generally managing the workloads on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SchedMD maintains the core documentation for Slurm and the&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/quickstart_admin.html#quick_start&#34;&gt;current installation documentation&lt;/a&gt;&amp;nbsp;can be found on their website.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Getting Slurm working is not particularly difficult, but fine-tuning it for your requirements can be a time-consuming task. Since performance is often a primary focus with these clusters and Slurm is designed to work with a very wide set of hardware and software libraries, it can take a lot of effort to fine-tune the installation, so that it utilizes the hardware to the best of its ability, and all the proper software is available to streamline the types of jobs that will be run on the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;At a minimum clusters will require node name resolution, a MySQL instance, shared block storage, a synchronized time source, synchronized users, the&amp;nbsp;&lt;a href=&#34;https://dun.github.io/munge/&#34;&gt;Munge&lt;/a&gt;&amp;nbsp;daemon for user authentication, a full set of custom-built Slurm executables, and various configuration files.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In addition to this, the nodes will need to contain the various hardware, programming languages, libraries, and other tools that the majority of jobs will require to run. This could include things like GPUs&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:gpu&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;and their related toolchains (CUDA&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cuda&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;/cuDNN&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cudnn&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;), MPI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:mpi&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;libraries, OCI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:oci&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;runtimes for&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/containers.html&#34;&gt;container support&lt;/a&gt;, and much more.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm’s greatest asset and liability is how flexible it is. This makes it something that can be highly optimized for the requirements of the organization, while also making it susceptible to misconfiguration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;workloads&#34;&gt;Workloads&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To provide some context, let’s take a look at an example user job and workflow that utilizes some of the features of Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the&amp;nbsp;&lt;code&gt;mpi-ping-pong.py&lt;/code&gt;&amp;nbsp;code below, you will find a&amp;nbsp;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt;&amp;nbsp;script that simulates a pseudo-game of&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Table_tennis&#34;&gt;Ping Pong/Table Tennis&lt;/a&gt;&amp;nbsp;by making use of MPI to pass a message (&lt;em&gt;“ball”&lt;/em&gt;) between as many processes as we have configured to participate in the parallel job.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;local-testing&#34;&gt;Local Testing&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run this code, you simply need to have&amp;nbsp;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/quickstart.html&#34;&gt;OpenMPI&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://mpi4py.readthedocs.io/en/stable/install.html&#34;&gt;mpi4py&lt;/a&gt;&amp;nbsp;installed on your system.mpi-ping-pong.py&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/mpi-ping-pong.py&#34;&gt;mpi-ping-pong.py&lt;/a&gt;&amp;nbsp;is what many would refer to as an&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Embarrassingly_parallel&#34;&gt;embarrassingly parallel&lt;/a&gt;&amp;nbsp;workload because it can be scaled with no real effort at all.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you run this Python application on your local system, you will see something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ python ./mpi-ping-pong.py&#xA;&#xA;Hello, World! I am 🏓 Player (Rank)  0 . Process  1  of  1  on  laptop.local&#xA;Player (Rank)  0 &#39;s receiving 🏓 neighbor is Player (Rank) 0 and my sending 🏓 neighbor is Player (Rank) 0.&#xA;Player 0 started round 1 by hitting the 🏓 ball towards the wall.&#xA;Player 0 received the 🏓 ball that bounced off the wall.&#xA;Player 0 started round 2 by hitting the 🏓 ball towards the wall.&#xA;Player 0 received the 🏓 ball that bounced off the wall.&#xA;…&#xA;Player 0 started round 299 by hitting the 🏓 ball towards the wall.&#xA;Player 0 received the 🏓 ball that bounced off the wall.&#xA;🎉 Player 0 won the game of 🏓 ping pong, since they were playing alone! 🎉&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As you can see the application runs perfectly fine, and uses a single rank&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:rank&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;/process (&lt;em&gt;CPU&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cpu&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;core, in this case&lt;/em&gt;). Since we only have a single process participating in this game of Ping Pong, the program simply sends and receives messages to and from from itself.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, assuming that we have at least two CPU cores, then we can run this application utilizing as many of them as we want, by leveraging the MPI command line tool&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run 12 instances of our application, which will all communicate with each other in parallel across 12 cores on our system, we can do the following on our local system:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: These messages will be somewhat out of order. This is the nature of parallel computing, buffers, etc. To deal with this you would likely need to send all the messages back to single process for ordering and printing.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ mpirun -np 12 ./mpi-ping-pong.py&#xA;&#xA;Player (Rank)  0 &#39;s receiving 🏓 neighbor is Player (Rank) 11 and my sending 🏓 neighbor is Player (Rank) 1.&#xA;Hello, World! I am 🏓 Player (Rank)  0 . Process  1  of  12  on  laptop.local&#xA;Player (Rank)  1 &#39;s receiving 🏓 neighbor is Player (Rank) 0 and my sending 🏓 neighbor is Player (Rank) 2.&#xA;Hello, World! I am 🏓 Player (Rank)  1 . Process  2  of  12  on  laptop.local&#xA;Player (Rank)  2 &#39;s receiving 🏓 neighbor is Player (Rank) 1 and my sending 🏓 neighbor is Player (Rank) 3.&#xA;Hello, World! I am 🏓 Player (Rank)  2 . Process  3  of  12  on  laptop.local&#xA;…&#xA;🛑 Player 0 is bored of 🏓 ping pong and is quitting! 🛑&#xA;🛑 Player 1 is bored of 🏓 ping pong and is quitting! 🛑&#xA;🛑 Player 2 is bored of 🏓 ping pong and is quitting! 🛑&#xA;…&#xA;🎉 Player 11 won the game of 🏓 ping pong, since everyone else quit! 🎉&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;massive-scale-with-slurm&#34;&gt;Massive Scale with Slurm&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Now, imagine that we want to run this across hundreds of CPUs. Slurm makes this trivial as long as you have enough systems in your cluster to support this work.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In Slurm, you can define a batch job with a shell script (&lt;em&gt;e.g. /shared_storage/mpi-ping-pong.slurm&lt;/em&gt;) that looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;#!/bin/bash&#xA;&#xA;#SBATCH -J mpi-ping-pong                         # Name the job&#xA;#SBATCH -o /shared_storage/mpi-ping-pong-%j.out  # The standard output file&#xA;#SBATCH -e /shared_storage/mpi-ping-pong-%j.err  # The standard error file&#xA;#SBATCH -t 0-2:00:00         # Run for a max time of 0d, 2h, 0m, 0s&#xA;#SBATCH --nodes=3            # Request 3 nodes&#xA;#SBATCH --ntasks-per-node=1  # Request 1 cores or task per node&#xA;&#xA;echo &#34;Date start                = $(date)&#34;&#xA;echo &#34;Initiating Host           = $(hostname)&#34;&#xA;echo &#34;Working Directory         = $(pwd)&#34;&#xA;echo &#34;&#34;&#xA;echo &#34;Number of Nodes Allocated = ${SLURM_JOB_NUM_NODES}&#34;&#xA;echo &#34;Number of Tasks Allocated = ${SLURM_NTASKS}&#34;&#xA;echo &#34;&#34;&#xA;&#xA;mpirun python /shared_storage/mpi-ping-pong.py&#xA;RETURN=${?}&#xA;&#xA;echo &#34;&#34;&#xA;echo &#34;Exit code                 = ${RETURN}&#34;&#xA;echo &#34;Date end                  = $(date)&#34;&#xA;echo &#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Each&amp;nbsp;&lt;code&gt;#SBATCH&lt;/code&gt;&amp;nbsp;line defines an&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/sbatch.html&#34;&gt;&lt;code&gt;sbatch&lt;/code&gt;&lt;/a&gt;&amp;nbsp;command line argument to be used for the job. In this case, we are going to request that the workload run on three nodes and that it will only use a single core on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that all the compute nodes in our cluster are configured correctly and our Python script exists on the shared storage at&amp;nbsp;&lt;em&gt;/shared_storage/mpi-ping-pong.py&lt;/em&gt;&amp;nbsp;then we can submit this job to the cluster, by running:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ sbatch /shared_storage/mpi-ping-pong.slurm&#xA;&#xA;Submitted batch job 87&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This job will run very fast, but if we needed to we could use commands like&amp;nbsp;&lt;code&gt;squeue&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;sinfo&lt;/code&gt;&amp;nbsp;to examine the state of the cluster and jobs while it was running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When the job is finished we can look at&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.err&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.out&lt;/code&gt;&amp;nbsp;to see all the output from our application. In this case, the error file is empty, but our output file looks like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;Date start                = Tue May 14 10:15:02 PM UTC 2024&#xA;Initiating Host           = worker-0&#xA;Working Directory         = /shared_storage&#xA;&#xA;Number of Nodes Allocated = 3&#xA;Number of Tasks Allocated = 3&#xA;&#xA;Hello, World! I am 🏓 Player (Rank)  1 . Process  2  of  3  on  worker-1&#xA;Player (Rank)  1 &#39;s receiving 🏓 neighbor is Player (Rank) 0 and my sending 🏓 neighbor is Player (Rank) 2.&#xA;Hello, World! I am 🏓 Player (Rank)  2 . Process  3  of  3  on  worker-2&#xA;Player (Rank)  2 &#39;s receiving 🏓 neighbor is Player (Rank) 1 and my sending 🏓 neighbor is Player (Rank) 0.&#xA;Hello, World! I am 🏓 Player (Rank)  0 . Process  1  of  3  on  worker-0&#xA;Player (Rank)  0 &#39;s receiving 🏓 neighbor is Player (Rank) 2 and my sending 🏓 neighbor is Player (Rank) 1.&#xA;Player 0 sent the 🏓 ball to Player 1.&#xA;Player 1 received the 🏓 ball from Player 0.&#xA;Player 1 sent the 🏓 ball to Player 2.&#xA;Player 2 received the 🏓 ball from Player 1.&#xA;Player 2 sent the 🏓 ball to Player 0.&#xA;Player 0 received the 🏓 ball from Player 2.&#xA;Player 0 started round 2 by sending the 🏓 ball to Player 1.&#xA;…&#xA;Player 0 started round 900 by sending the 🏓 ball to Player 1.&#xA;🛑 Player 0 is bored of 🏓 ping pong and is quitting! 🛑&#xA;Player 1 received the 🏓 ball from Player 0.&#xA;Player 1 sent the 🏓 ball to Player 2.&#xA;🛑 Player 1 is bored of 🏓 ping pong and is quitting! 🛑&#xA;Player 2 received the 🏓 ball from Player 1.&#xA;🎉 Player 2 won the game of 🏓 ping pong, since everyone else quit! 🎉&#xA;&#xA;Exit code                 = 0&#xA;Date end                  = Tue May 14 10:15:03 PM UTC 2024&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you look closely at the&amp;nbsp;&lt;code&gt;Hello, World!&lt;/code&gt;&amp;nbsp;lines for each rank/process you will notice that they are all running on completely different nodes (&lt;em&gt;worker-[0-2]&lt;/em&gt;), but the application still behaves exactly as we expect it to, and the messages are happily passed between each process running on a different core on different systems. Because of this, we could use Slurm to easily run our application across hundreds or even thousands of cores if that is what we needed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, could we do all of this with Kubernetes today? Well, yes and no.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The real power of Slurm is that you have so much control over how it is built, how you assign resources to your workloads, and how they are partitioned while running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;mpi---message-passing-interface&#34;&gt;MPI – Message Passing Interface&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although you could easily run this application inside a single pod on Kubernetes, to truly leverage the MPI capabilities that we are expecting we would need to install and utilize something like&amp;nbsp;&lt;a href=&#34;https://www.kubeflow.org/&#34;&gt;KubeFlow&lt;/a&gt;&amp;nbsp;along with&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/mpi-operator&#34;&gt;KubeFlow’s beta MPI Operator&lt;/a&gt;&amp;nbsp;to even get started. You might also want to consider something like the&amp;nbsp;&lt;a href=&#34;https://kueue.sigs.k8s.io/&#34;&gt;Kubernetes-native Job Queueing support&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;At the time of writing this&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/manifests/tree/v1.8.0?tab=readme-ov-file#prerequisites&#34;&gt;KubeFlow v.1.8.0 doesn’t support versions of Kubernetes greater than v1.26&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One obvious side effect of using Kubernetes is that we&amp;nbsp;&lt;strong&gt;MUST&lt;/strong&gt;&amp;nbsp;run our code inside one or more containers. This is possible with Slurm but is not a hard requirement. We will also need to ensure that Python, MPI, and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;are all installed and configured correctly in our container, as we will need them all to run our application and manage each of the MPI worker pods.&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is required because this is how&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;will connect to each of the pods to launch the parallel instances of our application. The manifest that we will need to apply to our Kubernetes cluster for use with&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;is simply named&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/manifests/mpi-ping-pong/mpijob.yaml&#34;&gt;mpijob.yaml&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;You can also find&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/Dockerfile&#34;&gt;the Dockerfile for the container that we are running&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground&#34;&gt;the GitHub repo&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;mpijob.yaml&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;em&gt;&lt;/em&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that Kubeflow and the mpi-operator are properly installed in our cluster, then applying the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;manifest should “just work”.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ git clone https://github.com/superorbital/mpi-playground.git&#xA;$ cd mpi-playground&#xA;$ kubectl apply -f ./manifests/mpi-ping-pong/mpijob.yaml&#xA;mpijob.kubeflow.org/mpi-ping-pong created&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If you want to try this out yourself, you will likely find that it is easiest to delete the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;and then re-apply it each time you want to run it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;code&gt;kubectl delete mpijobs.kubeflow.org mpi-ping-pong&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the animated GIF below you will see the manifest being launched, followed by the launcher and worker pods being spun up, once the workers are done, then the launcher will complete and the launcher logs will be displayed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/demo.gif&#34; alt=&#34;MPI Ping Pong in Kubernetes demo&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The sequence of events looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The required number of workers are started and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is run inside each worker container.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher is started and executes the desired&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;command.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;connects to each worker and starts the individual tasks.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The workers complete their tasks and exit.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher completes and exits.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The pod logs for the launcher will contain all of the output that we would normally expect to see from an&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;session.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, today it is technically possible to run at least some MPI-style jobs inside Kubernetes, but how practical is it?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Well, one of the big advantages of Kubernetes is its ability to scale. Unlike Slurm, which has no cluster scaling functionality, a Kubernetes cluster can be designed to scale up nodes for a very large workflow and then scale back down once that workflow is finished and those nodes are no longer required for any immediately upcoming workflows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But, I think the real answer here is that it depends a lot on what you are trying to do, how much control you have over the Kubernetes cluster setup, and how much effort you want to put into getting it all working.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;artificial-intelligence-and-machine-learning&#34;&gt;Artificial Intelligence and Machine Learning&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although the technical details are different, most of the basic observations in this article also apply to artificial intelligence (AI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ai&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;), machine learning (ML&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ml&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;), and deep learning (DL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:dl&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;) applications. In most instances, these workloads will make heavy use of systems equipped with advanced GPUs to significantly improve the performance of their computation tasks. Ensuring that each process has exclusive access to the required hardware resources is often critical for the success and performance of the task.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;slurm-on-kubernetes&#34;&gt;Slurm on Kubernetes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the last couple of years,&amp;nbsp;&lt;a href=&#34;https://www.linkedin.com/in/timwickberg/&#34;&gt;Tim Wickberg, SchedMD’s CTO&lt;/a&gt;, has given a talk entitled “&lt;a href=&#34;https://slurm.schedmd.com/SC23/Slurm-and-or-vs-Kubernetes.pdf&#34;&gt;Slurm and/or/vs Kubernetes&lt;/a&gt;”, which looks at Slurm and Kubernetes and explores how each of these tools could be extended to leverage their strengths and generally improve the current state of high-performance workload managers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One project that has come out of the effort to find synergy between these two tools and communities, is&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/blog/sunk-slurm-on-kubernetes-implementations&#34;&gt;SUNK&lt;/a&gt;, which stands for&amp;nbsp;&lt;em&gt;&lt;strong&gt;S&lt;/strong&gt;l&lt;strong&gt;U&lt;/strong&gt;rm o&lt;strong&gt;N&lt;/strong&gt;&amp;nbsp;&lt;strong&gt;K&lt;/strong&gt;ubernetes&lt;/em&gt;. It is a project in early development by&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/&#34;&gt;CoreWeave&lt;/a&gt;&amp;nbsp;to enable Slurm workloads in Kubernetes Clusters.&amp;nbsp;&lt;a href=&#34;https://www.youtube.com/watch?v=48ONt0UKiew&#34;&gt;The CoreWeave talk that officially introduced this project can viewed on YouTube&lt;/a&gt;&amp;nbsp;if you would like to hear more details about the work that is being done with this. You can also grab&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/SLUG23/CoreWeave-SLUG23.pdf&#34;&gt;a PDF of the slides&lt;/a&gt;&amp;nbsp;for the talk. Once the code is released it should be available at&amp;nbsp;&lt;a href=&#34;https://github.com/coreweave/sunk&#34;&gt;github.com/coreweave/sunk&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;kubernetes-device-management&#34;&gt;Kubernetes Device Management&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The&amp;nbsp;&lt;a href=&#34;https://community.arm.com/arm-research/b/articles/posts/a-smarter-device-manager-for-kubernetes-on-the-edge&#34;&gt;Smarter Device Project&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://www.arm.com/resources/research&#34;&gt;ARM research&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:arm&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;is also an interesting&amp;nbsp;&lt;a href=&#34;https://github.com/smarter-project/smarter-device-manager&#34;&gt;Kubernetes application&lt;/a&gt;&amp;nbsp;that might be useful for these sorts of workloads and makes it possible to expose, reserve, and manage any Linux device (&lt;em&gt;e.g. /dev/video*&lt;/em&gt;) within Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For GPUs,&amp;nbsp;&lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/k8s-device-plugin&#34;&gt;NVIDIA has a device plugin&lt;/a&gt;&amp;nbsp;to provide similar functionality, which is often deployed along with their&amp;nbsp;&lt;a href=&#34;https://github.com/NVIDIA/gpu-operator&#34;&gt;gpu-operator&lt;/a&gt;. This also enables features in Kubernetes like GPU time-slicing, something similar to the generic&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/gres.html#Sharding&#34;&gt;GPU sharding&lt;/a&gt;&amp;nbsp;available in Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;in-conclusion&#34;&gt;In Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm is a very useful tool for running parallel applications that require very specific resources inside a compute cluster. It was designed for a set of workloads that are outside the standard stateless microservice focus of Kubernetes, but even today Kubernetes can be made to work with some of these workloads without too much hassle, however, truly leveraging the advantages of Kubernetes for these more traditional high-performance computing workloads will require projects like SUNK and&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;to reach a reasonable level of maturity and then in the future, there may be newer projects that rethink the current approaches and build Kubernetes-first tools from the ground-up that leverage the strengths that are already available in the platform while extending its ability to handle the type of workloads that have been traditionally handled by tools like Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;further-reading&#34;&gt;Further Reading&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained&#34;&gt;Machine learning, explained&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.geeksforgeeks.org/machine-learning/&#34;&gt;Machine Learning Tutorial&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://carleton.ca/rcs/rcdc/introduction-to-mpi/&#34;&gt;Introduction to MPI&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://mpitutorial.com/&#34;&gt;A Comprehensive MPI Tutorial Resource&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Intro to Multi-Node Machine Learning&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-1-setting-up-an-hpc-cluster&#34;&gt;1: Setting up an HPC cluster&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-2-using-slurm&#34;&gt;2: Using Slurm&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;Unreleased as of 05/16/2024&lt;/em&gt;&amp;nbsp;–&amp;nbsp;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-3-multi-node-training&#34;&gt;3: Multi-Node Training&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://blog.kubeflow.org/integrations/operators/2020/03/16/mpi-operator.html&#34;&gt;Introduction to Kubeflow MPI Operator and Industry Adoption&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Cover image by&amp;nbsp;&lt;a href=&#34;https://pixabay.com/users/dlohner-4631193/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;dlohner&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://pixabay.com//?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;Pixabay&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Slurm architecture overview by SchedMD for their&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/network.html&#34;&gt;network overview&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/High-performance_computing&#34;&gt;HPC – high-performance computing&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:hpc&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Lawrence_Livermore_National_Laboratory&#34;&gt;LLNL – Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:llnl&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.schedmd.com/about-schedmd/slurm-history&#34;&gt;Slurm history&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:history&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Graphics_processing_unit&#34;&gt;GPU – graphics processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:gpu&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA® – Compute Unified Device Architecture&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cuda&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN – NVIDIA CUDA® Deep Neural Network library&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cudnn&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.open-mpi.org/&#34;&gt;MPI – Message Passing Interface&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:mpi&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Open_Container_Initiative&#34;&gt;OCI – Open Container Initiative&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:oci&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.uio.no/studier/emner/matnat/ifi/INF3380/v11/undervisningsmateriale/inf3380-week06.pdf&#34;&gt;Rank&lt;/a&gt;&amp;nbsp;– Each process has a unique rank, i.e. an integer identifier, within a communicator. The rank value is between zero and the number of processes minus one. The rank value is used to distinguish one process from another.&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:rank&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Central_processing_unit&#34;&gt;CPU – central processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cpu&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/artificial-intelligence&#34;&gt;AI – Artificial Intelligence&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ai&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/machine-learning&#34;&gt;ML – Machine Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ml&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/deep-learning&#34;&gt;DL – Deep Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:dl&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/ARM_architecture_family&#34;&gt;ARM – Advanced RISC Machine&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:arm&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Sun, 07 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Now what? Kubernetes troubleshooting with AI?】怎么办？ Kubernetes 使用 AI 进行故障排除？</title>
      <link>https://www.cncf.io/blog/2024/07/11/now-what-kubernetes-troubleshooting-with-ai/</link>
      <description>【&lt;p&gt;&lt;em&gt;Ambassador post originally published on &lt;a href=&#34;https://eminalemdar.medium.com/&#34;&gt;Medium &lt;/a&gt;by Emin Alemdar&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;cb51&#34;&gt;We all know that Kubernetes troubleshooting is difficult and it can get pretty complex from time to time. We can easily get lost in the logs, jumping between pods, searching through events and what have you. Most importantly, finding a meaningful explanation can be an extremely huge problem because not all of the logs are easily understandable. We also have to accept that it is time consuming. Of course we can use an external observability tool or even an observability stack with all of the components but still not all of the outputs are easy to understand and even read. At the end of the day, diagnose times and of course the troubleshooting times are extending drastically as a result.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;358a&#34;&gt;But let’s talk about the elephant in the room here. Can AI help us here? Especially for Kubernetes troubleshooting. Because we started using&amp;nbsp;&lt;strong&gt;AI&lt;/strong&gt;&amp;nbsp;and mostly&amp;nbsp;&lt;strong&gt;LLMs&lt;/strong&gt;&amp;nbsp;everyday not just in our jobs but in our daily activities as well. There are of course some&amp;nbsp;&lt;strong&gt;AIOps&lt;/strong&gt;&amp;nbsp;tools out there that help us implement the observability solution with the help of AI but almost all of those tools consume huge amounts of GPU resources and that increases the underlying cost and of course the maintainability issue here.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;140d&#34;&gt;So, in this blog post, I want to introduce a&amp;nbsp;&lt;strong&gt;CNCF Sandbox&lt;/strong&gt;&amp;nbsp;project to you which is a really powerful tool designed to simplify Kubernetes management using AI and natural language processing. The project is called&amp;nbsp;&lt;strong&gt;K8sGPT&lt;/strong&gt;&amp;nbsp;and K8sGPT is a tool for scanning your Kubernetes clusters, diagnosing and triaging issues in simple English. So basically, K8sGPT integrates with various AI backends, including OpenAI, Azure OpenAI, and Amazon Bedrock, to provide clear and actionable insights into your Kubernetes environment and it provides these insights in a user friendly format.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*rWs5LOWvxt5df7gW&#34; alt=&#34;Image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 class=&#34;wp-block-heading&#34; id=&#34;439b&#34;&gt;Key Features of K8sGPT&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;6ba7&#34;&gt;Before sharing some examples of K8sGPT usage, I want to share some specific key features of it. Let’s break them down into bullet points.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Workload Health Analysis&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;969d&#34;&gt;Of course we should start with this because that is one of the most important reasons this tool is here. K8sGPT scans your Kubernetes clusters to identify critical issues affecting your workloads. It transforms complex diagnostic data and logs into simple, understandable suggestions, making it easier to maintain cluster health. Yes, K8sGPT also provides suggestions for the problems it detects in the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;AI-Powered Diagnostics&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a06c&#34;&gt;This one is obvious. K8sGPT leverages AI and LLMs. K8sGPT provides detailed explanations of detected issues in plain English. This feature is powered by integrations with leading AI platforms like OpenAI. Basically, by using these AI platforms, K8sGPT transforms the diagnostic data into a very human friendly format.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Built-in and Custom Analysers&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;5583&#34;&gt;K8sGPT includes several built-in analyzers for various Kubernetes resources, such as Pods, Services, and Nodes. But of course with additional integrations, you can also create custom analysers to meet specific needs. K8sGPT has integrations with AWS, Prometheus, KEDA and Trivy. With these native integrations, you can have more detailed analysis. Of course I believe this list is going to be extended and more integrations will be added here in the future.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Continuous Monitoring and Integration&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;d67b&#34;&gt;You can also deploy K8sGPT as a Kubernetes Operator within a Kubernetes cluster. K8sGPT can continuously monitor the environment and integrate seamlessly with existing tools like Prometheus and Alertmanager in the cluster. You can see the components that the K8sGPT Operator instals and manages from the diagram below.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*loluayfTjzR3OyVv&#34; alt=&#34;image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Security CVE Review&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;e023&#34;&gt;With the Trivy integration, K8sGPT helps in identifying and addressing security vulnerabilities within your Kubernetes clusters. Once you activate the Trivy integration, Trivy Kubernetes Operator will be installed into the Kubernetes cluster and make it possible for K8sGPT to interact with the results of the Operator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;fbd6&#34;&gt;I also want to specifically talk about the&amp;nbsp;&lt;strong&gt;AWS&lt;/strong&gt;&amp;nbsp;integration here. I’m pretty sure you’re already familiar with the&amp;nbsp;&lt;strong&gt;AWS Controllers for Kubernetes (ACK)&lt;/strong&gt;&amp;nbsp;project but if not, I also have a blog post about that and you can check that out from&amp;nbsp;&lt;a href=&#34;https://medium.com/@eminalemdar/manage-your-aws-resources-from-kubernetes-with-ack-3cf06a4b0770&#34;&gt;here&lt;/a&gt;. In short, ACK allows you to manage AWS services from your Kubernetes clusters with CRDs. This integration also helps K8sGPT to interact with the AWS resources managed by ACK. As a result, you can use K8sGPT to analyse and manage not only your Kubernetes resources but also your AWS resources that are under the management of ACK.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;01bf&#34;&gt;Let’s play with K8sGPT&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;4756&#34;&gt;Installation of K8sGPT CLI is extremely easy and you can use popular packet managers like Brew to install it on your machine. After the installation, you will need to authenticate with your chosen AI provider. I’m using OpenAI here with GPT4 but you can of course choose the appropriate one for you:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;k8sgpt auth list&lt;br&gt;Default:&lt;br&gt;&amp;gt; openai&lt;br&gt;Active:&lt;br&gt;&amp;gt; openai&lt;br&gt;Unused:&lt;br&gt;&amp;gt; localai&lt;br&gt;&amp;gt; azureopenai&lt;br&gt;&amp;gt; cohere&lt;br&gt;&amp;gt; amazonbedrock&lt;br&gt;&amp;gt; amazonsagemaker&lt;br&gt;&amp;gt; google&lt;br&gt;&amp;gt; noopai&lt;br&gt;&amp;gt; huggingface&lt;br&gt;&amp;gt; googlevertexai&lt;br&gt;&amp;gt; oci&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;ddc5&#34;&gt;You can use&amp;nbsp;&lt;code&gt;k8sgpt auth add&lt;/code&gt;&amp;nbsp;command to add the provider backend authentication.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;0e6f&#34;&gt;After this step, it’s pretty straightforward actually. Let’s start the first analysis with the&amp;nbsp;&lt;code&gt;k8sgpt analyse&lt;/code&gt;&amp;nbsp;command. But before running the command let me just deploy a broken Pod to see the actual diagnosis part.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl apply -f - &amp;lt;&amp;lt;EOF&lt;br&gt;apiVersion: v1&lt;br&gt;kind: Pod&lt;br&gt;metadata:&lt;br&gt;name: broken-pod&lt;br&gt;namespace: default&lt;br&gt;spec:&lt;br&gt;containers:&lt;br&gt;- name: broken-pod&lt;br&gt;image: nginx:1.a.b.c&lt;br&gt;livenessProbe:&lt;br&gt;httpGet:&lt;br&gt;path: /&lt;br&gt;port: 81&lt;br&gt;initialDelaySeconds: 3&lt;br&gt;periodSeconds: 3&lt;br&gt;EOF&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;8645&#34;&gt;As you can see the image part of this Pod definition is wrong. So let me run the&amp;nbsp;&lt;code&gt;k8sgpt analyse&lt;/code&gt;&amp;nbsp;command and see the output.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*mSTa1ykjgzbeBas2&#34; alt=&#34;Code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;616a&#34;&gt;We can see the error but not in detail but there is a flag for us to use here. If we add the&amp;nbsp;&lt;code&gt;k8sgpt analyse --explain&lt;/code&gt;&amp;nbsp;flag here, K8sGPT will connect to the AI provider and LLM here.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*xAdRU8htHZ3VxrtY&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;637f&#34;&gt;So now we have some more details about the issue and also some recommendations as well. I know this is a very basic example but let’s extend this. I will now enable two integrations, AWS and Trivy. If we look at the filters we can use now, we see the added filters coming from the integrations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:390/0*s-zY2Cb0irELZNms&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a48f&#34;&gt;Let’s start with the EKS filter and see if there’s anything wrong with that.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:466/0*uwSRiMmwhveF5-k5&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;2054&#34;&gt;Brilliant! No problems here but let’s see the results of the Vulnerability Report from Trivy.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*5tX1K5Wi2SOZNAv2&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;3f28&#34;&gt;Wow! That’s a lot! Let’s dive in to understand some of these issues.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*smPQrxM4w4VOtjP0&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*YtfG2_J9R1f4oWMk&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a139&#34;&gt;As you can see from the screenshots, We have very detailed information about each issue and again some really good recommendations on how to solve the problems.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;ae22&#34;&gt;Let me also deploy some other resources to the cluster and see the information about those.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*5rbT8-9aQ-bXGYcz&#34; alt=&#34;code&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;2953&#34;&gt;As you can see from the broken resources, I now have a very good understanding of what is really happening in my cluster here.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;bdbf&#34;&gt;Some final words here. Of course we all have some questions around security of these AI backends but there is also another option here called anonymise which basically anonymise the data before sending it to the AI backend.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;2ade&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;25df&#34;&gt;In the future K8sGPT might turn into one of the tools that’s going to transform Kubernetes management, problem diagnosis and of course troubleshooting by making it more accessible and efficient. Whether you’re an SRE, DevOps Engineer or Platform Engineer, K8sGPT can help you reduce troubleshooting times. It is extremely easy to get started with K8sGPT, you can just install it on your Kubernetes environment or on your local machine, authenticate with your chosen AI backend, and connect it to your cluster. In my opinion, you should definitely give K8sGPT a try.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://eminalemdar.medium.com/?source=post_page-----9c68d26e00ac--------------------------------&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;大使帖子最初由 Emin Alemdar 在 &lt;a href=&#34;https://eminalemdar.medium.com/&#34;&gt;Medium&lt;/a&gt; 上发布&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;cb51&#34;&gt;我们都知道 Kubernetes 故障排除很困难，而且有时会变得非常复杂。我们很容易迷失在日志中，在 Pod 之间跳转，搜索事件以及你拥有的东西。最重要的是，找到有意义的解释可能是一个极其巨大的问题，因为并非所有日志都易于理解。我们还必须承认这很耗时。当然，我们可以使用外部可观察性工具，甚至可以使用包含所有组件的可观察性堆栈，但仍然并非所有输出都易于理解甚至阅读。最终，诊断时间以及故障排除时间都会因此而大幅延长。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;358a&#34;&gt;但是我们来谈谈房间里的大象。人工智能可以在这方面帮助我们吗？特别是对于 Kubernetes 故障排除。因为我们每天都开始使用&lt;strong&gt;人工智能&lt;/strong&gt;，并且主要是&lt;strong&gt;法学硕士&lt;/strong&gt;，不仅在我们的工作中，而且在我们的日常活动中。当然，有一些&lt;strong&gt;AIOps&lt;/strong&gt;工具可以帮助我们在人工智能的帮助下实现可观测性解决方案，但几乎所有这些工具都会消耗大量的 GPU 资源，这会增加潜在成本，当然这里的可维护性问题。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;140d&#34;&gt;因此，在这篇博文中，我想向您介绍一个&lt;strong&gt;CNCF Sandbox&lt;/strong&gt;项目，这是一个非常强大的工具，旨在使用人工智能和自然语言处理来简化 Kubernetes 管理。该项目名为&lt;strong&gt;K8sGPT&lt;/strong&gt;，K8sGPT 是一个用简单的英语扫描 Kubernetes 集群、诊断和分类问题的工具。因此，基本上，K8sGPT 与各种 AI 后端集成，包括 OpenAI、Azure OpenAI 和 Amazon Bedrock，为您的 Kubernetes 环境提供清晰且可操作的见解，并以用户友好的格式提供这些见解。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*rWs5LOWvxt5df7gW&#34; alt=&#34;Image&#34;referrerpolicy =“无引用者”&gt;&lt;/图&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 class=&#34;wp-block-heading&#34; id=&#34;439b&#34;&gt;K8sGPT 的主要功能&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;6ba7&#34;&gt;在分享一些 K8sGPT 使用示例之前，我想先分享一下它的一些具体关键功能。让我们将它们分解为要点。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;工作负载健康分析&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;969d&#34;&gt;当然我们应该从这个开始，因为这是这个工具出现的最重要原因之一。 K8sGPT 扫描您的 Kubernetes 集群以识别影响您工作负载的关键问题。它将复杂的诊断数据和日志转换为简单、易于理解的建议，从而更轻松地维护集群健康状况。是的，K8sGPT 还针对它在集群中检测到的问题提供建议。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;人工智能驱动的诊断&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a06c&#34;&gt;这一点是显而易见的。 K8sGPT 利用人工智能和法学硕士。 K8sGPT 用简单的英语提供了检测到的问题的详细解释。此功能由与 OpenAI 等领先 AI 平台的集成提供支持。基本上，通过使用这些人工智能平台，K8sGPT 将诊断数据转换为非常人性化的格式。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;内置和自定义分析器&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;5583&#34;&gt;K8sGPT 包含多个针对各种 Kubernetes 资源（例如 Pod、服务和节点）的内置分析器。当然，通过额外的集成，您还可以创建自定义分析器来满足特定需求。 K8sGPT 与 AWS、Prometheus、KEDA 和 Trivy 集成。通过这些原生集成，您可以进行更详细的分析。当然，我相信这个列表将会扩展，并且将来会添加更多集成。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;持续监控和集成&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;d67b&#34;&gt;您还可以将 K8sGPT 作为 Kubernetes Operator 在 Kubernetes 集群中部署。 K8sGPT 可以持续监控环境，并与集群中现有的 Prometheus、Alertmanager 等工具无缝集成。您可以从下图中看到 K8sGPT Operator 安装和管理的组件。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*loluayfTjzR3OyVv&#34; alt=&#34;image&#34;referrerpolicy =“无引用者”&gt;&lt;/图&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;安全 CVE 审核&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;e023&#34;&gt;通过 Trivy 集成，K8sGPT 有助于识别和解决 Kubernetes 集群内的安全漏洞。一旦激活 Trivy 集成，Trivy Kubernetes Operator 将被安装到 Kubernetes 集群中，并使 K8sGPT 能够与 Operator 的结果进行交互。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;fbd6&#34;&gt;我还想在此专门讨论 &lt;strong&gt;AWS&lt;/strong&gt; 集成。我很确定您已经熟悉&lt;strong&gt;AWS Controllers for Kubernetes (ACK)&lt;/strong&gt;项目，但如果不熟悉，我还有一篇关于该项目的博文，您可以从&lt;a href= “https://medium.com/@eminalemdar/manage-your-aws-resources-from-kubernetes-with-ack-3cf06a4b0770&#34;&gt;此处&lt;/a&gt;。简而言之，ACK 允许您使用 CRD 从 Kubernetes 集群管理 AWS 服务。这种集成还有助于 K8sGPT 与 ACK 管理的 AWS 资源进行交互。因此，您不仅可以使用 K8sGPT 来分析和管理 Kubernetes 资源，还可以分析和管理 ACK 管理下的 AWS 资源。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;01bf&#34;&gt;让我们来玩一下 K8sGPT&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;4756&#34;&gt;K8sGPT CLI 的安装非常简单，您可以使用 Brew 等流行的数据包管理器将其安装在您的计算机上。安装后，您需要向您选择的 AI 提供商进行身份验证。我在这里使用 OpenAI 和 GPT4，但您当然可以选择适合您的：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;k8sgpt 身份验证列表&lt;br&gt;默认：&lt;br&gt;&gt; openai&lt;br&gt;活动：&lt;br&gt;&gt; openai&lt;br&gt;未使用：&lt;br&gt; &gt; localai&lt;br&gt;&gt; azureopenai&lt;br&gt;&gt; cohere&lt;br&gt;&gt; amazonbedrock&lt;br&gt;&gt; amazonsagemaker&lt;br&gt;&gt; google&lt;br&gt;&gt; noopai&lt;br&gt;&gt; Huggingface&lt;br&gt;&gt; googlevertexai&lt;br&gt;&gt; oci&lt;/code &gt;&lt;/前&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;ddc5&#34;&gt;您可以使用&lt;code&gt;k8sgpt auth add&lt;/code&gt;命令添加提供商后端身份验证。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;0e6f&#34;&gt;完成这一步后，实际上非常简单。让我们使用 &lt;code&gt;k8sgpt analysis&lt;/code&gt; 命令开始第一次分析。但在运行命令之前，让我部署一个损坏的 Pod 来查看实际的诊断部分。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;kubectl apply -f - &lt;&lt;EOF&lt;br&gt;api版本：v1&lt;br&gt;种类：Pod&lt;br&gt;元数据：&lt;br&gt;名称：损坏-pod&lt;br&gt;命名空间：默认&lt;br&gt;规格：&lt;br&gt;容器：&lt;br&gt;- 名称：broken-pod&lt;br&gt;图像：nginx:1.a.b.c&lt;br&gt;livenessProbe:&lt;br&gt;httpGet:&lt;br&gt;路径：/&lt;br&gt;端口：81&lt;br&gt;initialDelaySeconds：3&lt;br&gt;periodSeconds：3&lt;br&gt;EOF&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;8645&#34;&gt;如您所见，此 Pod 定义的图像部分是错误的。那么让我运行 &lt;code&gt;k8sgpt analysis&lt;/code&gt; 命令并查看输出。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*mSTa1ykjgzbeBas2&#34; alt=&#34;代码&#34;referrerpolicy =“无引用者”&gt;&lt;/图&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;616a&#34;&gt;我们可以看到错误，但看不到详细信息，但这里有一个标志供我们使用。如果我们在此处添加 &lt;code&gt;k8sgptanalyze--explain&lt;/code&gt; 标志，K8sGPT 将在此处连接到 AI 提供商和 LLM。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*xAdRU8htHZ3VxrtY&#34; alt=&#34;code&#34;referrerpolicy =“无引用者”&gt;&lt;/图&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;637f&#34;&gt;现在我们有了有关该问题的更多详细信息以及一些建议。我知道这是一个非常基本的示例，但让我们扩展一下。我现在将启用两个集成：AWS 和 Trivy。如果我们查看现在可以使用的过滤器，我们会看到添加的过滤器来自集成。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:390/0*s-zY2Cb0irELZNms&#34; alt=&#34;code “referrerpolicy =“no-referrer”&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a48f&#34;&gt;让我们从 EKS 过滤器开始，看看是否有任何问题。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:466/0*uwSRiMmwhveF5-k5&#34; alt=&#34;code “referrerpolicy =“no-referrer”&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;2054&#34;&gt;太棒了！这里没有问题，但让我们看看 Trivy 的漏洞报告的结果。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*5tX1K5Wi2SOZNAv2&#34; alt=&#34;code&#34;referrerpolicy =“无引用者”&gt;&lt;/图&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;3f28&#34;&gt;哇！好多啊！让我们深入了解其中一些问题。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*smPQrxM4w4VOtjP0&#34; alt=&#34;code&#34;referrerpolicy =“无引用者”&gt;&lt;/图&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*YtfG2_J9R1f4oWMk&#34; alt=&#34;code&#34;referrerpolicy =“无引用者”&gt;&lt;/图&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;a139&#34;&gt;正如您从屏幕截图中看到的，我们提供了有关每个问题的非常详细的信息，并且还提供了一些关于如何解决问题的非常好的建议。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;ae22&#34;&gt;让我也将一些其他资源部署到集群并查看有关这些资源的信息。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://miro.medium.com/v2/resize:fit:700/0*5rbT8-9aQ-bXGYcz&#34; alt= “代码”referrerpolicy =“no-referrer”&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;2953&#34;&gt;正如您从损坏的资源中看到的那样，我现在对集群中实际发生的情况有了很好的了解。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;bdbf&#34;&gt;最后一些话。当然，我们对这些人工智能后端的安全性都有一些疑问，但这里还有另一个选项，称为匿名，它基本上在将数据发送到人工智能后端之前对数据进行匿名化。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;2ade&#34;&gt;结论&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p id=&#34;25df&#34;&gt;将来，K8sGPT 可能会成为一种工具，通过使其更易于访问和更高效，来改变 Kubernetes 管理、问题诊断和故障排除。无论您是 SRE、DevOps 工程师还是平台工程师，K8sGPT 都可以帮助您减少故障排除时间。 K8sGPT 入门非常简单，您只需将其安装在 Kubernetes 环境或本地计算机上，使用您选择的 AI 后端进行身份验证，然后将其连接到集群即可。在我看来，你绝对应该尝试一下 K8sGPT。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://eminalemdar.medium.com/?source=post_page-----9c68d26e00ac---------------------- ----------&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Wed, 10 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Embracing the future: our online store moves to a print-on-demand model】拥抱未来：我们的在线商店转向按需打印模式</title>
      <link>https://www.cncf.io/blog/2024/07/08/embracing-the-future-our-online-store-moves-to-a-print-on-demand-model/</link>
      <description>【&lt;p&gt;In today’s fast-paced digital world, businesses must evolve and adapt to meet their customers’ changing needs. We are excited to announce that our &lt;a href=&#34;https://store.cncf.io/&#34;&gt;online store&lt;/a&gt; is transitioning to a Print On Demand (POD) model. This significant change brings numerous benefits for us and, more importantly, our vibrant community.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;What is Print On Demand?&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Print On Demand is a fulfillment method in which items are printed as soon as an order is placed rather than stored in inventory. This model allows for greater flexibility and customization, ensuring each product is made specifically for the person who orders it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;The Benefits of Moving to Print-On-Demand&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Sustainability&lt;/strong&gt;:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reduced Waste&lt;/strong&gt;: Traditional inventory systems often result in overproduction and excess stock, leading to waste. With POD, we only produce what is needed, minimizing our environmental footprint.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Eco-Friendly Materials&lt;/strong&gt;: Many POD services use sustainable materials and eco-friendly printing processes, reducing environmental impact.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reduced Overhead Costs&lt;/strong&gt;: By eliminating the need for warehousing and managing excess inventory, we can focus on improving other aspects of our service, such as customer support and product quality.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;How This Benefits Our Community&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Our shift to a Print-on-Demand model is a testament to our commitment to our community. By reducing waste and promoting sustainability, we can allow more flexibility in our offerings and ensure we always have inventory.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We are excited about this new chapter and look forward to providing you an even better shopping experience. Your support and feedback have been invaluable in making this transition possible. Together, we can positively impact both the environment and the creative community.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Thank you for being a part of our journey. Explore our new&lt;a href=&#34;https://store.cncf.io/collections/all-projects&#34;&gt; Print-On-Demand offerings today&lt;/a&gt; and discover the endless possibilities!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In the Future&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the coming months, we look forward to adding customization to our products and sourcing print-on-demand options with global partners to reduce shipping costs.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Stay tuned for more updates and exciting new products!&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;在当今快节奏的数字世界中，企业必须不断发展和适应，以满足客户不断变化的需求。我们很高兴地宣布，我们的&lt;a href=&#34;https://store.cncf.io/&#34;&gt;在线商店&lt;/a&gt;正在转变为按需打印 (POD) 模式。这一重大变化为我们带来了众多好处，更重要的是，为我们充满活力的社区带来了众多好处。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;什么是按需打印？&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;按需打印是一种履行方法，在下订单后立即打印商品，而不是将其存储在库存中。这种模式提供了更大的灵活性和定制性，确保每件产品都是专门为订购者制造的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;转向按需打印的好处&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;可持续性&lt;/strong&gt;：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;减少浪费&lt;/strong&gt;：传统库存系统通常会导致生产过剩和库存过剩，从而导致浪费。通过 POD，我们只生产所需的产品，最大限度地减少对环境的影响。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;环保材料&lt;/strong&gt;：许多 POD 服务都使用可持续材料和环保印刷工艺，减少对环境的影响。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;效率：&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;降低管理成本&lt;/strong&gt;：通过消除仓储和管理过剩库存的需要，我们可以专注于改善服务的其他方面，例如客户支持和产品质量。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;这对我们的社区有何好处&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们向按需打印模式的转变证明了我们对社区的承诺。通过减少浪费和促进可持续发展，我们可以在我们的产品中提供更大的灵活性，并确保我们始终有库存。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们对这一新篇章感到兴奋，并期待为您提供更好的购物体验。您的支持和反馈对于实现这一转变至关重要。我们可以齐心协力，对环境和创意社区产生积极影响。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;感谢您参与我们的旅程。立即探索我们新的&lt;a href=&#34;https://store.cncf.io/collections/all-projects&#34;&gt;按需打印产品&lt;/a&gt;并发现无限的可能性！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;未来&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在接下来的几个月中，我们期待为我们的产品增加定制功能，并与全球合作伙伴一起采购按需打印选项，以降低运输成本。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;敬请关注更多更新和令人兴奋的新产品！&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Sun, 07 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Kubestronaut in Orbit: Eleni Grosdouli】在轨道上的 Kubetronaut：Eleni Grosdouli</title>
      <link>https://www.cncf.io/blog/2024/07/16/kubestronaut-in-orbit-eleni-grosdouli/</link>
      <description>【&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1800&#34; height=&#34;945&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5.jpg&#34; alt=&#34;Eleni headshot&#34; class=&#34;wp-image-114334&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5.jpg 1800w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-300x158.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-1024x538.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-768x403.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-194x102.jpg 194w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-388x204.jpg 388w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-776x408.jpg 776w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-1552x816.jpg 1552w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-900x473.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-381x200.jpg 381w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-762x400.jpg 762w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-590x310.jpg 590w, https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-1180x620.jpg 1180w&#34; sizes=&#34;(max-width: 1800px) 100vw, 1800px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Get to know Eleni&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This week’s Kubstronaut in Orbit, Eleni Grosdouli, brings diverse experiences to her role as a DevOps Consulting Engineer at Cisco Systems. She’s the go-to person for DevOps and Kubernetes Automation, with a passion for networking, security, endpoint management, and endpoint security,&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you’d like to be a Kubestronaut like Eleni, find more details on the&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt; CNCF Kubestronaut &lt;/a&gt;page.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;When did you get started with Kubernetes–what was your first project?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;My journey started back in 2019. At the time, I was a network consulting engineer involved in network automation projects. After learning about automation technologies, I was involved in container creation images and Docker. The next logical step was to work with an open source offering for container orchestration. That’s when I became familiar with Kubernetes. Setting up a home lab was just the beginning towards the exciting journey of the Kubestronaut certification.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What are the primary CNCF projects you work on or use today? What projects have you enjoyed the most in your career?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cilium/cilium&#34;&gt;Cilium&lt;/a&gt; as CNI and Service mesh&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kyverno&#34;&gt;Kyverno&lt;/a&gt; for applying policies&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/projectsveltos&#34;&gt;Sveltos&lt;/a&gt; for Kubernetes add-on and applications deployment&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/argoproj/argo-cd&#34;&gt;ArgoCD&lt;/a&gt; for continuous deployments on specific topics&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/aquasecurity/kube-bench/&#34;&gt;Kubebench&lt;/a&gt;, &lt;a href=&#34;https://github.com/trivy&#34;&gt;Trivy&lt;/a&gt;, &lt;a href=&#34;https://github.com/falco&#34;&gt;Falco&lt;/a&gt;, &lt;a href=&#34;http://kubesec.io/&#34;&gt;Kubesec.io&lt;/a&gt; for security and compliance&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/prometheus&#34;&gt;Prometheus&lt;/a&gt;, &lt;a href=&#34;https://github.com/grafana&#34;&gt;Grafana&lt;/a&gt;, &lt;a href=&#34;https://github.com/cortexproject/cortex&#34;&gt;Cortex&lt;/a&gt; for observability&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How have the certs helped you in your career?&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I would say a lot. As most certifications are hands-on, there is no way to pass the exam unless you know what you are doing. It is proof to your employees and the customers you interact with that you know what exactly you are talking about.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How has CNCF helped you or influenced your career?&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Getting familiar with all these open source projects. Understanding how the community works and how I can assist in the efforts, was a mindset change!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What are some other books/sites/courses you recommend for people who want to work with k8s?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;All training material from Sander Van Vugt on O’Reilly Media&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The Kodekloud team for the DevOps track including the certification preparation material&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The ‘The Linux DevOps Handbook’ By Damian Wojsław, Grzegorz Adamowicz&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The ‘Learning Helm’ By Matt Butcher, Matt Farina, Josh Dolitsky&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The ‘Programming Kubernetes’ By Michael Hausenblas, Stefan Schimanski&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The ‘Getting Started with Grafana: Real-Time Dashboards for IT and Business Operations’ By Ronald McCollam&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And many more!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What do you do in your free time?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I love nature and like to exercise! Usually, you will find me somewhere hiking and enjoying nature or practising rugby in a nearby local field. Spending quality time with family and friends is very important to me.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What would you tell someone who is just starting their K8s certification journey? Any tips or tricks?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Practise, practise, practise. Hands-on and working on projects is what will do the trick! Be confident to put yourself out there, surpass your limits and learn something new every day!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Today the cloud native ecosystem is way more than Kubernetes. Do you plan to get other cloud native certifications from the CNCF?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If it is hands-on, yes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How did you get involved with cloud native and Kubernetes in general?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When I started working with Kubernetes back in 2019-2020.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“1800”高度=“945”src=“https://www.cncf.io/ wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5.jpg&#34; alt=&#34;Eleni headshot&#34; class=&#34;wp-image-114334&#34; srcset=&#34;https://www.cncf.io/wp-内容/uploads/2024/07/Kubestronaut-in-Orbit-5.jpg 1800w，https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-300x158.jpg 300w，https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-1024x538.jpg 1024w，https://www.cncf.io/wp-content/uploads /2024/07/Kubestronaut-in-Orbit-5-768x403.jpg 768w，https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-194x102.jpg 194w ，https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-388x204.jpg 388w，https://www.cncf.io/wp-content/uploads/ 2024/07/Kubestronaut-in-Orbit-5-776x408.jpg 776w，https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-1552x816.jpg 1552w， https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-900x473.jpg 900w，https://www.cncf.io/wp-content/uploads/2024 /07/Kubestronaut-in-Orbit-5-381x200.jpg 381w，https://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-762x400.jpg 762w，https ://www.cncf.io/wp-content/uploads/2024/07/Kubestronaut-in-Orbit-5-590x310.jpg 590w，https://www.cncf.io/wp-content/uploads/2024/ 07/Kubestronaut-in-Orbit-5-1180x620.jpg 1180w“尺寸=”（最大宽度：1800px）100vw，1800px“referrerpolicy=“no-referrer”&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;了解 Eleni&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;本周 Orbit 的 Kubstronaut Eleni Grosdouli 为她担任 Cisco Systems 的 DevOps 咨询工程师带来了丰富的经验。她是 DevOps 和 Kubernetes 自动化的关键人物，对网络、安全、端点管理和端点安全充满热情，&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果您想成为像 Eleni 一样的 Kubestronaut，请在&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt;CNCF Kubestronaut&lt;/a&gt;页面上找到更多详细信息。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您什么时候开始使用 Kubernetes - 您的第一个项目是什么？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我的旅程始于 2019 年。当时，我是一名参与网络自动化项目的网络咨询工程师。在学习了自动化技术之后，我参与了容器创建镜像和Docker。下一个合乎逻辑的步骤是使用用于容器编排的开源产品。就在那时我开始熟悉 Kubernetes。建立家庭实验室只是 Kubetronaut 认证激动人心的旅程的开始。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您现在从事或使用的主要 CNCF 项目是什么？在您的职业生涯中，您最喜欢哪些项目？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cilium/cilium&#34;&gt;Cilium&lt;/a&gt; 作为 CNI 和服务网格&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kyverno&#34;&gt;Kyverno&lt;/a&gt; 用于应用策略&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/projectsveltos&#34;&gt;Sveltos&lt;/a&gt;，用于 Kubernetes 插件和应用程序部署&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/argoproj/argo-cd&#34;&gt;ArgoCD&lt;/a&gt;，用于特定主题的持续部署&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/aquasecurity/kube-bench/&#34;&gt;Kubebench&lt;/a&gt;、&lt;a href=&#34;https://github.com/trivy&#34;&gt;Trivy&lt;/a&gt; &gt;、&lt;a href=&#34;https://github.com/falco&#34;&gt;Falco&lt;/a&gt;、&lt;a href=&#34;http://kubesec.io/&#34;&gt;Kubesec.io&lt;/a&gt; 确保安全性和合规性&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/prometheus&#34;&gt;普罗米修斯&lt;/a&gt;、&lt;a href=&#34;https://github.com/grafana&#34;&gt;Grafana&lt;/a&gt;、&lt;a href =&#34;https://github.com/cortexproject/cortex&#34;&gt;Cortex&lt;/a&gt; 用于可观察性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;这些证书对您的职业生涯有何帮助？ &lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我想说很多。由于大多数认证都是实践性的，除非您知道自己在做什么，否则无法通过考试。这向您的员工和与您互动的客户证明您确切地知道自己在说什么。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;CNCF 如何帮助或影响您的职业生涯？ &lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;熟悉所有这些开源项目。了解社区如何运作以及我如何协助工作，是一种思维方式的改变！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您为想要使用 k8s 的人推荐了哪些其他书籍/网站/课程？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;O’Reilly Media 上 Sander Van Vugt 提供的所有培训材料&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Kodekloud 团队负责 DevOps 轨道，包括认证准备材料&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;《Linux DevOps 手册》，作者：Damian Wojsław、Grzegorz Adamowicz&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;《学习头盔》作者：Matt Butcher、Matt Farina、Josh Dolitsky&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;“Kubernetes 编程”，作者：Michael Hausenblas、Stefan Schimanski&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Ronald McCollam 的《Grafana 入门：IT 和业务运营实时仪表板》&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;还有更多！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;你空闲时间做什么？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我热爱大自然，喜欢运动！通常，你会发现我在某个地方徒步旅行，享受大自然，或者在附近的当地场地练习橄榄球。与家人和朋友共度美好时光对我来说非常重要。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您会对刚刚开始 K8s 认证之旅的人说些什么？有什么提示或技巧吗？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;练习，练习，练习。亲自动手并从事项目才是成功的秘诀！自信地展现自己，超越自己的极限，每天学习新东西！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;如今，云原生生态系统的意义远不止 Kubernetes。您是否计划获得 CNCF 的其他云原生认证？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果是动手的话，是的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您是如何涉足云原生和 Kubernetes 的？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;当我在 2019-2020 年开始使用 Kubernetes 时。&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Mon, 15 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Kubestronaut in Orbit: Peter Barczi】在轨道上的 Kubetronaut：Peter Barczi</title>
      <link>https://www.cncf.io/blog/2024/07/05/kubestronaut-in-orbit-peter-barczi/</link>
      <description>【&lt;h2 class=&#34;wp-block-heading&#34;&gt;Get to know Peter Barczi&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1650&#34; height=&#34;866&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4.jpg&#34; alt=&#34;Kubestronaut in Orbit - Peter Barczi&#34; class=&#34;wp-image-113542&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4.jpg 1650w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-300x157.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1024x537.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-768x403.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-194x102.jpg 194w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-388x204.jpg 388w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-776x408.jpg 776w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1552x816.jpg 1552w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-900x472.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-381x200.jpg 381w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-762x400.jpg 762w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-590x310.jpg 590w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1180x620.jpg 1180w&#34; sizes=&#34;(max-width: 1650px) 100vw, 1650px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Peter, one of our first Kubestronauts, has been working with Kubernetes only since 2021 but has still managed to pass all of CNCF’s Kubernetes certifications. He’s currently the Sr. DevOps Engineer / TechLead at a company building a cloud offering with focus on confidential computing. In his role, he also manages physical servers, clouds, operating Linux OSs and Kubernetes clusters at scale and is an Internal Linux Trainer.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you’d like to be a Kubestronaut, get more details on the&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt; CNCF Kubestronaut &lt;/a&gt;page.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;When did you get started with kubernetes–what was your first project?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I started with K8s as self-learner in 2021.&amp;nbsp;My first project (not surprisingly) was the NGINX web server running on Kubernetes.&amp;nbsp; Later on, I used K8s during the migration of our infra services into Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Nowadays, within my current project, almost everything is a Kubernetes object, even the network switches, so I use it daily.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What are the primary CNCF projects you work on or use today?&amp;nbsp; What projects have you enjoyed the most in your career?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The primary CNCF projects I work with the most are:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes&#34;&gt;Kubernetes&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/fluxcd/&#34;&gt;FluxCD&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/prometheus&#34;&gt;Prometheus&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://github.com/grafana&#34;&gt;Grafana&lt;/a&gt; stack&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cilium&#34;&gt;Cilium&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/rook&#34;&gt;Rook&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Discovering a GitOps approach was a real game-changer in my work life.&amp;nbsp; And thanks to tools like &lt;a href=&#34;https://github.com/argoproj/argo-cd&#34;&gt;ArgoCD &lt;/a&gt;and &lt;a href=&#34;https://github.com/fluxcd/&#34;&gt;FluxCD,&lt;/a&gt; I realised how “easy” it can be to deploy and track apps using git.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Furthermore, thanks to &lt;a href=&#34;https://rook.io/&#34;&gt;Rook-ceph storage provide&lt;/a&gt;r I’m now able to spin up and configure storage clusters in a matter of a few minutes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What motivated you to get all the kubernetes certs?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Each certification has its own story.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;KCNA was my first certification. I took it right before a job interview for extra confidence in my Kubernetes skills.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;CKA &amp;amp; CKAD&amp;nbsp; were two certs I needed to help with a project I was working on.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;CKS was a cert I got as a personal goal.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KCSA was my last one. I got this one right after Kubecon when the Kubestronaut program was announced and realized I only needed one more.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How have the certs helped you in your career?&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The certifications allow me to practice and learn about technology and tools I do not use on a daily basis. Preparation for the certification is a way to get the education needed to be ready for future projects.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What are some other books/sites/courses you recommend for people who want to work with k8s?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I found these online courses were a good starting point for my Kubernetes learning journey:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– CKA and CKAD courses from kodekloud.com&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– CKS online course from Kim Wuestkamp on YouTube&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– CKA/CKAD/CKS practise exams from aclodguru/pluralsight&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What do you do in your free time?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I spend all my free time with my family, my 15-month-old daughter is my single point of interest currently. I also spend time in my cottage house, so I balance my work time there.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;As you know Kubernetes is turning 10 this year, what are you most excited about for Kubernetes in the next 10 years?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;My magic ball doesn’t have answers so far into the future:)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What would you tell someone who is just starting their K8s certification journey? Any tips or tricks?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;There are no tips and tricks better than to practise, practise, practise.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Today the Cloud native ecosystem is way more than Kubernetes. Do you plan to get other cloud native certifications from the CNCF?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Absolutely, the next challenge for me is to dive into the Prometheus stack and become familiar enough with it to become a &lt;a href=&#34;https://www.cncf.io/training/certification/pca/&#34;&gt;Prometheus Certified Associate (PCA)&lt;/a&gt;&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How did you get involved with cloud native and Kubernetes in general?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I have a background in Linux administration, so from there it was a logical and natural to progress to Kubernetes and cloud native.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you’d like to be a Kubestronaut,&amp;nbsp;get more details on the&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt; CNCF Kubestronaut &lt;/a&gt;page.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;h2 class=&#34;wp-block-heading&#34;&gt;了解 Peter Barczi&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“1650”高度=“866”src=“https://www.cncf.io/ wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4.jpg&#34; alt=&#34;轨道上的 Kubestronaut - Peter Barczi&#34; class=&#34;wp-image-113542&#34; srcset=&#34;https://www.cncf. io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4.jpg 1650w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4 -300x157.jpg 300w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1024x537.jpg 1024w，https://www.cncf.io/wp -content/uploads/2024/06/Kubestronaut-in-Orbit-4-768x403.jpg 768w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4- 194x102.jpg 194w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-388x204.jpg 388w，https://www.cncf.io/wp-内容/uploads/2024/06/Kubestronaut-in-Orbit-4-776x408.jpg 776w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1552x816 .jpg 1552w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-900x472.jpg 900w，https://www.cncf.io/wp-content /uploads/2024/06/Kubestronaut-in-Orbit-4-381x200.jpg 381w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-762x400。 jpg 762w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-590x310.jpg 590w，https://www.cncf.io/wp-content/ uploads/2024/06/Kubestronaut-in-Orbit-4-1180x620.jpg 1180w&#34;sizes=&#34;(max-width: 1650px) 100vw, 1650px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Peter 是我们首批 Kubetronaut 之一，他从 2021 年才开始使用 Kubernetes，但仍然设法通过了 CNCF 的所有 Kubernetes 认证。他目前是一家公司的高级 DevOps 工程师/技术主管，该公司致力于构建专注于机密计算的云产品。在此职位上，他还管理物理服务器、云、大规模操作 Linux 操作系统和 Kubernetes 集群，并且是一名内部 Linux 培训师。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果您想成为 Kubestronaut，请在&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt;CNCF Kubestronaut&lt;/a&gt;页面获取更多详细信息。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您什么时候开始使用 kubernetes – 您的第一个项目是什么？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我于 2021 年开始自学 K8s。我的第一个项目（毫不奇怪）是在 Kubernetes 上运行的 NGINX Web 服务器。  后来，我在将基础设施服务迁移到 Kubernetes 的过程中使用了 K8s。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;现在，在我当前的项目中，几乎所有东西都是 Kubernetes 对象，甚至是网络交换机，所以我每天都使用它。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您现在从事或使用的主要 CNCF 项目是什么？  在您的职业生涯中，您最喜欢哪些项目？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我参与最多的主要 CNCF 项目是：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes&#34;&gt;Kubernetes&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/fluxcd/&#34;&gt;FluxCD&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/prometheus&#34;&gt;Prometheus&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/grafana&#34;&gt;Grafana&lt;/a&gt; 堆栈&lt;/li &gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cilium&#34;&gt;Cilium&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/rook&#34;&gt;Rook&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;发现 GitOps 方法真正改变了我的工作生活。  感谢 &lt;a href=&#34;https://github.com/argoproj/argo-cd&#34;&gt;ArgoCD &lt;/a&gt; 和 &lt;a href=&#34;https://github.com/fluxcd/&#34;&gt;FluxCD 等工具， &lt;/a&gt; 我意识到使用 git 部署和跟踪应用程序是多么“容易”。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;此外，感谢 &lt;a href=&#34;https://rook.io/&#34;&gt;Rook-ceph 存储提供商&lt;/a&gt;，我现在只需几分钟即可启动和配置存储集群分钟。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;是什么促使您获得所有 Kubernetes 证书？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;每个认证都有自己的故事。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;KCNA 是我的第一个认证。我在工作面试前拍摄了它，以便对我的 Kubernetes 技能更有信心。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;CKA 和 CKAD 是我需要帮助完成我正在开展的项目的两个证书。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;CKS 是我作为个人目标而获得的证书。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KCSA 是我的最后一个。 Kubecon 结束后，当 Kubetronaut 计划宣布时，我得到了这个，并意识到我只需要再一个。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;这些证书对您的职业生涯有何帮助？ &lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;这些认证使我能够练习和学习我日常不使用的技术和工具。准备认证是获得为未来项目做好准备所需教育的一种方式。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您为想要使用 k8s 的人推荐了哪些其他书籍/网站/课程？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我发现这些在线课程是我的 Kubernetes 学习之旅的一个很好的起点：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– 来自 kodekloud.com 的 CKA 和 CKAD 课程&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– Kim Wuestkamp 在 YouTube 上提供的 CKS 在线课程&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– 来自 aclodguru/pluralsight 的 CKA/CKAD/CKS 模拟考试&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;你空闲时间做什么？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我所有的空闲时间都和家人一起度过，我 15 个月大的女儿目前是我唯一的兴趣点。我也花时间在我的小屋里，所以我在那里平衡我的工作时间。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;正如您所知，Kubernetes 今年已满 10 岁了，您对 Kubernetes 未来 10 年最兴奋的是什么？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我的魔法球到目前为止还没有答案:)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您会对刚刚开始 K8s 认证之旅的人说些什么？有什么提示或技巧吗？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;没有比练习、练习、再练习更好的提示和技巧了。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;如今，云原生生态系统的意义远不止 Kubernetes。您是否计划获得 CNCF 的其他云原生认证？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;当然，我的下一个挑战是深入研究 Prometheus 堆栈并足够熟悉它，成为一名 &lt;a href=&#34;https://www.cncf.io/training/certification/pca/&#34;&gt;Prometheus认证助理 (PCA)&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您是如何涉足云原生和 Kubernetes 的？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我有 Linux 管理背景，因此从那时起，发展到 Kubernetes 和云原生是合乎逻辑且自然的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果您想成为 Kubestronaut，请在&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt;CNCF Kubestronaut&lt;/a&gt;页面获取更多详细信息。 &lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Thu, 04 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【As we reach mid-year 2024, a look at CNCF, Linux Foundation, and top 30 open source project velocity】到了 2024 年年中，我们来看看 CNCF、Linux 基金会和排名前 30 的开源项目速度</title>
      <link>https://www.cncf.io/blog/2024/07/11/as-we-reach-mid-year-2024-a-look-at-cncf-linux-foundation-and-top-30-open-source-project-velocity/</link>
      <description>【&lt;p&gt;&lt;em&gt;Staff post by Chris Aniszczyk&amp;nbsp;&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Date/Time:&lt;/strong&gt; July 11 at 8am&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For the last several years we have tracked open source project velocity, which has enabled us to monitor the trends and technologies that resonate with developers and end users. For comparison, have a look at past timeframes from our &lt;a href=&#34;https://www.cncf.io/?s=Velocity&#34;&gt;blogs&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Here are the main takeaways I see from these charts:&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Kubernetes continues to mature with its consistent and largest contributor base&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;OpenTelemetry continues to grow its contributor base and remains the second highest velocity project; they recently added &lt;a href=&#34;https://opentelemetry.io/blog/2024/profiling/&#34;&gt;profiling&lt;/a&gt; as a new signal type&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Backstage grows solving an important pain point around developer experience&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;GitOps continues to be important in the cloud native ecosystem, where projects like Argo and Flux continue to cultivate large communities&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Crossplane grew its contributor base by over 20% in the last year reflecting a desire for open source control planes in the era of open source relicensing issues&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KeyCloak joined CNCF last year as an incubating project and has a large community pushing open source identity and access management forward&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;In many cases, CNCF projects &lt;a href=&#34;https://www.cncf.io/case-studies/openai/&#34;&gt;underpin large scale AI infrastructure&lt;/a&gt; and we have Kubeflow appearing on the top 30 CNCF project list for the first time in 2023.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;CNCF projects – Last 12 months &lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;interactive map&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXeXEdq4E15JkoUMLwMRdFVFC0jeXT7El7VLVnVChbBOZk-v2I-03MLI5R8HMhgOrnlMS8LC-dQlKrKI4m8ybZHMzQ4Khm4_3dm5bboQhepmg4N9mf2IVWD4dOE4kxEv2T1rmQ0J33ATxZsT4Oi3XsxZh01x?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;cncf projects&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Linux Foundation Projects – Last 12 months &lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507&#34;&gt;interactive map&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXcBhcGzavp5kG5-l6g2MW-hNTvn2BIl2HIkb07TFm2dhUOJEymAhE18jAxE2_hgqitU9vJCRSDZ18EzppV9auq0eHq-qyGiYoT-JxQ5znQRMK_rq-mA_naXmtdQrNeuAeI8paEXZtUSxWxXLbsgRSBZi74?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;LF projects&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Top 30 open source projects – Last 12 months&lt;/strong&gt; (&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507&#34;&gt;interactive map&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdrzMUoz3xCWNweqwfxhzfI5koLh1Z68AbH4oWMjsQsH6FoBfZDx4bpsY2Cwyaxx7ZT9exDO1qtNy-oP00BuSczhhCZYCreLEDjdv0bQssLvSGEDkuVSaNCy2GQyjB0GUJIo4amk3C_St4m7tqM9zj8wAsv?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;Open source projects&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: We use bubble charts to show three axes of data: commits, authors, and comments/pull requests, and plot on a log-log chart to show the data across large scales.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;The bubble’s area is proportional to the number of authors&amp;nbsp;&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;The y-axis is the total number of pull requests and issues&amp;nbsp;&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;The x-axis is the number of commits&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;All of the the &lt;a href=&#34;https://github.com/cncf/velocity#current-reports&#34;&gt;current&lt;/a&gt; and &lt;a href=&#34;https://github.com/cncf/velocity#past-reports&#34;&gt;past&lt;/a&gt; reports are available on GitHub, as well as a list and charts on the Google sheets below:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;All CNCF projects for July 2023-July 2024&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507&#34;&gt;All Linux Foundation projects for July 2023-July 2024&lt;/a&gt;&amp;nbsp;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507&#34;&gt;Top 30 open source projects for July 2023-July 2024&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;All of the scripts used to generate this data are at &lt;a href=&#34;https://github.com/cncf/velocity&#34;&gt;https://github.com/cncf/velocity&lt;/a&gt; (under an Apache 2.0 &lt;a href=&#34;https://www.cncf.io/blog/2017/02/01/cncf-recommends-aslv2/&#34;&gt;license&lt;/a&gt;). If you see any errors, please open an issue there.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Past blog posts about project velocity:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2024/01/17/a-look-back-at-cncf-linux-foundation-and-top-30-open-source-project-velocity-in-2023/&#34;&gt;A look back at CNCF, Linux Foundation, and top 30 open source project velocity in 2023&amp;nbsp;&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/10/27/october-2023-where-we-are-with-velocity-of-cncf-lf-and-top-30-open-source-projects/&#34;&gt;October 2023: where we are with velocity of CNCF, LF, and top 30 open source projects&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/01/11/a-look-at-the-2022-velocity-of-cncf-linux-foundation-and-top-30-open-source-projects/&#34;&gt;A look at the 2022 velocity of CNCF, Linux Foundation, and top 30 open source projects&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2022/08/23/mid-year-update-on-2022-cncf-linux-foundation-and-open-source-velocity/&#34;&gt;Mid-year update on 2022 CNCF, Linux Foundation, and open source velocity&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/12/15/end-of-year-update-on-cncf-and-open-source-velocity-in-2021/&#34;&gt;End of year update on CNCF and open source velocity in 2021&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/08/02/update-on-cncf-and-open-source-project-velocity-2020/&#34;&gt;Update on CNCF and Open Source Project Velocity 2020&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2017/06/05/30-highest-velocity-open-source-projects/&#34;&gt;The 30 highest velocity open source projects&lt;/a&gt;&amp;nbsp;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;Chris Aniszczyk 的员工帖子&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;日期/时间：&lt;/strong&gt;7 月 11 日上午 8 点&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在过去的几年里，我们一直在跟踪开源项目的速度，这使我们能够监控与开发人员和最终用户产生共鸣的趋势和技术。为了进行比较，请查看我们的&lt;a href=&#34;https://www.cncf.io/?s=Velocity&#34;&gt;博客&lt;/a&gt;中过去的时间范围。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;以下是我从这些图表中看到的主要结论：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Kubernetes 凭借其一致且最大的贡献者群体不断走向成熟&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;OpenTelemetry 的贡献者基础不断扩大，并且仍然是第二高速度​​项目；他们最近添加了&lt;a href=&#34;https://opentelemetry.io/blog/2024/profiling/&#34;&gt;分析&lt;/a&gt;作为新的信号类型&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;后台不断发展，解决了开发者体验方面的一个重要痛点&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;GitOps 在云原生生态系统中仍然发挥着重要作用，Argo 和 Flux 等项目继续培育大型社区&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;去年，Crossplane 的贡献者数量增长了 20% 以上，反映出在开源重新许可问题时代对开源控制平面的渴望&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KeyCloak 去年作为孵化项目加入 CNCF，拥有推动开源身份和访问管理向前发展的大型社区&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;在许多情况下，CNCF 项目&lt;a href=&#34;https://www.cncf.io/case-studies/openai/&#34;&gt;支撑大规模人工智能基础设施&lt;/a&gt;，并且 Kubeflow 出现在前 30 名中2023年首次CNCF项目名单。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;CNCF 项目 – 过去 12 个月&lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;交互式地图&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXeXEdq4E15JkoUMLwMRdFVFC0jeXT7El7VLVnVChbBOZk-v2I-03MLI5R8HMhgOrnlMS8LC-dQlKrKI4m8ybZHMzQ4Khm4_ 3dm5bboQhepmg4N9mf2IVWD4dOE4kxEv2T1rmQ0J33ATxZsT4Oi3XsxZh01x?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;cncf 项目&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Linux 基金会项目 – 过去 12 个月&lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507 &#34;&gt;交互式地图&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXcBhcGzavp5kG5-l6g2MW-hNTvn2BIl2HIkb07TFm2dhUOJEymAhE18jAxE2_hgqitU9vJCRSDZ18EzppV9auq0eHq- qyGiYoT-JxQ5znQRMK_rq-mA_naXmtdQrNeuAeI8paEXZtUSxWxXLbsgRSBZi74?key= CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;LF 项目&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;前 30 个开源项目 - 过去 12 个月&lt;/strong&gt; (&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507 &#34;&gt;交互式地图&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdrzMUoz3xCWNweqwfxhzfI5koLh1Z68AbH4oWMjsQsH6FoBfZDx4bpsY2Cwyaxx7ZT9exDO1qtNy-oP00BuSczhhCZY CreLEDjdv0bQssLvSGEDkuVSaNCy2GQyjB0GUJIo4amk3C_St4m7tqM9zj8wAsv?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;打开源项目&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;注意&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;：我们使用气泡图来显示数据的三个轴：提交、作者和评论/拉取请求，并在对数日志上绘制图表显示大范围内的数据。 &lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;气泡面积与作者数量成正比&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;y 轴是拉取请求和问题的总数&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;x 轴是提交次数&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;所有&lt;a href=&#34;https://github.com/cncf/velocity#current-reports&#34;&gt;当前&lt;/a&gt;和&lt;a href=&#34;https://github.com/cncf/ velocity#past-reports&#34;&gt;过去的&lt;/a&gt;报告可在 GitHub 上找到，下面的 Google 工作表上还提供列表和图表：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;2023 年 7 月至 2024 年 7 月的所有 CNCF 项目&lt;/a&gt;&lt;/李&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507&#34;&gt;2023 年 7 月至 2024 年 7 月的所有 Linux 基金会项目&lt;/a &gt; &lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507&#34;&gt;2023 年 7 月至 2024 年 7 月排名前 30 的开源项目&lt;/a&gt; &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;用于生成此数据的所有脚本均位于 &lt;a href=&#34;https://github.com/cncf/velocity&#34;&gt;https://github.com/cncf/velocity&lt;/a&gt;（在Apache 2.0 &lt;a href=&#34;https://www.cncf.io/blog/2017/02/01/cncf-recommends-aslv2/&#34;&gt;许可证&lt;/a&gt;）。如果您发现任何错误，请在那里提出问题。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;过去有关项目速度的博客文章：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2024/01/17/a-look-back-at-cncf-linux-foundation-and-top-30-open-source- project-velocity-in-2023/&#34;&gt;CNCF、Linux 基金会和 2023 年排名前 30 的开源项目速度回顾&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/10/27/october-2023-where-we-are-with-velocity-of-cncf-lf-and-top- 30-open-source-projects/&#34;&gt;2023 年 10 月：CNCF、LF 和前 30 个开源项目的发展速度&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/01/11/a-look-at-the-2022-velocity-of-cncf-linux-foundation-and-top- 30-open-source-projects/&#34;&gt;CNCF、Linux 基金会和前 30 个开源项目 2022 年速度一览&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2022/08/23/mid-year-update-on-2022-cncf-linux-foundation-and-open-source-velocity/ &#34;&gt;2022 年 CNCF、Linux 基金会和开源速度的年中更新&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/12/15/end-of-year-update-on-cncf-and-open-source-velocity-in-2021/ &#34;&gt;CNCF 年终更新和 2021 年开源速度&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/08/02/update-on-cncf-and-open-source-project-velocity-2020/&#34;&gt;CNCF 和更新2020 年开源项目速度&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2017/06/05/30-highest-velocity-open-source-projects/&#34;&gt;30 个速度最快的开源项目&lt;/a &gt; &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Wed, 10 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【WebAssembly components: the next wave of cloud native computing】WebAssembly 组件：云原生计算的下一波浪潮</title>
      <link>https://www.cncf.io/blog/2024/07/09/webassembly-components-the-next-wave-of-cloud-native-computing/</link>
      <description>【&lt;p&gt;&lt;em&gt;Member post by Liam Randall, Cosmonic CEO and CNCF Ambassador and Bailey Hayes, Cosmonic CTO, Bytecode Alliance TSC director, and WASI SG co-chair&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The advent of containers marked an inflection point for computing in the 21st century—a paradigm shift (&lt;a href=&#34;https://en.wikipedia.org/wiki/Paradigm_shift&#34;&gt;per Thomas Kuhn&lt;/a&gt;) that gave rise to the entire cloud native landscape. In 2024, the arrival of WebAssembly components represents a new inflection point, and the next paradigm shift is already underway.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Why components are made for the cloud&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly (Wasm) has been around for about a decade now—much as Linux kernel namespaces were in use for over a decade before the debut of Docker (and 15 years before Kubernetes reached the mainstream). Like the Linux namespace, the core WebAssembly standard has provided a firm foundation to build on.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the case of Wasm, that means we have a bytecode format and virtual instruction set architecture (ISA) that enable us to compile code from any language to a common standard, without needing to build for a particular kernel or architecture. Over the last decade, Wasm’s flexibility has proven itself not only in the browser, but…pretty much everywhere else as well. Today, Wasm is used every where from &lt;a href=&#34;https://www.amazon.science/blog/how-prime-video-updates-its-app-for-more-than-8-000-device-types&#34;&gt;Amazon Prime Video&lt;/a&gt; to &lt;a href=&#34;https://www.youtube.com/watch?v=ms5_0wOl79I&#34;&gt;Google Earth&lt;/a&gt; to &lt;a href=&#34;https://www.cncf.io/blog/2022/11/17/better-together-a-kubernetes-and-wasm-case-study/&#34;&gt;Adobe&lt;/a&gt; to &lt;a href=&#34;https://www.cncf.io/blog/2024/01/05/bringing-webassembly-to-telecoms-with-cncf-wasmcloud/&#34;&gt;telecoms like Orange&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Core Wasm is particularly well-suited to the cloud. In addition to their flexibility, Wasm binaries are tiny, sandboxed, and efficient, allowing for much greater density and speed of download or startup in cloud environments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly &lt;strong&gt;components&lt;/strong&gt; take all of this one step further. (Or many big leaps further, as we’ll see.) Components are WebAssembly binaries conforming to an additional specification called the Component Model. Components bring all the same benefits of ordinary Wasm modules (as the Component Model is built on top of the Core WebAssembly specification), but they are also &lt;strong&gt;interoperable&lt;/strong&gt; and &lt;strong&gt;composable&lt;/strong&gt; with other components:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Interoperability&lt;/strong&gt; means that components can communicate over strictly-defined interfaces. There is a common set of standardized interfaces called the &lt;a href=&#34;https://github.com/WebAssembly/WASI/&#34;&gt;&lt;strong&gt;WASI&lt;/strong&gt;&lt;/a&gt;, consisting of high-level APIs (at various stages of proposal and standardization) for functionality like HTTP, CLI, blob storage, key-value storage, and much more. Developers can use their favorite libraries in their favorite languages, and once they compile the code to a Wasm component, other components can make use of the functions they expose—regardless of the language and libraries used to write &lt;em&gt;those&lt;/em&gt; components.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Composability&lt;/strong&gt; means that multiple components can be combined into a single component. Functions exposed (or “exported”) by one component on a given interface can be used (or “imported”) by another component, and those two components can be compiled together into a single binary. For a microservice—or any two pieces talking, really—composition is much more efficient than sending data over a network boundary where calls within a composed component happen with the same process in nanoseconds vs a network request in milliseconds. If you need to link components dynamically, you can achieve a compositional effect with distributed components using an open source transport protocol like &lt;a href=&#34;https://github.com/wrpc/wrpc&#34;&gt;wRPC (WIT over RPC)&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In short, if core WebAssembly binaries are flat-surfaced, fundamental building blocks, components are studded construction bricks—designed for building sophisticated, interconnected applications in new ways.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1086&#34; height=&#34;506&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/components.jpg&#34; alt=&#34;components&#34; class=&#34;wp-image-113994&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/components.jpg 1086w, https://www.cncf.io/wp-content/uploads/2024/07/components-300x140.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/components-1024x477.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/components-768x358.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/components-900x419.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/components-429x200.jpg 429w, https://www.cncf.io/wp-content/uploads/2024/07/components-858x400.jpg 858w&#34; sizes=&#34;(max-width: 1086px) 100vw, 1086px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If a WASI API doesn’t exist for your use case, no problem—you can write your own interface in the open &lt;strong&gt;WebAssembly Interface Type (WIT)&lt;/strong&gt; interface description language. Open standards make the ecosystem infinitely extensible.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly enthusiasts (like us) often share this quote from Docker founder Solomon Hykes:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;If WASM+WASI existed in 2008, we wouldn&#39;t have needed to create Docker.&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We love that quote! But if we use it without explaining how components work, we run the risk of obscuring what is so transformative about them. It’s a mistake to think of components as a more efficient alternative to containers. Yes, they are efficient, portable delivery mechanisms for cloud native workloads—but components are also an entirely new paradigm that unlocks entirely new models of computing.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;The next wave is already here&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The last couple of years have built to a convergence moment in which key pieces have fallen into place (or rather, been wrestled into place by many, many people working across the ecosystem). The most important of these is the release of WASI 0.2 and the Component Model in January 2024. With a common model and common APIs in place, the community has wasted no time building and updating a wide array of open source tools and native support in standard language libraries. For just a handful of examples:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasmtime&#34;&gt;&lt;strong&gt;Wasmtime&lt;/strong&gt;&lt;/a&gt;: Standalone runtime for WebAssembly components&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasm-tools&#34;&gt;&lt;strong&gt;&lt;code&gt;wasm-tools&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;: A multi-functional tool for interacting with components (read WIT interfaces, compose, and more)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://wasmcloud.com/&#34;&gt;&lt;strong&gt;wasmCloud&lt;/strong&gt;&lt;/a&gt;: Build and run components anywhere, including edge and distributed environments&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wit-deps&#34;&gt;&lt;strong&gt;&lt;code&gt;wit-deps&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;: Manage WIT dependencies for a component project&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasi-virt&#34;&gt;&lt;strong&gt;WASI Virt&lt;/strong&gt;&lt;/a&gt;: Use composition to virtualize a component within another encapsulating component&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Better yet, components increasingly integrate with common cloud native tools and standards:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;OCI has emerged as &lt;a href=&#34;https://tag-runtime.cncf.io/wgs/wasm/deliverables/wasm-oci-artifact/&#34;&gt;the standard for packaging components in registries&lt;/a&gt;.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Platforms like wasmCloud &lt;a href=&#34;https://wasmcloud.com/docs/kubernetes&#34;&gt;integrate with Kubernetes&lt;/a&gt;, OpenTelemetry, and Open Policy Agent.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Teams don’t have to start from square one to run components in production, but can use their existing cloud native tooling. For those brand new to components, the community is developing more and more resources like &lt;a href=&#34;https://component-model.bytecodealliance.org/&#34;&gt;The Component Model Book&lt;/a&gt; and &lt;a href=&#34;https://wasi.dev/&#34;&gt;WASI.dev&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;All the while, the component development experience is growing more and more polished across more and more languages. In the JavaScript ecosystem, &lt;a href=&#34;https://github.com/bytecodealliance/jco&#34;&gt;&lt;code&gt;jco&lt;/code&gt;&lt;/a&gt; enables JavaScript developers to write idiomatic code and compile to WebAssembly, while Rustaceans can compile directly to the &lt;a href=&#34;https://doc.rust-lang.org/nightly/rustc/platform-support/wasm32-wasip2.html&#34;&gt;&lt;code&gt;wasm32-wasip2&lt;/code&gt; target&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Around the world, teams are already unlocking new possibilities with components. In resource-constrained environments like edge devices on factory floors, manufacturing analytics company &lt;a href=&#34;https://www.youtube.com/watch?v=fQdkNGZqYZA&amp;amp;list=PLj6h78yzYM2MQteKoXxICTWiUdZYEw6RI&amp;amp;index=6&#34;&gt;MachineMetrics&lt;/a&gt; is using components to process high-frequency data directly on factory-floor devices and stream it to the cloud in real-time—running in places that containers typically don’t reach, all the while integrating with Kubernetes. Only a handful of months after the release of WASI 0.2, components are already changing what is possible.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Accelerating innovation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Developers—and ecosystems as a whole—don’t necessarily adopt new technologies because they are theoretically more efficient or secure. New ways of working emerge when a technology enables us to be more effective, more productive, more innovative. WebAssembly components are doing just that—breaking down language silos and revealing new opportunities.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Today, we’re about a decade out from the first days of Wasm. In the timeline of containers, this is where we reached an inflection point; the Component Model and WASI 0.2 are ushering in the same sort of paradigm shift. The tooling is there for developers to not just build with WebAssembly, but to be more productive, more effective, and more innovative. The common interfaces of WASI make component development cycles incredibly rapid, and components themselves incredibly flexible. Teams will take components to places that we can’t predict, and from here, the next wave of cloud native computing will only become more transformative.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;Cosmonic 首席执行官兼 CNCF 大使 Liam Randall 和 Cosmonic 首席技术官、字节码联盟 TSC 总监兼 WASI SG 联合主席 Bailey Hayes 发表的会员帖子&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;容器的出现标志着 21 世纪计算的拐点——范式转变（&lt;a href=&#34;https://en.wikipedia.org/wiki/Paradigm_shift&#34;&gt;每 Thomas Kuhn&lt;/a&gt;）这催生了整个云原生景观。 2024 年，WebAssembly 组件的到来代表着一个新的拐点，下一个范式转变已经开始。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;为什么组件是为云而设计的&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly (Wasm) 已经存在了大约十年，就像 Linux 内核命名空间在 Docker 出现之前已经使用了十多年一样（以及 Kubernetes 成为主流之前的 15 年）。与 Linux 命名空间一样，核心 WebAssembly 标准提供了坚实的基础。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;就 Wasm 而言，这意味着我们拥有字节码格式和虚拟指令集架构 (ISA)，使我们能够将任何语言的代码编译为通用标准，而无需针对特定内核或架构进行构建。在过去的十年中，Wasm 的灵活性不仅在浏览器中得到了证明，而且......几乎在其他任何地方都得到了证明。如今，Wasm 已被广泛使用 &lt;a href=&#34;https://www.amazon.science/blog/how-prime-video-updates-its-app-for-more-than-8-000-device-types &#34;&gt;Amazon Prime Video&lt;/a&gt; 至 &lt;a href=&#34;https://www.youtube.com/watch?v=ms5_0wOl79I&#34;&gt;Google 地球&lt;/a&gt; 至 &lt;a href=&#34;https://www. cncf.io/blog/2022/11/17/better-together-a-kubernetes-and-wasm-case-study/&#34;&gt;Adobe&lt;/a&gt; 至 &lt;a href=&#34;https://www.cncf.io /blog/2024/01/05/bringing-web assembly-to-telecoms-with-cncf-wasmcloud/&#34;&gt;像 Orange 这样的电信公司&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Core Wasm 特别适合云。除了灵活性之外，Wasm 二进制文件体积小、沙箱化且高效，可在云环境中实现更高的下载或启动密度和速度。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly &lt;strong&gt;组件&lt;/strong&gt;使这一切更进一步。 （或者我们将看到更多的巨大飞跃。）组件是符合称为组件模型的附加规范的 WebAssembly 二进制文件。组件带来了普通 Wasm 模块的所有相同优势（因为组件模型构建在 Core WebAssembly 规范之上），但它们也可以与其他组件&lt;strong&gt;互操作&lt;/strong&gt;和&lt;strong&gt;可组合&lt;/strong&gt; ：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;互操作性&lt;/strong&gt;意味着组件可以通过严格定义的接口进行通信。有一组通用的标准化接口，称为 &lt;a href=&#34;https://github.com/WebAssembly/WASI/&#34;&gt;&lt;strong&gt;WASI&lt;/strong&gt;&lt;/a&gt;，由高级 API（位于提案和标准化的各个阶段），用于 HTTP、CLI、blob 存储、键值存储等功能。开发人员可以用他们喜欢的语言使用他们喜欢的库，一旦他们将代码编译为 Wasm 组件，其他组件就可以使用它们公开的功能 - 无论用于编写这些功能的语言和库是什么&lt;em&gt;&lt;/em&gt;组件。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;可组合性&lt;/strong&gt;意味着多个组件可以组合成一个组件。给定接口上的一个组件公开（或“导出”）的函数可以由另一组件使用（或“导入”），并且这两个组件可以一起编译成单个二进制文件。对于微服务（或者任何两个正在说话的部分）来说，组合比通过网络边界发送数据要高效得多，在网络边界中，组合组件内的调用在纳秒内通过相同的进程进行，而网络请求则在毫秒内发生。如果您需要动态链接组件，您可以使用开源传输协议（如 &lt;a href=&#34;https://github.com/wrpc/wrpc&#34;&gt;wRPC (WIT over RPC)&lt;/一个&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;简而言之，如果核心 WebAssembly 二进制文件是平面的基本构建块，那么组件就是镶嵌的构造块，旨在以新的方式构建复杂的互连应用程序。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“1086”高度=“506”src=“https://www.cncf.io/ wp-content/uploads/2024/07/components.jpg&#34; alt=&#34;组件&#34; class=&#34;wp-image-113994&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07 /components.jpg 1086w，https://www.cncf.io/wp-content/uploads/2024/07/components-300x140.jpg 300w，https://www.cncf.io/wp-content/uploads/2024 /07/components-1024x477.jpg 1024w，https://www.cncf.io/wp-content/uploads/2024/07/components-768x358.jpg 768w，https://www.cncf.io/wp-content /uploads/2024/07/components-900x419.jpg 900w，https://www.cncf.io/wp-content/uploads/2024/07/components-429x200.jpg 429w，https://www.cncf.io /wp-content/uploads/2024/07/components-858x400.jpg 858w“尺寸=”（最大宽度：1086px）100vw，1086px“referrerpolicy=“no-referrer”&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果您的用例不存在 WASI API，也没有问题 - 您可以使用开放的 &lt;strong&gt;WebAssembly 接口类型 (WIT)&lt;/strong&gt; 接口描述语言编写自己的接口。开放标准使生态系统无限扩展。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly 爱好者（像我们一样）经常分享 Docker 创始人 Solomon Hykes 的这句话：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;如果 WASM+WASI 在 2008 年就存在，我们就不需要创建 Docker。&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们喜欢这句话！但如果我们在不解释组件如何工作的情况下使用它，我们就有可能掩盖它们的变革性。将组件视为比容器更有效的替代品是错误的。是的，它们是云原生工作负载的高效、可移植的交付机制，但组件也是一种全新的范例，可以解锁全新的计算模型。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;下一波浪潮已经到来&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;过去几年已经形成了一个融合时刻，关键部分已经就位（或者更确切地说，是由整个生态系统中的许多人努力就位）。其中最重要的是 2024 年 1 月发布的 WASI 0.2 和组件模型。有了通用模型和通用 API，社区立即构建和更新了各种开源工具和标准的本机支持语言库。仅举几个例子：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasmtime&#34;&gt;&lt;strong&gt;Wasmtime&lt;/strong&gt;&lt;/a&gt;：WebAssembly 组件的独立运行时&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasm-tools&#34;&gt;&lt;strong&gt;&lt;code&gt;wasm-tools&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;：多功能工具用于与组件交互（阅读 WIT 接口、撰写等）&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://wasmcloud.com/&#34;&gt;&lt;strong&gt;wasmCloud&lt;/strong&gt;&lt;/a&gt;：在任何地方构建和运行组件，包括边缘和分布式环境&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wit-deps&#34;&gt;&lt;strong&gt;&lt;code&gt;wit-deps&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;：管理 WIT 依赖项组件项目&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasi-virt&#34;&gt;&lt;strong&gt;WASI Virt&lt;/strong&gt;&lt;/a&gt;：使用组合在另一个封装组件中虚拟化一个组件&lt;/li &gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;更好的是，组件越来越多地与常见的云原生工具和标准集成：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;OCI 已成为&lt;a href=&#34;https://tag-runtime.cncf.io/wgs/wasm/deliverables/wasm-oci-artifact/&#34;&gt;在注册表中打包组件的标准&lt;/a&gt;。 &lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;wasmCloud 等平台&lt;a href=&#34;https://wasmcloud.com/docs/kubernetes&#34;&gt;与 Kubernetes 集成&lt;/a&gt;、OpenTelemetry 和开放策略代理。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;团队不必从头开始在生产中运行组件，而是可以使用现有的云原生工具。对于那些全新的组件，社区正在开发越来越多的资源，例如 &lt;a href=&#34;https://component-model.bytecodealliance.org/&#34;&gt;组件模型手册&lt;/a&gt; 和 &lt;a href=&#34;https ://wasi.dev/&#34;&gt;WASI.dev&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;与此同时，组件开发体验在越来越多的语言中变得越来越完善。在 JavaScript 生态系统中，&lt;a href=&#34;https://github.com/bytecodealliance/jco&#34;&gt;&lt;code&gt;jco&lt;/code&gt;&lt;/a&gt; 使 JavaScript 开发人员能够编写惯用代码并编译为 WebAssembly，而 Rustaceans 可以直接编译到 &lt;a href=&#34;https://doc.rust-lang.org/nightly/rustc/platform-support/wasm32-wasip2.html&#34;&gt;&lt;code&gt;wasm32-wasip2&lt;/code&gt; 目标&lt;/a &gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;世界各地的团队已经在利用组件解锁新的可能性。在工厂车间边缘设备等资源受限的环境中，制造分析公司 &lt;a href=&#34;https://www.youtube.com/watch?v=fQdkNGZqYZA&amp;list=PLj6h78yzYM2MQteKoXxICTWiUdZYEw6RI&amp;index=6&#34;&gt;MachineMetrics&lt;/a&gt; 正在使用组件来直接在工厂车间设备上处理高频数据并将其实时传输到云端——在容器通常无法到达的地方运行，同时与 Kubernetes 集成。 WASI 0.2 发布仅几个月后，组件就已经改变了一切可能。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;加速创新&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;开发者以及整个生态系统不一定会采用新技术，因为它们在理论上更高效或更安全。当技术使我们变得更有效、更有生产力、更具创新性时，新的工作方式就会出现。 WebAssembly 组件正在这样做——打破语言孤岛并揭示新的机会。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;今天，距 Wasm 诞生已有十年了。在容器的时间轴上，这是我们到达一个拐点的地方；组件模型和 WASI 0.2 正在迎来同样的范式转变。该工具不仅可以让开发人员使用 WebAssembly 进行构建，还可以提高生产力、效率和创新性。 WASI 的通用接口使组件开发周期变得异常快速，并且组件本身非常灵活。团队将把组件带到我们无法预测的地方，从这里开始，下一波云原生计算只会变得更具变革性。&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Mon, 08 Jul 2024 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>