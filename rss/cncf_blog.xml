<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>CNCF - Blog</title>
    <link>http://rsshub.rssforever.com/cncf</link>
    <description>CNCF - Blog - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)</description>
    <managingEditor>i@diygod.me (DIYgod)</managingEditor>
    <item>
      <title>【Building the future of 5G with cloud native tech: insights from Joel and Ashan from Swisscom】利用云原生技术构建 5G 的未来：Swisscom 的 Joel 和 Ashan 的见解</title>
      <link>https://www.cncf.io/blog/2024/07/02/building-the-future-of-5g-with-cloud-native-tech-insights-from-joel-and-ashan-from-swisscom/</link>
      <description>【&lt;p&gt;&lt;em&gt;Member post by &lt;a href=&#34;https://www.linkedin.com/in/anjali-udasi/&#34;&gt;Anjali Udasi&lt;/a&gt;, Technical Writer, Zenduty and &lt;a href=&#34;https://www.linkedin.com/in/shubham67/&#34;&gt;Shubham Srivastava&lt;/a&gt;, Developer Relations Engineer, Zenduty&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/joelstudler/&#34;&gt;Joel Studler&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/ashan-senevirathne/&#34;&gt;Ashan Senevirathne&lt;/a&gt; took the stage at KubeCon + CloudNativeCon Europe in Paris with their presentation, “&lt;a href=&#34;https://www.youtube.com/watch?v=crmTnB6Zwt8&#34;&gt;From GitOps to Kubernetes Resource Model&lt;/a&gt;,” highlighting Swisscom’s automation journey in the 5G Core and reflecting the company’s evolution from telco to TechCo.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Their talk was truly compelling, sparking our interest in learning more about their experiences and journey at &lt;a href=&#34;https://www.linkedin.com/company/swisscom/&#34;&gt;Swisscom&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/shubham67/&#34;&gt;Shubham Srivastava&lt;/a&gt;, who currently leads Developer Relations at &lt;a href=&#34;https://www.zenduty.com/&#34;&gt;Zenduty&lt;/a&gt;, had the pleasure of speaking with this dynamic duo: Joel, a DevOps Engineer and System Architect dedicated to building the next generation of mobile networking using cloud-native technologies, and Ashan, a Product Owner overseeing the design, implementation, and delivery of a cloud-native orchestration framework for the mobile organization.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1600&#34; height=&#34;988&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-2.jpg&#34; alt=&#34;Building the Future of 5G with Cloud-Native Tech presented by Ashan Senevirathne and Joel Studler from swisscom&#34; class=&#34;wp-image-113961&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-2.jpg 1600w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-300x185.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-1024x632.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-768x474.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-900x556.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-324x200.jpg 324w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-648x400.jpg 648w&#34; sizes=&#34;(max-width: 1600px) 100vw, 1600px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this blog featuring Joel and Ashan, we peel the layers of the telco world, the struggles of modernization, and the cutting-edge tools these minds put to use. This is a conversation you don’t want to miss!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: It’s great to chat with you both. We’d love to know what an average day for you both looks like. You’re leading DevOps and reliability at Telco with very tight error budgets and room for failure. So what does that look like behind the scenes?&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan: &lt;/strong&gt;For us, our main focus is on developing tooling capabilities for the upcoming 5G core technology, which we find applicable to other areas of the business as well. We put a lot of emphasis on community-driven initiatives. While our main focus is on Kubernetes environments, we also address the transition from legacy-based change management to cloud-native approaches, which requires a shift in organizational mindset.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: My role involves handling technical interfaces with the product, and collaborating closely with Ashan on architecture and engineering. Our daily tasks involve building reliable tools and automation, predominantly through &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; operators. We prioritize designing sustainable and efficient solutions while optimizing existing workloads. Testing and deployment typically occur on live or pre-production clusters.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: Ashan, you mentioned that half of the job is not migrating processes, it’s also migrating the mindsets of the people. So stemming from that, what do you think is the hardest part about maintaining and updating reliability and tooling in a telecom industry that’s typically viewed as being very archaic and having a lot of legacy processes?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan: &lt;/strong&gt;The biggest hurdle in the telecom industry is adapting to a more open and flexible approach to network management. Traditionally, telecom relies on vendor-provided “black box” software, making it difficult to maintain and update tools reliably.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But, now we’re tackling this by:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Greenfield approach:&lt;/strong&gt; Building new 5G core tools from scratch instead of relying on legacy systems.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;CNCF adoption:&lt;/strong&gt; Utilizing tools and concepts from the Cloud Native Computing Foundation (CNCF) for automation and containerization.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Shifting mindset:&lt;/strong&gt; Moving from a “black box” to a “white box” mentality, where software is open and modifiable for better control and reliability.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And also with the fact that we strategically decided to go forward with Kubernetes operators and the Kubernetes concepts for automation, has a big impact on many other topics like change management.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance: How do you implement a change? How do you plan with Kubernetes resources? You don’t control when the change happens. The operator just rolls it out now and then. You don’t control it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It’s a dynamic system, sparking a range of questions that we’ll be discussing a lot in the near future. This shift impacts not just technology but also requires a cultural change within the organization. The company is focusing on education and demonstrations to promote this new way of working.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; As an organization that’s so large and serves millions of people every day with mission-critical services, how do you handle the transition to Kubernetes? Specifically, how do you support Kubernetes and other open-source tools that enhance its capabilities? How do you vet open-source tools and new technologies from the CNCF ecosystem or elsewhere to ensure they’re stable and suitable for our organization?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan:&lt;/strong&gt; From a Kubernetes perspective, we use a vendor-provided distribution for our infrastructure. For deployment, we utilize &lt;a href=&#34;https://fluxcd.io/&#34;&gt;Flux&lt;/a&gt;, along with the external secret operator, cert-manager, and several other mature tools within the ecosystem.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For anything telco-specific, we often develop our solutions and have strong support internally to open-source these projects. This allows us to contribute to the community and encourage contributions from other operators, integrating telco-specific use cases into the Kubernetes ecosystem.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When selecting tools, we prioritize maturity and support from the community and other industry players over novelty. This ensures we choose reliable, well-supported tools rather than simply the latest trends.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: Joel, are there any tools that have caught your eye recently, tools that you’d love to play around with and are watching closely?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: For the development flow, I’m interested in &lt;a href=&#34;https://microcks.io/&#34;&gt;Microcks&lt;/a&gt;. It’s a mocking framework and its innovation lies in its usability both within your IDE and on the Kubernetes cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We’re also exploring testing tools like &lt;a href=&#34;https://litmuschaos.io/&#34;&gt;Litmus&lt;/a&gt; for &lt;a href=&#34;https://www.zenduty.com/blog/chaos-engineering/&#34;&gt;chaos engineering&lt;/a&gt; and &lt;a href=&#34;https://testkube.io/&#34;&gt;Testkube&lt;/a&gt;, a testing wrapper.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance, we’re adopting cert-manager, but in a Mobile Core on-prem environment with black box applications, it’s challenging. We’re pushing vendors to ensure compatibility with cert-manager, despite their tendency to fork and maintain their own versions.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan&lt;/strong&gt;: Additionally, we’re looking into a project called &lt;a href=&#34;https://nephio.org/&#34;&gt;Nephio&lt;/a&gt;, driven by the Linux Foundation. It’s designed for deploying and managing the 5G core in a cloud-native way. While we don’t use Nephio tooling directly, we adapt its framework and thinking. For instance, we’re contributing to and leveraging the &lt;a href=&#34;https://docs.sdcio.dev/&#34;&gt;SDC (Schema Driven Configuration)&lt;/a&gt; tool within the project.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: Are there any problems that you guys face currently and you’re waiting for a tool to come up and solve it? Like a hard problem that you guys would not want to build something to solve and you’re looking for someone else to come up and solve?&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan&lt;/strong&gt;: The problem that we’re seeing is, or the technical challenge that we have is, we have these telco applications and it’s treated as an appliance. And during the lifecycle space or the deployment and the configuration phase, we need to do the deployment in a cloud-native way.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And then what comes on top on the appliance level or the configuration, it’s done in a telco way. So there’s this proprietary interface defined by the telco standards and you need to do this configuration or apply or define these network services outside of the Kubernetes layer. And to achieve this, you need to do all these workarounds on top of that where we need to implement custom operators or find certain ways to bring this, what’s done outside of the Kubernetes layer, more into the in-bed Kubernetes layer.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If there’s an ask, the ask would be to have this configuration done in a Kubernetes native way, which means moving away from these &lt;a href=&#34;https://en.wikipedia.org/wiki/NETCONF&#34;&gt;NETCONF&lt;/a&gt;-based files, into a Kubernetes resource model. This shift would provide significant benefits, especially considering the time and effort we currently invest in making the configuration Kubernetes-native.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: Our biggest pain point right now is that we can’t handle our applications as true cloud-native citizens. The applications we receive from vendors are still treated like traditional hardware, with manual configurations akin to putting a server into a rack or setting up a bare metal appliance.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The mindset remains tied to the idea of a permanent system, like a &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt; release, where changes are made directly on the running system. This approach prevents us from implementing practices like blue-green deployments, and even a simple redeployment becomes a huge effort due to the manual steps involved.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We believe that introducing a cloud-native configuration interface would simplify lifecycle management, updates, and configurations.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;&lt;strong&gt;: &lt;/strong&gt;Observability must be crucial in your journey, especially as you ramp up. What does your observability framework look like? What metrics are you spending most of your time monitoring? We’d love to know more about how you handle &lt;a href=&#34;https://www.zenduty.com/blog/observability-vs-monitoring/&#34;&gt;monitoring and observability&lt;/a&gt; at Swisscom.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel: &lt;/strong&gt;Currently, we use a standard observability stack with &lt;a href=&#34;https://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt; in our clusters and &lt;a href=&#34;https://grafana.com/oss/loki/&#34;&gt;Loki&lt;/a&gt; for logging. For centralized deployment, we use &lt;a href=&#34;https://thanos.io/&#34;&gt;Thanos&lt;/a&gt;. Additionally, we consume an internal Observability-as-a-Service stack that any Swisscom team can use, built on the standard Prometheus-Grafana stack, which integrates well with our Incident Management tooling.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We focus on a minimal, relevant subset of metrics to ensure service health. The 5G core and applications are more complex due to their black-box nature, so we work closely with vendors to identify the right metrics.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan:&lt;/strong&gt; We use HTTP-based requests to monitor golden signals and key performance indicators (KPIs) such as user attachments, latency, and DNS metrics. For example in 5G core, we will have how many users are attached, what’s the latency or some metrics on the DNS.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While some metrics are standard, others are more telco-specific, requiring vendor collaboration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; You mentioned a while ago that you’re spending a lot of effort bolstering site reliability movements within your organization. SRE can be very demanding, as we’re all familiar with it. What’s the story at your org? How are you managing work-life balance, especially in a Telco environment where nothing can go wrong?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan&lt;/strong&gt;: I can speak on behalf of the mobile organization at Swisscom. We have a significant IT side as well, but our focus here is on the mobile sector.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Throughout our SRE journey, we’ve learned that not all Google-defined site reliability engineering practices directly apply to the Telco space. Instead, we’ve shifted our focus to service reliability, defining specific services offered by our mobile organization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance, 5G is infrastructure, but the service is mobile data—like users browsing YouTube. We start by defining these services, identifying the underlying resources, and establishing &lt;a href=&#34;https://www.zenduty.com/blog/understanding-sla-slo-and-sli/&#34;&gt;SLAs and SLOs&lt;/a&gt; for each service. From there, we implement best practices in release engineering, observability, reliability, and security.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the 4G era, particularly with the Evolved Packet Core on virtual machines, we’ve heavily invested in these principles. As we transition to the 5G core, we will apply the same principles, but in a cloud-native way, simplifying processes. This convergence of SRE and cloud-native transformations is key to our approach in the 5G domain.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: Another unique aspect of Swisscom’s approach is encouraging a cultural shift among our engineers. We emphasize that every decision in engineering or operations has a reliability impact. Encouraging individual responsibility and continuous improvement in operations is crucial. Moreover, having management that supports and encourages this mindset is crucial. This cultural shift has had the most significant impact on our organization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We’ve started defining SLOs and maintaining error budgets for our services, but we apply them selectively, not at every resource level. When moving to Kubernetes operators, many SRE concepts, such as reconciliation, are automated by the Kubernetes layer. This automation puts us on the right track, and we’re excited to see the benefits it will bring to the organization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And that wraps up our conversation with Joel and Ashan! It’s always insightful to discuss observability, the demanding nature of SRE, and the innovative tools these reliability heroes are using to build products used by millions of people.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you’re fascinated by reliability and the intricate process of recovering from downtime, check out our &lt;a href=&#34;https://www.zenduty.com/podcast/&#34;&gt;podcast – Incidentally Reliable&lt;/a&gt;, where veterans from Docker, Amazon, Walmart, and other industry-leading organizations, share their experiences, challenges, and success stories from the Cloud Native world.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Authors:&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/anjali-udasi/&#34;&gt;Anjali Udasi&lt;/a&gt; (Technical Writer), &lt;a href=&#34;https://www.linkedin.com/in/shubham67/&#34;&gt;Shubham Srivastava&lt;/a&gt; (Developer Relations Engineer)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Author’s headshot:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image aligncenter size-full is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1104&#34; height=&#34;1600&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image.jpeg&#34; alt=&#34;Anjali Udasi&#34; class=&#34;wp-image-113963&#34; style=&#34;width:220px;height:auto&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image.jpeg 1104w, https://www.cncf.io/wp-content/uploads/2024/07/image-207x300.jpeg 207w, https://www.cncf.io/wp-content/uploads/2024/07/image-707x1024.jpeg 707w, https://www.cncf.io/wp-content/uploads/2024/07/image-768x1113.jpeg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-900x1304.jpeg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-138x200.jpeg 138w, https://www.cncf.io/wp-content/uploads/2024/07/image-276x400.jpeg 276w&#34; sizes=&#34;(max-width: 1104px) 100vw, 1104px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Anjali Udasi &lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image aligncenter size-full is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;746&#34; height=&#34;1004&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19.jpg&#34; alt=&#34;Shubham Srivastava &#34; class=&#34;wp-image-113537&#34; style=&#34;width:218px;height:auto&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19.jpg 746w, https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19-223x300.jpg 223w, https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19-149x200.jpg 149w, https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19-297x400.jpg 297w&#34; sizes=&#34;(max-width: 746px) 100vw, 746px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Shubham Srivastava&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;Member post by &lt;a href=&#34;https://www.linkedin.com/in/anjali-udasi/&#34;&gt;Anjali Udasi&lt;/a&gt;, Technical Writer, Zenduty and &lt;a href=&#34;https://www.linkedin.com/in/shubham67/&#34;&gt;Shubham Srivastava&lt;/a&gt;, Developer Relations Engineer, Zenduty&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/joelstudler/&#34;&gt;Joel Studler&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/ashan-senevirathne/&#34;&gt;Ashan Senevirathne&lt;/a&gt; took the stage at KubeCon + CloudNativeCon Europe in Paris with their presentation, “&lt;a href=&#34;https://www.youtube.com/watch?v=crmTnB6Zwt8&#34;&gt;From GitOps to Kubernetes Resource Model&lt;/a&gt;,” highlighting Swisscom’s automation journey in the 5G Core and reflecting the company’s evolution from telco to TechCo.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Their talk was truly compelling, sparking our interest in learning more about their experiences and journey at &lt;a href=&#34;https://www.linkedin.com/company/swisscom/&#34;&gt;Swisscom&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/shubham67/&#34;&gt;Shubham Srivastava&lt;/a&gt;, who currently leads Developer Relations at &lt;a href=&#34;https://www.zenduty.com/&#34;&gt;Zenduty&lt;/a&gt;, had the pleasure of speaking with this dynamic duo: Joel, a DevOps Engineer and System Architect dedicated to building the next generation of mobile networking using cloud-native technologies, and Ashan, a Product Owner overseeing the design, implementation, and delivery of a cloud-native orchestration framework for the mobile organization.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1600&#34; height=&#34;988&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-2.jpg&#34; alt=&#34;Building the Future of 5G with Cloud-Native Tech presented by Ashan Senevirathne and Joel Studler from swisscom&#34; class=&#34;wp-image-113961&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-2.jpg 1600w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-300x185.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-1024x632.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-768x474.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-900x556.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-324x200.jpg 324w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-2-648x400.jpg 648w&#34; sizes=&#34;(max-width: 1600px) 100vw, 1600px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this blog featuring Joel and Ashan, we peel the layers of the telco world, the struggles of modernization, and the cutting-edge tools these minds put to use. This is a conversation you don’t want to miss!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: It’s great to chat with you both. We’d love to know what an average day for you both looks like. You’re leading DevOps and reliability at Telco with very tight error budgets and room for failure. So what does that look like behind the scenes?&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan: &lt;/strong&gt;For us, our main focus is on developing tooling capabilities for the upcoming 5G core technology, which we find applicable to other areas of the business as well. We put a lot of emphasis on community-driven initiatives. While our main focus is on Kubernetes environments, we also address the transition from legacy-based change management to cloud-native approaches, which requires a shift in organizational mindset.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: My role involves handling technical interfaces with the product, and collaborating closely with Ashan on architecture and engineering. Our daily tasks involve building reliable tools and automation, predominantly through &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; operators. We prioritize designing sustainable and efficient solutions while optimizing existing workloads. Testing and deployment typically occur on live or pre-production clusters.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: Ashan, you mentioned that half of the job is not migrating processes, it’s also migrating the mindsets of the people. So stemming from that, what do you think is the hardest part about maintaining and updating reliability and tooling in a telecom industry that’s typically viewed as being very archaic and having a lot of legacy processes?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan: &lt;/strong&gt;The biggest hurdle in the telecom industry is adapting to a more open and flexible approach to network management. Traditionally, telecom relies on vendor-provided “black box” software, making it difficult to maintain and update tools reliably.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But, now we’re tackling this by:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Greenfield approach:&lt;/strong&gt; Building new 5G core tools from scratch instead of relying on legacy systems.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;CNCF adoption:&lt;/strong&gt; Utilizing tools and concepts from the Cloud Native Computing Foundation (CNCF) for automation and containerization.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Shifting mindset:&lt;/strong&gt; Moving from a “black box” to a “white box” mentality, where software is open and modifiable for better control and reliability.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And also with the fact that we strategically decided to go forward with Kubernetes operators and the Kubernetes concepts for automation, has a big impact on many other topics like change management.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance: How do you implement a change? How do you plan with Kubernetes resources? You don’t control when the change happens. The operator just rolls it out now and then. You don’t control it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It’s a dynamic system, sparking a range of questions that we’ll be discussing a lot in the near future. This shift impacts not just technology but also requires a cultural change within the organization. The company is focusing on education and demonstrations to promote this new way of working.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; As an organization that’s so large and serves millions of people every day with mission-critical services, how do you handle the transition to Kubernetes? Specifically, how do you support Kubernetes and other open-source tools that enhance its capabilities? How do you vet open-source tools and new technologies from the CNCF ecosystem or elsewhere to ensure they’re stable and suitable for our organization?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan:&lt;/strong&gt; From a Kubernetes perspective, we use a vendor-provided distribution for our infrastructure. For deployment, we utilize &lt;a href=&#34;https://fluxcd.io/&#34;&gt;Flux&lt;/a&gt;, along with the external secret operator, cert-manager, and several other mature tools within the ecosystem.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For anything telco-specific, we often develop our solutions and have strong support internally to open-source these projects. This allows us to contribute to the community and encourage contributions from other operators, integrating telco-specific use cases into the Kubernetes ecosystem.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When selecting tools, we prioritize maturity and support from the community and other industry players over novelty. This ensures we choose reliable, well-supported tools rather than simply the latest trends.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: Joel, are there any tools that have caught your eye recently, tools that you’d love to play around with and are watching closely?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: For the development flow, I’m interested in &lt;a href=&#34;https://microcks.io/&#34;&gt;Microcks&lt;/a&gt;. It’s a mocking framework and its innovation lies in its usability both within your IDE and on the Kubernetes cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We’re also exploring testing tools like &lt;a href=&#34;https://litmuschaos.io/&#34;&gt;Litmus&lt;/a&gt; for &lt;a href=&#34;https://www.zenduty.com/blog/chaos-engineering/&#34;&gt;chaos engineering&lt;/a&gt; and &lt;a href=&#34;https://testkube.io/&#34;&gt;Testkube&lt;/a&gt;, a testing wrapper.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance, we’re adopting cert-manager, but in a Mobile Core on-prem environment with black box applications, it’s challenging. We’re pushing vendors to ensure compatibility with cert-manager, despite their tendency to fork and maintain their own versions.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan&lt;/strong&gt;: Additionally, we’re looking into a project called &lt;a href=&#34;https://nephio.org/&#34;&gt;Nephio&lt;/a&gt;, driven by the Linux Foundation. It’s designed for deploying and managing the 5G core in a cloud-native way. While we don’t use Nephio tooling directly, we adapt its framework and thinking. For instance, we’re contributing to and leveraging the &lt;a href=&#34;https://docs.sdcio.dev/&#34;&gt;SDC (Schema Driven Configuration)&lt;/a&gt; tool within the project.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;: Are there any problems that you guys face currently and you’re waiting for a tool to come up and solve it? Like a hard problem that you guys would not want to build something to solve and you’re looking for someone else to come up and solve?&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan&lt;/strong&gt;: The problem that we’re seeing is, or the technical challenge that we have is, we have these telco applications and it’s treated as an appliance. And during the lifecycle space or the deployment and the configuration phase, we need to do the deployment in a cloud-native way.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And then what comes on top on the appliance level or the configuration, it’s done in a telco way. So there’s this proprietary interface defined by the telco standards and you need to do this configuration or apply or define these network services outside of the Kubernetes layer. And to achieve this, you need to do all these workarounds on top of that where we need to implement custom operators or find certain ways to bring this, what’s done outside of the Kubernetes layer, more into the in-bed Kubernetes layer.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If there’s an ask, the ask would be to have this configuration done in a Kubernetes native way, which means moving away from these &lt;a href=&#34;https://en.wikipedia.org/wiki/NETCONF&#34;&gt;NETCONF&lt;/a&gt;-based files, into a Kubernetes resource model. This shift would provide significant benefits, especially considering the time and effort we currently invest in making the configuration Kubernetes-native.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: Our biggest pain point right now is that we can’t handle our applications as true cloud-native citizens. The applications we receive from vendors are still treated like traditional hardware, with manual configurations akin to putting a server into a rack or setting up a bare metal appliance.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The mindset remains tied to the idea of a permanent system, like a &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt; release, where changes are made directly on the running system. This approach prevents us from implementing practices like blue-green deployments, and even a simple redeployment becomes a huge effort due to the manual steps involved.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We believe that introducing a cloud-native configuration interface would simplify lifecycle management, updates, and configurations.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;&lt;strong&gt;: &lt;/strong&gt;Observability must be crucial in your journey, especially as you ramp up. What does your observability framework look like? What metrics are you spending most of your time monitoring? We’d love to know more about how you handle &lt;a href=&#34;https://www.zenduty.com/blog/observability-vs-monitoring/&#34;&gt;monitoring and observability&lt;/a&gt; at Swisscom.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel: &lt;/strong&gt;Currently, we use a standard observability stack with &lt;a href=&#34;https://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt; in our clusters and &lt;a href=&#34;https://grafana.com/oss/loki/&#34;&gt;Loki&lt;/a&gt; for logging. For centralized deployment, we use &lt;a href=&#34;https://thanos.io/&#34;&gt;Thanos&lt;/a&gt;. Additionally, we consume an internal Observability-as-a-Service stack that any Swisscom team can use, built on the standard Prometheus-Grafana stack, which integrates well with our Incident Management tooling.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We focus on a minimal, relevant subset of metrics to ensure service health. The 5G core and applications are more complex due to their black-box nature, so we work closely with vendors to identify the right metrics.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan:&lt;/strong&gt; We use HTTP-based requests to monitor golden signals and key performance indicators (KPIs) such as user attachments, latency, and DNS metrics. For example in 5G core, we will have how many users are attached, what’s the latency or some metrics on the DNS.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While some metrics are standard, others are more telco-specific, requiring vendor collaboration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shubham&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; You mentioned a while ago that you’re spending a lot of effort bolstering site reliability movements within your organization. SRE can be very demanding, as we’re all familiar with it. What’s the story at your org? How are you managing work-life balance, especially in a Telco environment where nothing can go wrong?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ashan&lt;/strong&gt;: I can speak on behalf of the mobile organization at Swisscom. We have a significant IT side as well, but our focus here is on the mobile sector.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Throughout our SRE journey, we’ve learned that not all Google-defined site reliability engineering practices directly apply to the Telco space. Instead, we’ve shifted our focus to service reliability, defining specific services offered by our mobile organization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For instance, 5G is infrastructure, but the service is mobile data—like users browsing YouTube. We start by defining these services, identifying the underlying resources, and establishing &lt;a href=&#34;https://www.zenduty.com/blog/understanding-sla-slo-and-sli/&#34;&gt;SLAs and SLOs&lt;/a&gt; for each service. From there, we implement best practices in release engineering, observability, reliability, and security.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the 4G era, particularly with the Evolved Packet Core on virtual machines, we’ve heavily invested in these principles. As we transition to the 5G core, we will apply the same principles, but in a cloud-native way, simplifying processes. This convergence of SRE and cloud-native transformations is key to our approach in the 5G domain.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Joel&lt;/strong&gt;: Another unique aspect of Swisscom’s approach is encouraging a cultural shift among our engineers. We emphasize that every decision in engineering or operations has a reliability impact. Encouraging individual responsibility and continuous improvement in operations is crucial. Moreover, having management that supports and encourages this mindset is crucial. This cultural shift has had the most significant impact on our organization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We’ve started defining SLOs and maintaining error budgets for our services, but we apply them selectively, not at every resource level. When moving to Kubernetes operators, many SRE concepts, such as reconciliation, are automated by the Kubernetes layer. This automation puts us on the right track, and we’re excited to see the benefits it will bring to the organization.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;And that wraps up our conversation with Joel and Ashan! It’s always insightful to discuss observability, the demanding nature of SRE, and the innovative tools these reliability heroes are using to build products used by millions of people.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you’re fascinated by reliability and the intricate process of recovering from downtime, check out our &lt;a href=&#34;https://www.zenduty.com/podcast/&#34;&gt;podcast – Incidentally Reliable&lt;/a&gt;, where veterans from Docker, Amazon, Walmart, and other industry-leading organizations, share their experiences, challenges, and success stories from the Cloud Native world.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Authors:&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/anjali-udasi/&#34;&gt;Anjali Udasi&lt;/a&gt; (Technical Writer), &lt;a href=&#34;https://www.linkedin.com/in/shubham67/&#34;&gt;Shubham Srivastava&lt;/a&gt; (Developer Relations Engineer)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Author’s headshot:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image aligncenter size-full is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1104&#34; height=&#34;1600&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image.jpeg&#34; alt=&#34;Anjali Udasi&#34; class=&#34;wp-image-113963&#34; style=&#34;width:220px;height:auto&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image.jpeg 1104w, https://www.cncf.io/wp-content/uploads/2024/07/image-207x300.jpeg 207w, https://www.cncf.io/wp-content/uploads/2024/07/image-707x1024.jpeg 707w, https://www.cncf.io/wp-content/uploads/2024/07/image-768x1113.jpeg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-900x1304.jpeg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-138x200.jpeg 138w, https://www.cncf.io/wp-content/uploads/2024/07/image-276x400.jpeg 276w&#34; sizes=&#34;(max-width: 1104px) 100vw, 1104px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Anjali Udasi &lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image aligncenter size-full is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;746&#34; height=&#34;1004&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19.jpg&#34; alt=&#34;Shubham Srivastava &#34; class=&#34;wp-image-113537&#34; style=&#34;width:218px;height:auto&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19.jpg 746w, https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19-223x300.jpg 223w, https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19-149x200.jpg 149w, https://www.cncf.io/wp-content/uploads/2024/06/Screenshot-2024-06-28-at-07.31.19-297x400.jpg 297w&#34; sizes=&#34;(max-width: 746px) 100vw, 746px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Shubham Srivastava&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Mon, 01 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Celebrating 10 years of Kubernetes: the evolution of database operators】庆祝 Kubernetes 十周年：数据库运营商的演变</title>
      <link>https://www.cncf.io/blog/2024/06/28/celebrating-10-years-of-kubernetes-the-evolution-of-database-operators/</link>
      <description>【&lt;p&gt;&lt;em&gt;Ambassador post by Edith Puclla, CNCF Ambassador and Tech Evangelist at Percona&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Since its launch in June 2014, Kubernetes has revolutionized container orchestration, transforming how applications are managed and scaled.&amp;nbsp; The Data on Kubernetes Community (DoKC) created an infographic to celebrate Kubernetes’ tenth anniversary and highlight key milestones and community contributions to the evolution of operators for managing stateful applications. This infographic was made possible through the collaboration of several members of the DoKC: Sergey Pronin, Robert Hodges, Gabriele Bartolini, Chris Malarky, &lt;a href=&#34;mailto:mark.kember@onebite.co.uk&#34;&gt;Mark Kember&lt;/a&gt;, &lt;a href=&#34;mailto:paul@constantia.io&#34;&gt;Paul Au&lt;/a&gt; and &lt;a href=&#34;mailto:luciano.stabel@percona.com&#34;&gt;Luciano Stabel&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1600&#34; height=&#34;1134&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6.jpg&#34; alt=&#34;Timeline for Database on Kubernetes&#34; class=&#34;wp-image-113957&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6.jpg 1600w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-300x213.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-1024x726.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-768x544.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-900x638.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-282x200.jpg 282w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-564x400.jpg 564w&#34; sizes=&#34;(max-width: 1600px) 100vw, 1600px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Early Days and Key Developments&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;2014&lt;/strong&gt;: Kubernetes was introduced by Google as an open-source container orchestration platform. The initial version 1.0 was released in July 2015, supporting stateless applications but not stateful workloads like databases.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;2015&lt;/strong&gt;: Kubernetes 1.1 brought performance upgrades and new features. However, data and storage management remained underdeveloped.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;2016&lt;/strong&gt;: CoreOS introduced the operator concept, significantly simplifying the deployment and management of complex applications, including databases on Kubernetes. StatefulSets were also introduced providing stable network identifiers and persistent storage which are crucial for database management.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Database Operators and Community Innovations&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;2017&lt;/strong&gt;: The first Kubernetes Operators for databases emerged:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;MySQL Operator&lt;/strong&gt;: Launched by Oracle.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;MongoDB Operator&lt;/strong&gt;: Developed by MongoDB Inc.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;2018&lt;/strong&gt;: Couchbase and PostgreSQL Operators were introduced, enhancing the automated management of Couchbase clusters and providing support for PostgreSQL databases.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;2019&lt;/strong&gt;: ClickHouse and&lt;strong&gt; Cassandra Operators&lt;/strong&gt; were launched. &lt;a href=&#34;https://www.percona.com/software/percona-operators&#34;&gt;Percona&lt;/a&gt; introduced its own set of Kubernetes Operators for managing Percona server instances for MySQL and MongoDB, improving database management capabilities in Kubernetes environments.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Community Growth and Collaborative Efforts&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;By 2020, the Data on Kubernetes Community (DoKC) was established with the main goal of collaboration and sharing best practices for running data-intensive applications on Kubernetes.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Later, the Kubernetes Special Interest Groups (SIGs) focusing on storage, big data, and applications emerged as key collaborative working groups. These groups have produced valuable resources, such as the &lt;a href=&#34;https://github.com/cncf/tag-storage/blob/master/data-on-kubernetes-whitepaper/data-on-kubernetes-whitepaper-databases.md&#34;&gt;Data on Kubernetes Whitepaper&lt;/a&gt;, produced in collaboration with the CNCF Storage TAG.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Adoption and Impact&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;According to the CNCF, 84% of organizations are either using or evaluating Kubernetes, and 70% run stateful applications on Kubernetes in production. More users and containers have been added over time, evidenced by an increased number of contributors, greater adoption of cloud-native technologies, and more use cases for handling stateful applications in Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Looking Ahead&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As we celebrate a decade of Kubernetes, the integration of databases continues to evolve driven by community collaboration and technological advancements. A good example of this is Percona Everest. Percona Everest goes beyond being just a Kubernetes Operator for databases. It represents the future of databases on Kubernetes. It provides an easy way to run any type of database on Kubernetes clusters in the cloud and is completely open-source. Would you like to give it a try? Feel free to visit our &lt;a href=&#34;https://github.com/percona/everest&#34;&gt;GitHub repository&lt;/a&gt; and give us a star if you find it useful. For any feedback or comments, you can write to us in our &lt;a href=&#34;https://forums.percona.com/c/percona-everest/81&#34;&gt;Pecona Forum for Percona Everest&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;945&#34; height=&#34;221&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-1.jpg&#34; alt=&#34;Percona forum on github for Perscona Everest&#34; class=&#34;wp-image-113958&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/image-6-1.jpg 945w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-1-300x70.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-1-768x180.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-1-900x210.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/image-6-1-600x140.jpg 600w&#34; sizes=&#34;(max-width: 945px) 100vw, 945px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For more details, explore the rich history and ongoing developments in the Kubernetes ecosystem at&lt;a href=&#34;https://dok.community/&#34;&gt; Data on Kubernetes Community&lt;/a&gt; and join the conversation for the future of database management on Kubernetes.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;由 CNCF 大使兼 Percona 技术布道师 Edith Puclla 担任大使职务&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;自 2014 年 6 月推出以来，Kubernetes 彻底改变了容器编排，改变了应用程序的管理和扩展方式。  Data on Kubernetes Community (DoKC) 创建了一个信息图来庆祝 Kubernetes 十周年，并强调了关键里程碑和社区对管理有状态应用程序的操作员发展的贡献。此信息图是通过 DoKC 的几位成员的合作而得以实现的：Sergey Pronin、Robert Hodges、Gabriele Bartolini、Chris Malarky、&lt;a href=&#34;mailto:mark.kember@onebite.co.uk&#34;&gt;Mark Kember&lt;/a &gt;、&lt;a href=&#34;mailto:paul@constantia.io&#34;&gt;Paul Au&lt;/a&gt; 和 &lt;a href=&#34;mailto:luciano.stabel@percona.com&#34;&gt;Luciano Stabel&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“1600”高度=“1134”src=“https://www.cncf.io/ wp-content/uploads/2024/07/image-6.jpg&#34; alt=&#34;Kubernetes 上数据库的时间线&#34; class=&#34;wp-image-113957&#34; srcset=&#34;https://www.cncf.io/wp-content /uploads/2024/07/image-6.jpg 1600w，https://www.cncf.io/wp-content/uploads/2024/07/image-6-300x213.jpg 300w，https://www.cncf .io/wp-content/uploads/2024/07/image-6-1024x726.jpg 1024w，https://www.cncf.io/wp-content/uploads/2024/07/image-6-768x544.jpg 768w ，https://www.cncf.io/wp-content/uploads/2024/07/image-6-900x638.jpg 900w，https://www.cncf.io/wp-content/uploads/2024/07/ image-6-282x200.jpg 282w，https://www.cncf.io/wp-content/uploads/2024/07/image-6-564x400.jpg 564w“尺寸=”（最大宽度：1600px）100vw， 1600px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;早期和主要发展&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;2014 年&lt;/strong&gt;：Google 推出 Kubernetes 作为开源容器编排平台。初始版本 1.0 于 2015 年 7 月发布，支持无状态应用程序，但不支持数据库等有状态工作负载。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;2015&lt;/strong&gt;：Kubernetes 1.1 带来了性能升级和新功能。然而，数据和存储管理仍然不发达。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;2016&lt;/strong&gt;：CoreOS 引入了 Operator 概念，显着简化了复杂应用程序的部署和管理，包括 Kubernetes 上的数据库。还引入了 StatefulSet，提供稳定的网络标识符和持久存储，这对于数据库管理至关重要。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;数据库运营商和社区创新&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;2017&lt;/strong&gt;：第一个数据库 Kubernetes Operator 出现：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;MySQL Operator&lt;/strong&gt;：由 Oracle 推出。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;MongoDB Operator&lt;/strong&gt;：由 MongoDB Inc. 开发&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;2018&lt;/strong&gt;：推出 Couchbase 和 PostgreSQL Operator，增强了 Couchbase 集群的自动化管理并提供对 PostgreSQL 数据库的支持。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;2019&lt;/strong&gt;：ClickHouse 和&lt;strong&gt;Cassandra Operators&lt;/strong&gt; 推出。 &lt;a href=&#34;https://www.percona.com/software/percona-operators&#34;&gt;Percona&lt;/a&gt; 引入了自己的一组 Kubernetes Operators，用于管理 MySQL 和 MongoDB 的 Percona 服务器实例，从而提高 Kubernetes 中的数据库管理功能环境。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;社区发展和协作努力&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;到 2020 年，Data on Kubernetes Community (DoKC) 成立，主要目标是协作和分享在 Kubernetes 上运行数据密集型应用程序的最佳实践。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;后来，专注于存储、大数据和应用程序的 Kubernetes 特别兴趣组 (SIG) 成为关键的协作工作组。这些小组提供了宝贵的资源，例如 &lt;a href=&#34;https://github.com/cncf/tag-storage/blob/master/data-on-kubernetes-whitepaper/data-on-kubernetes-whitepaper-databases .md&#34;&gt;Kubernetes 白皮书上的数据&lt;/a&gt;，与 CNCF Storage TAG 合作制作。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;采用和影响&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;根据 CNCF 的数据，84% 的组织正在使用或评估 Kubernetes，70% 的组织在生产中在 Kubernetes 上运行有状态应用程序。随着时间的推移，添加了更多的用户和容器，贡献者数量的增加、云原生技术的更多采用以及在 Kubernetes 中处理有状态应用程序的更多用例就证明了这一点。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;展望未来&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在我们庆祝 Kubernetes 十周年之际，数据库的集成在社区协作和技术进步的推动下不断发展。 Percona Everest 就是一个很好的例子。 Percona Everest 不仅仅是一个数据库 Kubernetes Operator。它代表了 Kubernetes 上数据库的未来。它提供了一种在云中的 Kubernetes 集群上运行任何类型数据库的简单方法，并且完全开源。您想尝试一下吗？请随时访问我们的 &lt;a href=&#34;https://github.com/percona/everest&#34;&gt;GitHub 存储库&lt;/a&gt;，如果您觉得有用，请给我们一个星星。如需任何反馈或意见，您可以在我们的 &lt;a href=&#34;https://forums.percona.com/c/percona-everest/81&#34;&gt;Percona Everest 的 Pecona 论坛&lt;/a&gt;中给我们写信。&lt;/p &gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“945”高度=“221”src=“https://www.cncf.io/ wp-content/uploads/2024/07/image-6-1.jpg&#34; alt=&#34;Percona Everest github 上的 Percona 论坛&#34; class=&#34;wp-image-113958&#34; srcset=&#34;https://www.cncf.io /wp-content/uploads/2024/07/image-6-1.jpg 945w，https://www.cncf.io/wp-content/uploads/2024/07/image-6-1-300x70.jpg 300w ，https://www.cncf.io/wp-content/uploads/2024/07/image-6-1-768x180.jpg 768w，https://www.cncf.io/wp-content/uploads/2024/ 07/image-6-1-900x210.jpg 900w，https://www.cncf.io/wp-content/uploads/2024/07/image-6-1-600x140.jpg 600w&#34; 尺寸 = &#34;(最大-宽度：945px) 100vw，945px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如需了解更多详细信息，请访问&lt;a href=&#34;https://dok.community/&#34;&gt;Kubernetes 社区数据&lt;/a&gt;探索 Kubernetes 生态系统的悠久历史和持续发展，并加入关于未来的对话Kubernetes 上的数据库管理。&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Thu, 27 Jun 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Slurm: An HPC workload manager】Slurm：HPC 工作负载管理器</title>
      <link>https://www.cncf.io/blog/2024/07/08/slurm-an-hpc-workload-manager/</link>
      <description>【&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/&#34;&gt;SuperOrbital’s blog&lt;/a&gt; by Sean Kane&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/cover.jpg&#34; alt=&#34;datacenter servers&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this article, we are going to explore&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm&#34;&gt;Slurm&lt;/a&gt;, a popular open-source high-performance computing (HPC&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:hpc&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;) workload manager, and discover what it is, why people use it, and how it differs from Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For context, when we talk about high-performance computing, we are primarily talking about computational tasks that require a lot of time on very powerful computer systems and are often designed, so that they can be&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Parallel_computing&#34;&gt;highly parallelized&lt;/a&gt;, across resources within a super-computer or large cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These sorts of jobs are very common in scientific research,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_simulation&#34;&gt;computer simulation&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_intelligence&#34;&gt;artificial intelligence&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&gt;machine learning&lt;/a&gt;&amp;nbsp;tasks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;what-is-slurm&#34;&gt;What is Slurm?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To almost directly quote the Slurm repository&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm/blob/master/README.rst#slurm-workload-manager&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;Slurm is an open-source cluster resource management and job scheduling system for Linux (&lt;em&gt;primarily&lt;/em&gt;) that strives to be simple, scalable, portable, fault-tolerant, and interconnect agnostic. As a cluster resource manager, Slurm provides three key functions:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Second, it provides a framework for starting, executing, and monitoring work (&lt;em&gt;normally a parallel job&lt;/em&gt;) on the set of allocated nodes.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;And finally, it arbitrates conflicting requests for resources by managing a queue of pending work.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In short, Slurm is a system that is primarily designed to manage&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Batch_processing&#34;&gt;batch processing&lt;/a&gt;&amp;nbsp;workloads running in a shared&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_cluster&#34;&gt;computing cluster&lt;/a&gt;. Slurm can be configured to run any type of batch job but is often utilized on systems that support multiple users with parallel computing tasks to run.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;According to&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;, the primary support and development company for the Slurm community, Slurm is being used on approximately 65% of the&amp;nbsp;&lt;a href=&#34;https://top500.org/lists/top500/2024/06/&#34;&gt;TOP500 supercomputers in the world&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;a-bit-of-history&#34;&gt;A bit of history&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the mid-1990s, as Linux became more popular, there was an increased interest in making large-scale computer processing more accessible through the use of commodity hardware and projects like&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Beowulf_cluster&#34;&gt;Beowulf&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In 2001, engineers at the&amp;nbsp;&lt;a href=&#34;https://www.llnl.gov/&#34;&gt;Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;(LLNL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:llnl&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;) in the United States, started looking into how they might migrate from proprietary HPC systems to open-source solutions. They determined that there weren’t any good resource managers available at the time, and this directly led to the creation of Slurm, which was initially released in 2002.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;a href=&#34;https://slurm.schedmd.com/slurm_design.pdf&#34;&gt;The name Slurm is a reference&lt;/a&gt;&amp;nbsp;to the&amp;nbsp;&lt;a href=&#34;https://futurama.fandom.com/wiki/Slurm&#34;&gt;soda&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://www.imdb.com/title/tt0149460/&#34;&gt;Futurama&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the years, Slurm has evolved to support a wide variety of hardware configurations and in 2010, some initial scheduling logic was added to Slurm, enabling it to function as both a stand-alone resource manager and a scheduler&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:history&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-architecture&#34;&gt;The Architecture&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Like many similar systems, a Slurm setup consists of at least one controller node, and at least one compute node, which can be on the same physical systems.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/slurm-layout.gif&#34; alt=&#34;Diagram of the basic Slurm architecture&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;the-controller&#34;&gt;The Controller&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Two daemons are responsible for managing the cluster. These two processes can be run on a single node or separated, depending on the performance and resiliency required by the administrator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmctld&#34;&gt;slurmctld&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmctld&lt;/code&gt;&amp;nbsp;is the primary management daemon for Slurm. It is responsible for all the daemons, resources, and jobs, within the cluster. Because it is a critical component in the cluster, it is often configured with a passive backup that can take over management if the primary daemon ever fails.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmdbd&#34;&gt;slurmdbd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmdbd&lt;/code&gt;&amp;nbsp;provides an interface for the&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/MySQL&#34;&gt;MySQL database&lt;/a&gt;&amp;nbsp;that Slurm uses for stateful data, like job accounting records.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;compute-nodes&#34;&gt;Compute Nodes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These represent the nodes whose resources are available to any jobs that might need to be scheduled in the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmd&#34;&gt;slurmd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmd&lt;/code&gt;&amp;nbsp;is the daemon that runs on each compute node and is responsible for accepting work, launching tasks, and generally managing the workloads on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SchedMD maintains the core documentation for Slurm and the&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/quickstart_admin.html#quick_start&#34;&gt;current installation documentation&lt;/a&gt;&amp;nbsp;can be found on their website.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Getting Slurm working is not particularly difficult, but fine-tuning it for your requirements can be a time-consuming task. Since performance is often a primary focus with these clusters and Slurm is designed to work with a very wide set of hardware and software libraries, it can take a lot of effort to fine-tune the installation, so that it utilizes the hardware to the best of its ability, and all the proper software is available to streamline the types of jobs that will be run on the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;At a minimum clusters will require node name resolution, a MySQL instance, shared block storage, a synchronized time source, synchronized users, the&amp;nbsp;&lt;a href=&#34;https://dun.github.io/munge/&#34;&gt;Munge&lt;/a&gt;&amp;nbsp;daemon for user authentication, a full set of custom-built Slurm executables, and various configuration files.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In addition to this, the nodes will need to contain the various hardware, programming languages, libraries, and other tools that the majority of jobs will require to run. This could include things like GPUs&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:gpu&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;and their related toolchains (CUDA&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cuda&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;/cuDNN&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cudnn&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;), MPI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:mpi&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;libraries, OCI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:oci&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;runtimes for&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/containers.html&#34;&gt;container support&lt;/a&gt;, and much more.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm’s greatest asset and liability is how flexible it is. This makes it something that can be highly optimized for the requirements of the organization, while also making it susceptible to misconfiguration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;workloads&#34;&gt;Workloads&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To provide some context, let’s take a look at an example user job and workflow that utilizes some of the features of Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the&amp;nbsp;&lt;code&gt;mpi-ping-pong.py&lt;/code&gt;&amp;nbsp;code below, you will find a&amp;nbsp;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt;&amp;nbsp;script that simulates a pseudo-game of&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Table_tennis&#34;&gt;Ping Pong/Table Tennis&lt;/a&gt;&amp;nbsp;by making use of MPI to pass a message (&lt;em&gt;“ball”&lt;/em&gt;) between as many processes as we have configured to participate in the parallel job.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;local-testing&#34;&gt;Local Testing&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run this code, you simply need to have&amp;nbsp;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/quickstart.html&#34;&gt;OpenMPI&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://mpi4py.readthedocs.io/en/stable/install.html&#34;&gt;mpi4py&lt;/a&gt;&amp;nbsp;installed on your system.mpi-ping-pong.py&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/mpi-ping-pong.py&#34;&gt;mpi-ping-pong.py&lt;/a&gt;&amp;nbsp;is what many would refer to as an&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Embarrassingly_parallel&#34;&gt;embarrassingly parallel&lt;/a&gt;&amp;nbsp;workload because it can be scaled with no real effort at all.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you run this Python application on your local system, you will see something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ python ./mpi-ping-pong.py&#xA;&#xA;Hello, World! I am 🏓 Player (Rank)  0 . Process  1  of  1  on  laptop.local&#xA;Player (Rank)  0 &#39;s receiving 🏓 neighbor is Player (Rank) 0 and my sending 🏓 neighbor is Player (Rank) 0.&#xA;Player 0 started round 1 by hitting the 🏓 ball towards the wall.&#xA;Player 0 received the 🏓 ball that bounced off the wall.&#xA;Player 0 started round 2 by hitting the 🏓 ball towards the wall.&#xA;Player 0 received the 🏓 ball that bounced off the wall.&#xA;…&#xA;Player 0 started round 299 by hitting the 🏓 ball towards the wall.&#xA;Player 0 received the 🏓 ball that bounced off the wall.&#xA;🎉 Player 0 won the game of 🏓 ping pong, since they were playing alone! 🎉&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As you can see the application runs perfectly fine, and uses a single rank&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:rank&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;/process (&lt;em&gt;CPU&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cpu&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;core, in this case&lt;/em&gt;). Since we only have a single process participating in this game of Ping Pong, the program simply sends and receives messages to and from from itself.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, assuming that we have at least two CPU cores, then we can run this application utilizing as many of them as we want, by leveraging the MPI command line tool&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run 12 instances of our application, which will all communicate with each other in parallel across 12 cores on our system, we can do the following on our local system:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: These messages will be somewhat out of order. This is the nature of parallel computing, buffers, etc. To deal with this you would likely need to send all the messages back to single process for ordering and printing.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ mpirun -np 12 ./mpi-ping-pong.py&#xA;&#xA;Player (Rank)  0 &#39;s receiving 🏓 neighbor is Player (Rank) 11 and my sending 🏓 neighbor is Player (Rank) 1.&#xA;Hello, World! I am 🏓 Player (Rank)  0 . Process  1  of  12  on  laptop.local&#xA;Player (Rank)  1 &#39;s receiving 🏓 neighbor is Player (Rank) 0 and my sending 🏓 neighbor is Player (Rank) 2.&#xA;Hello, World! I am 🏓 Player (Rank)  1 . Process  2  of  12  on  laptop.local&#xA;Player (Rank)  2 &#39;s receiving 🏓 neighbor is Player (Rank) 1 and my sending 🏓 neighbor is Player (Rank) 3.&#xA;Hello, World! I am 🏓 Player (Rank)  2 . Process  3  of  12  on  laptop.local&#xA;…&#xA;🛑 Player 0 is bored of 🏓 ping pong and is quitting! 🛑&#xA;🛑 Player 1 is bored of 🏓 ping pong and is quitting! 🛑&#xA;🛑 Player 2 is bored of 🏓 ping pong and is quitting! 🛑&#xA;…&#xA;🎉 Player 11 won the game of 🏓 ping pong, since everyone else quit! 🎉&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;massive-scale-with-slurm&#34;&gt;Massive Scale with Slurm&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Now, imagine that we want to run this across hundreds of CPUs. Slurm makes this trivial as long as you have enough systems in your cluster to support this work.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In Slurm, you can define a batch job with a shell script (&lt;em&gt;e.g. /shared_storage/mpi-ping-pong.slurm&lt;/em&gt;) that looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;#!/bin/bash&#xA;&#xA;#SBATCH -J mpi-ping-pong                         # Name the job&#xA;#SBATCH -o /shared_storage/mpi-ping-pong-%j.out  # The standard output file&#xA;#SBATCH -e /shared_storage/mpi-ping-pong-%j.err  # The standard error file&#xA;#SBATCH -t 0-2:00:00         # Run for a max time of 0d, 2h, 0m, 0s&#xA;#SBATCH --nodes=3            # Request 3 nodes&#xA;#SBATCH --ntasks-per-node=1  # Request 1 cores or task per node&#xA;&#xA;echo &#34;Date start                = $(date)&#34;&#xA;echo &#34;Initiating Host           = $(hostname)&#34;&#xA;echo &#34;Working Directory         = $(pwd)&#34;&#xA;echo &#34;&#34;&#xA;echo &#34;Number of Nodes Allocated = ${SLURM_JOB_NUM_NODES}&#34;&#xA;echo &#34;Number of Tasks Allocated = ${SLURM_NTASKS}&#34;&#xA;echo &#34;&#34;&#xA;&#xA;mpirun python /shared_storage/mpi-ping-pong.py&#xA;RETURN=${?}&#xA;&#xA;echo &#34;&#34;&#xA;echo &#34;Exit code                 = ${RETURN}&#34;&#xA;echo &#34;Date end                  = $(date)&#34;&#xA;echo &#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Each&amp;nbsp;&lt;code&gt;#SBATCH&lt;/code&gt;&amp;nbsp;line defines an&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/sbatch.html&#34;&gt;&lt;code&gt;sbatch&lt;/code&gt;&lt;/a&gt;&amp;nbsp;command line argument to be used for the job. In this case, we are going to request that the workload run on three nodes and that it will only use a single core on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that all the compute nodes in our cluster are configured correctly and our Python script exists on the shared storage at&amp;nbsp;&lt;em&gt;/shared_storage/mpi-ping-pong.py&lt;/em&gt;&amp;nbsp;then we can submit this job to the cluster, by running:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ sbatch /shared_storage/mpi-ping-pong.slurm&#xA;&#xA;Submitted batch job 87&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This job will run very fast, but if we needed to we could use commands like&amp;nbsp;&lt;code&gt;squeue&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;sinfo&lt;/code&gt;&amp;nbsp;to examine the state of the cluster and jobs while it was running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When the job is finished we can look at&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.err&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.out&lt;/code&gt;&amp;nbsp;to see all the output from our application. In this case, the error file is empty, but our output file looks like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;Date start                = Tue May 14 10:15:02 PM UTC 2024&#xA;Initiating Host           = worker-0&#xA;Working Directory         = /shared_storage&#xA;&#xA;Number of Nodes Allocated = 3&#xA;Number of Tasks Allocated = 3&#xA;&#xA;Hello, World! I am 🏓 Player (Rank)  1 . Process  2  of  3  on  worker-1&#xA;Player (Rank)  1 &#39;s receiving 🏓 neighbor is Player (Rank) 0 and my sending 🏓 neighbor is Player (Rank) 2.&#xA;Hello, World! I am 🏓 Player (Rank)  2 . Process  3  of  3  on  worker-2&#xA;Player (Rank)  2 &#39;s receiving 🏓 neighbor is Player (Rank) 1 and my sending 🏓 neighbor is Player (Rank) 0.&#xA;Hello, World! I am 🏓 Player (Rank)  0 . Process  1  of  3  on  worker-0&#xA;Player (Rank)  0 &#39;s receiving 🏓 neighbor is Player (Rank) 2 and my sending 🏓 neighbor is Player (Rank) 1.&#xA;Player 0 sent the 🏓 ball to Player 1.&#xA;Player 1 received the 🏓 ball from Player 0.&#xA;Player 1 sent the 🏓 ball to Player 2.&#xA;Player 2 received the 🏓 ball from Player 1.&#xA;Player 2 sent the 🏓 ball to Player 0.&#xA;Player 0 received the 🏓 ball from Player 2.&#xA;Player 0 started round 2 by sending the 🏓 ball to Player 1.&#xA;…&#xA;Player 0 started round 900 by sending the 🏓 ball to Player 1.&#xA;🛑 Player 0 is bored of 🏓 ping pong and is quitting! 🛑&#xA;Player 1 received the 🏓 ball from Player 0.&#xA;Player 1 sent the 🏓 ball to Player 2.&#xA;🛑 Player 1 is bored of 🏓 ping pong and is quitting! 🛑&#xA;Player 2 received the 🏓 ball from Player 1.&#xA;🎉 Player 2 won the game of 🏓 ping pong, since everyone else quit! 🎉&#xA;&#xA;Exit code                 = 0&#xA;Date end                  = Tue May 14 10:15:03 PM UTC 2024&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you look closely at the&amp;nbsp;&lt;code&gt;Hello, World!&lt;/code&gt;&amp;nbsp;lines for each rank/process you will notice that they are all running on completely different nodes (&lt;em&gt;worker-[0-2]&lt;/em&gt;), but the application still behaves exactly as we expect it to, and the messages are happily passed between each process running on a different core on different systems. Because of this, we could use Slurm to easily run our application across hundreds or even thousands of cores if that is what we needed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, could we do all of this with Kubernetes today? Well, yes and no.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The real power of Slurm is that you have so much control over how it is built, how you assign resources to your workloads, and how they are partitioned while running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;mpi---message-passing-interface&#34;&gt;MPI – Message Passing Interface&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although you could easily run this application inside a single pod on Kubernetes, to truly leverage the MPI capabilities that we are expecting we would need to install and utilize something like&amp;nbsp;&lt;a href=&#34;https://www.kubeflow.org/&#34;&gt;KubeFlow&lt;/a&gt;&amp;nbsp;along with&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/mpi-operator&#34;&gt;KubeFlow’s beta MPI Operator&lt;/a&gt;&amp;nbsp;to even get started. You might also want to consider something like the&amp;nbsp;&lt;a href=&#34;https://kueue.sigs.k8s.io/&#34;&gt;Kubernetes-native Job Queueing support&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;At the time of writing this&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/manifests/tree/v1.8.0?tab=readme-ov-file#prerequisites&#34;&gt;KubeFlow v.1.8.0 doesn’t support versions of Kubernetes greater than v1.26&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One obvious side effect of using Kubernetes is that we&amp;nbsp;&lt;strong&gt;MUST&lt;/strong&gt;&amp;nbsp;run our code inside one or more containers. This is possible with Slurm but is not a hard requirement. We will also need to ensure that Python, MPI, and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;are all installed and configured correctly in our container, as we will need them all to run our application and manage each of the MPI worker pods.&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is required because this is how&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;will connect to each of the pods to launch the parallel instances of our application. The manifest that we will need to apply to our Kubernetes cluster for use with&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;is simply named&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/manifests/mpi-ping-pong/mpijob.yaml&#34;&gt;mpijob.yaml&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;You can also find&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/Dockerfile&#34;&gt;the Dockerfile for the container that we are running&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground&#34;&gt;the GitHub repo&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;mpijob.yaml&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;em&gt;&lt;/em&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that Kubeflow and the mpi-operator are properly installed in our cluster, then applying the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;manifest should “just work”.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ git clone https://github.com/superorbital/mpi-playground.git&#xA;$ cd mpi-playground&#xA;$ kubectl apply -f ./manifests/mpi-ping-pong/mpijob.yaml&#xA;mpijob.kubeflow.org/mpi-ping-pong created&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If you want to try this out yourself, you will likely find that it is easiest to delete the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;and then re-apply it each time you want to run it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;code&gt;kubectl delete mpijobs.kubeflow.org mpi-ping-pong&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the animated GIF below you will see the manifest being launched, followed by the launcher and worker pods being spun up, once the workers are done, then the launcher will complete and the launcher logs will be displayed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/demo.gif&#34; alt=&#34;MPI Ping Pong in Kubernetes demo&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The sequence of events looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The required number of workers are started and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is run inside each worker container.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher is started and executes the desired&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;command.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;connects to each worker and starts the individual tasks.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The workers complete their tasks and exit.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher completes and exits.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The pod logs for the launcher will contain all of the output that we would normally expect to see from an&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;session.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, today it is technically possible to run at least some MPI-style jobs inside Kubernetes, but how practical is it?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Well, one of the big advantages of Kubernetes is its ability to scale. Unlike Slurm, which has no cluster scaling functionality, a Kubernetes cluster can be designed to scale up nodes for a very large workflow and then scale back down once that workflow is finished and those nodes are no longer required for any immediately upcoming workflows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But, I think the real answer here is that it depends a lot on what you are trying to do, how much control you have over the Kubernetes cluster setup, and how much effort you want to put into getting it all working.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;artificial-intelligence-and-machine-learning&#34;&gt;Artificial Intelligence and Machine Learning&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although the technical details are different, most of the basic observations in this article also apply to artificial intelligence (AI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ai&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;), machine learning (ML&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ml&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;), and deep learning (DL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:dl&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;) applications. In most instances, these workloads will make heavy use of systems equipped with advanced GPUs to significantly improve the performance of their computation tasks. Ensuring that each process has exclusive access to the required hardware resources is often critical for the success and performance of the task.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;slurm-on-kubernetes&#34;&gt;Slurm on Kubernetes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the last couple of years,&amp;nbsp;&lt;a href=&#34;https://www.linkedin.com/in/timwickberg/&#34;&gt;Tim Wickberg, SchedMD’s CTO&lt;/a&gt;, has given a talk entitled “&lt;a href=&#34;https://slurm.schedmd.com/SC23/Slurm-and-or-vs-Kubernetes.pdf&#34;&gt;Slurm and/or/vs Kubernetes&lt;/a&gt;”, which looks at Slurm and Kubernetes and explores how each of these tools could be extended to leverage their strengths and generally improve the current state of high-performance workload managers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One project that has come out of the effort to find synergy between these two tools and communities, is&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/blog/sunk-slurm-on-kubernetes-implementations&#34;&gt;SUNK&lt;/a&gt;, which stands for&amp;nbsp;&lt;em&gt;&lt;strong&gt;S&lt;/strong&gt;l&lt;strong&gt;U&lt;/strong&gt;rm o&lt;strong&gt;N&lt;/strong&gt;&amp;nbsp;&lt;strong&gt;K&lt;/strong&gt;ubernetes&lt;/em&gt;. It is a project in early development by&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/&#34;&gt;CoreWeave&lt;/a&gt;&amp;nbsp;to enable Slurm workloads in Kubernetes Clusters.&amp;nbsp;&lt;a href=&#34;https://www.youtube.com/watch?v=48ONt0UKiew&#34;&gt;The CoreWeave talk that officially introduced this project can viewed on YouTube&lt;/a&gt;&amp;nbsp;if you would like to hear more details about the work that is being done with this. You can also grab&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/SLUG23/CoreWeave-SLUG23.pdf&#34;&gt;a PDF of the slides&lt;/a&gt;&amp;nbsp;for the talk. Once the code is released it should be available at&amp;nbsp;&lt;a href=&#34;https://github.com/coreweave/sunk&#34;&gt;github.com/coreweave/sunk&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;kubernetes-device-management&#34;&gt;Kubernetes Device Management&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The&amp;nbsp;&lt;a href=&#34;https://community.arm.com/arm-research/b/articles/posts/a-smarter-device-manager-for-kubernetes-on-the-edge&#34;&gt;Smarter Device Project&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://www.arm.com/resources/research&#34;&gt;ARM research&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:arm&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;is also an interesting&amp;nbsp;&lt;a href=&#34;https://github.com/smarter-project/smarter-device-manager&#34;&gt;Kubernetes application&lt;/a&gt;&amp;nbsp;that might be useful for these sorts of workloads and makes it possible to expose, reserve, and manage any Linux device (&lt;em&gt;e.g. /dev/video*&lt;/em&gt;) within Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For GPUs,&amp;nbsp;&lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/k8s-device-plugin&#34;&gt;NVIDIA has a device plugin&lt;/a&gt;&amp;nbsp;to provide similar functionality, which is often deployed along with their&amp;nbsp;&lt;a href=&#34;https://github.com/NVIDIA/gpu-operator&#34;&gt;gpu-operator&lt;/a&gt;. This also enables features in Kubernetes like GPU time-slicing, something similar to the generic&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/gres.html#Sharding&#34;&gt;GPU sharding&lt;/a&gt;&amp;nbsp;available in Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;in-conclusion&#34;&gt;In Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm is a very useful tool for running parallel applications that require very specific resources inside a compute cluster. It was designed for a set of workloads that are outside the standard stateless microservice focus of Kubernetes, but even today Kubernetes can be made to work with some of these workloads without too much hassle, however, truly leveraging the advantages of Kubernetes for these more traditional high-performance computing workloads will require projects like SUNK and&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;to reach a reasonable level of maturity and then in the future, there may be newer projects that rethink the current approaches and build Kubernetes-first tools from the ground-up that leverage the strengths that are already available in the platform while extending its ability to handle the type of workloads that have been traditionally handled by tools like Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;further-reading&#34;&gt;Further Reading&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained&#34;&gt;Machine learning, explained&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.geeksforgeeks.org/machine-learning/&#34;&gt;Machine Learning Tutorial&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://carleton.ca/rcs/rcdc/introduction-to-mpi/&#34;&gt;Introduction to MPI&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://mpitutorial.com/&#34;&gt;A Comprehensive MPI Tutorial Resource&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Intro to Multi-Node Machine Learning&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-1-setting-up-an-hpc-cluster&#34;&gt;1: Setting up an HPC cluster&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-2-using-slurm&#34;&gt;2: Using Slurm&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;Unreleased as of 05/16/2024&lt;/em&gt;&amp;nbsp;–&amp;nbsp;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-3-multi-node-training&#34;&gt;3: Multi-Node Training&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://blog.kubeflow.org/integrations/operators/2020/03/16/mpi-operator.html&#34;&gt;Introduction to Kubeflow MPI Operator and Industry Adoption&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Cover image by&amp;nbsp;&lt;a href=&#34;https://pixabay.com/users/dlohner-4631193/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;dlohner&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://pixabay.com//?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;Pixabay&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Slurm architecture overview by SchedMD for their&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/network.html&#34;&gt;network overview&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/High-performance_computing&#34;&gt;HPC – high-performance computing&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:hpc&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Lawrence_Livermore_National_Laboratory&#34;&gt;LLNL – Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:llnl&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.schedmd.com/about-schedmd/slurm-history&#34;&gt;Slurm history&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:history&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Graphics_processing_unit&#34;&gt;GPU – graphics processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:gpu&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA® – Compute Unified Device Architecture&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cuda&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN – NVIDIA CUDA® Deep Neural Network library&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cudnn&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.open-mpi.org/&#34;&gt;MPI – Message Passing Interface&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:mpi&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Open_Container_Initiative&#34;&gt;OCI – Open Container Initiative&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:oci&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.uio.no/studier/emner/matnat/ifi/INF3380/v11/undervisningsmateriale/inf3380-week06.pdf&#34;&gt;Rank&lt;/a&gt;&amp;nbsp;– Each process has a unique rank, i.e. an integer identifier, within a communicator. The rank value is between zero and the number of processes minus one. The rank value is used to distinguish one process from another.&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:rank&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Central_processing_unit&#34;&gt;CPU – central processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cpu&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/artificial-intelligence&#34;&gt;AI – Artificial Intelligence&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ai&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/machine-learning&#34;&gt;ML – Machine Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ml&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/deep-learning&#34;&gt;DL – Deep Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:dl&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/ARM_architecture_family&#34;&gt;ARM – Advanced RISC Machine&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:arm&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/&#34;&gt;SuperOrbital’s blog&lt;/a&gt; by Sean Kane&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/cover.jpg&#34; alt=&#34;datacenter servers&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In this article, we are going to explore&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm&#34;&gt;Slurm&lt;/a&gt;, a popular open-source high-performance computing (HPC&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:hpc&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;) workload manager, and discover what it is, why people use it, and how it differs from Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For context, when we talk about high-performance computing, we are primarily talking about computational tasks that require a lot of time on very powerful computer systems and are often designed, so that they can be&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Parallel_computing&#34;&gt;highly parallelized&lt;/a&gt;, across resources within a super-computer or large cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These sorts of jobs are very common in scientific research,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_simulation&#34;&gt;computer simulation&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_intelligence&#34;&gt;artificial intelligence&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&gt;machine learning&lt;/a&gt;&amp;nbsp;tasks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;what-is-slurm&#34;&gt;What is Slurm?&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To almost directly quote the Slurm repository&amp;nbsp;&lt;a href=&#34;https://github.com/SchedMD/slurm/blob/master/README.rst#slurm-workload-manager&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;Slurm is an open-source cluster resource management and job scheduling system for Linux (&lt;em&gt;primarily&lt;/em&gt;) that strives to be simple, scalable, portable, fault-tolerant, and interconnect agnostic. As a cluster resource manager, Slurm provides three key functions:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Second, it provides a framework for starting, executing, and monitoring work (&lt;em&gt;normally a parallel job&lt;/em&gt;) on the set of allocated nodes.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;And finally, it arbitrates conflicting requests for resources by managing a queue of pending work.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In short, Slurm is a system that is primarily designed to manage&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Batch_processing&#34;&gt;batch processing&lt;/a&gt;&amp;nbsp;workloads running in a shared&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computer_cluster&#34;&gt;computing cluster&lt;/a&gt;. Slurm can be configured to run any type of batch job but is often utilized on systems that support multiple users with parallel computing tasks to run.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;According to&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;, the primary support and development company for the Slurm community, Slurm is being used on approximately 65% of the&amp;nbsp;&lt;a href=&#34;https://top500.org/lists/top500/2024/06/&#34;&gt;TOP500 supercomputers in the world&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;a-bit-of-history&#34;&gt;A bit of history&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the mid-1990s, as Linux became more popular, there was an increased interest in making large-scale computer processing more accessible through the use of commodity hardware and projects like&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Beowulf_cluster&#34;&gt;Beowulf&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In 2001, engineers at the&amp;nbsp;&lt;a href=&#34;https://www.llnl.gov/&#34;&gt;Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;(LLNL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:llnl&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;) in the United States, started looking into how they might migrate from proprietary HPC systems to open-source solutions. They determined that there weren’t any good resource managers available at the time, and this directly led to the creation of Slurm, which was initially released in 2002.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;a href=&#34;https://slurm.schedmd.com/slurm_design.pdf&#34;&gt;The name Slurm is a reference&lt;/a&gt;&amp;nbsp;to the&amp;nbsp;&lt;a href=&#34;https://futurama.fandom.com/wiki/Slurm&#34;&gt;soda&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://www.imdb.com/title/tt0149460/&#34;&gt;Futurama&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the years, Slurm has evolved to support a wide variety of hardware configurations and in 2010, some initial scheduling logic was added to Slurm, enabling it to function as both a stand-alone resource manager and a scheduler&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:history&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;the-architecture&#34;&gt;The Architecture&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Like many similar systems, a Slurm setup consists of at least one controller node, and at least one compute node, which can be on the same physical systems.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/slurm-layout.gif&#34; alt=&#34;Diagram of the basic Slurm architecture&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;the-controller&#34;&gt;The Controller&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Two daemons are responsible for managing the cluster. These two processes can be run on a single node or separated, depending on the performance and resiliency required by the administrator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmctld&#34;&gt;slurmctld&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmctld&lt;/code&gt;&amp;nbsp;is the primary management daemon for Slurm. It is responsible for all the daemons, resources, and jobs, within the cluster. Because it is a critical component in the cluster, it is often configured with a passive backup that can take over management if the primary daemon ever fails.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmdbd&#34;&gt;slurmdbd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmdbd&lt;/code&gt;&amp;nbsp;provides an interface for the&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/MySQL&#34;&gt;MySQL database&lt;/a&gt;&amp;nbsp;that Slurm uses for stateful data, like job accounting records.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;compute-nodes&#34;&gt;Compute Nodes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These represent the nodes whose resources are available to any jobs that might need to be scheduled in the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;slurmd&#34;&gt;slurmd&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;slurmd&lt;/code&gt;&amp;nbsp;is the daemon that runs on each compute node and is responsible for accepting work, launching tasks, and generally managing the workloads on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SchedMD maintains the core documentation for Slurm and the&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/quickstart_admin.html#quick_start&#34;&gt;current installation documentation&lt;/a&gt;&amp;nbsp;can be found on their website.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Getting Slurm working is not particularly difficult, but fine-tuning it for your requirements can be a time-consuming task. Since performance is often a primary focus with these clusters and Slurm is designed to work with a very wide set of hardware and software libraries, it can take a lot of effort to fine-tune the installation, so that it utilizes the hardware to the best of its ability, and all the proper software is available to streamline the types of jobs that will be run on the cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;At a minimum clusters will require node name resolution, a MySQL instance, shared block storage, a synchronized time source, synchronized users, the&amp;nbsp;&lt;a href=&#34;https://dun.github.io/munge/&#34;&gt;Munge&lt;/a&gt;&amp;nbsp;daemon for user authentication, a full set of custom-built Slurm executables, and various configuration files.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In addition to this, the nodes will need to contain the various hardware, programming languages, libraries, and other tools that the majority of jobs will require to run. This could include things like GPUs&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:gpu&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;and their related toolchains (CUDA&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cuda&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;/cuDNN&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cudnn&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;), MPI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:mpi&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;libraries, OCI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:oci&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;runtimes for&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/containers.html&#34;&gt;container support&lt;/a&gt;, and much more.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm’s greatest asset and liability is how flexible it is. This makes it something that can be highly optimized for the requirements of the organization, while also making it susceptible to misconfiguration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;workloads&#34;&gt;Workloads&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To provide some context, let’s take a look at an example user job and workflow that utilizes some of the features of Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the&amp;nbsp;&lt;code&gt;mpi-ping-pong.py&lt;/code&gt;&amp;nbsp;code below, you will find a&amp;nbsp;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt;&amp;nbsp;script that simulates a pseudo-game of&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Table_tennis&#34;&gt;Ping Pong/Table Tennis&lt;/a&gt;&amp;nbsp;by making use of MPI to pass a message (&lt;em&gt;“ball”&lt;/em&gt;) between as many processes as we have configured to participate in the parallel job.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;local-testing&#34;&gt;Local Testing&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run this code, you simply need to have&amp;nbsp;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/quickstart.html&#34;&gt;OpenMPI&lt;/a&gt;, and&amp;nbsp;&lt;a href=&#34;https://mpi4py.readthedocs.io/en/stable/install.html&#34;&gt;mpi4py&lt;/a&gt;&amp;nbsp;installed on your system.mpi-ping-pong.py&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/mpi-ping-pong.py&#34;&gt;mpi-ping-pong.py&lt;/a&gt;&amp;nbsp;is what many would refer to as an&amp;nbsp;&lt;a href=&#34;https://en.wikipedia.org/wiki/Embarrassingly_parallel&#34;&gt;embarrassingly parallel&lt;/a&gt;&amp;nbsp;workload because it can be scaled with no real effort at all.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you run this Python application on your local system, you will see something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ python ./mpi-ping-pong.py&#xA;&#xA;Hello, World! I am 🏓 Player (Rank)  0 . Process  1  of  1  on  laptop.local&#xA;Player (Rank)  0 &#39;s receiving 🏓 neighbor is Player (Rank) 0 and my sending 🏓 neighbor is Player (Rank) 0.&#xA;Player 0 started round 1 by hitting the 🏓 ball towards the wall.&#xA;Player 0 received the 🏓 ball that bounced off the wall.&#xA;Player 0 started round 2 by hitting the 🏓 ball towards the wall.&#xA;Player 0 received the 🏓 ball that bounced off the wall.&#xA;…&#xA;Player 0 started round 299 by hitting the 🏓 ball towards the wall.&#xA;Player 0 received the 🏓 ball that bounced off the wall.&#xA;🎉 Player 0 won the game of 🏓 ping pong, since they were playing alone! 🎉&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As you can see the application runs perfectly fine, and uses a single rank&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:rank&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;/process (&lt;em&gt;CPU&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:cpu&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;core, in this case&lt;/em&gt;). Since we only have a single process participating in this game of Ping Pong, the program simply sends and receives messages to and from from itself.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, assuming that we have at least two CPU cores, then we can run this application utilizing as many of them as we want, by leveraging the MPI command line tool&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To run 12 instances of our application, which will all communicate with each other in parallel across 12 cores on our system, we can do the following on our local system:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: These messages will be somewhat out of order. This is the nature of parallel computing, buffers, etc. To deal with this you would likely need to send all the messages back to single process for ordering and printing.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ mpirun -np 12 ./mpi-ping-pong.py&#xA;&#xA;Player (Rank)  0 &#39;s receiving 🏓 neighbor is Player (Rank) 11 and my sending 🏓 neighbor is Player (Rank) 1.&#xA;Hello, World! I am 🏓 Player (Rank)  0 . Process  1  of  12  on  laptop.local&#xA;Player (Rank)  1 &#39;s receiving 🏓 neighbor is Player (Rank) 0 and my sending 🏓 neighbor is Player (Rank) 2.&#xA;Hello, World! I am 🏓 Player (Rank)  1 . Process  2  of  12  on  laptop.local&#xA;Player (Rank)  2 &#39;s receiving 🏓 neighbor is Player (Rank) 1 and my sending 🏓 neighbor is Player (Rank) 3.&#xA;Hello, World! I am 🏓 Player (Rank)  2 . Process  3  of  12  on  laptop.local&#xA;…&#xA;🛑 Player 0 is bored of 🏓 ping pong and is quitting! 🛑&#xA;🛑 Player 1 is bored of 🏓 ping pong and is quitting! 🛑&#xA;🛑 Player 2 is bored of 🏓 ping pong and is quitting! 🛑&#xA;…&#xA;🎉 Player 11 won the game of 🏓 ping pong, since everyone else quit! 🎉&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;massive-scale-with-slurm&#34;&gt;Massive Scale with Slurm&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Now, imagine that we want to run this across hundreds of CPUs. Slurm makes this trivial as long as you have enough systems in your cluster to support this work.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In Slurm, you can define a batch job with a shell script (&lt;em&gt;e.g. /shared_storage/mpi-ping-pong.slurm&lt;/em&gt;) that looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;#!/bin/bash&#xA;&#xA;#SBATCH -J mpi-ping-pong                         # Name the job&#xA;#SBATCH -o /shared_storage/mpi-ping-pong-%j.out  # The standard output file&#xA;#SBATCH -e /shared_storage/mpi-ping-pong-%j.err  # The standard error file&#xA;#SBATCH -t 0-2:00:00         # Run for a max time of 0d, 2h, 0m, 0s&#xA;#SBATCH --nodes=3            # Request 3 nodes&#xA;#SBATCH --ntasks-per-node=1  # Request 1 cores or task per node&#xA;&#xA;echo &#34;Date start                = $(date)&#34;&#xA;echo &#34;Initiating Host           = $(hostname)&#34;&#xA;echo &#34;Working Directory         = $(pwd)&#34;&#xA;echo &#34;&#34;&#xA;echo &#34;Number of Nodes Allocated = ${SLURM_JOB_NUM_NODES}&#34;&#xA;echo &#34;Number of Tasks Allocated = ${SLURM_NTASKS}&#34;&#xA;echo &#34;&#34;&#xA;&#xA;mpirun python /shared_storage/mpi-ping-pong.py&#xA;RETURN=${?}&#xA;&#xA;echo &#34;&#34;&#xA;echo &#34;Exit code                 = ${RETURN}&#34;&#xA;echo &#34;Date end                  = $(date)&#34;&#xA;echo &#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Each&amp;nbsp;&lt;code&gt;#SBATCH&lt;/code&gt;&amp;nbsp;line defines an&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/sbatch.html&#34;&gt;&lt;code&gt;sbatch&lt;/code&gt;&lt;/a&gt;&amp;nbsp;command line argument to be used for the job. In this case, we are going to request that the workload run on three nodes and that it will only use a single core on each node.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that all the compute nodes in our cluster are configured correctly and our Python script exists on the shared storage at&amp;nbsp;&lt;em&gt;/shared_storage/mpi-ping-pong.py&lt;/em&gt;&amp;nbsp;then we can submit this job to the cluster, by running:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ sbatch /shared_storage/mpi-ping-pong.slurm&#xA;&#xA;Submitted batch job 87&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This job will run very fast, but if we needed to we could use commands like&amp;nbsp;&lt;code&gt;squeue&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;sinfo&lt;/code&gt;&amp;nbsp;to examine the state of the cluster and jobs while it was running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When the job is finished we can look at&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.err&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;/shared_storage/mpi-ping-pong-87.out&lt;/code&gt;&amp;nbsp;to see all the output from our application. In this case, the error file is empty, but our output file looks like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;Date start                = Tue May 14 10:15:02 PM UTC 2024&#xA;Initiating Host           = worker-0&#xA;Working Directory         = /shared_storage&#xA;&#xA;Number of Nodes Allocated = 3&#xA;Number of Tasks Allocated = 3&#xA;&#xA;Hello, World! I am 🏓 Player (Rank)  1 . Process  2  of  3  on  worker-1&#xA;Player (Rank)  1 &#39;s receiving 🏓 neighbor is Player (Rank) 0 and my sending 🏓 neighbor is Player (Rank) 2.&#xA;Hello, World! I am 🏓 Player (Rank)  2 . Process  3  of  3  on  worker-2&#xA;Player (Rank)  2 &#39;s receiving 🏓 neighbor is Player (Rank) 1 and my sending 🏓 neighbor is Player (Rank) 0.&#xA;Hello, World! I am 🏓 Player (Rank)  0 . Process  1  of  3  on  worker-0&#xA;Player (Rank)  0 &#39;s receiving 🏓 neighbor is Player (Rank) 2 and my sending 🏓 neighbor is Player (Rank) 1.&#xA;Player 0 sent the 🏓 ball to Player 1.&#xA;Player 1 received the 🏓 ball from Player 0.&#xA;Player 1 sent the 🏓 ball to Player 2.&#xA;Player 2 received the 🏓 ball from Player 1.&#xA;Player 2 sent the 🏓 ball to Player 0.&#xA;Player 0 received the 🏓 ball from Player 2.&#xA;Player 0 started round 2 by sending the 🏓 ball to Player 1.&#xA;…&#xA;Player 0 started round 900 by sending the 🏓 ball to Player 1.&#xA;🛑 Player 0 is bored of 🏓 ping pong and is quitting! 🛑&#xA;Player 1 received the 🏓 ball from Player 0.&#xA;Player 1 sent the 🏓 ball to Player 2.&#xA;🛑 Player 1 is bored of 🏓 ping pong and is quitting! 🛑&#xA;Player 2 received the 🏓 ball from Player 1.&#xA;🎉 Player 2 won the game of 🏓 ping pong, since everyone else quit! 🎉&#xA;&#xA;Exit code                 = 0&#xA;Date end                  = Tue May 14 10:15:03 PM UTC 2024&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you look closely at the&amp;nbsp;&lt;code&gt;Hello, World!&lt;/code&gt;&amp;nbsp;lines for each rank/process you will notice that they are all running on completely different nodes (&lt;em&gt;worker-[0-2]&lt;/em&gt;), but the application still behaves exactly as we expect it to, and the messages are happily passed between each process running on a different core on different systems. Because of this, we could use Slurm to easily run our application across hundreds or even thousands of cores if that is what we needed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, could we do all of this with Kubernetes today? Well, yes and no.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The real power of Slurm is that you have so much control over how it is built, how you assign resources to your workloads, and how they are partitioned while running.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;mpi---message-passing-interface&#34;&gt;MPI – Message Passing Interface&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although you could easily run this application inside a single pod on Kubernetes, to truly leverage the MPI capabilities that we are expecting we would need to install and utilize something like&amp;nbsp;&lt;a href=&#34;https://www.kubeflow.org/&#34;&gt;KubeFlow&lt;/a&gt;&amp;nbsp;along with&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/mpi-operator&#34;&gt;KubeFlow’s beta MPI Operator&lt;/a&gt;&amp;nbsp;to even get started. You might also want to consider something like the&amp;nbsp;&lt;a href=&#34;https://kueue.sigs.k8s.io/&#34;&gt;Kubernetes-native Job Queueing support&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;At the time of writing this&amp;nbsp;&lt;a href=&#34;https://github.com/kubeflow/manifests/tree/v1.8.0?tab=readme-ov-file#prerequisites&#34;&gt;KubeFlow v.1.8.0 doesn’t support versions of Kubernetes greater than v1.26&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One obvious side effect of using Kubernetes is that we&amp;nbsp;&lt;strong&gt;MUST&lt;/strong&gt;&amp;nbsp;run our code inside one or more containers. This is possible with Slurm but is not a hard requirement. We will also need to ensure that Python, MPI, and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;are all installed and configured correctly in our container, as we will need them all to run our application and manage each of the MPI worker pods.&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is required because this is how&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;will connect to each of the pods to launch the parallel instances of our application. The manifest that we will need to apply to our Kubernetes cluster for use with&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;is simply named&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/manifests/mpi-ping-pong/mpijob.yaml&#34;&gt;mpijob.yaml&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;You can also find&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground/blob/main/containers/mpi-ping-pong/Dockerfile&#34;&gt;the Dockerfile for the container that we are running&lt;/a&gt;&amp;nbsp;in&amp;nbsp;&lt;a href=&#34;https://github.com/superorbital/mpi-playground&#34;&gt;the GitHub repo&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;mpijob.yaml&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;em&gt;&lt;/em&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Assuming that Kubeflow and the mpi-operator are properly installed in our cluster, then applying the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;manifest should “just work”.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;$ git clone https://github.com/superorbital/mpi-playground.git&#xA;$ cd mpi-playground&#xA;$ kubectl apply -f ./manifests/mpi-ping-pong/mpijob.yaml&#xA;mpijob.kubeflow.org/mpi-ping-pong created&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If you want to try this out yourself, you will likely find that it is easiest to delete the&amp;nbsp;&lt;strong&gt;MPIJob&lt;/strong&gt;&amp;nbsp;and then re-apply it each time you want to run it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p class=&#34;has-normal-font-size&#34;&gt;&lt;code&gt;kubectl delete mpijobs.kubeflow.org mpi-ping-pong&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the animated GIF below you will see the manifest being launched, followed by the launcher and worker pods being spun up, once the workers are done, then the launcher will complete and the launcher logs will be displayed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/demo.gif&#34; alt=&#34;MPI Ping Pong in Kubernetes demo&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The sequence of events looks something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The required number of workers are started and&amp;nbsp;&lt;code&gt;sshd&lt;/code&gt;&amp;nbsp;is run inside each worker container.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher is started and executes the desired&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;command.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;connects to each worker and starts the individual tasks.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The workers complete their tasks and exit.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The launcher completes and exits.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The pod logs for the launcher will contain all of the output that we would normally expect to see from an&amp;nbsp;&lt;code&gt;mpirun&lt;/code&gt;&amp;nbsp;session.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;So, today it is technically possible to run at least some MPI-style jobs inside Kubernetes, but how practical is it?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Well, one of the big advantages of Kubernetes is its ability to scale. Unlike Slurm, which has no cluster scaling functionality, a Kubernetes cluster can be designed to scale up nodes for a very large workflow and then scale back down once that workflow is finished and those nodes are no longer required for any immediately upcoming workflows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;But, I think the real answer here is that it depends a lot on what you are trying to do, how much control you have over the Kubernetes cluster setup, and how much effort you want to put into getting it all working.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;artificial-intelligence-and-machine-learning&#34;&gt;Artificial Intelligence and Machine Learning&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although the technical details are different, most of the basic observations in this article also apply to artificial intelligence (AI&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ai&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;), machine learning (ML&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:ml&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;), and deep learning (DL&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:dl&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;) applications. In most instances, these workloads will make heavy use of systems equipped with advanced GPUs to significantly improve the performance of their computation tasks. Ensuring that each process has exclusive access to the required hardware resources is often critical for the success and performance of the task.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;slurm-on-kubernetes&#34;&gt;Slurm on Kubernetes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Over the last couple of years,&amp;nbsp;&lt;a href=&#34;https://www.linkedin.com/in/timwickberg/&#34;&gt;Tim Wickberg, SchedMD’s CTO&lt;/a&gt;, has given a talk entitled “&lt;a href=&#34;https://slurm.schedmd.com/SC23/Slurm-and-or-vs-Kubernetes.pdf&#34;&gt;Slurm and/or/vs Kubernetes&lt;/a&gt;”, which looks at Slurm and Kubernetes and explores how each of these tools could be extended to leverage their strengths and generally improve the current state of high-performance workload managers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;One project that has come out of the effort to find synergy between these two tools and communities, is&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/blog/sunk-slurm-on-kubernetes-implementations&#34;&gt;SUNK&lt;/a&gt;, which stands for&amp;nbsp;&lt;em&gt;&lt;strong&gt;S&lt;/strong&gt;l&lt;strong&gt;U&lt;/strong&gt;rm o&lt;strong&gt;N&lt;/strong&gt;&amp;nbsp;&lt;strong&gt;K&lt;/strong&gt;ubernetes&lt;/em&gt;. It is a project in early development by&amp;nbsp;&lt;a href=&#34;https://www.schedmd.com/&#34;&gt;SchedMD&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://www.coreweave.com/&#34;&gt;CoreWeave&lt;/a&gt;&amp;nbsp;to enable Slurm workloads in Kubernetes Clusters.&amp;nbsp;&lt;a href=&#34;https://www.youtube.com/watch?v=48ONt0UKiew&#34;&gt;The CoreWeave talk that officially introduced this project can viewed on YouTube&lt;/a&gt;&amp;nbsp;if you would like to hear more details about the work that is being done with this. You can also grab&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/SLUG23/CoreWeave-SLUG23.pdf&#34;&gt;a PDF of the slides&lt;/a&gt;&amp;nbsp;for the talk. Once the code is released it should be available at&amp;nbsp;&lt;a href=&#34;https://github.com/coreweave/sunk&#34;&gt;github.com/coreweave/sunk&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;kubernetes-device-management&#34;&gt;Kubernetes Device Management&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The&amp;nbsp;&lt;a href=&#34;https://community.arm.com/arm-research/b/articles/posts/a-smarter-device-manager-for-kubernetes-on-the-edge&#34;&gt;Smarter Device Project&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://www.arm.com/resources/research&#34;&gt;ARM research&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fn:arm&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;&amp;nbsp;is also an interesting&amp;nbsp;&lt;a href=&#34;https://github.com/smarter-project/smarter-device-manager&#34;&gt;Kubernetes application&lt;/a&gt;&amp;nbsp;that might be useful for these sorts of workloads and makes it possible to expose, reserve, and manage any Linux device (&lt;em&gt;e.g. /dev/video*&lt;/em&gt;) within Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For GPUs,&amp;nbsp;&lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/k8s-device-plugin&#34;&gt;NVIDIA has a device plugin&lt;/a&gt;&amp;nbsp;to provide similar functionality, which is often deployed along with their&amp;nbsp;&lt;a href=&#34;https://github.com/NVIDIA/gpu-operator&#34;&gt;gpu-operator&lt;/a&gt;. This also enables features in Kubernetes like GPU time-slicing, something similar to the generic&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/gres.html#Sharding&#34;&gt;GPU sharding&lt;/a&gt;&amp;nbsp;available in Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;in-conclusion&#34;&gt;In Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Slurm is a very useful tool for running parallel applications that require very specific resources inside a compute cluster. It was designed for a set of workloads that are outside the standard stateless microservice focus of Kubernetes, but even today Kubernetes can be made to work with some of these workloads without too much hassle, however, truly leveraging the advantages of Kubernetes for these more traditional high-performance computing workloads will require projects like SUNK and&amp;nbsp;&lt;code&gt;mpi-operator&lt;/code&gt;&amp;nbsp;to reach a reasonable level of maturity and then in the future, there may be newer projects that rethink the current approaches and build Kubernetes-first tools from the ground-up that leverage the strengths that are already available in the platform while extending its ability to handle the type of workloads that have been traditionally handled by tools like Slurm.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;further-reading&#34;&gt;Further Reading&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained&#34;&gt;Machine learning, explained&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.geeksforgeeks.org/machine-learning/&#34;&gt;Machine Learning Tutorial&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://carleton.ca/rcs/rcdc/introduction-to-mpi/&#34;&gt;Introduction to MPI&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://mpitutorial.com/&#34;&gt;A Comprehensive MPI Tutorial Resource&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Intro to Multi-Node Machine Learning&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-1-setting-up-an-hpc-cluster&#34;&gt;1: Setting up an HPC cluster&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-2-using-slurm&#34;&gt;2: Using Slurm&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;Unreleased as of 05/16/2024&lt;/em&gt;&amp;nbsp;–&amp;nbsp;&lt;a href=&#34;https://www.unitary.ai/articles/intro-to-multi-node-machine-learning-3-multi-node-training&#34;&gt;3: Multi-Node Training&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://blog.kubeflow.org/integrations/operators/2020/03/16/mpi-operator.html&#34;&gt;Introduction to Kubeflow MPI Operator and Industry Adoption&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Cover image by&amp;nbsp;&lt;a href=&#34;https://pixabay.com/users/dlohner-4631193/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;dlohner&lt;/a&gt;&amp;nbsp;from&amp;nbsp;&lt;a href=&#34;https://pixabay.com//?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3100049&#34;&gt;Pixabay&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li class=&#34;has-normal-font-size&#34;&gt;Slurm architecture overview by SchedMD for their&amp;nbsp;&lt;a href=&#34;https://slurm.schedmd.com/network.html&#34;&gt;network overview&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/High-performance_computing&#34;&gt;HPC – high-performance computing&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:hpc&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Lawrence_Livermore_National_Laboratory&#34;&gt;LLNL – Lawrence Livermore National Laboratory&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:llnl&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.schedmd.com/about-schedmd/slurm-history&#34;&gt;Slurm history&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:history&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Graphics_processing_unit&#34;&gt;GPU – graphics processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:gpu&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA® – Compute Unified Device Architecture&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cuda&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN – NVIDIA CUDA® Deep Neural Network library&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cudnn&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.open-mpi.org/&#34;&gt;MPI – Message Passing Interface&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:mpi&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Open_Container_Initiative&#34;&gt;OCI – Open Container Initiative&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:oci&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.uio.no/studier/emner/matnat/ifi/INF3380/v11/undervisningsmateriale/inf3380-week06.pdf&#34;&gt;Rank&lt;/a&gt;&amp;nbsp;– Each process has a unique rank, i.e. an integer identifier, within a communicator. The rank value is between zero and the number of processes minus one. The rank value is used to distinguish one process from another.&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:rank&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Central_processing_unit&#34;&gt;CPU – central processing unit&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:cpu&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/artificial-intelligence&#34;&gt;AI – Artificial Intelligence&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ai&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/machine-learning&#34;&gt;ML – Machine Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:ml&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/topics/deep-learning&#34;&gt;DL – Deep Learning&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:dl&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/ARM_architecture_family&#34;&gt;ARM – Advanced RISC Machine&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://superorbital.io/blog/slurm-an-hpc-scheduler-for-batch-workloads/#fnref:arm&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Sun, 07 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【WebAssembly components: the next wave of cloud native computing】WebAssembly 组件：云原生计算的下一波浪潮</title>
      <link>https://www.cncf.io/blog/2024/07/09/webassembly-components-the-next-wave-of-cloud-native-computing/</link>
      <description>【&lt;p&gt;&lt;em&gt;Member post by Liam Randall, Cosmonic CEO and CNCF Ambassador and Bailey Hayes, Cosmonic CTO, Bytecode Alliance TSC director, and WASI SG co-chair&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The advent of containers marked an inflection point for computing in the 21st century—a paradigm shift (&lt;a href=&#34;https://en.wikipedia.org/wiki/Paradigm_shift&#34;&gt;per Thomas Kuhn&lt;/a&gt;) that gave rise to the entire cloud native landscape. In 2024, the arrival of WebAssembly components represents a new inflection point, and the next paradigm shift is already underway.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Why components are made for the cloud&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly (Wasm) has been around for about a decade now—much as Linux kernel namespaces were in use for over a decade before the debut of Docker (and 15 years before Kubernetes reached the mainstream). Like the Linux namespace, the core WebAssembly standard has provided a firm foundation to build on.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the case of Wasm, that means we have a bytecode format and virtual instruction set architecture (ISA) that enable us to compile code from any language to a common standard, without needing to build for a particular kernel or architecture. Over the last decade, Wasm’s flexibility has proven itself not only in the browser, but…pretty much everywhere else as well. Today, Wasm is used every where from &lt;a href=&#34;https://www.amazon.science/blog/how-prime-video-updates-its-app-for-more-than-8-000-device-types&#34;&gt;Amazon Prime Video&lt;/a&gt; to &lt;a href=&#34;https://www.youtube.com/watch?v=ms5_0wOl79I&#34;&gt;Google Earth&lt;/a&gt; to &lt;a href=&#34;https://www.cncf.io/blog/2022/11/17/better-together-a-kubernetes-and-wasm-case-study/&#34;&gt;Adobe&lt;/a&gt; to &lt;a href=&#34;https://www.cncf.io/blog/2024/01/05/bringing-webassembly-to-telecoms-with-cncf-wasmcloud/&#34;&gt;telecoms like Orange&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Core Wasm is particularly well-suited to the cloud. In addition to their flexibility, Wasm binaries are tiny, sandboxed, and efficient, allowing for much greater density and speed of download or startup in cloud environments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly &lt;strong&gt;components&lt;/strong&gt; take all of this one step further. (Or many big leaps further, as we’ll see.) Components are WebAssembly binaries conforming to an additional specification called the Component Model. Components bring all the same benefits of ordinary Wasm modules (as the Component Model is built on top of the Core WebAssembly specification), but they are also &lt;strong&gt;interoperable&lt;/strong&gt; and &lt;strong&gt;composable&lt;/strong&gt; with other components:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Interoperability&lt;/strong&gt; means that components can communicate over strictly-defined interfaces. There is a common set of standardized interfaces called the &lt;a href=&#34;https://github.com/WebAssembly/WASI/&#34;&gt;&lt;strong&gt;WASI&lt;/strong&gt;&lt;/a&gt;, consisting of high-level APIs (at various stages of proposal and standardization) for functionality like HTTP, CLI, blob storage, key-value storage, and much more. Developers can use their favorite libraries in their favorite languages, and once they compile the code to a Wasm component, other components can make use of the functions they expose—regardless of the language and libraries used to write &lt;em&gt;those&lt;/em&gt; components.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Composability&lt;/strong&gt; means that multiple components can be combined into a single component. Functions exposed (or “exported”) by one component on a given interface can be used (or “imported”) by another component, and those two components can be compiled together into a single binary. For a microservice—or any two pieces talking, really—composition is much more efficient than sending data over a network boundary where calls within a composed component happen with the same process in nanoseconds vs a network request in milliseconds. If you need to link components dynamically, you can achieve a compositional effect with distributed components using an open source transport protocol like &lt;a href=&#34;https://github.com/wrpc/wrpc&#34;&gt;wRPC (WIT over RPC)&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In short, if core WebAssembly binaries are flat-surfaced, fundamental building blocks, components are studded construction bricks—designed for building sophisticated, interconnected applications in new ways.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1086&#34; height=&#34;506&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/components.jpg&#34; alt=&#34;components&#34; class=&#34;wp-image-113994&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/components.jpg 1086w, https://www.cncf.io/wp-content/uploads/2024/07/components-300x140.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/components-1024x477.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/components-768x358.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/components-900x419.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/components-429x200.jpg 429w, https://www.cncf.io/wp-content/uploads/2024/07/components-858x400.jpg 858w&#34; sizes=&#34;(max-width: 1086px) 100vw, 1086px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If a WASI API doesn’t exist for your use case, no problem—you can write your own interface in the open &lt;strong&gt;WebAssembly Interface Type (WIT)&lt;/strong&gt; interface description language. Open standards make the ecosystem infinitely extensible.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly enthusiasts (like us) often share this quote from Docker founder Solomon Hykes:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;If WASM+WASI existed in 2008, we wouldn&#39;t have needed to create Docker.&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We love that quote! But if we use it without explaining how components work, we run the risk of obscuring what is so transformative about them. It’s a mistake to think of components as a more efficient alternative to containers. Yes, they are efficient, portable delivery mechanisms for cloud native workloads—but components are also an entirely new paradigm that unlocks entirely new models of computing.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;The next wave is already here&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The last couple of years have built to a convergence moment in which key pieces have fallen into place (or rather, been wrestled into place by many, many people working across the ecosystem). The most important of these is the release of WASI 0.2 and the Component Model in January 2024. With a common model and common APIs in place, the community has wasted no time building and updating a wide array of open source tools and native support in standard language libraries. For just a handful of examples:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasmtime&#34;&gt;&lt;strong&gt;Wasmtime&lt;/strong&gt;&lt;/a&gt;: Standalone runtime for WebAssembly components&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasm-tools&#34;&gt;&lt;strong&gt;&lt;code&gt;wasm-tools&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;: A multi-functional tool for interacting with components (read WIT interfaces, compose, and more)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://wasmcloud.com/&#34;&gt;&lt;strong&gt;wasmCloud&lt;/strong&gt;&lt;/a&gt;: Build and run components anywhere, including edge and distributed environments&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wit-deps&#34;&gt;&lt;strong&gt;&lt;code&gt;wit-deps&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;: Manage WIT dependencies for a component project&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasi-virt&#34;&gt;&lt;strong&gt;WASI Virt&lt;/strong&gt;&lt;/a&gt;: Use composition to virtualize a component within another encapsulating component&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Better yet, components increasingly integrate with common cloud native tools and standards:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;OCI has emerged as &lt;a href=&#34;https://tag-runtime.cncf.io/wgs/wasm/deliverables/wasm-oci-artifact/&#34;&gt;the standard for packaging components in registries&lt;/a&gt;.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Platforms like wasmCloud &lt;a href=&#34;https://wasmcloud.com/docs/kubernetes&#34;&gt;integrate with Kubernetes&lt;/a&gt;, OpenTelemetry, and Open Policy Agent.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Teams don’t have to start from square one to run components in production, but can use their existing cloud native tooling. For those brand new to components, the community is developing more and more resources like &lt;a href=&#34;https://component-model.bytecodealliance.org/&#34;&gt;The Component Model Book&lt;/a&gt; and &lt;a href=&#34;https://wasi.dev/&#34;&gt;WASI.dev&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;All the while, the component development experience is growing more and more polished across more and more languages. In the JavaScript ecosystem, &lt;a href=&#34;https://github.com/bytecodealliance/jco&#34;&gt;&lt;code&gt;jco&lt;/code&gt;&lt;/a&gt; enables JavaScript developers to write idiomatic code and compile to WebAssembly, while Rustaceans can compile directly to the &lt;a href=&#34;https://doc.rust-lang.org/nightly/rustc/platform-support/wasm32-wasip2.html&#34;&gt;&lt;code&gt;wasm32-wasip2&lt;/code&gt; target&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Around the world, teams are already unlocking new possibilities with components. In resource-constrained environments like edge devices on factory floors, manufacturing analytics company &lt;a href=&#34;https://www.youtube.com/watch?v=fQdkNGZqYZA&amp;amp;list=PLj6h78yzYM2MQteKoXxICTWiUdZYEw6RI&amp;amp;index=6&#34;&gt;MachineMetrics&lt;/a&gt; is using components to process high-frequency data directly on factory-floor devices and stream it to the cloud in real-time—running in places that containers typically don’t reach, all the while integrating with Kubernetes. Only a handful of months after the release of WASI 0.2, components are already changing what is possible.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Accelerating innovation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Developers—and ecosystems as a whole—don’t necessarily adopt new technologies because they are theoretically more efficient or secure. New ways of working emerge when a technology enables us to be more effective, more productive, more innovative. WebAssembly components are doing just that—breaking down language silos and revealing new opportunities.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Today, we’re about a decade out from the first days of Wasm. In the timeline of containers, this is where we reached an inflection point; the Component Model and WASI 0.2 are ushering in the same sort of paradigm shift. The tooling is there for developers to not just build with WebAssembly, but to be more productive, more effective, and more innovative. The common interfaces of WASI make component development cycles incredibly rapid, and components themselves incredibly flexible. Teams will take components to places that we can’t predict, and from here, the next wave of cloud native computing will only become more transformative.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;Cosmonic 首席执行官兼 CNCF 大使 Liam Randall 和 Cosmonic 首席技术官、字节码联盟 TSC 总监兼 WASI SG 联合主席 Bailey Hayes 发表的会员帖子&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;容器的出现标志着 21 世纪计算的拐点——范式转变（&lt;a href=&#34;https://en.wikipedia.org/wiki/Paradigm_shift&#34;&gt;每 Thomas Kuhn&lt;/a&gt;）这催生了整个云原生景观。 2024 年，WebAssembly 组件的到来代表着一个新的拐点，下一个范式转变已经开始。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;为什么组件是为云而设计的&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly (Wasm) 已经存在了大约十年，就像 Linux 内核命名空间在 Docker 出现之前已经使用了十多年一样（以及 Kubernetes 成为主流之前的 15 年）。与 Linux 命名空间一样，核心 WebAssembly 标准提供了坚实的基础。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;就 Wasm 而言，这意味着我们拥有字节码格式和虚拟指令集架构 (ISA)，使我们能够将任何语言的代码编译为通用标准，而无需针对特定内核或架构进行构建。在过去的十年中，Wasm 的灵活性不仅在浏览器中得到了证明，而且......几乎在其他任何地方都得到了证明。如今，Wasm 已被广泛使用 &lt;a href=&#34;https://www.amazon.science/blog/how-prime-video-updates-its-app-for-more-than-8-000-device-types &#34;&gt;Amazon Prime Video&lt;/a&gt; 至 &lt;a href=&#34;https://www.youtube.com/watch?v=ms5_0wOl79I&#34;&gt;Google 地球&lt;/a&gt; 至 &lt;a href=&#34;https://www. cncf.io/blog/2022/11/17/better-together-a-kubernetes-and-wasm-case-study/&#34;&gt;Adobe&lt;/a&gt; 至 &lt;a href=&#34;https://www.cncf.io /blog/2024/01/05/bringing-web assembly-to-telecoms-with-cncf-wasmcloud/&#34;&gt;像 Orange 这样的电信公司&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Core Wasm 特别适合云。除了灵活性之外，Wasm 二进制文件体积小、沙箱化且高效，可在云环境中实现更高的下载或启动密度和速度。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly &lt;strong&gt;组件&lt;/strong&gt;使这一切更进一步。 （或者我们将看到更多的巨大飞跃。）组件是符合称为组件模型的附加规范的 WebAssembly 二进制文件。组件带来了普通 Wasm 模块的所有相同优势（因为组件模型构建在 Core WebAssembly 规范之上），但它们也可以与其他组件&lt;strong&gt;互操作&lt;/strong&gt;和&lt;strong&gt;可组合&lt;/strong&gt; ：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;互操作性&lt;/strong&gt;意味着组件可以通过严格定义的接口进行通信。有一组通用的标准化接口，称为 &lt;a href=&#34;https://github.com/WebAssembly/WASI/&#34;&gt;&lt;strong&gt;WASI&lt;/strong&gt;&lt;/a&gt;，由高级 API（位于提案和标准化的各个阶段），用于 HTTP、CLI、blob 存储、键值存储等功能。开发人员可以用他们喜欢的语言使用他们喜欢的库，一旦他们将代码编译为 Wasm 组件，其他组件就可以使用它们公开的功能 - 无论用于编写这些功能的语言和库是什么&lt;em&gt;&lt;/em&gt;组件。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;可组合性&lt;/strong&gt;意味着多个组件可以组合成一个组件。给定接口上的一个组件公开（或“导出”）的函数可以由另一组件使用（或“导入”），并且这两个组件可以一起编译成单个二进制文件。对于微服务（或者任何两个正在说话的部分）来说，组合比通过网络边界发送数据要高效得多，在网络边界中，组合组件内的调用在纳秒内通过相同的进程进行，而网络请求则在毫秒内发生。如果您需要动态链接组件，您可以使用开源传输协议（如 &lt;a href=&#34;https://github.com/wrpc/wrpc&#34;&gt;wRPC (WIT over RPC)&lt;/一个&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;简而言之，如果核心 WebAssembly 二进制文件是平面的基本构建块，那么组件就是镶嵌的构造块，旨在以新的方式构建复杂的互连应用程序。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“1086”高度=“506”src=“https://www.cncf.io/ wp-content/uploads/2024/07/components.jpg&#34; alt=&#34;组件&#34; class=&#34;wp-image-113994&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07 /components.jpg 1086w，https://www.cncf.io/wp-content/uploads/2024/07/components-300x140.jpg 300w，https://www.cncf.io/wp-content/uploads/2024 /07/components-1024x477.jpg 1024w，https://www.cncf.io/wp-content/uploads/2024/07/components-768x358.jpg 768w，https://www.cncf.io/wp-content /uploads/2024/07/components-900x419.jpg 900w，https://www.cncf.io/wp-content/uploads/2024/07/components-429x200.jpg 429w，https://www.cncf.io /wp-content/uploads/2024/07/components-858x400.jpg 858w“尺寸=”（最大宽度：1086px）100vw，1086px“referrerpolicy=“no-referrer”&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果您的用例不存在 WASI API，也没有问题 - 您可以使用开放的 &lt;strong&gt;WebAssembly 接口类型 (WIT)&lt;/strong&gt; 接口描述语言编写自己的接口。开放标准使生态系统无限扩展。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly 爱好者（像我们一样）经常分享 Docker 创始人 Solomon Hykes 的这句话：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;如果 WASM+WASI 在 2008 年就存在，我们就不需要创建 Docker。&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们喜欢这句话！但如果我们在不解释组件如何工作的情况下使用它，我们就有可能掩盖它们的变革性。将组件视为比容器更有效的替代品是错误的。是的，它们是云原生工作负载的高效、可移植的交付机制，但组件也是一种全新的范例，可以解锁全新的计算模型。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;下一波浪潮已经到来&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;过去几年已经形成了一个融合时刻，关键部分已经就位（或者更确切地说，是由整个生态系统中的许多人努力就位）。其中最重要的是 2024 年 1 月发布的 WASI 0.2 和组件模型。有了通用模型和通用 API，社区立即构建和更新了各种开源工具和标准的本机支持语言库。仅举几个例子：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasmtime&#34;&gt;&lt;strong&gt;Wasmtime&lt;/strong&gt;&lt;/a&gt;：WebAssembly 组件的独立运行时&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasm-tools&#34;&gt;&lt;strong&gt;&lt;code&gt;wasm-tools&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;：多功能工具用于与组件交互（阅读 WIT 接口、撰写等）&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://wasmcloud.com/&#34;&gt;&lt;strong&gt;wasmCloud&lt;/strong&gt;&lt;/a&gt;：在任何地方构建和运行组件，包括边缘和分布式环境&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wit-deps&#34;&gt;&lt;strong&gt;&lt;code&gt;wit-deps&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;：管理 WIT 依赖项组件项目&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/bytecodealliance/wasi-virt&#34;&gt;&lt;strong&gt;WASI Virt&lt;/strong&gt;&lt;/a&gt;：使用组合在另一个封装组件中虚拟化一个组件&lt;/li &gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;更好的是，组件越来越多地与常见的云原生工具和标准集成：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;OCI 已成为&lt;a href=&#34;https://tag-runtime.cncf.io/wgs/wasm/deliverables/wasm-oci-artifact/&#34;&gt;在注册表中打包组件的标准&lt;/a&gt;。 &lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;wasmCloud 等平台&lt;a href=&#34;https://wasmcloud.com/docs/kubernetes&#34;&gt;与 Kubernetes 集成&lt;/a&gt;、OpenTelemetry 和开放策略代理。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;团队不必从头开始在生产中运行组件，而是可以使用现有的云原生工具。对于那些全新的组件，社区正在开发越来越多的资源，例如 &lt;a href=&#34;https://component-model.bytecodealliance.org/&#34;&gt;组件模型手册&lt;/a&gt; 和 &lt;a href=&#34;https ://wasi.dev/&#34;&gt;WASI.dev&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;与此同时，组件开发体验在越来越多的语言中变得越来越完善。在 JavaScript 生态系统中，&lt;a href=&#34;https://github.com/bytecodealliance/jco&#34;&gt;&lt;code&gt;jco&lt;/code&gt;&lt;/a&gt; 使 JavaScript 开发人员能够编写惯用代码并编译为 WebAssembly，而 Rustaceans 可以直接编译到 &lt;a href=&#34;https://doc.rust-lang.org/nightly/rustc/platform-support/wasm32-wasip2.html&#34;&gt;&lt;code&gt;wasm32-wasip2&lt;/code&gt; 目标&lt;/a &gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;世界各地的团队已经在利用组件解锁新的可能性。在工厂车间边缘设备等资源受限的环境中，制造分析公司 &lt;a href=&#34;https://www.youtube.com/watch?v=fQdkNGZqYZA&amp;list=PLj6h78yzYM2MQteKoXxICTWiUdZYEw6RI&amp;index=6&#34;&gt;MachineMetrics&lt;/a&gt; 正在使用组件来直接在工厂车间设备上处理高频数据并将其实时传输到云端——在容器通常无法到达的地方运行，同时与 Kubernetes 集成。 WASI 0.2 发布仅几个月后，组件就已经改变了一切可能。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;加速创新&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;开发者以及整个生态系统不一定会采用新技术，因为它们在理论上更高效或更安全。当技术使我们变得更有效、更有生产力、更具创新性时，新的工作方式就会出现。 WebAssembly 组件正在这样做——打破语言孤岛并揭示新的机会。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;今天，距 Wasm 诞生已有十年了。在容器的时间轴上，这是我们到达一个拐点的地方；组件模型和 WASI 0.2 正在迎来同样的范式转变。该工具不仅可以让开发人员使用 WebAssembly 进行构建，还可以提高生产力、效率和创新性。 WASI 的通用接口使组件开发周期变得异常快速，并且组件本身非常灵活。团队将把组件带到我们无法预测的地方，从这里开始，下一波云原生计算只会变得更具变革性。&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Mon, 08 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【A new App Development WG has now been launched!】新的应用程序开发工作组现已启动！</title>
      <link>https://www.cncf.io/blog/2024/07/05/a-new-app-development-wg-has-now-been-launched/</link>
      <description>【&lt;p&gt;&lt;em&gt;TAG post from TAG App Delivery &lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Calling all developers!&lt;/strong&gt; We’re excited to announce the launch of the new &lt;a href=&#34;https://tag-app-delivery.cncf.io/wgs/app-development/&#34;&gt;App Development Working Group&lt;/a&gt; within the &lt;a href=&#34;https://tag-app-delivery.cncf.io/&#34;&gt;TAG App Delivery&lt;/a&gt;. This group is dedicated to bridging the gap between developers and CNCF projects directly affecting your daily workflow.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Get involved and help shape the future of cloud-native application development! The &lt;a href=&#34;https://tag-app-delivery.cncf.io/wgs/app-development/charter/charter.md/&#34;&gt;charter&lt;/a&gt; provides more information about the working group’s goals and direction.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The working group is co-chaired by Daniel Oh from Red Hat, Thomas Vitale from Systematic, and Mauricio Salatino from Diagrid. Leveraging the deep expertise of the co-chairs and group members (e.g., Ryan Nowak – Microsoft, Eli Aleyner – Docker, Marcos Lilljedahl – Dagger, Sonali Srivastava – InfraCloud Technologies, and Yacine Kheddache – Microcks) in development practices, platform engineering, and the CNCF ecosystem, the group is initially focused on highlighting Graduated and Incubating projects that directly benefit developers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Beyond raising awareness, the working group actively shapes the cloud-native development landscape by classifying tools and fostering collaboration between CNCF projects and app developers. This makes it easier for the developers to find the right tools and share best practices. The working group also actively seeks out and integrates new projects prioritizing best practices for cloud-native application development.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Join the movement!&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Are you a cloud-native developer passionate about improving development? We’d love for you to be part of it!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://tag-app-delivery.cncf.io/wgs/app-development/&#34;&gt;Bi-weekly meetings&lt;/a&gt;: Participate in our discussions on each month’s first and third Wednesdays.&amp;nbsp;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/1L7e2szHX_gpYnC0cs_BTQH6hTNzh8acWzqkzhrPOJds/edit#wg-app-developmen&#34;&gt;CNCF Slack Channel&lt;/a&gt;: Connect with us on the #wg-app-development channel on the CNCF Slack workspace:&amp;nbsp;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Are you heading to &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&#34;&gt;KubeCon + CLoudNativeCon North America 2024&lt;/a&gt;? Don’t miss out on connecting with fellow developers at the co-located &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&#34;&gt;AppDevelopmentCon&lt;/a&gt; event! We’re also going to &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-india/&#34;&gt;KubeCon + CloudNativeCon India 2024&lt;/a&gt;. Let’s meet up and chat about cloud-native development.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;来自 TAG App Delivery 的 TAG 帖子&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;呼叫所有开发者！&lt;/strong&gt;我们很高兴地宣布推出新的&lt;a href=&#34;https://tag-app-delivery.cncf.io/wgs/app-development/ &lt;a href=&#34;https://tag-app-delivery.cncf.io/&#34;&gt;TAG 应用交付&lt;/a&gt;内的&#34;&gt;应用开发工作组&lt;/a&gt;。该小组致力于弥合开发人员与直接影响您日常工作流程的 CNCF 项目之间的差距。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;参与并帮助塑造云原生应用程序开发的未来！ &lt;a href=&#34;https://tag-app-delivery.cncf.io/wgs/app-development/charter/charter.md/&#34;&gt;章程&lt;/a&gt;提供了有关工作组目标和方向的更多信息。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;该工作组由来自 Red Hat 的 Daniel Oh、来自 Systematic 的 Thomas Vitale 和来自 Diagrid 的 Mauricio Salatino 担任联合主席。利用联合主席和小组成员（例如，Ryan Nowak – Microsoft、Eli Aleyner – Docker、Marcos Lilljedahl – Dagger、Sonali Srivastava – InfraCloud Technologies 和 Yacine Kheddache – Microcks）在开发实践、平台工程和在 CNCF 生态系统中，该小组最初专注于突出直接使开发人员受益的毕业和孵化项目。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;除了提高认识之外，工作组还通过对工具进行分类并促进 CNCF 项目和应用开发人员之间的协作，积极塑造云原生开发格局。这使得开发人员更容易找到合适的工具并分享最佳实践。该工作组还积极寻找并整合新项目，优先考虑云原生应用程序开发的最佳实践。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;加入运动！&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;您是一位热衷于改进开发的云原生开发人员吗？我们希望您能参与其中！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://tag-app-delivery.cncf.io/wgs/app-development/&#34;&gt;每两周一次的会议&lt;/a&gt;：参与我们每月第一个和第三个周三的讨论。 &lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/1L7e2szHX_gpYnC0cs_BTQH6hTNzh8acWzqkzhrPOJds/edit#wg-app-developmen&#34;&gt;CNCF Slack 频道&lt;/a&gt;：通过 #wg-app 与我们联系-CNCF Slack 工作区上的开发频道：&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;您要去参加&lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&#34;&gt;KubeCon + CLoudNativeCon North America 2024&lt;/a&gt;吗？不要错过在同期举行的 &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&#34;&gt;AppDevelopmentCon&lt;/a&gt; 活动中与其他开发人员交流！我们还将参加 &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-india/&#34;&gt;KubeCon + CloudNativeCon India 2024&lt;/a&gt;。让我们聚在一起聊聊云原生开发吧。&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Thu, 04 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【SQL simplifies TSDB – how to migrate from InfluxQL to SQL】SQL 简化 TSDB – 如何从 InfluxQL 迁移到 SQL</title>
      <link>https://www.cncf.io/blog/2024/07/10/sql-simplifies-tsdb-how-to-migrate-from-influxql-to-sql/</link>
      <description>【&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql&#34;&gt;Greptime’s blog&lt;/a&gt; by tison&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. SQL is a more common and general language for querying time series data, making migrating from InfluxQL to SQL a growing trend.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/coverimage1.png&#34; alt=&#34;Image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB uses SQL as its primary query language. Once users ingest data into GreptimeDB via the InfluxDB line protocol or other APIs, a common question arises: how can I analyze the data ingested? Specifically, how can existing InfluxQL queries be migrated to SQL queries?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To address the question above, this article outlines the differences between the query languages of InfluxDB (InfluxQL or Flux) and SQL, as well as a cheat sheet for migrating from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;overview-of-query-languages&#34;&gt;Overview of Query Languages&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#overview-of-query-languages&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/&#34;&gt;InfluxQL&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V1. It’s a SQL-like query language but not a SQL dialect. Below are some examples of InfluxQL queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT * FROM h2o_feet;&#xA;SELECT * FROM h2o_feet LIMIT 5;&#xA;SELECT COUNT(&#34;water_level&#34;) FROM h2o_feet;&#xA;SELECT &#34;level description&#34;, &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;SELECT *::field FROM &#34;h2o_feet&#34;;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When InfluxDB was designed and developed, there weren’t as many database developers as today. Consequently, despite InfluxQL’s efforts to closely resemble SQL syntax, implementing basic SQL capabilities supported by relational algebra and adding time series query extensions was quite challenging.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL instead implemented functions and syntax specifically designed for time series data analysis. For instance, all InfluxQL queries default to returning the timestamp column in ascending order, and all queries must include field columns to return results.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, special query syntax is designed for querying over time series rather than rows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Essentially, InfluxQL was developed from the raw need for time series data analysis focused on numerical metrics. As InfluxDB evolved, InfluxQL also supported continuous queries and retention policies to solve some requirements of real-time data processing.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL can still be used in InfluxDB V2, it faces&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v2/query-data/influxql/&#34;&gt;a series of challenges due to model mismatches&lt;/a&gt;, as InfluxDB V2 mainly promotes the Flux query language.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/&#34;&gt;Flux&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V2. Unlike InfluxQL, which has a SQL-like syntax, Flux uses a DataFrame style syntax. Developers who have written programs in Elixir will find the syntax familiar. Here are some examples of Flux queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;erlang&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;from(bucket: &#34;example-bucket&#34;)&#xA;    |&amp;gt; range(start: -1d)&#xA;    |&amp;gt; filter(fn: (r) =&amp;gt; r._measurement == &#34;example-measurement&#34;)&#xA;    |&amp;gt; mean()&#xA;    |&amp;gt; yield(name: &#34;_result&#34;)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Designed to support joint analysis of time series data across various data sources, Flux allows users to fetch data from time series databases (InfluxDB), relational databases (PostgreSQL or MySQL), and CSV files for analysis. For example,&amp;nbsp;&lt;code&gt;sql.from&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;csv.from&lt;/code&gt;&amp;nbsp;can replace&amp;nbsp;&lt;code&gt;from(bucket)&lt;/code&gt;&amp;nbsp;in the example above, allowing fetching data from other sources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Flux can only be used in InfluxDB V2; it is not implemented in V1 and has been&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/future-of-flux/&#34;&gt;abandoned in V3&lt;/a&gt;. The reason is apparent: the learning curve is too steep. Without professional language developers, expanding syntax while fixing various design and implementation issues is almost impossible, resulting in unsustainable engineering costs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, the Structured Query Language, is familiar to data analysts and is based on relational algebra.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Unlike DSLs tailored for specific business scenarios, SQL has a solid theoretical foundation. Since E. F. Codd published the seminal paper “&lt;a href=&#34;https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf&#34;&gt;A Relational Model of Data for Large Shared Data Banks&lt;/a&gt;,” research on relational databases has flourished for over fifty years.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Despite unique extensions in various SQL databases that sometimes confuse users, the basic query and analysis capabilities are consistently implemented across all SQL databases, supported by relational algebra. One or two decades ago, there might have been debates about SQL’s relevance. However, SQL has undoubtedly reasserted itself as the default choice for data analysis today. Over the years, SQL has been continuously improved and expanded, and it is widely adopted globally through a series of proven implementations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL is the primary query language for InfluxDB V3 and GreptimeDB. Both now recommend users analyze time series data using SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In GreptimeDB,&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql&#34;&gt;you can use standard SQL to query your data&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT ts, idc, AVG(memory_util)&#xA;FROM system_metrics&#xA;GROUP BY idc&#xA;ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The solid theoretical foundation of SQL helps emerging time series databases reliably implement complex query logic and data management tasks. Also, the broader SQL ecosystem enables emerging time series databases to quickly integrate into the data analysis tech stack. For example, in the previous&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-03-19-keyboard-monitoring&#34;&gt;input behavior analysis demos&lt;/a&gt;, we showcase an integration between GreptimeDB and Streamlit for visualizing time series by leveraging GreptimeDB’s MySQL protocol support.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;challenges-in-time-series-analysis&#34;&gt;Challenges in Time Series Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#challenges-in-time-series-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql-1&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While SQL has a solid theoretical foundation and a broader analytical ecosystem, traditional SQL databases suffer when handling time series data, primarily due to their large size.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The value provided from a single data point of a time series is often very low. Most metrics uploaded by devices aren’t explicitly handled, and the healthy status reported doesn’t require special attention. Thus, the cost-efficiency of storing time series data is crucial. How to leverage modern cloud commodity storage to reduce costs and use cutting-edge compression for time series data are key points for time series databases.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Furthermore, extracting essential information efficiently from vast amounts of time series data often requires specific query extensions for optimization. GreptimeDB’s support for&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY&lt;/a&gt;&amp;nbsp;to help users analyze data aggregation within specific time windows is one such example.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux-1&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The learning curve itself essentially doomed this dialect. As mentioned above, being a DSL solely supported by a single provider, Flux faced significant challenges in language robustness, performance optimization, and ecosystem development. The sole provider has since abandoned further development of Flux, making it a language of the past.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql-1&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL syntax resembles SQL, subtle differences can be frustrating. Despite efforts to mimic SQL syntax, InfluxQL fundamentally remains a DSL tailored to time series analysis needs focusing on metrics. Its challenges in development and maintenance costs are similar to those faced by Flux.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For example, InfluxQL does not support&amp;nbsp;&lt;code&gt;JOIN&lt;/code&gt;&amp;nbsp;queries. Although one can write queries like&amp;nbsp;&lt;code&gt;SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&lt;/code&gt;, it simply reads data from both measurements separately:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&amp;gt; SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&#xA;&#xA;name: h2o_feet&#xA;--------------&#xA;time                   level description      location       pH   water_level&#xA;2015-08-18T00:00:00Z   below 3 feet           santa_monica        2.064&#xA;2015-08-18T00:00:00Z   between 6 and 9 feet   coyote_creek        8.12&#xA;[...]&#xA;2015-09-18T21:36:00Z   between 3 and 6 feet   santa_monica        5.066&#xA;2015-09-18T21:42:00Z   between 3 and 6 feet   santa_monica        4.938&#xA;&#xA;name: h2o_pH&#xA;------------&#xA;time                   level description   location       pH   water_level&#xA;2015-08-18T00:00:00Z                       santa_monica   6&#xA;2015-08-18T00:00:00Z                       coyote_creek   7&#xA;[...]&#xA;2015-09-18T21:36:00Z                       santa_monica   8&#xA;2015-09-18T21:42:00Z                       santa_monica   7&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Moreover, despite InfluxDB V3 supporting InfluxQL due to strong user demand to facilitate migration, InfluxDB V3 primarily promotes SQL-based queries. Thus, it’s fair to say that InfluxQL is also fading away.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;migrating-to-sql-analysis&#34;&gt;Migrating to SQL Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#migrating-to-sql-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Today, many existing time series data analysis logics are written in InfluxQL. This section outlines the core differences between InfluxQL and SQL and illustrates how to migrate from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;timestamp-column&#34;&gt;Timestamp Column&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#timestamp-column&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;A key difference in application logic migration is that&amp;nbsp;&lt;strong&gt;SQL does not treat the time column especially, while InfluxQL returns the time column by default and sorts results in ascending order by timestamp&lt;/strong&gt;. SQL queries need to explicitly specify the time column to include timestamps in the result set and manually specify sorting logic.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;SELECT &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;SELECT ts, location, water_level FROM h2o_feet ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When writing data, InfluxQL automatically populates the time column with the current time, whereas SQL requires manual specification of the time column value. If using the current time, it must be explicitly written:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;INSERT INTO &#34;measurement&#34; (tag, value) VALUES (&#39;my_tag&#39;, 42);&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support inserting multiple rows in one INSERT statement, whereas SQL databases typically support this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag_0&#39;, 42), (NOW(), &#39;my_tag_1&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, InfluxQL uses the&amp;nbsp;&lt;code&gt;tz()&lt;/code&gt;&amp;nbsp;function to specify the query timezone, while SQL typically has other ways to set the timezone. GreptimeDB supports&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/mysql#time-zone&#34;&gt;MySQL&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/postgresql#time-zone&#34;&gt;PostgreSQL&lt;/a&gt;&amp;nbsp;syntax for setting the timezone.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;time-series&#34;&gt;Time Series&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#time-series&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL implements time series granularity query syntax, such as SLIMIT and&amp;nbsp;&lt;code&gt;SOFFSET&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;SLIMIT&lt;/code&gt;&amp;nbsp;limits the number of data points returned for each time series in the result set. For example,&amp;nbsp;&lt;code&gt;SLIMIT 1&lt;/code&gt;&amp;nbsp;means, at most, one result per time series that meets the filter condition.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, not specifically designed for time series data analysis, requires some workarounds:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT DISTINCT ON (host) * FROM monitor ORDER BY host, ts DESC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This query returns one result per time series, distinguished by the host tag:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;+-----------+---------------------+------+--------+&#xA;| host      | ts                  | cpu  | memory |&#xA;+-----------+---------------------+------+--------+&#xA;| 127.0.0.1 | 2022-11-03 03:39:58 |  0.5 |    0.2 |&#xA;| 127.0.0.2 | 2022-11-03 03:39:58 |  0.2 |    0.3 |&#xA;+-----------+---------------------+------+--------+&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;interval-literals&#34;&gt;Interval Literals&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#interval-literals&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL’s interval syntax resembles&amp;nbsp;&lt;code&gt;1d&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;12m&lt;/code&gt;, while SQL has standard syntax for time intervals:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INTERVAL &#39;1 DAY&#39;&#xA;INTERVAL &#39;1 YEAR 3 HOURS 20 MINUTES&#39;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;data-columns-and-tag-columns&#34;&gt;Data Columns and Tag Columns&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#data-columns-and-tag-columns&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL distinguishes between data columns and tag columns at the model level; queries that only SELECT tag columns will not return data. InfluxQL also supports the&amp;nbsp;&lt;code&gt;::field&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;::tag&lt;/code&gt;&amp;nbsp;suffixes to specify data columns or tag columns, allowing for columns with the same name.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL standards do not differentiate between data columns and tag columns, treating them all as regular columns. However, specific implementations may map these concepts differently. For example, GreptimeDB’s&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/concepts/data-model&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;data model&lt;/a&gt;&amp;nbsp;distinguishes between timestamp columns, tag columns, and data columns and has corresponding mapping rules.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image1.png&#34; alt=&#34;The Data Structure of GreptimeDB&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;The Data Structure of GreptimeDB&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;function-names&#34;&gt;Function Names&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#function-names&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Some function names differ between InfluxQL and SQL. For instance, the&amp;nbsp;&lt;code&gt;MEAN&lt;/code&gt;&amp;nbsp;function in InfluxQL corresponds to the&amp;nbsp;&lt;code&gt;AVG&lt;/code&gt;&amp;nbsp;function in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, many other functions, such as&amp;nbsp;&lt;code&gt;COUNT&lt;/code&gt;,&amp;nbsp;&lt;code&gt;SUM&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;MIN,&lt;/code&gt;&amp;nbsp;remain the same in both languages.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;identifiers&#34;&gt;Identifiers&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#identifiers&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In InfluxQL, identifiers are always double-quoted, while SQL supports unquoted identifiers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It is worth noting that SQL identifiers are case-insensitive by default. If case sensitivity is needed, the identifiers should be enclosed in the appropriate quotes. In GreptimeDB, double quotes are used by default. However, when connecting via MySQL or PostgreSQL clients, the corresponding dialect’s syntax is respected.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Examples of identifier usage differences between InfluxQL and SQL are as follows:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image2.png&#34; alt=&#34;The Usage Differences between InfluxQL and SQL&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;join&#34;&gt;JOIN&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#join&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support JOIN queries, while one of the fundamental capabilities of SQL databases is support for JOIN queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches&lt;/em&gt;&#xA;SELECT a.* FROM system_metrics a JOIN idc_info b ON a.idc = b.idc_id;&#xA;&#xA;&lt;em&gt;-- Select all rows from the idc_info table and system_metrics table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT a.* FROM idc_info a LEFT JOIN system_metrics b ON a.idc_id = b.idc;&#xA;&#xA;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT b.* FROM system_metrics a RIGHT JOIN idc_info b ON a.idc = b.idc_id;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These are examples of&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/reference/sql/join&#34;&gt;JOIN queries in GreptimeDB&lt;/a&gt;, which supports:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;INNER JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;LEFT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;RIGHT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;FULL OUTER JOIN&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;queries-over-time-windows&#34;&gt;Queries over Time Windows&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#queries-over-time-windows&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL’s&amp;nbsp;&lt;code&gt;GROUP BY&lt;/code&gt;&amp;nbsp;statement supports passing a time window to aggregate data within a specific length of time windows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL does not have such specific query capabilities; the closest equivalent is the&amp;nbsp;&lt;code&gt;OVER ... PARTITION BY&lt;/code&gt;&amp;nbsp;syntax, which can be quite complex to understand.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB implements its own&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY extension syntax&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT &#xA;    ts, &#xA;    host, &#xA;    avg(cpu) RANGE &#39;10s&#39; FILL LINEAR&#xA;FROM monitor&#xA;ALIGN &#39;5s&#39; TO &#39;2023-12-01T00:00:00&#39; BY (host) ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;continuous-aggregation&#34;&gt;Continuous Aggregation&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#continuous-aggregation&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL supports&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/continuous_queries/&#34;&gt;continuous aggregation&lt;/a&gt;, which corresponds to the standard concept of materialized views in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, the implementation of materialized views in most SQL databases is still fragile and remains an area for further exploration.&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-06-04-flow-engine&#34;&gt;GreptimeDB supports continuous aggregation&lt;/a&gt;&amp;nbsp;to meet these needs based on its flow engine.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#conclusion&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. While InfluxQL and Flux are used by InfluxDB and specifically created for handling time series data, SQL is a widely used query language in relational databases. Its robust theoretical foundation and rich ecosystem allow data analysts to quickly get started and use effective tools for time series data analysis.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB natively supports SQL queries. Visit our&amp;nbsp;&lt;a href=&#34;https://greptime.com/&#34;&gt;homepage&lt;/a&gt;&amp;nbsp;for more information or&amp;nbsp;&lt;a href=&#34;https://console.greptime.cloud/signup&#34;&gt;create a free cloud service&lt;/a&gt;&amp;nbsp;instance to start your trial today.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;about-greptime&#34;&gt;About Greptime&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#about-greptime&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We help industries that generate large amounts of time-series data, such as Connected Vehicles (CV), IoT, and Observability, to efficiently uncover the hidden value of data in real-time.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Visit the&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;latest version&lt;/a&gt;&amp;nbsp;from any device to get started and get the most out of your data.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GreptimeDB&lt;/a&gt;, written in Rust, is a distributed, open-source, time-series database designed for scalability, efficiency, and powerful analytics.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://greptime.com/product/carcloud&#34;&gt;Edge-Cloud Integrated TSDB&lt;/a&gt;&amp;nbsp;is designed for the unique demands of edge storage and compute in IoT. It tackles the exponential growth of edge data by integrating a multimodal edge-side database with cloud-based GreptimeDB Enterprise. This combination reduces traffic, computing, and storage costs while enhancing data timeliness and business insights.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.greptime.com/product/cloud&#34;&gt;GreptimeCloud&lt;/a&gt;&amp;nbsp;is a fully-managed cloud database-as-a-service (DBaaS) solution built on GreptimeDB. It efficiently supports applications in fields such as observability, IoT, and finance.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Star us on&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GitHub&lt;/a&gt;&amp;nbsp;or join GreptimeDB Community on&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/slack&#34;&gt;Slack&lt;/a&gt;&amp;nbsp;to get connected. Also, you can go to our&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb/contribute&#34;&gt;contribution page&lt;/a&gt;&amp;nbsp;to find some interesting issues to start with.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;Member post originally published on &lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql&#34;&gt;Greptime’s blog&lt;/a&gt; by tison&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. SQL is a more common and general language for querying time series data, making migrating from InfluxQL to SQL a growing trend.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/coverimage1.png&#34; alt=&#34;Image&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB uses SQL as its primary query language. Once users ingest data into GreptimeDB via the InfluxDB line protocol or other APIs, a common question arises: how can I analyze the data ingested? Specifically, how can existing InfluxQL queries be migrated to SQL queries?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;To address the question above, this article outlines the differences between the query languages of InfluxDB (InfluxQL or Flux) and SQL, as well as a cheat sheet for migrating from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;overview-of-query-languages&#34;&gt;Overview of Query Languages&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#overview-of-query-languages&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/&#34;&gt;InfluxQL&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V1. It’s a SQL-like query language but not a SQL dialect. Below are some examples of InfluxQL queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT * FROM h2o_feet;&#xA;SELECT * FROM h2o_feet LIMIT 5;&#xA;SELECT COUNT(&#34;water_level&#34;) FROM h2o_feet;&#xA;SELECT &#34;level description&#34;, &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;SELECT *::field FROM &#34;h2o_feet&#34;;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When InfluxDB was designed and developed, there weren’t as many database developers as today. Consequently, despite InfluxQL’s efforts to closely resemble SQL syntax, implementing basic SQL capabilities supported by relational algebra and adding time series query extensions was quite challenging.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL instead implemented functions and syntax specifically designed for time series data analysis. For instance, all InfluxQL queries default to returning the timestamp column in ascending order, and all queries must include field columns to return results.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, special query syntax is designed for querying over time series rather than rows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Essentially, InfluxQL was developed from the raw need for time series data analysis focused on numerical metrics. As InfluxDB evolved, InfluxQL also supported continuous queries and retention policies to solve some requirements of real-time data processing.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL can still be used in InfluxDB V2, it faces&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v2/query-data/influxql/&#34;&gt;a series of challenges due to model mismatches&lt;/a&gt;, as InfluxDB V2 mainly promotes the Flux query language.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/&#34;&gt;Flux&lt;/a&gt;&amp;nbsp;is the primary query language for InfluxDB V2. Unlike InfluxQL, which has a SQL-like syntax, Flux uses a DataFrame style syntax. Developers who have written programs in Elixir will find the syntax familiar. Here are some examples of Flux queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;erlang&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;from(bucket: &#34;example-bucket&#34;)&#xA;    |&amp;gt; range(start: -1d)&#xA;    |&amp;gt; filter(fn: (r) =&amp;gt; r._measurement == &#34;example-measurement&#34;)&#xA;    |&amp;gt; mean()&#xA;    |&amp;gt; yield(name: &#34;_result&#34;)&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Designed to support joint analysis of time series data across various data sources, Flux allows users to fetch data from time series databases (InfluxDB), relational databases (PostgreSQL or MySQL), and CSV files for analysis. For example,&amp;nbsp;&lt;code&gt;sql.from&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;csv.from&lt;/code&gt;&amp;nbsp;can replace&amp;nbsp;&lt;code&gt;from(bucket)&lt;/code&gt;&amp;nbsp;in the example above, allowing fetching data from other sources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Flux can only be used in InfluxDB V2; it is not implemented in V1 and has been&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/flux/v0/future-of-flux/&#34;&gt;abandoned in V3&lt;/a&gt;. The reason is apparent: the learning curve is too steep. Without professional language developers, expanding syntax while fixing various design and implementation issues is almost impossible, resulting in unsustainable engineering costs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, the Structured Query Language, is familiar to data analysts and is based on relational algebra.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Unlike DSLs tailored for specific business scenarios, SQL has a solid theoretical foundation. Since E. F. Codd published the seminal paper “&lt;a href=&#34;https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf&#34;&gt;A Relational Model of Data for Large Shared Data Banks&lt;/a&gt;,” research on relational databases has flourished for over fifty years.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Despite unique extensions in various SQL databases that sometimes confuse users, the basic query and analysis capabilities are consistently implemented across all SQL databases, supported by relational algebra. One or two decades ago, there might have been debates about SQL’s relevance. However, SQL has undoubtedly reasserted itself as the default choice for data analysis today. Over the years, SQL has been continuously improved and expanded, and it is widely adopted globally through a series of proven implementations.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL is the primary query language for InfluxDB V3 and GreptimeDB. Both now recommend users analyze time series data using SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In GreptimeDB,&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql&#34;&gt;you can use standard SQL to query your data&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT ts, idc, AVG(memory_util)&#xA;FROM system_metrics&#xA;GROUP BY idc&#xA;ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The solid theoretical foundation of SQL helps emerging time series databases reliably implement complex query logic and data management tasks. Also, the broader SQL ecosystem enables emerging time series databases to quickly integrate into the data analysis tech stack. For example, in the previous&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-03-19-keyboard-monitoring&#34;&gt;input behavior analysis demos&lt;/a&gt;, we showcase an integration between GreptimeDB and Streamlit for visualizing time series by leveraging GreptimeDB’s MySQL protocol support.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;challenges-in-time-series-analysis&#34;&gt;Challenges in Time Series Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#challenges-in-time-series-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;sql-1&#34;&gt;SQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#sql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;While SQL has a solid theoretical foundation and a broader analytical ecosystem, traditional SQL databases suffer when handling time series data, primarily due to their large size.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The value provided from a single data point of a time series is often very low. Most metrics uploaded by devices aren’t explicitly handled, and the healthy status reported doesn’t require special attention. Thus, the cost-efficiency of storing time series data is crucial. How to leverage modern cloud commodity storage to reduce costs and use cutting-edge compression for time series data are key points for time series databases.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Furthermore, extracting essential information efficiently from vast amounts of time series data often requires specific query extensions for optimization. GreptimeDB’s support for&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY&lt;/a&gt;&amp;nbsp;to help users analyze data aggregation within specific time windows is one such example.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;flux-1&#34;&gt;Flux&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#flux-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The learning curve itself essentially doomed this dialect. As mentioned above, being a DSL solely supported by a single provider, Flux faced significant challenges in language robustness, performance optimization, and ecosystem development. The sole provider has since abandoned further development of Flux, making it a language of the past.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;influxql-1&#34;&gt;InfluxQL&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#influxql-1&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Although InfluxQL syntax resembles SQL, subtle differences can be frustrating. Despite efforts to mimic SQL syntax, InfluxQL fundamentally remains a DSL tailored to time series analysis needs focusing on metrics. Its challenges in development and maintenance costs are similar to those faced by Flux.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For example, InfluxQL does not support&amp;nbsp;&lt;code&gt;JOIN&lt;/code&gt;&amp;nbsp;queries. Although one can write queries like&amp;nbsp;&lt;code&gt;SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&lt;/code&gt;, it simply reads data from both measurements separately:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;&amp;gt; SELECT * FROM &#34;h2o_feet&#34;, &#34;h2o_pH&#34;&#xA;&#xA;name: h2o_feet&#xA;--------------&#xA;time                   level description      location       pH   water_level&#xA;2015-08-18T00:00:00Z   below 3 feet           santa_monica        2.064&#xA;2015-08-18T00:00:00Z   between 6 and 9 feet   coyote_creek        8.12&#xA;[...]&#xA;2015-09-18T21:36:00Z   between 3 and 6 feet   santa_monica        5.066&#xA;2015-09-18T21:42:00Z   between 3 and 6 feet   santa_monica        4.938&#xA;&#xA;name: h2o_pH&#xA;------------&#xA;time                   level description   location       pH   water_level&#xA;2015-08-18T00:00:00Z                       santa_monica   6&#xA;2015-08-18T00:00:00Z                       coyote_creek   7&#xA;[...]&#xA;2015-09-18T21:36:00Z                       santa_monica   8&#xA;2015-09-18T21:42:00Z                       santa_monica   7&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Moreover, despite InfluxDB V3 supporting InfluxQL due to strong user demand to facilitate migration, InfluxDB V3 primarily promotes SQL-based queries. Thus, it’s fair to say that InfluxQL is also fading away.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;migrating-to-sql-analysis&#34;&gt;Migrating to SQL Analysis&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#migrating-to-sql-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Today, many existing time series data analysis logics are written in InfluxQL. This section outlines the core differences between InfluxQL and SQL and illustrates how to migrate from InfluxQL to SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;timestamp-column&#34;&gt;Timestamp Column&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#timestamp-column&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;A key difference in application logic migration is that&amp;nbsp;&lt;strong&gt;SQL does not treat the time column especially, while InfluxQL returns the time column by default and sorts results in ascending order by timestamp&lt;/strong&gt;. SQL queries need to explicitly specify the time column to include timestamps in the result set and manually specify sorting logic.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;SELECT &#34;location&#34;, &#34;water_level&#34; FROM &#34;h2o_feet&#34;;&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;SELECT ts, location, water_level FROM h2o_feet ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;When writing data, InfluxQL automatically populates the time column with the current time, whereas SQL requires manual specification of the time column value. If using the current time, it must be explicitly written:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- InfluxQL&lt;/em&gt;&#xA;INSERT INTO &#34;measurement&#34; (tag, value) VALUES (&#39;my_tag&#39;, 42);&#xA;&lt;em&gt;-- SQL&lt;/em&gt;&#xA;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support inserting multiple rows in one INSERT statement, whereas SQL databases typically support this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INSERT INTO measurement (ts, tag, value) VALUES (NOW(), &#39;my_tag_0&#39;, 42), (NOW(), &#39;my_tag_1&#39;, 42);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Additionally, InfluxQL uses the&amp;nbsp;&lt;code&gt;tz()&lt;/code&gt;&amp;nbsp;function to specify the query timezone, while SQL typically has other ways to set the timezone. GreptimeDB supports&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/mysql#time-zone&#34;&gt;MySQL&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/clients/postgresql#time-zone&#34;&gt;PostgreSQL&lt;/a&gt;&amp;nbsp;syntax for setting the timezone.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;time-series&#34;&gt;Time Series&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#time-series&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL implements time series granularity query syntax, such as SLIMIT and&amp;nbsp;&lt;code&gt;SOFFSET&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;code&gt;SLIMIT&lt;/code&gt;&amp;nbsp;limits the number of data points returned for each time series in the result set. For example,&amp;nbsp;&lt;code&gt;SLIMIT 1&lt;/code&gt;&amp;nbsp;means, at most, one result per time series that meets the filter condition.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL, not specifically designed for time series data analysis, requires some workarounds:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT DISTINCT ON (host) * FROM monitor ORDER BY host, ts DESC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This query returns one result per time series, distinguished by the host tag:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code class=&#34;&#34;&gt;+-----------+---------------------+------+--------+&#xA;| host      | ts                  | cpu  | memory |&#xA;+-----------+---------------------+------+--------+&#xA;| 127.0.0.1 | 2022-11-03 03:39:58 |  0.5 |    0.2 |&#xA;| 127.0.0.2 | 2022-11-03 03:39:58 |  0.2 |    0.3 |&#xA;+-----------+---------------------+------+--------+&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;interval-literals&#34;&gt;Interval Literals&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#interval-literals&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL’s interval syntax resembles&amp;nbsp;&lt;code&gt;1d&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;12m&lt;/code&gt;, while SQL has standard syntax for time intervals:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;INTERVAL &#39;1 DAY&#39;&#xA;INTERVAL &#39;1 YEAR 3 HOURS 20 MINUTES&#39;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;data-columns-and-tag-columns&#34;&gt;Data Columns and Tag Columns&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#data-columns-and-tag-columns&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL distinguishes between data columns and tag columns at the model level; queries that only SELECT tag columns will not return data. InfluxQL also supports the&amp;nbsp;&lt;code&gt;::field&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;::tag&lt;/code&gt;&amp;nbsp;suffixes to specify data columns or tag columns, allowing for columns with the same name.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL standards do not differentiate between data columns and tag columns, treating them all as regular columns. However, specific implementations may map these concepts differently. For example, GreptimeDB’s&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/concepts/data-model&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;data model&lt;/a&gt;&amp;nbsp;distinguishes between timestamp columns, tag columns, and data columns and has corresponding mapping rules.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image1.png&#34; alt=&#34;The Data Structure of GreptimeDB&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;The Data Structure of GreptimeDB&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;function-names&#34;&gt;Function Names&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#function-names&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Some function names differ between InfluxQL and SQL. For instance, the&amp;nbsp;&lt;code&gt;MEAN&lt;/code&gt;&amp;nbsp;function in InfluxQL corresponds to the&amp;nbsp;&lt;code&gt;AVG&lt;/code&gt;&amp;nbsp;function in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, many other functions, such as&amp;nbsp;&lt;code&gt;COUNT&lt;/code&gt;,&amp;nbsp;&lt;code&gt;SUM&lt;/code&gt;, and&amp;nbsp;&lt;code&gt;MIN,&lt;/code&gt;&amp;nbsp;remain the same in both languages.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;identifiers&#34;&gt;Identifiers&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#identifiers&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In InfluxQL, identifiers are always double-quoted, while SQL supports unquoted identifiers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;It is worth noting that SQL identifiers are case-insensitive by default. If case sensitivity is needed, the identifiers should be enclosed in the appropriate quotes. In GreptimeDB, double quotes are used by default. However, when connecting via MySQL or PostgreSQL clients, the corresponding dialect’s syntax is respected.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Examples of identifier usage differences between InfluxQL and SQL are as follows:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql/image2.png&#34; alt=&#34;The Usage Differences between InfluxQL and SQL&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;join&#34;&gt;JOIN&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#join&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL does not support JOIN queries, while one of the fundamental capabilities of SQL databases is support for JOIN queries:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches&lt;/em&gt;&#xA;SELECT a.* FROM system_metrics a JOIN idc_info b ON a.idc = b.idc_id;&#xA;&#xA;&lt;em&gt;-- Select all rows from the idc_info table and system_metrics table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT a.* FROM idc_info a LEFT JOIN system_metrics b ON a.idc_id = b.idc;&#xA;&#xA;&lt;em&gt;-- Select all rows from the system_metrics table and idc_info table where the idc_id matches, and include null values for idc_info without any matching system_metrics&lt;/em&gt;&#xA;SELECT b.* FROM system_metrics a RIGHT JOIN idc_info b ON a.idc = b.idc_id;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;These are examples of&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/reference/sql/join&#34;&gt;JOIN queries in GreptimeDB&lt;/a&gt;, which supports:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;INNER JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;LEFT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;RIGHT JOIN&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;FULL OUTER JOIN&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;queries-over-time-windows&#34;&gt;Queries over Time Windows&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#queries-over-time-windows&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL’s&amp;nbsp;&lt;code&gt;GROUP BY&lt;/code&gt;&amp;nbsp;statement supports passing a time window to aggregate data within a specific length of time windows.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;SQL does not have such specific query capabilities; the closest equivalent is the&amp;nbsp;&lt;code&gt;OVER ... PARTITION BY&lt;/code&gt;&amp;nbsp;syntax, which can be quite complex to understand.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB implements its own&amp;nbsp;&lt;a href=&#34;https://docs.greptime.com/user-guide/query-data/sql#aggregate-data-by-time-window&#34;&gt;RANGE QUERY extension syntax&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;wp-block-code&#34;&gt;&lt;code lang=&#34;sql&#34; class=&#34;language-sql&#34;&gt;SELECT &#xA;    ts, &#xA;    host, &#xA;    avg(cpu) RANGE &#39;10s&#39; FILL LINEAR&#xA;FROM monitor&#xA;ALIGN &#39;5s&#39; TO &#39;2023-12-01T00:00:00&#39; BY (host) ORDER BY ts ASC;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 class=&#34;wp-block-heading&#34; id=&#34;continuous-aggregation&#34;&gt;Continuous Aggregation&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#continuous-aggregation&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;InfluxQL supports&amp;nbsp;&lt;a href=&#34;https://docs.influxdata.com/influxdb/v1/query_language/continuous_queries/&#34;&gt;continuous aggregation&lt;/a&gt;, which corresponds to the standard concept of materialized views in SQL.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;However, the implementation of materialized views in most SQL databases is still fragile and remains an area for further exploration.&amp;nbsp;&lt;a href=&#34;https://greptime.com/blogs/2024-06-04-flow-engine&#34;&gt;GreptimeDB supports continuous aggregation&lt;/a&gt;&amp;nbsp;to meet these needs based on its flow engine.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34; id=&#34;conclusion&#34;&gt;Conclusion&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#conclusion&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;This article introduced the differences between InfluxQL, Flux, and SQL as query languages. While InfluxQL and Flux are used by InfluxDB and specifically created for handling time series data, SQL is a widely used query language in relational databases. Its robust theoretical foundation and rich ecosystem allow data analysts to quickly get started and use effective tools for time series data analysis.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;GreptimeDB natively supports SQL queries. Visit our&amp;nbsp;&lt;a href=&#34;https://greptime.com/&#34;&gt;homepage&lt;/a&gt;&amp;nbsp;for more information or&amp;nbsp;&lt;a href=&#34;https://console.greptime.cloud/signup&#34;&gt;create a free cloud service&lt;/a&gt;&amp;nbsp;instance to start your trial today.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34; id=&#34;about-greptime&#34;&gt;About Greptime&lt;a href=&#34;https://www.greptime.com/blogs/2024-06-20-influx-sql#about-greptime&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We help industries that generate large amounts of time-series data, such as Connected Vehicles (CV), IoT, and Observability, to efficiently uncover the hidden value of data in real-time.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Visit the&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;latest version&lt;/a&gt;&amp;nbsp;from any device to get started and get the most out of your data.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GreptimeDB&lt;/a&gt;, written in Rust, is a distributed, open-source, time-series database designed for scalability, efficiency, and powerful analytics.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://greptime.com/product/carcloud&#34;&gt;Edge-Cloud Integrated TSDB&lt;/a&gt;&amp;nbsp;is designed for the unique demands of edge storage and compute in IoT. It tackles the exponential growth of edge data by integrating a multimodal edge-side database with cloud-based GreptimeDB Enterprise. This combination reduces traffic, computing, and storage costs while enhancing data timeliness and business insights.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.greptime.com/product/cloud&#34;&gt;GreptimeCloud&lt;/a&gt;&amp;nbsp;is a fully-managed cloud database-as-a-service (DBaaS) solution built on GreptimeDB. It efficiently supports applications in fields such as observability, IoT, and finance.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Star us on&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GitHub&lt;/a&gt;&amp;nbsp;or join GreptimeDB Community on&amp;nbsp;&lt;a href=&#34;https://www.greptime.com/slack&#34;&gt;Slack&lt;/a&gt;&amp;nbsp;to get connected. Also, you can go to our&amp;nbsp;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb/contribute&#34;&gt;contribution page&lt;/a&gt;&amp;nbsp;to find some interesting issues to start with.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Tue, 09 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Kubestronaut in Orbit: Peter Barczi】在轨道上的 Kubetronaut：Peter Barczi</title>
      <link>https://www.cncf.io/blog/2024/07/05/kubestronaut-in-orbit-peter-barczi/</link>
      <description>【&lt;h2 class=&#34;wp-block-heading&#34;&gt;Get to know Peter Barczi&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1650&#34; height=&#34;866&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4.jpg&#34; alt=&#34;Kubestronaut in Orbit - Peter Barczi&#34; class=&#34;wp-image-113542&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4.jpg 1650w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-300x157.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1024x537.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-768x403.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-194x102.jpg 194w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-388x204.jpg 388w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-776x408.jpg 776w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1552x816.jpg 1552w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-900x472.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-381x200.jpg 381w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-762x400.jpg 762w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-590x310.jpg 590w, https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1180x620.jpg 1180w&#34; sizes=&#34;(max-width: 1650px) 100vw, 1650px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Peter, one of our first Kubestronauts, has been working with Kubernetes only since 2021 but has still managed to pass all of CNCF’s Kubernetes certifications. He’s currently the Sr. DevOps Engineer / TechLead at a company building a cloud offering with focus on confidential computing. In his role, he also manages physical servers, clouds, operating Linux OSs and Kubernetes clusters at scale and is an Internal Linux Trainer.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you’d like to be a Kubestronaut, get more details on the&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt; CNCF Kubestronaut &lt;/a&gt;page.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;When did you get started with kubernetes–what was your first project?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I started with K8s as self-learner in 2021.&amp;nbsp;My first project (not surprisingly) was the NGINX web server running on Kubernetes.&amp;nbsp; Later on, I used K8s during the migration of our infra services into Kubernetes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Nowadays, within my current project, almost everything is a Kubernetes object, even the network switches, so I use it daily.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What are the primary CNCF projects you work on or use today?&amp;nbsp; What projects have you enjoyed the most in your career?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The primary CNCF projects I work with the most are:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes&#34;&gt;Kubernetes&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/fluxcd/&#34;&gt;FluxCD&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/prometheus&#34;&gt;Prometheus&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://github.com/grafana&#34;&gt;Grafana&lt;/a&gt; stack&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cilium&#34;&gt;Cilium&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/rook&#34;&gt;Rook&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Discovering a GitOps approach was a real game-changer in my work life.&amp;nbsp; And thanks to tools like &lt;a href=&#34;https://github.com/argoproj/argo-cd&#34;&gt;ArgoCD &lt;/a&gt;and &lt;a href=&#34;https://github.com/fluxcd/&#34;&gt;FluxCD,&lt;/a&gt; I realised how “easy” it can be to deploy and track apps using git.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Furthermore, thanks to &lt;a href=&#34;https://rook.io/&#34;&gt;Rook-ceph storage provide&lt;/a&gt;r I’m now able to spin up and configure storage clusters in a matter of a few minutes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What motivated you to get all the kubernetes certs?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Each certification has its own story.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;KCNA was my first certification. I took it right before a job interview for extra confidence in my Kubernetes skills.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;CKA &amp;amp; CKAD&amp;nbsp; were two certs I needed to help with a project I was working on.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;CKS was a cert I got as a personal goal.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KCSA was my last one. I got this one right after Kubecon when the Kubestronaut program was announced and realized I only needed one more.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How have the certs helped you in your career?&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The certifications allow me to practice and learn about technology and tools I do not use on a daily basis. Preparation for the certification is a way to get the education needed to be ready for future projects.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What are some other books/sites/courses you recommend for people who want to work with k8s?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I found these online courses were a good starting point for my Kubernetes learning journey:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– CKA and CKAD courses from kodekloud.com&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– CKS online course from Kim Wuestkamp on YouTube&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– CKA/CKAD/CKS practise exams from aclodguru/pluralsight&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What do you do in your free time?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I spend all my free time with my family, my 15-month-old daughter is my single point of interest currently. I also spend time in my cottage house, so I balance my work time there.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;As you know Kubernetes is turning 10 this year, what are you most excited about for Kubernetes in the next 10 years?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;My magic ball doesn’t have answers so far into the future:)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What would you tell someone who is just starting their K8s certification journey? Any tips or tricks?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;There are no tips and tricks better than to practise, practise, practise.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Today the Cloud native ecosystem is way more than Kubernetes. Do you plan to get other cloud native certifications from the CNCF?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Absolutely, the next challenge for me is to dive into the Prometheus stack and become familiar enough with it to become a &lt;a href=&#34;https://www.cncf.io/training/certification/pca/&#34;&gt;Prometheus Certified Associate (PCA)&lt;/a&gt;&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How did you get involved with cloud native and Kubernetes in general?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;I have a background in Linux administration, so from there it was a logical and natural to progress to Kubernetes and cloud native.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If you’d like to be a Kubestronaut,&amp;nbsp;get more details on the&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt; CNCF Kubestronaut &lt;/a&gt;page.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;h2 class=&#34;wp-block-heading&#34;&gt;了解 Peter Barczi&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“1650”高度=“866”src=“https://www.cncf.io/ wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4.jpg&#34; alt=&#34;轨道上的 Kubestronaut - Peter Barczi&#34; class=&#34;wp-image-113542&#34; srcset=&#34;https://www.cncf. io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4.jpg 1650w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4 -300x157.jpg 300w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1024x537.jpg 1024w，https://www.cncf.io/wp -content/uploads/2024/06/Kubestronaut-in-Orbit-4-768x403.jpg 768w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4- 194x102.jpg 194w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-388x204.jpg 388w，https://www.cncf.io/wp-内容/uploads/2024/06/Kubestronaut-in-Orbit-4-776x408.jpg 776w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-1552x816 .jpg 1552w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-900x472.jpg 900w，https://www.cncf.io/wp-content /uploads/2024/06/Kubestronaut-in-Orbit-4-381x200.jpg 381w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-762x400。 jpg 762w，https://www.cncf.io/wp-content/uploads/2024/06/Kubestronaut-in-Orbit-4-590x310.jpg 590w，https://www.cncf.io/wp-content/ uploads/2024/06/Kubestronaut-in-Orbit-4-1180x620.jpg 1180w&#34;sizes=&#34;(max-width: 1650px) 100vw, 1650px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Peter 是我们首批 Kubetronaut 之一，他从 2021 年才开始使用 Kubernetes，但仍然设法通过了 CNCF 的所有 Kubernetes 认证。他目前是一家公司的高级 DevOps 工程师/技术主管，该公司致力于构建专注于机密计算的云产品。在此职位上，他还管理物理服务器、云、大规模操作 Linux 操作系统和 Kubernetes 集群，并且是一名内部 Linux 培训师。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果您想成为 Kubestronaut，请在&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt;CNCF Kubestronaut&lt;/a&gt;页面获取更多详细信息。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您什么时候开始使用 kubernetes – 您的第一个项目是什么？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我从 2021 年开始自学 K8s。我的第一个项目（毫不奇怪）是在 Kubernetes 上运行的 NGINX Web 服务器。  后来，我在将基础设施服务迁移到 Kubernetes 的过程中使用了 K8s。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;现在，在我当前的项目中，几乎所有东西都是 Kubernetes 对象，甚至是网络交换机，所以我每天都使用它。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您现在从事或使用的主要 CNCF 项目是什么？  在您的职业生涯中，您最喜欢哪些项目？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我参与最多的主要 CNCF 项目是：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes&#34;&gt;Kubernetes&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/fluxcd/&#34;&gt;FluxCD&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/prometheus&#34;&gt;Prometheus&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/grafana&#34;&gt;Grafana&lt;/a&gt; 堆栈&lt;/li &gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cilium&#34;&gt;Cilium&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/rook&#34;&gt;Rook&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;发现 GitOps 方法真正改变了我的工作生活。  感谢 &lt;a href=&#34;https://github.com/argoproj/argo-cd&#34;&gt;ArgoCD &lt;/a&gt; 和 &lt;a href=&#34;https://github.com/fluxcd/&#34;&gt;FluxCD 等工具， &lt;/a&gt; 我意识到使用 git 部署和跟踪应用程序是多么“容易”。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;此外，感谢 &lt;a href=&#34;https://rook.io/&#34;&gt;Rook-ceph 存储提供商&lt;/a&gt;，我现在只需几分钟即可启动和配置存储集群分钟。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;是什么促使您获得所有 Kubernetes 证书？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;每个认证都有自己的故事。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;KCNA 是我的第一个认证。我在工作面试前拍摄了它，以便对我的 Kubernetes 技能更有信心。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;CKA 和 CKAD 是我需要帮助完成我正在开展的项目的两个证书。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;CKS 是我作为个人目标而获得的证书。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KCSA 是我的最后一个。 Kubecon 结束后，当 Kubetronaut 计划宣布时，我得到了这个，并意识到我只需要再一个。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;这些证书对您的职业生涯有何帮助？ &lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;这些认证使我能够练习和学习我日常不使用的技术和工具。准备认证是获得为未来项目做好准备所需教育的一种方式。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您为想要使用 k8s 的人推荐了哪些其他书籍/网站/课程？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我发现这些在线课程是我的 Kubernetes 学习之旅的一个很好的起点：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– 来自 kodekloud.com 的 CKA 和 CKAD 课程&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– Kim Wuestkamp 在 YouTube 上提供的 CKS 在线课程&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;– 来自 aclodguru/pluralsight 的 CKA/CKAD/CKS 模拟考试&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;你空闲时间做什么？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我所有的空闲时间都和家人在一起，我 15 个月大的女儿目前是我唯一的兴趣点。我也花时间在我的小屋里，所以我在那里平衡我的工作时间。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;正如您所知，Kubernetes 今年已满 10 岁了，您对 Kubernetes 未来 10 年最兴奋的是什么？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我的魔法球到目前为止还没有答案:)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您会对刚刚开始 K8s 认证之旅的人说些什么？有什么提示或技巧吗？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;没有比练习、练习、再练习更好的提示和技巧了。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;如今，云原生生态系统的意义远不止 Kubernetes。您是否计划获得 CNCF 的其他云原生认证？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;当然，我的下一个挑战是深入研究 Prometheus 堆栈并足够熟悉它，成为一名 &lt;a href=&#34;https://www.cncf.io/training/certification/pca/&#34;&gt;Prometheus认证助理 (PCA)&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;您是如何涉足云原生和 Kubernetes 的？&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我有 Linux 管理背景，因此从那时起，发展到 Kubernetes 和云原生是合乎逻辑且自然的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果您想成为 Kubestronaut，请在&lt;a href=&#34;https://www.cncf.io/training/kubestronaut/&#34;&gt;CNCF Kubestronaut&lt;/a&gt;页面获取更多详细信息。 &lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Thu, 04 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【How WebAssembly components extend the frontiers of Kubernetes to multi-cloud, edge, and beyond】WebAssembly 组件如何将 Kubernetes 的前沿扩展到多云、边缘及其他领域</title>
      <link>https://www.cncf.io/blog/2024/07/01/how-webassembly-components-extend-the-frontiers-of-kubernetes-to-multi-cloud-edge-and-beyond/</link>
      <description>【&lt;p&gt;&lt;em&gt;Ambassador post by Taylor Thomas, CNCF Ambassador and Director of Engineering at Cosmonic&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly (Wasm) components are here and already unlocking new computing patterns. Meanwhile, CNCF’s &lt;a href=&#34;https://wasmcloud.com/&#34;&gt;&lt;strong&gt;wasmCloud&lt;/strong&gt;&lt;/a&gt; offers Wasm-native orchestration for distributed components—in essence, a Kubernetes for WebAssembly.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;That’s great for using components in large-scale production environments, but there’s one question: &lt;em&gt;Why would you want to replace your entire infrastructure?&lt;/em&gt; If you’re invested in Kubernetes, you have robust infrastructure management solved, and you’d prefer not to make changes at the node level, much less switch up your orchestrator.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Fortunately, wasmCloud integrates seamlessly with your existing cloud native setup in a quick, non-invasive, Kubernetes-native way. There’s no fiddling with custom node pools or configuration: Kubernetes’ operator pattern gives us a way to treat WebAssembly as more than a runtime and take full advantage of components’ capabilities. With the flexibility of components, we can extend the frontiers of Kubernetes in traditional challenge areas like multi-region, multi-cluster, multi-cloud, and edge environments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Kubernetes, meet components&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;WebAssembly components&lt;/strong&gt; are Wasm binaries that snap together like building blocks, regardless of the language in which they were first written. One service might be written in Go, another in Rust, and another in JavaScript, but they can all communicate with one another over shared APIs. Like any Wasm binary, they can run on any operating system and any architecture.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;2358&#34; height=&#34;992&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/compile.jpg&#34; alt=&#34;Multi-language diagram&#34; class=&#34;wp-image-113602&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/compile.jpg 2358w, https://www.cncf.io/wp-content/uploads/2024/07/compile-300x126.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/compile-1024x431.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/compile-768x323.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/compile-900x379.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/compile-1800x757.jpg 1800w, https://www.cncf.io/wp-content/uploads/2024/07/compile-475x200.jpg 475w, https://www.cncf.io/wp-content/uploads/2024/07/compile-951x400.jpg 951w&#34; sizes=&#34;(max-width: 2358px) 100vw, 2358px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Components are portable, small (ranging from kilobytes to single-digit megabytes), and packaged as &lt;a href=&#34;https://tag-runtime.cncf.io/wgs/wasm/deliverables/wasm-oci-artifact/&#34;&gt;OCI artifacts&lt;/a&gt;, so a certain analogy with containers is inevitable. But they’re also something brand new: reusable building blocks written against well-known APIs and interfaces with real multi-language interoperability.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In a world with components, software development becomes much more flexible and platform engineering becomes much less of an exercise in bug-whacking. Developers don’t have to worry about rewriting something for their language of choice and platform engineers have the flexibility to compose their platform in the way that best suits their requirements.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The pluggable nature of components was designed with a cloud native world in mind. In cases where you want to minimize communication over network boundaries, you can &lt;a href=&#34;https://wasmcloud.com/docs/concepts/linking-components/linking-at-build#overview&#34;&gt;&lt;strong&gt;compose&lt;/strong&gt; all those services together into a single binary&lt;/a&gt;. In cases where you want to scale (or maintain) those services independently, or services need to run close to distributed data sources, you can run your various components distributedly with wasmCloud, which serves as an orchestrator for WebAssembly components. (Of course, you can run build-time composed components on wasmCloud, too.)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;If components are comparable to containers, then wasmCloud is analogous to Kubernetes, providing Wasm-native orchestration so teams can utilize components fully in distributed environments and at scale. The architecture bears some high-level similarities:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;2400&#34; height=&#34;1264&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2.jpg&#34; alt=&#34;K8s &amp;amp; wasmCloud comparison diagram&#34; class=&#34;wp-image-113603&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2.jpg 2400w, https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-300x158.jpg 300w, https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-1024x539.jpg 1024w, https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-768x404.jpg 768w, https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-194x102.jpg 194w, https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-388x204.jpg 388w, https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-776x408.jpg 776w, https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-1552x816.jpg 1552w, https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-900x474.jpg 900w, https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-1800x948.jpg 1800w, https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-380x200.jpg 380w, https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-759x400.jpg 759w, https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-590x310.jpg 590w, https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-1180x620.jpg 1180w&#34; sizes=&#34;(max-width: 2400px) 100vw, 2400px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The &lt;strong&gt;wasmCloud Application Deployment Manager (wadm)&lt;/strong&gt; is the system’s orchestration layer and analogous to the Kubernetes scheduler, controller manager, and API server (for a subset of APIs) all in one.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;The &lt;strong&gt;control interface&lt;/strong&gt; for wasmCloud routes messages across the connectivity layer (called the “lattice”), much as the API server acts as Kubernetes’ air-traffic control system.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;wasmCloud’s workloads are components rather than containers in pods, but they are similarly packaged as &lt;strong&gt;OCI artifacts&lt;/strong&gt; (and can therefore be pulled down from the same registries).&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;In wasmCloud, the WebAssembly runtime &lt;strong&gt;Wasmtime&lt;/strong&gt; runs components in a given host environment, whereas the Kubelet mediates pod execution on a Kubernetes node.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;As with components and containers, there are some important differences: for example, wasmCloud uses the CNCF’s &lt;a href=&#34;https://nats.io/&#34;&gt;NATS&lt;/a&gt; project as a connectivity layer for a “distributed-first” approach, &lt;em&gt;and&lt;/em&gt; as a distributed key-value store for state. These characteristics make wasmCloud extremely well-suited to distributing workloads easily and securely across clouds, clusters, and regions‐and Kubernetes’ extensibility means that it’s simple to bring those features to existing cloud native infrastructure using CRDs and the operator pattern.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Multi-cloud, edge, and beyond&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;By integrating with Kubernetes via the open source &lt;a href=&#34;https://github.com/wasmCloud/wasmcloud-operator&#34;&gt;&lt;strong&gt;&lt;code&gt;wasmcloud-operator&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;, wasmCloud complements and extends Kubernetes in areas that have historically posed challenges like multi-cluster, multi-cloud, and edge. The size and efficiency of both WebAssembly binaries and wasmCloud makes it much more practical to run distributed apps in resource-constrained environments. Meanwhile, the wasmCloud lattice provides a means to not only connect Kubernetes clusters across various clouds or regions, but move mission-critical workloads safely and at scale.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;2048&#34; height=&#34;855&#34; src=&#34;https://www.cncf.io/wp-content/uploads/2024/07/regions.png&#34; alt=&#34;Deployment diagram&#34; class=&#34;wp-image-113605&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/07/regions.png 2048w, https://www.cncf.io/wp-content/uploads/2024/07/regions-300x125.png 300w, https://www.cncf.io/wp-content/uploads/2024/07/regions-1024x428.png 1024w, https://www.cncf.io/wp-content/uploads/2024/07/regions-768x321.png 768w, https://www.cncf.io/wp-content/uploads/2024/07/regions-900x376.png 900w, https://www.cncf.io/wp-content/uploads/2024/07/regions-1800x751.png 1800w, https://www.cncf.io/wp-content/uploads/2024/07/regions-479x200.png 479w, https://www.cncf.io/wp-content/uploads/2024/07/regions-958x400.png 958w&#34; sizes=&#34;(max-width: 2048px) 100vw, 2048px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;With these tools, it becomes fairly straightforward to architect a “supercluster” composed of smaller clusters, with workloads scheduled to one cluster or another based on region or other criteria. Architectural patterns that were previously highly challenging are first-class use cases with wasmCloud and Kubernetes working together, with no changes to your underlying infrastructure.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;With &lt;code&gt;wasmcloud-operator&lt;/code&gt;, you can run component workloads &lt;em&gt;exactly the same way you would run anything else in Kubernetes&lt;/em&gt;. Moreover, you can deploy wasmCloud application manifests directly with &lt;code&gt;kubectl apply&lt;/code&gt; and check the status of wasmCloud apps with &lt;code&gt;kubectl get applications&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;The operator is already in use on factory floors. With components, manufacturing analytics company &lt;a href=&#34;https://www.youtube.com/watch?v=fQdkNGZqYZA&amp;amp;list=PLj6h78yzYM2MQteKoXxICTWiUdZYEw6RI&amp;amp;index=6&#34;&gt;MachineMetrics&lt;/a&gt; can process high-frequency data directly on edge devices and stream to the cloud in real-time—running in environments that are highly challenging for containers, and integrating with Kubernetes all the while. MachineMetrics’ Data Platform Team Engineer Jochen Rau notes: “The wasmCloud-operator makes this really simple to manage with our existing tools. We deploy our compute workloads on the lattice on wasmCloud. We’re crossing the boundaries between edge and cloud.”&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Components are ushering in a new wave of computing in which new patterns of interoperability and reusability are possible. Kubernetes’ extensibility means that teams who want to start leveraging components right now can do so without changes to their infrastructure—and with integrations for existing cloud-native tooling in areas like policy, observability, and packaging.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;You can try out wasmCloud on Kubernetes right now—check out the &lt;a href=&#34;https://wasmcloud.com/docs/deployment/k8s/&#34;&gt;walkthrough in the wasmCloud documentation&lt;/a&gt;. If you’re just getting started with WebAssembly components, the &lt;a href=&#34;https://wasmcloud.com/docs/tour/hello-world&#34;&gt;wasmCloud quickstart&lt;/a&gt; is a great place to get started. If you’d like to learn more about running components on Kubernetes, or you have questions about the ecosystem, &lt;a href=&#34;https://slack.wasmcloud.com/&#34;&gt;join the wasmCloud Slack&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;由 CNCF 大使兼 Cosmonic 工程总监 Taylor Thomas 担任大使职务&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;WebAssembly (Wasm) 组件已经出现，并且已经解锁了新的计算模式。与此同时，CNCF 的 &lt;a href=&#34;https://wasmcloud.com/&#34;&gt;&lt;strong&gt;wasmCloud&lt;/strong&gt;&lt;/a&gt; 为分布式组件提供 Wasm 原生编排，本质上是用于 WebAssembly 的 Kubernetes。&lt;/p &gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;这对于在大规模生产环境中使用组件来说非常有用，但有一个问题：&lt;em&gt;您为什么要更换整个基础设施？&lt;/em&gt;如果您投资了 Kubernetes，那么您就拥有强大的基础设施管理已解决，并且您不希望在节点级别进行更改，更不用说切换您的编排器了。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;幸运的是，wasmCloud 以快速、非侵入性的 Kubernetes 原生方式与您现有的云原生设置无缝集成。无需摆弄自定义节点池或配置：Kubernetes 的运算符模式为我们提供了一种将 WebAssembly 视为不仅仅是运行时并充分利用组件功能的方法。凭借组件的灵活性，我们可以在多区域、多集群、多云和边缘环境等传统挑战领域扩展 Kubernetes 的前沿。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;Kubernetes，满足组件&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;WebAssembly 组件&lt;/strong&gt;是 Wasm 二进制文件，它们像构建块一样组合在一起，无论它们最初是用什么语言编写的。一项服务可能是用 Go 编写的，另一项是用 Rust 编写的，另一项是用 JavaScript 编写的，但它们都可以通过共享 API 相互通信。与任何 Wasm 二进制文件一样，它们可以在任何操作系统和任何架构上运行。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“2358”高度=“992”src=“https://www.cncf.io/ wp-content/uploads/2024/07/compile.jpg&#34; alt=&#34;多语言图&#34; class=&#34;wp-image-113602&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/ 2024/07/compile.jpg 2358w，https://www.cncf.io/wp-content/uploads/2024/07/compile-300x126.jpg 300w，https://www.cncf.io/wp-content/ uploads/2024/07/compile-1024x431.jpg 1024w，https://www.cncf.io/wp-content/uploads/2024/07/compile-768x323.jpg 768w，https://www.cncf.io/ wp-content/uploads/2024/07/compile-900x379.jpg 900w，https://www.cncf.io/wp-content/uploads/2024/07/compile-1800x757.jpg 1800w，https://www. cncf.io/wp-content/uploads/2024/07/compile-475x200.jpg 475w，https://www.cncf.io/wp-content/uploads/2024/07/compile-951x400.jpg 951w&#34; 尺寸= “（最大宽度：2358px）100vw，2358px”referrerpolicy =“no-referrer”&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;组件是便携式的、小型的（范围从千字节到个位数兆字节），并打包为 &lt;a href=&#34;https://tag-runtime.cncf.io/wgs/wasm/deliverables/wasm-oci-artifact /&#34;&gt;OCI 工件&lt;/a&gt;，因此与容器的某种类比是不可避免的。但它们也是全新的：根据众所周知的 API 和接口编写的可重用构建块，具有真正的多语言互操作性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在一个有组件的世界中，软件开发变得更加灵活，平台工程也不再是一种修复错误的练习。开发人员不必担心为他们选择的语言重写某些内容，平台工程师可以灵活地以最适合他们要求的方式构建平台。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;组件的可插拔性质是在设计时考虑到云原生世界的。如果您希望最大程度地减少网络边界上的通信，您可以&lt;a href=&#34;https://wasmcloud.com/docs/concepts/linking-components/linking-at-build#overview&#34;&gt;&lt;strong&gt;撰写&lt;/ strong&gt;将所有这些服务整合到一个二进制文件中&lt;/a&gt;。如果您想要独立扩展（或维护）这些服务，或者服务需要在分布式数据源附近运行，您可以使用 wasmCloud 分布式运行各种组件，它充当 WebAssembly 组件的协调器。 （当然，您也可以在 wasmCloud 上运行构建时组合组件。）&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;如果组件比作容器，那么 wasmCloud 就类似于 Kubernetes，提供 Wasm 原生编排，以便团队可以在分布式环境中大规模地充分利用组件。该架构具有一些高级相似之处：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“2400”高度=“1264”src=“https://www.cncf.io/ wp-content/uploads/2024/07/k8s-wasmCloud-v2.jpg&#34; alt=&#34;K8s &amp; wasmCloud 对比图&#34; class=&#34;wp-image-113603&#34; srcset=&#34;https://www.cncf.io/wp -content/uploads/2024/07/k8s-wasmCloud-v2.jpg 2400w，https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-300x158.jpg 300w，https ://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-1024x539.jpg 1024w，https://www.cncf.io/wp-content/uploads/2024/07/ k8s-wasmCloud-v2-768x404.jpg 768w，https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-194x102.jpg 194w，https://www.cncf。 io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-388x204.jpg 388w，https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-776x408 .jpg 776w，https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-1552x816.jpg 1552w，https://www.cncf.io/wp-content/uploads /2024/07/k8s-wasmCloud-v2-900x474.jpg 900w，https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-1800x948.jpg 1800w，https:// /www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-380x200.jpg 380w，https://www.cncf.io/wp-content/uploads/2024/07/k8s- wasmCloud-v2-759x400.jpg 759w，https://www.cncf.io/wp-content/uploads/2024/07/k8s-wasmCloud-v2-590x310.jpg 590w，https://www.cncf.io/ wp-content/uploads/2024/07/k8s-wasmCloud-v2-1180x620.jpg 1180w“尺寸=”（最大宽度：2400px）100vw，2400px“referrerpolicy=”no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;wasmCloud 应用程序部署管理器 (wadm)&lt;/strong&gt; 是系统的编排层，类似于 Kubernetes 调度程序、控制器管理器和 API 服务器（针对 API 的子集）。&lt;/li &gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;wasmCloud 的&lt;strong&gt;控制接口&lt;/strong&gt;在连接层（称为“网格”）上路由消息，就像 API 服务器充当 Kubernetes 的空中交通控制系统一样。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;wasmCloud 的工作负载是组件，而不是 Pod 中的容器，但它们类似地打包为 &lt;strong&gt;OCI 工件&lt;/strong&gt;（因此可以从相同的注册表中提取）。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;在 wasmCloud 中，WebAssembly 运行时 &lt;strong&gt;Wasmtime&lt;/strong&gt; 在给定主机环境中运行组件，而 Kubelet 则在 Kubernetes 节点上协调 Pod 执行。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;与组件和容器一样，存在一些重要的区别：例如，wasmCloud 使用 CNCF 的 &lt;a href=&#34;https://nats.io/&#34;&gt;NATS&lt;/a&gt; 项目作为“分布式优先”方法，&lt;em&gt;和&lt;/em&gt;作为状态的分布式键值存储。这些特性使得 wasmCloud 非常适合跨云、集群和区域轻松、安全地分配工作负载，而 Kubernetes 的可扩展性意味着可以使用 CRD 和运算符模式轻松地将这些功能引入现有的云原生基础设施。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;多云、边缘及其他&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;通过开源与 Kubernetes 集成&lt;a href=&#34;https://github.com/wasmCloud/wasmcloud-operator&#34;&gt;&lt;strong&gt;&lt;code&gt;wasmcloud-operator&lt;/code&gt;&lt;/strong&gt;&lt;/ a&gt;，wasmCloud 在多集群、多云和边缘等历史上提出挑战的领域补充和扩展了 Kubernetes。 WebAssembly 二进制文件和 wasmCloud 的大小和效率使得在资源受限的环境中运行分布式应用程序更加实用。同时，wasmCloud 网格提供了一种方法，不仅可以跨不同云或区域连接 Kubernetes 集群，还可以安全、大规模地移动关键任务工作负载。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class =“wp-block-image size-full”&gt;&lt;img加载=“lazy”解码=“异步”宽度=“2048”高度=“855”src=“https://www.cncf.io/ wp-content/uploads/2024/07/regions.png&#34; alt=&#34;部署图&#34; class=&#34;wp-image-113605&#34; srcset=&#34;https://www.cncf.io/wp-content/uploads/2024/ 07/regions.png 2048w，https://www.cncf.io/wp-content/uploads/2024/07/regions-300x125.png 300w，https://www.cncf.io/wp-content/uploads/ 2024/07/regions-1024x428.png 1024w，https://www.cncf.io/wp-content/uploads/2024/07/regions-768x321.png 768w，https://www.cncf.io/wp- content/uploads/2024/07/regions-900x376.png 900w，https://www.cncf.io/wp-content/uploads/2024/07/regions-1800x751.png 1800w，https://www.cncf。 io/wp-content/uploads/2024/07/regions-479x200.png 479w，https://www.cncf.io/wp-content/uploads/2024/07/regions-958x400.png 958w&#34; 尺寸 = &#34;(最大宽度：2048px) 100vw，2048px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;使用这些工具，构建一个由较小集群组成的“超级集群”变得相当简单，并根据区域或其他标准将工作负载安排到一个或另一个集群。以前极具挑战性的架构模式是 wasmCloud 和 Kubernetes 协同工作的一流用例，无需更改底层基础设施。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;使用 &lt;code&gt;wasmcloud-operator&lt;/code&gt;，您可以运行组件工作负载，&lt;em&gt;与在 Kubernetes 中运行其他任何东西的方式完全相同&lt;/em&gt;。此外，您可以直接使用 &lt;code&gt;kubectl apply&lt;/code&gt; 部署 wasmCloud 应用程序清单，并使用 &lt;code&gt;kubectl get applications&lt;/code&gt; 检查 wasmCloud 应用程序的状态。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;操作员已在工厂车间使用。借助组件，制造分析公司 &lt;a href=&#34;https://www.youtube.com/watch?v=fQdkNGZqYZA&amp;list=PLj6h78yzYM2MQteKoXxICTWiUdZYEw6RI&amp;index=6&#34;&gt;MachineMetrics&lt;/a&gt; 可以直接在边缘设备上处理高频数据并将其流式传输到实时云——在对容器来说极具挑战性的环境中运行，并始终与 Kubernetes 集成。 MachineMetrics 的数据平台团队工程师 Jochen Rau 指出：“wasmCloud-operator 使使用我们现有工具的管理变得非常简单。我们将计算工作负载部署在 wasmCloud 上的网格上。我们正在跨越边缘和云之间的界限。”&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 class=&#34;wp-block-heading&#34;&gt;结论&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;组件正在引领新的计算浪潮，其中新的互操作性和可重用性模式成为可能。 Kubernetes 的可扩展性意味着想要立即开始利用组件的团队可以在不更改基础设施的情况下实现这一目标，并且可以在策略、可观察性和打包等领域集成现有的云原生工具。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;您现在就可以在 Kubernetes 上尝试 wasmCloud - 查看&lt;a href=&#34;https://wasmcloud.com/docs/deployment/k8s/&#34;&gt;wasmCloud 文档中的演练&lt;/a&gt;。如果您刚刚开始使用 WebAssembly 组件，&lt;a href=&#34;https://wasmcloud.com/docs/tour/hello-world&#34;&gt;wasmCloud 快速入门&lt;/a&gt; 是一个很好的起点。如果您想了解有关在 Kubernetes 上运行组件的更多信息，或者对生态系统有疑问，请&lt;a href=&#34;https://slack.wasmcloud.com/&#34;&gt;加入 wasmCloud Slack&lt;/a&gt;。&lt;/a&gt; p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Sun, 30 Jun 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Embracing the future: our online store moves to a print-on-demand model】拥抱未来：我们的在线商店转向按需打印模式</title>
      <link>https://www.cncf.io/blog/2024/07/08/embracing-the-future-our-online-store-moves-to-a-print-on-demand-model/</link>
      <description>【&lt;p&gt;In today’s fast-paced digital world, businesses must evolve and adapt to meet their customers’ changing needs. We are excited to announce that our &lt;a href=&#34;https://store.cncf.io/&#34;&gt;online store&lt;/a&gt; is transitioning to a Print On Demand (POD) model. This significant change brings numerous benefits for us and, more importantly, our vibrant community.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;What is Print On Demand?&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Print On Demand is a fulfillment method in which items are printed as soon as an order is placed rather than stored in inventory. This model allows for greater flexibility and customization, ensuring each product is made specifically for the person who orders it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;The Benefits of Moving to Print-On-Demand&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Sustainability&lt;/strong&gt;:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reduced Waste&lt;/strong&gt;: Traditional inventory systems often result in overproduction and excess stock, leading to waste. With POD, we only produce what is needed, minimizing our environmental footprint.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Eco-Friendly Materials&lt;/strong&gt;: Many POD services use sustainable materials and eco-friendly printing processes, reducing environmental impact.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reduced Overhead Costs&lt;/strong&gt;: By eliminating the need for warehousing and managing excess inventory, we can focus on improving other aspects of our service, such as customer support and product quality.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;How This Benefits Our Community&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Our shift to a Print-on-Demand model is a testament to our commitment to our community. By reducing waste and promoting sustainability, we can allow more flexibility in our offerings and ensure we always have inventory.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;We are excited about this new chapter and look forward to providing you an even better shopping experience. Your support and feedback have been invaluable in making this transition possible. Together, we can positively impact both the environment and the creative community.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Thank you for being a part of our journey. Explore our new&lt;a href=&#34;https://store.cncf.io/collections/all-projects&#34;&gt; Print-On-Demand offerings today&lt;/a&gt; and discover the endless possibilities!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In the Future&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;In the coming months, we look forward to adding customization to our products and sourcing print-on-demand options with global partners to reduce shipping costs.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Stay tuned for more updates and exciting new products!&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;在当今快节奏的数字世界中，企业必须不断发展和适应，以满足客户不断变化的需求。我们很高兴地宣布，我们的&lt;a href=&#34;https://store.cncf.io/&#34;&gt;在线商店&lt;/a&gt;正在转变为按需打印 (POD) 模式。这一重大变化为我们带来了众多好处，更重要的是，为我们充满活力的社区带来了众多好处。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;什么是按需打印？&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;按需打印是一种在下订单后立即打印商品的履行方法，而不是将其存储在库存中。这种模式提供了更大的灵活性和定制性，确保每件产品都是专门为订购者制造的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;转向按需打印的好处&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;可持续性&lt;/strong&gt;：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;减少浪费&lt;/strong&gt;：传统库存系统通常会导致生产过剩和库存过剩，从而导致浪费。通过 POD，我们只生产所需的产品，最大限度地减少对环境的影响。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;环保材料&lt;/strong&gt;：许多 POD 服务都使用可持续材料和环保印刷工艺，减少对环境的影响。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;strong&gt;效率：&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;降低管理成本&lt;/strong&gt;：通过消除仓储和管理过剩库存的需要，我们可以专注于改善服务的其他方面，例如客户支持和产品质量。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 class=&#34;wp-block-heading&#34;&gt;&lt;strong&gt;这对我们的社区有何好处&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们向按需打印模式的转变证明了我们对社区的承诺。通过减少浪费和促进可持续发展，我们可以在我们的产品中提供更大的灵活性，并确保我们始终有库存。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;我们对这个新篇章感到兴奋，并期待为您提供更好的购物体验。您的支持和反馈对于实现这一转变至关重要。我们可以齐心协力，对环境和创意社区产生积极影响。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;感谢您参与我们的旅程。立即探索我们新的&lt;a href=&#34;https://store.cncf.io/collections/all-projects&#34;&gt;按需打印产品&lt;/a&gt;并发现无限的可能性！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;未来&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在接下来的几个月中，我们期待为我们的产品增加定制功能，并与全球合作伙伴一起采购按需打印选项，以降低运输成本。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;敬请关注更多更新和令人兴奋的新产品！&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Sun, 07 Jul 2024 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【As we reach mid-year 2024, a look at CNCF, Linux Foundation, and top 30 open source project velocity】到了 2024 年年中，我们来看看 CNCF、Linux 基金会和排名前 30 的开源项目速度</title>
      <link>https://www.cncf.io/blog/2024/07/11/as-we-reach-mid-year-2024-a-look-at-cncf-linux-foundation-and-top-30-open-source-project-velocity/</link>
      <description>【&lt;p&gt;&lt;em&gt;Staff post by Chris Aniszczyk&amp;nbsp;&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Date/Time:&lt;/strong&gt; July 11 at 8am&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;For the last several years we have tracked open source project velocity, which has enabled us to monitor the trends and technologies that resonate with developers and end users. For comparison, have a look at past timeframes from our &lt;a href=&#34;https://www.cncf.io/?s=Velocity&#34;&gt;blogs&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Here are the main takeaways I see from these charts:&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Kubernetes continues to mature with its consistent and largest contributor base&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;OpenTelemetry continues to grow its contributor base and remains the second highest velocity project; they recently added &lt;a href=&#34;https://opentelemetry.io/blog/2024/profiling/&#34;&gt;profiling&lt;/a&gt; as a new signal type&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Backstage grows solving an important pain point around developer experience&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;GitOps continues to be important in the cloud native ecosystem, where projects like Argo and Flux continue to cultivate large communities&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;Crossplane grew its contributor base by over 20% in the last year reflecting a desire for open source control planes in the era of open source relicensing issues&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KeyCloak joined CNCF last year as an incubating project and has a large community pushing open source identity and access management forward&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;In many cases, CNCF projects &lt;a href=&#34;https://www.cncf.io/case-studies/openai/&#34;&gt;underpin large scale AI infrastructure&lt;/a&gt; and we have Kubeflow appearing on the top 30 CNCF project list for the first time in 2023.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;CNCF projects – Last 12 months &lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;interactive map&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXeXEdq4E15JkoUMLwMRdFVFC0jeXT7El7VLVnVChbBOZk-v2I-03MLI5R8HMhgOrnlMS8LC-dQlKrKI4m8ybZHMzQ4Khm4_3dm5bboQhepmg4N9mf2IVWD4dOE4kxEv2T1rmQ0J33ATxZsT4Oi3XsxZh01x?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;cncf projects&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Linux Foundation Projects – Last 12 months &lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507&#34;&gt;interactive map&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXcBhcGzavp5kG5-l6g2MW-hNTvn2BIl2HIkb07TFm2dhUOJEymAhE18jAxE2_hgqitU9vJCRSDZ18EzppV9auq0eHq-qyGiYoT-JxQ5znQRMK_rq-mA_naXmtdQrNeuAeI8paEXZtUSxWxXLbsgRSBZi74?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;LF projects&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Top 30 open source projects – Last 12 months&lt;/strong&gt; (&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507&#34;&gt;interactive map&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdrzMUoz3xCWNweqwfxhzfI5koLh1Z68AbH4oWMjsQsH6FoBfZDx4bpsY2Cwyaxx7ZT9exDO1qtNy-oP00BuSczhhCZYCreLEDjdv0bQssLvSGEDkuVSaNCy2GQyjB0GUJIo4amk3C_St4m7tqM9zj8wAsv?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;Open source projects&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: We use bubble charts to show three axes of data: commits, authors, and comments/pull requests, and plot on a log-log chart to show the data across large scales.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;The bubble’s area is proportional to the number of authors&amp;nbsp;&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;The y-axis is the total number of pull requests and issues&amp;nbsp;&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;The x-axis is the number of commits&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;All of the the &lt;a href=&#34;https://github.com/cncf/velocity#current-reports&#34;&gt;current&lt;/a&gt; and &lt;a href=&#34;https://github.com/cncf/velocity#past-reports&#34;&gt;past&lt;/a&gt; reports are available on GitHub, as well as a list and charts on the Google sheets below:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;All CNCF projects for July 2023-July 2024&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507&#34;&gt;All Linux Foundation projects for July 2023-July 2024&lt;/a&gt;&amp;nbsp;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507&#34;&gt;Top 30 open source projects for July 2023-July 2024&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;All of the scripts used to generate this data are at &lt;a href=&#34;https://github.com/cncf/velocity&#34;&gt;https://github.com/cncf/velocity&lt;/a&gt; (under an Apache 2.0 &lt;a href=&#34;https://www.cncf.io/blog/2017/02/01/cncf-recommends-aslv2/&#34;&gt;license&lt;/a&gt;). If you see any errors, please open an issue there.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;Past blog posts about project velocity:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2024/01/17/a-look-back-at-cncf-linux-foundation-and-top-30-open-source-project-velocity-in-2023/&#34;&gt;A look back at CNCF, Linux Foundation, and top 30 open source project velocity in 2023&amp;nbsp;&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/10/27/october-2023-where-we-are-with-velocity-of-cncf-lf-and-top-30-open-source-projects/&#34;&gt;October 2023: where we are with velocity of CNCF, LF, and top 30 open source projects&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/01/11/a-look-at-the-2022-velocity-of-cncf-linux-foundation-and-top-30-open-source-projects/&#34;&gt;A look at the 2022 velocity of CNCF, Linux Foundation, and top 30 open source projects&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2022/08/23/mid-year-update-on-2022-cncf-linux-foundation-and-open-source-velocity/&#34;&gt;Mid-year update on 2022 CNCF, Linux Foundation, and open source velocity&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/12/15/end-of-year-update-on-cncf-and-open-source-velocity-in-2021/&#34;&gt;End of year update on CNCF and open source velocity in 2021&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/08/02/update-on-cncf-and-open-source-project-velocity-2020/&#34;&gt;Update on CNCF and Open Source Project Velocity 2020&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2017/06/05/30-highest-velocity-open-source-projects/&#34;&gt;The 30 highest velocity open source projects&lt;/a&gt;&amp;nbsp;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;】&lt;p&gt;&lt;em&gt;Chris Aniszczyk 的员工帖子&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;日期/时间：&lt;/strong&gt;7 月 11 日上午 8 点&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;在过去的几年里，我们一直在跟踪开源项目的速度，这使我们能够监控与开发人员和最终用户产生共鸣的趋势和技术。为了进行比较，请查看我们的&lt;a href=&#34;https://www.cncf.io/?s=Velocity&#34;&gt;博客&lt;/a&gt;中过去的时间范围。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;以下是我从这些图表中看到的主要结论：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Kubernetes 凭借其一致且最大的贡献者群体不断走向成熟&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;OpenTelemetry 的贡献者基础不断扩大，并且仍然是第二高速度​​项目；他们最近添加了&lt;a href=&#34;https://opentelemetry.io/blog/2024/profiling/&#34;&gt;分析&lt;/a&gt;作为新的信号类型&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;后台不断发展，解决了开发者体验方面的一个重要痛点&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;GitOps 在云原生生态系统中仍然发挥着重要作用，Argo 和 Flux 等项目继续培育大型社区&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;去年，Crossplane 的贡献者数量增长了 20% 以上，反映出在开源重新许可问题时代对开源控制平面的渴望&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;KeyCloak 去年作为孵化项目加入 CNCF，拥有推动开源身份和访问管理向前发展的大型社区&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;在许多情况下，CNCF 项目&lt;a href=&#34;https://www.cncf.io/case-studies/openai/&#34;&gt;支撑大规模人工智能基础设施&lt;/a&gt;，并且 Kubeflow 出现在前 30 名中2023年首次CNCF项目名单。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;CNCF 项目 – 过去 12 个月&lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;交互式地图&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXeXEdq4E15JkoUMLwMRdFVFC0jeXT7El7VLVnVChbBOZk-v2I-03MLI5R8HMhgOrnlMS8LC-dQlKrKI4m8ybZHMzQ4Khm4_ 3dm5bboQhepmg4N9mf2IVWD4dOE4kxEv2T1rmQ0J33ATxZsT4Oi3XsxZh01x?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;cncf 项目&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Linux 基金会项目 – 过去 12 个月&lt;/strong&gt;(&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507 &#34;&gt;交互式地图&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXcBhcGzavp5kG5-l6g2MW-hNTvn2BIl2HIkb07TFm2dhUOJEymAhE18jAxE2_hgqitU9vJCRSDZ18EzppV9auq0eHq- qyGiYoT-JxQ5znQRMK_rq-mA_naXmtdQrNeuAeI8paEXZtUSxWxXLbsgRSBZi74?key= CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;LF 项目&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;前 30 个开源项目 - 过去 12 个月&lt;/strong&gt; (&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507 &#34;&gt;交互式地图&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdrzMUoz3xCWNweqwfxhzfI5koLh1Z68AbH4oWMjsQsH6FoBfZDx4bpsY2Cwyaxx7ZT9exDO1qtNy-oP00BuSczhhCZY CreLEDjdv0bQssLvSGEDkuVSaNCy2GQyjB0GUJIo4amk3C_St4m7tqM9zj8wAsv?key=CJ6na1qTM-2MQJWw3h925g&#34; alt=&#34;打开源项目&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;注意&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;：我们使用气泡图来显示数据的三个轴：提交、作者和评论/拉取请求，并在对数日志上绘制图表显示大范围内的数据。 &lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;气泡面积与作者数量成正比&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;y 轴是拉取请求和问题的总数&lt;/em&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;em&gt;x 轴是提交次数&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;所有&lt;a href=&#34;https://github.com/cncf/velocity#current-reports&#34;&gt;当前&lt;/a&gt;和&lt;a href=&#34;https://github.com/cncf/ velocity#past-reports&#34;&gt;过去的&lt;/a&gt;报告可在 GitHub 上找到，下面的 Google 工作表上还提供列表和图表：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1FyVjdO9kMnz4hmEkyYLpl101cBDg_ZGHMkR5j_ce5JU/edit?gid=976519966#gid=976519966&#34;&gt;2023 年 7 月至 2024 年 7 月的所有 CNCF 项目&lt;/a&gt;&lt;/李&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1Aiyb2qoQ-vgTXkP3F5OdSB8nrVI43Al0EJfkkZjRqqY/edit?gid=134798507#gid=134798507&#34;&gt;2023 年 7 月至 2024 年 7 月的所有 Linux 基金会项目&lt;/a &gt; &lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1N4DCm7vtYNYY2iPglQWBZRWuoZeksXtV6DWkpcPL_kY/edit?gid=134798507#gid=134798507&#34;&gt;2023 年 7 月至 2024 年 7 月排名前 30 的开源项目&lt;/a&gt; &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;用于生成此数据的所有脚本均位于 &lt;a href=&#34;https://github.com/cncf/velocity&#34;&gt;https://github.com/cncf/velocity&lt;/a&gt;（在Apache 2.0 &lt;a href=&#34;https://www.cncf.io/blog/2017/02/01/cncf-recommends-aslv2/&#34;&gt;许可证&lt;/a&gt;）。如果您发现任何错误，请在那里提出问题。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;过去有关项目速度的博客文章：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2024/01/17/a-look-back-at-cncf-linux-foundation-and-top-30-open-source- project-velocity-in-2023/&#34;&gt;CNCF、Linux 基金会和 2023 年排名前 30 的开源项目速度回顾&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/10/27/october-2023-where-we-are-with-velocity-of-cncf-lf-and-top- 30-open-source-projects/&#34;&gt;2023 年 10 月：CNCF、LF 和前 30 个开源项目的发展速度&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2023/01/11/a-look-at-the-2022-velocity-of-cncf-linux-foundation-and-top- 30-open-source-projects/&#34;&gt;CNCF、Linux 基金会和前 30 个开源项目 2022 年速度一览&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2022/08/23/mid-year-update-on-2022-cncf-linux-foundation-and-open-source-velocity/ &#34;&gt;2022 年 CNCF、Linux 基金会和开源速度的年中更新&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/12/15/end-of-year-update-on-cncf-and-open-source-velocity-in-2021/ &#34;&gt;CNCF 年终更新和 2021 年开源速度&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/08/02/update-on-cncf-and-open-source-project-velocity-2020/&#34;&gt;CNCF 和2020 年开源项目速度&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2017/06/05/30-highest-velocity-open-source-projects/&#34;&gt;30 个速度最快的开源项目&lt;/a &gt; &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div style=&#34;height:80px&#34; aria-hidden=&#34;true&#34; class=&#34;wp-block-spacer is-style-80-120&#34;&gt;&#xA;&lt;/div&gt;</description>
      <pubDate>Wed, 10 Jul 2024 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>