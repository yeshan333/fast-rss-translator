<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DZone.com Feed</title>
    <link>https://feeds.dzone.com/home</link>
    <description>Recent posts on DZone.com</description>
    <item>
      <title>【Developing a Nationwide Real-Time Telemetry Analytics Platform Using Google Cloud Platform and Apache Airflow】使用Google Cloud Platform和Apache气流开发全国实时遥测分析平台</title>
      <link>https://dzone.com/articles/real-time-telemetry-gcp-airflow</link>
      <description>【&lt;p&gt;In my tenure at TELUS, I was assigned a prominent project requiring substantial technical expertise: the development of a telemetry analytics platform that could analyze data in real-time from over 100,000 set-top boxes (STBs) deployed throughout Canada. The objective was not just about scale; it aimed to assist teams to make quicker operational decisions and enhance the experience for millions of customers. Initially, I recognized the outdated data infrastructure as a bottleneck, obstructing the data from reaching the teams who required it the most. This article portrays the methodologies we employed to modernize our infrastructure using &lt;a href=&#34;https://dzone.com/articles/5-best-google-cloud-platform-gcp-courses-for-begin&#34;&gt;Google Cloud Platform&lt;/a&gt; (GCP), &lt;a href=&#34;https://dzone.com/articles/scalable-resilient-data-pipelines-apache-airflow&#34;&gt;Apache Airflow&lt;/a&gt;, and &lt;a href=&#34;http://Infrastructure-as-Code&#34;&gt;Infrastructure-as-Code&lt;/a&gt; tools to surmount the obstacles and deliver a future-proof solution.&lt;/p&gt;&#xA;&lt;h2&gt;&lt;strong&gt;The Predicament: Ancient Bottlenecks and Unseen Black Spots&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Prior to this revamp, we predominantly relied on segregated and batch-oriented data pipelines incapable of supporting real-time diagnostics. Key concerns encompassed:&lt;/p&gt;】&lt;p&gt;在我在Telus任职期间，我被分配了一个著名的项目，需要实质性的技术专长：遥测分析平台的开发，该平台可以从整个加拿大部署的100,000多个机顶盒（STB）实时分析数据。目标不只是规模；它旨在协助团队做出更快的运营决策，并为数百万客户提供体验。最初，我将过时的数据基础架构视为一种瓶颈，阻碍了数据到达最需要它的团队的数据。本文描绘了使用&lt;a href =“ https://dzone.com/articles/5-best-google-cloud-cloud-platform-gcp-courses-for-begin-for-bebe-for-begin”&gt; Google Cloud Platfort &lt;/a&gt;（GCP），&lt;A（GCP），&lt;a A&gt;（GCP），&lt;a a https：//5-best-google-cloud-gcp-courses-forform-gcp），&lt;a href=&#34;https://dzone.com/articles/scalable-resilient-data-pipelines-apache-airflow&#34;&gt;Apache Airflow&lt;/a&gt;, and &lt;a href=&#34;http://Infrastructure-as-Code&#34;&gt;Infrastructure-as-Code&lt;/a&gt; tools to surmount the obstacles and deliver a future-proof solution.&lt;/p&gt;&#xA;&lt;h2&gt; &lt;strong&gt;困境：古老的瓶颈和看不见的黑点&lt;/strong&gt; &lt;/h2&gt;&#xA;&lt;p&gt;在此改建之前，我们主要依赖于隔离和面向批处理的数据管道无法支持实时诊断。关键问题包括：&lt;/p&gt;</description>
      <pubDate>Wed, 03 Sep 2025 20:00:01 +0000</pubDate>
    </item>
    <item>
      <title>【DSLs vs. Libraries: Evaluating Language Design in the GenAI Era】DSL与图书馆：评估Genai时代的语言设计</title>
      <link>https://dzone.com/articles/gpl-vs-dsl-ai-evolution</link>
      <description>【&lt;p dir=&#34;ltr&#34;&gt;Programming languages are the fundamental tools used to shape the digital world. Every developer has to choose at some point in their careers between &lt;a href=&#34;https://dzone.com/articles/tiobe-index-popular-programming-languages&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;general-purpose languages&lt;/a&gt; such as Python, Java, and C# and specialized &lt;a href=&#34;https://dzone.com/articles/evolving-domain-specific-languages&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;domain-specific languages&lt;/a&gt; like SQL, CSS, or XAML. But with the evolution of AI the lines are getting blurred. We are observing shifts in not only how we write code but the definitions of productivity, maintainability, and innovation are beginning to change as well. As a result, the conventional trade-offs between DSLs and libraries are changing, and long-standing issues like expressiveness, integration complexity, and learning curves are being approached from new perspectives.&lt;/p&gt;&#xA;&lt;h2 dir=&#34;ltr&#34;&gt;The Traditional DSL vs Library Paradigm&lt;/h2&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;General-Purpose Languages (GPLs) are very versatile. They are packed with extensive libraries that allow developers to tackle problems across multiple domains. But this flexibility comes at the cost of writing more code and the need for significant domain knowledge to implement specialized solutions effectively.&amp;nbsp;&lt;/p&gt;】&lt;p dir =“ ltr”&gt;编程语言是用于塑造数字世界的基本工具。每个开发人员都必须在&lt;a href =“ https://dzone.com/articles/tiobe-index-popular-progular-programpommming-languages”中的职业生涯中进行选择。 href =“ https://dzone.com/articles/evolving-domain-spexific-languages” rel =“ noopener noreferrer” target =“ _ blank”&gt; domain特定语言&lt;/a&gt;像sql，css或xaml一样。但是随着AI的演变，线条变得模糊。我们正在观察不仅要编写代码的方式，而且在生产力，可维护性和创新的定义也开始改变。结果，DSL和图书馆之间的常规权衡正在发生变化，从新的角度来实现了长期存在的问题，例如表达，整合复杂性和学习曲线。&lt;/p&gt;&#xA;&lt;h2 dir =“ ltr”&gt;传统的DSL vs库范式&lt;/h2&gt;&#xA;&lt;p dir =“ ltr”&gt;通用语言（GPLS）非常通用。他们包装了广泛的库，使开发人员可以解决多个领域的问题。但是，这种灵活性是以编写更多代码和重要领域知识来有效实施专业解决方案的需要的成本来实现的。 &lt;/p&gt;</description>
      <pubDate>Wed, 03 Sep 2025 19:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Observability for the Invisible: Tracing Message Drops in Kafka Pipelines】无形的可观察性：跟踪消息在kafka管道中下降</title>
      <link>https://dzone.com/articles/observability-tracing-message-drops-kafka-pipelines</link>
      <description>【&lt;p&gt;When an event drops silently in a distributed system, it is not a bug, it is an architectural blind spot. In high-scale messaging platforms, particularly those serving real-time APIs like WhatsApp Business or IoT command chains, telemetry failures are often mistaken for application errors. But the root cause lies deeper: observability gaps in event streams.&lt;/p&gt;&#xA;&lt;p&gt;This article explores how backend engineers and DevOps teams can detect, debug, and prevent message loss in &lt;a href=&#34;https://dzone.com/articles/building-robust-real-time-data-pipelines-with-pyth&#34;&gt;Kafka-based streaming pipelines&lt;/a&gt; using tools like &lt;a href=&#34;https://dzone.com/articles/guide-to-opentelemetry-intro-to-observability&#34;&gt;OpenTelemetry&lt;/a&gt;, &lt;a href=&#34;https://dzone.com/articles/install-fluent-bit-from-source-part-two&#34;&gt;Fluent Bit&lt;/a&gt;, &lt;a href=&#34;https://dzone.com/articles/tracing-with-opentelemetry-and-jaeger&#34;&gt;Jaeger&lt;/a&gt;, and dead-letter queues. If your distributed messaging system handles millions of events, this guide outlines exactly how to make those events accountable.&lt;/p&gt;】&lt;p&gt;当事件在分布式系统中静静地掉落时，它不是错误，它是一个建筑盲点。在高规模的消息平台中，尤其是那些服务于实时API（例如WhatsApp Business或IoT命令链）的人，遥测失败通常被误认为是应用程序错误。但是根本原因更深：事件流中的可观察性差距。&lt;/p&gt;&#xA;&lt;p&gt;本文探讨了后端工程师和DevOps团队如何检测，调试和防止&lt;a href =“ https://dzone.com/articles/building-robust-robust-real time-data-pipelines-with-pipelines-with-pyth”&gt; kafka基于kafka的流媒体管道&lt;/a&gt;使用&lt;a&gt;的工具&lt;/a&gt; href =“ https://dzone.com/articles/guide-to-to-popentelemetry-intro-to-observability”&gt; opentelemetry &lt;/a&gt;，&lt;a href =“ https://dzone.com/articles/articles/articles/articles/articles/articles/articles/Install-fluent-fluent-fluent-fluent-bit-from-source-part-part-part-part-part-part-two，2 href =“ https://dzone.com/articles/tracing-with-with-popentelemetry-and-jaeger”&gt; jaeger &lt;/a&gt;和Dead-Letter队列。如果您的分布式消息系统处理数百万事件，则本指南准确概述了如何使这些事件负责。&lt;/p&gt;</description>
      <pubDate>Wed, 03 Sep 2025 18:00:07 +0000</pubDate>
    </item>
    <item>
      <title>【Simple Efficient Spring/Kafka Datastreams】简单有效的弹簧/kafka数据串</title>
      <link>https://dzone.com/articles/simple-efficient-springkafka-datastreams</link>
      <description>【&lt;p&gt;I had the opportunity to work with &lt;a href=&#34;https://dataflow.spring.io/&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;Spring Cloud Data Flow&lt;/a&gt; streams and batches. The streams work in production and perform well. The main streams used &lt;a href=&#34;https://debezium.io/&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;Debezium&lt;/a&gt; to send the database deltas to Soap endpoints or provided Soap endpoints to write into the database. The events where send via &lt;a href=&#34;https://kafka.apache.org/documentation/streams/&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;Kafka&lt;/a&gt;. Spring Cloud Data Flow also provides a application to manage the streams and jobs.&lt;/p&gt;&#xA;&lt;p&gt;The streams are build with a data source and a data sink that are separate applications and are decoupled by the events send via &lt;a href=&#34;https://dzone.com/refcardz/apache-kafka&#34;&gt;Kafka&lt;/a&gt;. Stream 1 has a Debezium source and sends the database deltas via Kafka to the sink that transforms the event into a soap request to the application. Stream 2 receives a soap request from the application and sends an event to Kafka. The sink receives the event and creates the database entries for the event.&lt;/p&gt;】&lt;p&gt;我有机会与&lt;a href =“ https://dataflow.spring.io/” rel =“ noopener noreferrer” target =“ _ black”&gt;春季云数据流&lt;/a&gt;流和批处理。这些溪流在生产方面工作并表现良好。使用的主流&lt;a href =“ https://debezium.io/” rel =“ noopener noreferrer” target =“ _ black”&gt; debezium &lt;/a&gt;将数据库deltas发送到肥皂端点或提供肥皂端点以写入数据库中。通过&lt;a href =“ https://kafka.apache.org/documentation/streams/”发送的事件。春季云数据流还提供了管理流和作业的应用程序。&lt;/p&gt;&#xA;&lt;p&gt;这些流是使用数据源构建的，并且数据源是单独的应用程序，并通过&lt;a href =“ https://dzone.com/refcardz/refcardz/apache-kafka”&gt; kafka“&gt; kafka” &lt;/a&gt;将事件解耦。 Stream 1具有Debezium源，并通过KAFKA将数据库三角洲发送到接收器，将事件转换为应用程序的肥皂请求。 Stream 2收到了该应用程序的肥皂请求，并将事件发送到Kafka。水槽接收事件并为事件创建数据库条目。&lt;/p&gt;</description>
      <pubDate>Wed, 03 Sep 2025 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Understanding Zero-Copy】了解零拷贝</title>
      <link>https://dzone.com/articles/understanding-zero-copy</link>
      <description>【&lt;p&gt;In the realm of high-performance computing and network applications, efficient data handling is important. Traditional &lt;a href=&#34;https://dzone.com/articles/decoding-database-speed-essential-server-resources&#34;&gt;Input/Output (I/O)&lt;/a&gt; operations often involve redundant data copies, creating performance bottlenecks that can limit throughput and increase latency. Zero-copy is a powerful optimization technique that minimizes or eliminates these unnecessary data movements, leading to significant performance gains.&lt;/p&gt;&#xA;&lt;h2&gt;Traditional Input/Output Path&lt;/h2&gt;&#xA;&lt;p&gt;Consider a common scenario: an application needs to read a file from disk and transmit it over a network. In a traditional I/O model, this seemingly straightforward operation entails a series of data copies:&lt;/p&gt;】&lt;p&gt;在高性能计算和网络应用领域中，有效的数据处理很重要。传统&lt;a href =“ https://dzone.com/articles/decoding-database-peed-sential-server-server-resources”&gt; input/output（i/o）&lt;/a&gt;操作通常涉及冗余数据副本，创建性能瓶颈，可以限制跨度并增加延迟。零拷贝是一种强大的优化技术，可最大程度地减少或消除这些不必要的数据移动，从而导致大量的性能增长。&lt;/p&gt;&#xA;&lt;h2&gt;传统输入/输出路径&lt;/h2&gt;&#xA;&lt;p&gt;考虑一个常见的方案：应用程序需要从磁盘读取文件并通过网络传输。在传统的I/O模型中，这种看似简单的操作需要一系列数据副本：&lt;/p&gt;</description>
      <pubDate>Wed, 03 Sep 2025 16:00:07 +0000</pubDate>
    </item>
    <item>
      <title>【Understanding Apache Spark Join Types】了解Apache Spark加入类型</title>
      <link>https://dzone.com/articles/spark-join-types-explained</link>
      <description>【&lt;p dir=&#34;ltr&#34;&gt;In this article, we are going to discuss three essential joins of Apache Spark.&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;The data frame or table join operation is most commonly used for data transformations in Apache Spark. With &lt;a href=&#34;https://dzone.com/articles/apache-spark-all-you-need-to-know&#34;&gt;Apache Spark&lt;/a&gt;, a developer can use joins to merge two or more data frames according to specific (sortable) keys. Writing a join operation has a straightforward syntax, but occasionally the inner workings are obscured. Apache Spark internal API suggests several algorithms for joins and selects one. A basic join operation could become costly if you do not know what these core algorithms are or which one Spark uses.&lt;/p&gt;】&lt;p dir =“ ltr”&gt;在本文中，我们将讨论Apache Spark的三个基本连接。&lt;/p&gt;&#xA;&lt;p dir =“ ltr”&gt;数据框架或表接合操作最常用于Apache Spark中的数据转换。使用&lt;a href =“ https://dzone.com/articles/apache-spark-all-you-need-to-know-”&gt; apache spark &lt;/a&gt;，开发人员可以根据特定（可分配）键使用加入来合并两个或多个数据帧。编写连接操作具有直接的语法，但有时内部工作被遮盖了。 Apache Spark Internal API建议连接的几种算法并选择一种算法。如果您不知道这些核心算法是什么或Spark使用的基本连接操作可能会变得昂贵。&lt;/p&gt;</description>
      <pubDate>Wed, 03 Sep 2025 15:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Container Security Essentials: From Images to Runtime Protection】集装箱安全性必需品：从图像到运行时保护</title>
      <link>https://dzone.com/articles/container-security-essentials</link>
      <description>【&lt;p&gt;Container security is all about making sure you run an image that is exceptionally low in vulnerability and malware. I would love to say having zero vulnerabilities, but it is rarely possible in the real world.&lt;/p&gt;&#xA;&lt;p&gt;In the worst case, you at least want to address critical to medium vulnerabilities to have a good night&#39;s sleep and avoid potential compromise from bad actors. You could also think of container security like peeling an onion, where each layer adds resilience against potential threats. As part of this article, we will learn what the different steps are that we could take to increase the overall safety of the container infrastructure.&lt;/p&gt;】&lt;p&gt;容器安全性就是确保您运行漏洞和恶意软件异常低的图像。我想说的是零脆弱性，但在现实世界中很少可能。&lt;/p&gt;&#xA;&lt;p&gt;在最坏的情况下，您至少想解决对中等脆弱性至关重要的问题，以使睡眠良好，并避免不良演员的潜在妥协。您还可以想到容器安全性，例如剥离洋葱，每一层都会为潜在威胁带来弹性。作为本文的一部分，我们将了解我们可以采取的不同步骤来提高容器基础设施的整体安全性。&lt;/p&gt;</description>
      <pubDate>Wed, 03 Sep 2025 14:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Cryptography Libraries on Ampere®】Ampere®上的加密图书馆</title>
      <link>https://dzone.com/articles/cryptography-libraries-on-ampere</link>
      <description>【&lt;h2&gt;Background&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://dzone.com/articles/fundamentals-of-cryptography&#34;&gt;Cryptography&lt;/a&gt; is the science of securing communication and data through mathematical techniques, ensuring confidentiality, integrity, and authenticity. It is widely used in web services, load balance proxies, databases, etc.&lt;/p&gt;&#xA;&lt;p&gt;Cryptography can be divided into three categories:&lt;/p&gt;】&lt;h2&gt;背景&lt;/h2&gt;&#xA;&lt;p&gt; &lt;a href =“ https://dzone.com/articles/fundamentals-of-cryptography”&gt;加密术&lt;/a&gt;是通过数学技术确保通信和数据的科学，确保了机密性，完整性和真实性。它广泛用于Web服务，加载余额代理，数据库等。&lt;/p&gt;&#xA;&lt;p&gt;密码学可以分为三类：&lt;/p&gt;</description>
      <pubDate>Wed, 03 Sep 2025 13:30:00 +0000</pubDate>
    </item>
    <item>
      <title>【Why Zero Trust Is Not a Product but a Strategy You Can’t Ignore in 2025】为什么零信任不是产品，而是您在2025年不能忽略的策略</title>
      <link>https://dzone.com/articles/zero-trust-2025-guide</link>
      <description>【&lt;p dir=&#34;ltr&#34;&gt;&#34;We recently purchased a Zero Trust solution.&#34; &amp;nbsp;A statement like that makes even the most seasoned security experts cringe. &lt;a href=&#34;https://dzone.com/articles/immutable-secrets-management-zero-trust-approach?fromrel=true&#34;&gt;Zero Trust&lt;/a&gt; is a ubiquitous notion in 2025, appearing in product packaging, seminars, and sales presentations. However, the fundamental idea is still gravely misinterpreted.&amp;nbsp;&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;There is no such thing as buying Zero Trust. It&#39;s a way of thinking, a plan you follow, and a path you dedicate yourself to. In light of growing attack surfaces, heterogeneous workforces, and more complex threat actors, it is not only inefficient but also risky to approach Zero Trust as a checkbox.&lt;/p&gt;】&lt;p dir =“ ltr”&gt;“我们最近购买了零信任解决方案。”  这样的声明即使是经验丰富的安全专家也畏缩了一下。 &lt;a href =“ https://dzone.com/articles/immutable-secrets-management-management-management-zero-trust-pable--fromrel=true”&gt;零信任&lt;/a&gt;是2025年无处不在的概念，出现在产品包装，研讨会，研讨会和销售介绍中。但是，基本思想仍然被严重误解。 &lt;/p&gt;&#xA;&lt;p dir =“ ltr”&gt;没有购买零信任的东西。这是一种思维方式，您遵循的计划以及您奉献自己的道路。鉴于攻击表面，异构劳动力和更复杂的威胁行为者的越来越多，不仅效率低下，而且还是有风险作为复选框的零信任。&lt;/p&gt;</description>
      <pubDate>Wed, 03 Sep 2025 13:00:03 +0000</pubDate>
    </item>
    <item>
      <title>【File Systems &amp;lt;&amp;gt; Database: Full Circle】文件系统&lt;&gt;数据库：完整圈子</title>
      <link>https://dzone.com/articles/file-systems-and-database-full-circle</link>
      <description>【&lt;p&gt;File-based systems were the original data storage systems before the invention of database management systems (DBMS). Back in the 1970s, organizations manually stored data across servers in numerous files, such as flat files. These files have a fixed, rigid format and multiple copies of data stored for each department, resulting in data redundancy. These led to various challenges, especially data consistency, sharing, security, and retrieval. Analyzing these files was also challenging if we needed to join multiple files for one end-to-end record. As a result, file-based systems could not keep up with the changing data and innovations.&amp;nbsp;&lt;/p&gt;&#xA;&lt;p&gt;With the &lt;a href=&#34;https://dzone.com/articles/the-types-of-modern-databases&#34;&gt;invention of DBMS&lt;/a&gt;, data transactions comply with ACID properties (atomicity, consistency, isolation, durability), which allows for data consistency, integrity, recovery, and concurrency. In addition, today&#39;s advanced DBMS system provides disaster recovery, backup and restore, data searching, and data encryption and security. Even though the DBMS has evolved, due to the advancement of big data, cloud technologies, the Internet, social media, and advancing data formats, file storage is again a hot topic.&amp;nbsp;&lt;/p&gt;】&lt;p&gt;基于文件的系统是数据库管理系统（DBMS）发明之前的原始数据存储系统。早在1970年代，组织就在许多文件（例如Flat Files）中手动存储了跨服务器的数据。这些文件具有固定的，严格的格式和为每个部门存储的数据副本，从而导致数据冗余。这些导致了各种挑战，尤其是数据一致性，共享，安全性和检索。如果我们需要加入多个文件以获取一个端到端记录，则分析这些文件也很具有挑战性。结果，基于文件的系统无法跟上不断变化的数据和创新。 &lt;/p&gt;&#xA;&lt;p&gt;使用&lt;a href =“ https://dzone.com/articles/the-types-of-modern-databases”&gt; dbms的发明&lt;/a&gt;，数据交易符合酸性属性（原子，一致性，隔离性，耐用性），从而允许数据一致性，完整性，综合性，恢复和相关性。此外，当今的高级DBMS系统还提供灾难恢复，备份和还原，数据搜索以及数据加密和安全性。即使DBM的发展，由于大数据，云技术，互联网，社交媒体和推进数据格式的发展，文件存储再次成为一个热门话题。 &lt;/p&gt;</description>
      <pubDate>Wed, 03 Sep 2025 12:00:11 +0000</pubDate>
    </item>
  </channel>
</rss>