<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DZone.com Feed</title>
    <link>https://feeds.dzone.com/home</link>
    <description>Recent posts on DZone.com</description>
    <item>
      <title>【Death by a Thousand YAMLs: Surviving Kubernetes Tool Sprawl】一千个山羊的死亡：幸存的库伯尼特工具蔓延</title>
      <link>https://dzone.com/articles/kubernetes-tool-sprawl-yaml</link>
      <description>【&lt;p style=&#34;font-size: 17px;&#34;&gt;&lt;em&gt;Editor&#39;s Note: The following is an article written for and published in&amp;nbsp;&lt;/em&gt;&lt;em&gt;DZone&#39;s 2025 Trend Report,&amp;nbsp;&lt;/em&gt;&lt;a href=&#34;https://dzone.com/trendreports/kubernetes-in-the-enterprise-4&#34;&gt;&lt;em&gt;Kubernetes in the Enterprise: Optimizing the Scale, Speed, and Intelligence of Cloud Operations&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;Kubernetes is eating the world.&amp;nbsp;&lt;/p&gt;】&lt;p style =“ font-size：17px;”&gt; &lt;em&gt;编辑注：以下是为&lt;/em&gt; &lt;em&gt; &lt;em&gt; dzone的2025年2025年趋势报告，&lt;/em&gt; &lt;a href =“ https://///dzone.com/dzone.com/trendreports/trendreports/kubernetes-in-kubernetes-in-in-the-the-the-prise-.- &lt;/em&gt; &lt;a href =”云操作的速度和智能&lt;/em&gt; &lt;/a&gt; &lt;em&gt;。&lt;/em&gt; &lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p dir =“ ltr”&gt; kubernetes正在吃世界。 &lt;/p&gt;</description>
      <pubDate>Wed, 24 Sep 2025 18:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【The New API Economy With LLMs】LLM的新API经济</title>
      <link>https://dzone.com/articles/new-api-economy-with-llms</link>
      <description>【&lt;p dir=&#34;ltr&#34;&gt;Large language models (LLMs) are becoming more advanced in understanding context in natural language. With this, a new paradigm is emerging — using LLMs as APIs.&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;Traditionally, an API call would be &lt;code&gt;GET /users/123/orders&lt;/code&gt; and you would receive a JSON in return, which would return the orders for the user 123. APIs facilitate the interaction between different software systems.&lt;/p&gt;】&lt;p dir =“ ltr”&gt;大语言模型（LLM）在理解自然语言的上下文中变得越来越先进。因此，新的范式正在出现 - 使用LLMS作为API。&lt;/p&gt;&#xA;&lt;p dir =“ ltr”&gt;传统上，API调用将为&lt;code&gt; get/users/123/orders &lt;/code&gt;，您将收到一个返回的JSON，这将返回用户123的订单。APIS促进不同软件系统之间的交互。&lt;/p&gt;</description>
      <pubDate>Wed, 24 Sep 2025 17:00:04 +0000</pubDate>
    </item>
    <item>
      <title>【Key Principles of API-First Development for SaaS】SaaS的API优先开发的关键原则</title>
      <link>https://dzone.com/articles/api-first-development-principles-saas</link>
      <description>【&lt;p dir=&#34;ltr&#34;&gt;Having worked in software development for over 8 years, I have repeatedly watched developers struggle to integrate APIs into platforms as an afterthought. The situation is common. Someone builds a beautiful web app, then the business team asks for mobile support, third-party integrations, and suddenly you&#39;re reverse-engineering your own application to expose endpoints that make sense.&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;Luckily, this is changing. With&amp;nbsp;&lt;a href=&#34;https://dzone.com/articles/what-is-api-first&#34;&gt;API-first development&lt;/a&gt;, we can design the architecture with the API as part of it from day one. This is especially beneficial for SaaS products as they rely on third-party integrations and ecosystem support.&amp;nbsp;&lt;/p&gt;】&lt;p dir =“ ltr”&gt;在软件开发工作已有8年以上，我一再观察开发人员努力将API作为事后的想法整合到平台中。情况很普遍。有人构建了一个漂亮的网络应用程序，然后业务团队要求提供移动支持，第三方集成，突然之间，您正在逆转自己的应用程序，以揭示有意义的端点。&lt;/p&gt;&#xA;&lt;p dir =“ ltr”&gt;幸运的是，这正在发生变化。使用&lt;a href =“ https://dzone.com/articles/what-is-api-first”&gt; api-first开发&lt;/a&gt;，我们可以从第一天开始使用API​​设计该体系结构。这对SaaS产品尤其有益，因为它们依靠第三方整合和生态系统支持。 &lt;/p&gt;</description>
      <pubDate>Wed, 24 Sep 2025 16:00:01 +0000</pubDate>
    </item>
    <item>
      <title>【Using TanStack Query for Scalable React Applications】使用Tanstack查询进行可扩展的反应应用程序</title>
      <link>https://dzone.com/articles/tanstack-query-scalable-react-apps</link>
      <description>【&lt;p&gt;&lt;span data-preserver-spaces=&#34;true&#34;&gt;When building React applications, data fetching often starts with the native fetch API or tools like Axios. While this approach works for small projects, larger applications require features such as caching, retries, synchronization, and request cancellation, and it is here that TanStack Query, formerly React Query, excels.&lt;/span&gt;&lt;span data-preserver-spaces=&#34;true&#34;&gt;&amp;nbsp;It provides a battle-tested abstraction for CRUD operations with powerful state management built&amp;nbsp;&lt;/span&gt;&lt;span data-preserver-spaces=&#34;true&#34;&gt;in.&lt;/span&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;span data-preserver-spaces=&#34;true&#34;&gt;In this article, we’ll walk through fetching data with useQuery, performing mutations with useMutation, and highlighting some features that make&amp;nbsp;&lt;/span&gt;&lt;a href=&#34;https://tanstack.com/query/latest/docs/framework/react/overview&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;&lt;span data-preserver-spaces=&#34;true&#34;&gt;TanStack Query&lt;/span&gt;&lt;/a&gt;&lt;span data-preserver-spaces=&#34;true&#34;&gt;&amp;nbsp;a helpful tool for scaling &lt;/span&gt;&lt;a href=&#34;https://dzone.com/articles/optimizing-react-apps-for-web-development-a-compre&#34;&gt;&lt;span data-preserver-spaces=&#34;true&#34;&gt;React apps&lt;/span&gt;&lt;/a&gt;&lt;span data-preserver-spaces=&#34;true&#34;&gt;.&lt;/span&gt;&lt;/p&gt;】&lt;p&gt; &lt;span data-proserver spaces =“ true”&gt;在构建反应应用程序时，数据获取通常始于本机提取API或AXIOS等工具。尽管这种方法适用于小型项目，但较大的应用程序需要诸如缓存，重试，同步和请求取消之类的功能，并且在这里，Tanstack查询（以前是反应查询）擅长的。 data-preserver spaces =“ true”&gt; in。&lt;/span&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;span data data-preserver-spaces =“ true”&gt;在本文中，我们将浏览用USEQUERY获取数据，执行具有USEMONT的突变，并突出一些使&lt;/span&gt; &lt;a href =“ https：////////////tanstack.com.com.com.com.com.com.com.com./query/query/latest/docs/docs/ddocs/framework/framework/framework/reaeact/react/over/overecte noore n nore n n re ne n nre n n n n n n n re =” target=&#34;_blank&#34;&gt;&lt;span data-preserver-spaces=&#34;true&#34;&gt;TanStack Query&lt;/span&gt;&lt;/a&gt;&lt;span data-preserver-spaces=&#34;true&#34;&gt; a helpful tool for scaling &lt;/span&gt;&lt;a href=&#34;https://dzone.com/articles/optimizing-react-apps-for-web-development-a-compre&#34;&gt;&lt;span data-preserver spaces =“ true”&gt; React Apps &lt;/span&gt; &lt;/a&gt; &lt;span data-proserver-spaces =“ true”&gt;。&lt;/span&gt; &lt;/p&gt;</description>
      <pubDate>Wed, 24 Sep 2025 15:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Resilient Data Pipelines in GCP: Handling Failures and Latency in Distributed Systems】GCP中的弹性数据管道：分布式系统的处理故障和延迟</title>
      <link>https://dzone.com/articles/resilient-data-pipelines-in-gcp</link>
      <description>【&lt;p dir=&#34;ltr&#34;&gt;I have spent years designing and operating data pipelines in Google Cloud, and one thing has not changed: resilience is not optional. It does not matter how nice your design diagrams look or how scalable the architecture is. In practice, nodes die, quotas are exhausted, regions are shaded, schemas alter unannounced, and message queues are clogged up at the most unpredictable moments. The main distinction between a functional pipeline and a resilient pipeline lies in the fact that the former can withstand failures and still meet latency requirements.&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;The article explains my philosophy on resilience in distributed data pipelines on GCP, based not only on the experience of running these systems, but also more broadly on systems research and Google operational experience.&lt;/p&gt;】&lt;p dir =“ ltr”&gt;我花了数年的时间在Google Cloud中设计和操作数据管道，而一件事没有改变：弹性不是可选的。您的设计图外观或架构的可扩展性无关紧要。在实践中，节点模具，配额耗尽，区域被阴影，图式架构，未通知，并在最不可预测的时刻堵塞了消息队列。功能管道和弹性管道之间的主要区别在于，前者可以承受失败并仍然满足潜伏期的要求。&lt;/p&gt;&#xA;&lt;p dir =“ ltr”&gt;本文解释了我对GCP分布式数据管道中的弹性的理念，不仅基于运行这些系统的经验，而且基于系统研究和Google操作经验的更广泛的经验。&lt;/p&gt;。&lt;/p&gt;</description>
      <pubDate>Wed, 24 Sep 2025 14:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Why I Ditched Redis for Cloudflare Durable Objects in My Rate Limiter】为什么我将redis抛弃在我的费率限制器中的Cloudflare耐用物体</title>
      <link>https://dzone.com/articles/why-ditch-redis-for-cloudflare-durable-objects</link>
      <description>【&lt;p&gt;Have you ever watched your serverless application crumble under unexpected traffic? Last month, our AI-powered image generator went viral on social media, and within hours, we were drowning in requests. Our traditional rate-limiting setup couldn&#39;t keep up with the distributed load across Cloudflare&#39;s edge network.&lt;/p&gt;&#xA;&lt;p&gt;This experience taught me that &lt;a href=&#34;https://dzone.com/articles/what-you-must-know-about-rate-limiting&#34;&gt;rate limiting&lt;/a&gt; in serverless environments requires a fundamentally different approach. Here&#39;s how I built a production-ready rate limiter using Cloudflare Durable Objects that handles thousands of concurrent requests while running at the edge.&lt;/p&gt;】&lt;p&gt;您是否曾经在意外流量下观看过无服务器的应用程序崩溃？上个月，我们的AI驱动图像生成器在社交媒体上传播开来，在几个小时内，我们被要求淹没了。我们的传统限制性设置无法跟上Cloudflare的边缘网络的分布式负载。&lt;/p&gt;&#xA;&lt;p&gt;这种经历告诉我&lt;a href =“ https://dzone.com/articles/what-you-must-know-bout-rate-late-limition”&gt;限制速率&lt;/a&gt;在无服务器环境中，需要一种根本不同的方法。这是我使用Cloudflare耐用对象构建一个可以生产的速率限制器，该对象在边缘运行时处理数千个并发请求。&lt;/p&gt;</description>
      <pubDate>Wed, 24 Sep 2025 13:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Shipping Responsible AI Without Slowing Down】运输负责人的AI而不减速</title>
      <link>https://dzone.com/articles/shipping-responsible-ai-without-slowing-down</link>
      <description>【&lt;p&gt;In software engineering, launch day rarely fails because a unit test was missing; in machine learning (ML), that’s not the case. Inputs far from training data, adversarial prompts, proxies that drift away from human goals, or an upstream artefact that isn’t what it claims to be can all sink a release. The question is not “can every failure be prevented?” but “can failures be bounded, detected quickly, and recovered from predictably?”&lt;/p&gt;&#xA;&lt;p&gt;Two research threads shape this approach. The first maps where &lt;a href=&#34;https://arxiv.org/abs/2109.13916&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;ML goes wrong&lt;/a&gt; in production: robustness gaps, weak runtime monitoring, misalignment with real human objectives, and systemic issues across the stack (&lt;a href=&#34;https://dzone.com/articles/redefining-supply-chains-through-tech-a-developers&#34;&gt;supply chain&lt;/a&gt;, access, blast radius). The second focuses on how teams make decisions that stand up to scrutiny: a deliberative loop that’s open, informed, multi-vocal, and responsive. Put together, the operating model feels like standard software engineering — just opinionated for ML.&lt;/p&gt;】&lt;p&gt;在软件工程中，启动日很少失败，因为缺少单位测试；在机器学习（ML）中，情况并非如此。远离培训数据，对抗性提示，偏离人类目标的代理或上游的人工制品的投入远远没有散发出释放。这个问题不是“可以防止每一次失败？”但是“失败可以被界定，迅速检测并从可预测地恢复吗？” &lt;/p&gt;&#xA;&lt;p&gt;两个研究线程塑造了这种方法。第一个地图其中&lt;a href =“ https://arxiv.org/abs/2109.13916” rel =“ noopener noreferrer” target =“ _ black”&gt; ML出现问题&lt;/a&gt;在生产中出错：&lt;/a&gt; &lt;/a&gt; &lt;/a&gt; &lt;/a&gt; &lt;/a&gt; &lt;/a&gt;稳健性差距，稳健的运行时间监测，与真实的人类目的和跨越的人类目的和系统问题（&lt;a and Systemic Chorege） href =“ https://dzone.com/articles/redefining-supply-chains-chains-though-tech-a-developers”&gt;供应链&lt;/a&gt;，访问，爆炸半径）。第二个重点是团队如何做出应对审查的决策：一个开放，知情，多声音和响应迅速的审议循环。组合在一起，操作模型感觉就像是标准软件工程 - 仅为ML。&lt;/p&gt;</description>
      <pubDate>Wed, 24 Sep 2025 12:00:13 +0000</pubDate>
    </item>
    <item>
      <title>【Top 7 Mistakes When Testing JavaFX Applications】测试Javafx应用程序时犯了前7个错误</title>
      <link>https://dzone.com/articles/top-javafx-testing-mistakes</link>
      <description>【&lt;p dir=&#34;ltr&#34;&gt;JavaFX is a versatile tool for creating rich enterprise-grade GUI applications. Testing these applications is an integral part of the development lifecycle. However, Internet sources are very scarce when it comes to defining best practices and guidelines for testing JavaFX apps. Therefore, developers must rely on commercial offerings for JavaFX testing services or write their test suites following trial-and-error approaches.&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;This article summarises the seven most common mistakes programmers make when testing JavaFX applications and ways to avoid them.&lt;/p&gt;】&lt;p dir =“ ltr”&gt; javafx是用于创建丰富企业级GUI应用程序的多功能工具。测试这些应用是开发生命周期不可或缺的一部分。但是，在定义测试Javafx应用程序的最佳实践和准则时，互联网来源非常稀缺。因此，开发人员必须依靠商业产品来用于Javafx测试服务或在试用方法之后编写其测试套件。&lt;/p&gt;&#xA;&lt;p dir =“ ltr”&gt;本文总结了测试JAVAFX应用程序和避免使用方法时的七个最常见错误。&lt;/p&gt;</description>
      <pubDate>Wed, 24 Sep 2025 11:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【LLMs at the Edge: Decentralized Power and Control】边缘的LLM：分散的功率和控制</title>
      <link>https://dzone.com/articles/lms-edge-decentralized-power-control</link>
      <description>【&lt;p&gt;&lt;span&gt;Most of the large language models&#39; applications have been implemented in centralized cloud environments, raising concerns about latency, privacy, and energy consumption. This chapter examines the potential application of LLMs in decentralized edge computing, where computing tasks are distributed across interconnected devices rather than centralized hosts. Therefore, by applying approaches like quantization, model compression, distributed inference, and federated learning, LLMs solve the problems of limited computational and memory resources on edge devices, making them suitable for practical use in real-world settings.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;span&gt;Several advantages of &lt;/span&gt;&lt;a href=&#34;https://dzone.com/articles/decentralized-artificial-intelligence-the-future-o&#34;&gt;&lt;span&gt;decentralization&lt;/span&gt;&lt;/a&gt;&lt;span&gt; are outlined in the chapter, such as increased privacy, user control, and enhanced system robustness. Additionally, it focuses on the potential of employing energy-efficient methods and dynamic power modes to enhance edge systems. The conclusion re-emphasizes that edge AI is the way forward&amp;nbsp;&lt;/span&gt;&lt;span&gt;as a responsible and performant solution for&lt;/span&gt;&lt;span&gt;&amp;nbsp;the future&lt;/span&gt;&lt;span&gt;&amp;nbsp;of decentralized AI technologies, which would be privacy-centric, high-performing, and put the user first.&lt;/span&gt;&lt;/p&gt;】&lt;p&gt; &lt;span&gt;大多数大型语言模型的应用程序已经在集中式的云环境中实施，从而引起了人们对延迟，隐私和能源消耗的担忧。本章介绍了LLM在分散的边缘计算中的潜在应用，其中计算任务是在互连设备而不是集中式主机上分布的。因此，通过应用量化，模型压缩，分布式推理和联合学习等方法，LLMS解决了边缘设备上有限的计算和内存资源的问题，使其适合于现实世界中的实际使用。 &lt;/span&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;span&gt; &lt;/span&gt; &lt;a href =“ https://dzone.com/articles/decentralized-ainderalized-aindercer-intelligence-therelligence-the-future-o”&gt; &lt;span&gt;分散式中心化&lt;/span&gt; &lt;/a&gt; &lt;/a&gt; &lt;/a&gt; &lt;/a&gt; &lt;/a&gt; &lt;/a&gt; &lt;ga&gt;在本章中概述了隐私，用户控制功能，并增强了系统，并增强了系统。此外，它重点是采用节能方法和动态功率模式来增强边缘系统的潜力。结论重新强调，边缘AI是前进的方式&lt;/span&gt; &lt;span&gt;作为&lt;/span&gt; &lt;span&gt;的负责任和绩效解决方案，即未来的分散AI技术的未来&lt;/span&gt; &lt;span&gt;，这将是以隐私为中心的，以隐私为中心的，高性能，并将用户放在首位。</description>
      <pubDate>Tue, 23 Sep 2025 19:00:08 +0000</pubDate>
    </item>
    <item>
      <title>【Running AI/ML on Kubernetes: From Prototype to Production — Use MLflow, KServe, and vLLM on Kubernetes to Ship Models With Confidence】在Kubernetes上运行AI/ML：从原型到生产 - 使用MLFlow，Kserve和Kubernetes上的VLLM自信地运送模型</title>
      <link>https://dzone.com/articles/ai-ml-kubernetes-mlflow-kserve-vllm</link>
      <description>【&lt;p style=&#34;font-size: 17px;&#34;&gt;&lt;em&gt;Editor&#39;s Note: The following is an article written for and published in&amp;nbsp;&lt;/em&gt;&lt;em&gt;DZone&#39;s 2025 Trend Report,&amp;nbsp;&lt;/em&gt;&lt;a href=&#34;https://dzone.com/trendreports/kubernetes-in-the-enterprise-4&#34;&gt;&lt;em&gt;Kubernetes in the Enterprise: Optimizing the Scale, Speed, and Intelligence of Cloud Operations&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;After training a machine learning model, the inference phase must be fast, reliable, and cost efficient in production. Serving inference at scale, however, brings difficult problems: GPU/resource management, latency and batching, model/version rollout, observability, and orchestration of ancillary services (preprocessors, feature stores, and vector databases). Running artificial intelligence and machine learning (AI/ML) on Kubernetes gives us a scalable, portable platform for training and serving models. Kubernetes schedules GPUs and other resources so that we can pack workloads efficiently and autoscale to match traffic for both batch jobs and real-time inference. It also coordinates multi-component stacks — like model servers, preprocessors, vector DBs, and feature stores — so that complex pipelines and low-latency endpoints run reliably.&amp;nbsp;&lt;/p&gt;】&lt;p style =“ font-size：17px;”&gt; &lt;em&gt;编辑注：以下是为&lt;/em&gt; &lt;em&gt; &lt;em&gt; dzone的2025年2025年趋势报告，&lt;/em&gt; &lt;a href =“ https://///dzone.com/dzone.com/trendreports/trendreports/kubernetes-in-kubernetes-in-in-the-the-the-prise-.- &lt;/em&gt; &lt;a href =”云操作的速度和智能&lt;/em&gt; &lt;/a&gt; &lt;em&gt;。&lt;/em&gt; &lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p dir =“ ltr”&gt;训练机器学习模型后，推理阶段必须快速，可靠且生产成本效率。但是，在大规模上进行推断，带来了困难的问题：GPU/资源管理，延迟和批处理，模型/版本的推出，可观察性和辅助服务的编排（预处理器，功能商店和矢量数据库）。在Kubernetes上运行人工智能和机器学习（AI/ML）为我们提供了一个可扩展的，可移植的培训和服务模型平台。 Kubernetes安排了GPU和其他资源，以便我们可以有效地打包工作负载，并自动打包以匹配批处理作业和实时推理的流量。它还协调多组分堆栈（例如模型服务器，预处理程序，向量DB和功能存储），从而使复杂的管道和低延迟端点可靠地运行。 &lt;/p&gt;</description>
      <pubDate>Tue, 23 Sep 2025 18:00:08 +0000</pubDate>
    </item>
  </channel>
</rss>