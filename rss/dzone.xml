<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DZone.com Feed</title>
    <link>https://feeds.dzone.com/home</link>
    <description>Recent posts on DZone.com</description>
    <item>
      <title>【Building a Simple MCP Server and Client: An In-Memory Database】构建简单的 MCP 服务器和客户端：内存数据库</title>
      <link>https://dzone.com/articles/simple-mcp-server-client-database</link>
      <description>【&lt;p dir=&#34;ltr&#34;&gt;If you&#39;ve been diving into the world of AI-assisted programming or tool-calling protocols, you might have come across Model Context Protocol (MCP). MCP is an open-source standard for connecting AI applications to external systems. It is a lightweight framework that lets you expose functions as &#34;&lt;a href=&#34;https://modelcontextprotocol.io/docs/learn/server-concepts#tools&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;tools&lt;/a&gt;&#34; to language models, enabling seamless interaction between AI agents and your code. Think of it as a bridge that turns your functions into callable endpoints for models.&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;In this post, we’ll build a basic in-memory database server using MCP, with code samples to extend and learn from. We&#39;ll dissect the code step by step, and by the end, you&#39;ll have a working prototype. Plus, I&#39;ll ask you to extend it with update, delete, and drop functionalities. Let&#39;s turn your terminal into a mini SQL playground!&lt;/p&gt;】&lt;p dir=&#34;ltr&#34;&gt;如果您一直在深入研究人工智能辅助编程或工具调用协议的世界，您可能已经遇到过模型上下文协议 (MCP)。 MCP 是一种用于将 AI 应用程序连接到外部系统的开源标准。它是一个轻量级框架，可让您将函数作为“&lt;a href=&#34;https://modelcontextprotocol.io/docs/learn/server-concepts#tools&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;工具&lt;/a&gt;”公开给语言模型，从而实现 AI 代理与代码之间的无缝交互。将其视为将您的函数转变为模型的可调用端点的桥梁。&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;在这篇文章中，我们将使用 MCP 构建一个基本的内存数据库服务器，并提供可供扩展和学习的代码示例。我们将逐步剖析代码，最后，您将拥有一个工作原型。另外，我会要求您通过更新、删除和删除功能来扩展它。让我们将您的终端变成一个迷你 SQL 游乐场！&lt;/p&gt;</description>
      <pubDate>Thu, 27 Nov 2025 20:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【How To Restore a Deleted Branch In Azure DevOps】如何在 Azure DevOps 中恢复已删除的分支</title>
      <link>https://dzone.com/articles/restore-deleted-branch-azure-devops</link>
      <description>【&lt;p&gt;Human error is one of the most common causes of data loss or breaches. In the ITIC report, they state that 64 % of downtime incidents have their roots in human errors.&lt;/p&gt;&#xA;&lt;p&gt;If you think that in SaaS environments all your data is safe, you need to think once again. All SaaS providers, including Microsoft, follow the &lt;a href=&#34;https://dzone.com/articles/rules-for-better-cloud-security&#34;&gt;shared responsibility model&lt;/a&gt;, which states that the service provider is responsible for the accessibility of its infrastructure and services, while a user is responsible for their data availability, including backup and disaster recovery.&lt;/p&gt;】&lt;p&gt;人为错误是数据丢失或泄露的最常见原因之一。在 ITIC 报告中，他们指出 64% 的停机事件根源在于人为错误。&lt;/p&gt;&#xA;&lt;p&gt;如果您认为在 SaaS 环境中您的所有数据都是安全的，那么您需要再考虑一下。所有 SaaS 提供商（包括 Microsoft）都遵循&lt;a href=&#34;https://dzone.com/articles/rules-for-better-cloud-security&#34;&gt;共担责任模型&lt;/a&gt;，该模型规定服务提供商负责其基础设施和服务的可访问性，而用户则负责其数据可用性，包括备份和灾难恢复。&lt;/p&gt;</description>
      <pubDate>Thu, 27 Nov 2025 19:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Mastering Fluent Bit: Controlling Logs with Fluent Bit on Kubernetes (update to Part 4)】掌握 Fluent Bit：在 Kubernetes 上使用 Fluent Bit 控制日志（更新至第 4 部分）</title>
      <link>https://dzone.com/articles/mastering-fluent-bit-controlling-logs-with-fluent</link>
      <description>【&lt;p&gt;&lt;strong&gt;NOTE: This is a special update to the original &lt;a href=&#34;https://dzone.com/articles/controlling-logs-with-fluent-bit-kubernetes&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;Controlling Logs with &amp;nbsp;Fluent Bit on Kubernetes (Part 4)&lt;/a&gt; article published previously.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;The issue requiring this update arose over the weekend when I discovered that Broadcom, who acquired VMWare, who were the custodians of the Bitnami catalog, did something not so nice to all of us.&lt;/p&gt;】&lt;p&gt;&lt;strong&gt;注意：这是对之前发布的&lt;a href=&#34;https://dzone.com/articles/controlling-logs-with- Fluent-bit-kubernetes&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;在 Kubernetes 上使用 Fluent Bit 控制日志（第 4 部分）&lt;/a&gt;文章的特殊更新。&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;周末出现了需要此更新的问题，当时我发现收购 VMWare（Bitnami 目录的托管人）的 Broadcom 做了一些对我们所有人都不太好的事情。&lt;/p&gt;</description>
      <pubDate>Thu, 27 Nov 2025 18:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Solving Real-Time Event Correlation in Distributed Systems】解决分布式系统中的实时事件关联</title>
      <link>https://dzone.com/articles/real-time-event-correlation-distributed-systems</link>
      <description>【&lt;p&gt;&lt;span&gt;Modern digital platforms operate as distributed ecosystems — microservices emitting events, APIs exchanging data, and asynchronous communication becoming the norm. In such environments, &lt;strong&gt;correlating events across multiple sources in real time&lt;/strong&gt; becomes a critical requirement.&lt;/span&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;span&gt;Think of payments, orders, customer metadata, &lt;/span&gt;&lt;a href=&#34;https://dzone.com/articles/type-of-sensors-and-actuators-in-iot&#34;&gt;&lt;span&gt;IoT sensors&lt;/span&gt;&lt;/a&gt;&lt;span&gt;, logistics tracking — all flowing continuously.&lt;/span&gt;&lt;/p&gt;】&lt;p&gt;&lt;span&gt;现代数字平台作为分布式生态系统运行——微服务发出事件、API 交换数据以及异步通信成为常态。在这种环境中，&lt;strong&gt;实时关联多个来源的事件&lt;/strong&gt;成为一项关键要求。&lt;/span&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;span&gt;想想付款、订单、客户元数据、&lt;/span&gt;&lt;a href=&#34;https://dzone.com/articles/type-of-sensors-and-actuators-in-iot&#34;&gt;&lt;span&gt;物联网传感器&lt;/span&gt;&lt;/a&gt;&lt;span&gt;、物流跟踪 - 一切都在持续流动。&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 27 Nov 2025 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Run LLMs Locally Using Ollama】使用 Ollama 在本地运行法学硕士</title>
      <link>https://dzone.com/articles/run-llms-locally-using-ollama</link>
      <description>【&lt;p dir=&#34;ltr&#34;&gt;Over the past few months, I’ve increasingly shifted my LLM experimentation from cloud APIs to running models directly on my laptop. The reason is simple: local inference has matured to the point where it’s fast, private, offline-friendly, and surprisingly easy to set up.&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;Tools like &lt;a href=&#34;https://dzone.com/articles/firebase-genkit-with-ollama&#34;&gt;Ollama&lt;/a&gt; have lowered the barrier dramatically. Instead of wrestling with GPU drivers, manually downloading weights, or wiring up custom runtimes, you get a single lightweight tool that can run models such as Llama 3.1, Mistral, Phi-3, DeepSeek R1, Gemma, and many others, all with minimal configuration.&lt;/p&gt;】&lt;p dir=&#34;ltr&#34;&gt;在过去的几个月里，我逐渐将 LLM 实验从云 API 转移到直接在笔记本电脑上运行模型。原因很简单：本地推理已经成熟到快速、私密、离线友好且易于设置的地步。&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;像 &lt;a href=&#34;https://dzone.com/articles/firebase-genkit-with-ollama&#34;&gt;Ollama&lt;/a&gt; 这样的工具极大地降低了障碍。您无需苦苦挣扎于 GPU 驱动程序、手动下载权重或连接自定义运行时，而是获得一个轻量级工具，可以运行 Llama 3.1、Mistral、Phi-3、DeepSeek R1、Gemma 等模型，所有这些都只需最少的配置。&lt;/p&gt;</description>
      <pubDate>Thu, 27 Nov 2025 16:30:00 +0000</pubDate>
    </item>
    <item>
      <title>【AWS Airflow vs Step Functions: The Data Engineering Orchestration Dilemma】AWS Airflow 与 Step Functions：数据工程编排困境</title>
      <link>https://dzone.com/articles/aws-airflow-vs-step-functions-the-data-engineering</link>
      <description>【&lt;p&gt;There&#39;s a moment in every data engineering project when you realize your growing collection of batch jobs, data transformations, and scheduled tasks needs proper orchestration. You&#39;ve probably duct-taped together some Lambda functions with CloudWatch Events, maybe written a few shell scripts with cron jobs, and now you&#39;re looking at AWS, wondering: should I go with Managed Airflow (MWAA) or Step Functions?&lt;/p&gt;&#xA;&lt;p&gt;I&#39;ve seen teams make both choices, and here&#39;s the truth: neither is universally &#34;better.&#34; The right answer depends on what you&#39;re actually building, who&#39;s maintaining it, and how your data engineering team thinks about workflows.&lt;/p&gt;】&lt;p&gt;在每个数据工程项目中，都有一个时刻，您会意识到不断增长的批处理作业、数据转换和计划任务需要适当的编排。您可能已经将一些 Lambda 函数与 CloudWatch Events 组合在一起，也许使用 cron 作业编写了一些 shell 脚本，现在您正在查看 AWS，想知道：我应该使用 Managed Airflow (MWAA) 还是 Step Functions？&lt;/p&gt;&#xA;&lt;p&gt;我见过团队做出这两种选择，事实是：这两种选择都不是普遍“更好”。正确的答案取决于您实际构建的内容、维护人员以及您的数据工程团队如何看待工作流程。&lt;/p&gt;</description>
      <pubDate>Thu, 27 Nov 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Automating FastAPI Deployments With a GitHub Actions Pipeline】使用 GitHub Actions 管道自动化 FastAPI 部署</title>
      <link>https://dzone.com/articles/fastapi-github-actions-deploy</link>
      <description>【&lt;p dir=&#34;ltr&#34;&gt;Deploying FastAPI apps manually gets old fast. You SSH into a server, pull the latest code, restart the service, and hope nothing breaks. Maybe you remember to run tests first. Maybe you don&#39;t.&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;One forgotten environment variable or skipped test, and your API is down. Users get 500 errors. You&#39;re frantically SSHing back in to fix it.&lt;/p&gt;】&lt;p dir=&#34;ltr&#34;&gt;手动部署 FastAPI 应用程序很快就会过时。您通过 SSH 连接到服务器，提取最新代码，重新启动服务，并希望不会出现任何问题。也许您记得先运行测试。也许你不知道。&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;忘记一个环境变量或跳过测试，并且您的 API 已关闭。用户收到 500 个错误。您疯狂地通过 SSH 重新连接来修复它。&lt;/p&gt;</description>
      <pubDate>Thu, 27 Nov 2025 15:30:00 +0000</pubDate>
    </item>
    <item>
      <title>【Optimizing Trino Performance With Materialized Views in a Data Lake】利用数据湖中的物化视图优化 Trino 性能</title>
      <link>https://dzone.com/articles/trino-materialized-views</link>
      <description>【&lt;p&gt;In this article, I share how we improved the performance of our Trino-based data lake by using materialized views. Our service evolved from a dual-storage system built on HBase and Elasticsearch to a simplified, cost-efficient data lake architecture powered by Iceberg, Spark Streaming, and Trino. The transition brought significant advantages but also unexpected performance challenges that we solved through careful use of Trino’s materialized views.&lt;/p&gt;&#xA;&lt;h2&gt;Business Description&lt;/h2&gt;&#xA;&lt;p&gt;Our service receives data from a Kafka source on three different topics and inserts it into HBase and Elasticsearch. HBase was used for get-by-ID operations, while &lt;a href=&#34;https://dzone.com/articles/introduction-to-elasticsearch-1&#34;&gt;Elasticsearch&lt;/a&gt; handled GraphQL-style search queries. HBase is known for excellent insert performance and fast get-by-ID operations, and Elasticsearch provides powerful full-text search capabilities. Over time, however, we realized that we were not using most of Elasticsearch’s advanced search features. Maintaining both systems was costly, and the operational complexity of supporting two clusters — HBase and Elasticsearch — was high. We decided to migrate to a modern data lake architecture to improve scalability and cost efficiency.&lt;/p&gt;】&lt;p&gt;在本文中，我将分享我们如何通过使用物化视图来提高基于 Trino 的数据湖的性能。我们的服务从基于 HBase 和 Elasticsearch 构建的双存储系统发展到由 Iceberg、Spark Streaming 和 Trino 提供支持的简化且经济高效的数据湖架构。这一转变带来了显着的优势，但也带来了意想不到的性能挑战，我们通过仔细使用 Trino 的物化视图解决了这些挑战。&lt;/p&gt;&#xA;&lt;h2&gt;业务描述&lt;/h2&gt;&#xA;&lt;p&gt;我们的服务从 Kafka 源接收关于三个不同主题的数据，并将其插入到 HBase 和 Elasticsearch 中。 HBase 用于按 ID 获取操作，而 &lt;a href=&#34;https://dzone.com/articles/introduction-to-elasticsearch-1&#34;&gt;Elasticsearch&lt;/a&gt; 处理 GraphQL 样式的搜索查询。 HBase以优异的插入性能和快速的按ID获取操作而闻名，Elasticsearch提供强大的全文搜索功能。然而，随着时间的推移，我们意识到我们并没有使用 Elasticsearch 的大部分高级搜索功能。维护这两个系统的成本很高，而且支持两个集群（HBase 和 Elasticsearch）的操作复杂性也很高。我们决定迁移到现代数据湖架构，以提高可扩展性和成本效率。&lt;/p&gt;</description>
      <pubDate>Thu, 27 Nov 2025 15:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【The Fake &#34;Multi&#34; in Multi-Tenant: When SaaS Tenancy Models Backfire at Scale】多租户中的假“多”：当 SaaS 租户模型大规模适得其反时</title>
      <link>https://dzone.com/articles/when-saas-tenancy-models-backfire-at-scale</link>
      <description>【&lt;h2 dir=&#34;ltr&#34;&gt;One SaaS, Many Users, One Big Lie&lt;/h2&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;Your “multi-tenant” SaaS architecture is probably a single-tenant app with commitment issues.&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;That sounds harsh until you look at the actual implementation. Customer A gets one deployment with hardcoded settings. Customer B gets the same codebase, but now wrapped in a flag-laden logic bomb. By the time you reach customer C, your team has a 60-page Confluence doc titled “How to onboard a new tenant without waking the VP of Engineering.”&lt;/p&gt;】&lt;h2 dir=&#34;ltr&#34;&gt;一个 SaaS，许多用户，一个弥天大谎&lt;/h2&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;您的“多租户”SaaS 架构可能是一个存在承诺问题的单租户应用程序。&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;在你看看实际的实现之前，这听起来很刺耳。客户 A 获得了一项具有硬编码设置的部署。客户 B 获得相同的代码库，但现在被包裹在一个充满标志的逻辑炸弹中。当您联系客户 C 时，您的团队已经有了一份 60 页的 Confluence 文档，标题为“如何在不唤醒工程副总裁的情况下加入新租户。”&lt;/p&gt;</description>
      <pubDate>Thu, 27 Nov 2025 13:00:06 +0000</pubDate>
    </item>
    <item>
      <title>【How to Push Docker Images to AWS Elastic Container Repository Using GitHub Actions】如何使用 GitHub Actions 将 Docker 映像推送到 AWS Elastic Container Repository</title>
      <link>https://dzone.com/articles/push-docker-images-to-aws-ecr-using-github-actions</link>
      <description>【&lt;p&gt;GitHub Actions enables the CI/CD, short for continuous integration or continuous deployment, process to build, test, and deploy the code through the workflows within the same GitHub repository. GitHub Actions builds images and pushes them to cloud providers such as AWS and Docker Hub. We can choose the different OS platforms, Windows or Linux, to run the workflows.&lt;/p&gt;&#xA;&lt;p&gt;In this article, we will demonstrate how we can streamline the build and deploy process to push Docker Images to AWS ECR, short for Elastic Container Repository, by using GitHub Actions.&lt;/p&gt;】&lt;p&gt;GitHub Actions 支持 CI/CD（持续集成或持续部署的缩写），通过同一 GitHub 存储库中的工作流程来构建、测试和部署代码。 GitHub Actions 构建镜像并将其推送到 AWS 和 Docker Hub 等云提供商。我们可以选择不同的操作系统平台（Windows 或 Linux）来运行工作流程。&lt;/p&gt;&#xA;&lt;p&gt;在本文中，我们将演示如何使用 GitHub Actions 简化构建和部署流程，将 Docker 映像推送到 AWS ECR（弹性容器存储库的缩写）。&lt;/p&gt;</description>
      <pubDate>Wed, 26 Nov 2025 20:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>