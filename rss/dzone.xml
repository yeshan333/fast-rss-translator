<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DZone.com Feed</title>
    <link>https://feeds.dzone.com/home</link>
    <description>Recent posts on DZone.com</description>
    <item>
      <title>【RAG at Scale: The Data Engineering Challenges】大规模 RAG：数据工程挑战</title>
      <link>https://dzone.com/articles/rag-at-scale-data-engineering-challenges</link>
      <description>【&lt;p dir=&#34;ltr&#34;&gt;Retrieval-augmented generation (RAG) has emerged as a powerful technique for building AI systems that can access and reason over external knowledge bases. RAG enabled us to build accurate and up-to-date systems by combining the content-generative capabilities of LLMs with user-context-specific, precise information retrieval.&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;However, deploying &lt;a href=&#34;https://dzone.com/articles/introduction-to-retrieval-augmented-generation-rag&#34;&gt;RAG systems&lt;/a&gt; at scale in production reveals a different reality that most blog posts and conference talks gloss over. While the core RAG concept is straightforward, the engineering challenges required to make it work reliably, efficiently, and cost-effectively at production scale are substantial and often underestimated.&lt;/p&gt;】&lt;p dir=&#34;ltr&#34;&gt;检索增强生成 (RAG) 已成为一种强大的技术，用于构建可以访问外部知识库并进行推理的人工智能系统。 RAG 使我们能够将法学硕士的内容生成功能与特定于用户上下文的精确信息检索相结合，构建准确且最新的系统。&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;但是，在生产中大规模部署&lt;a href=&#34;https://dzone.com/articles/introduction-to-retrieval-augmented- Generation-rag&#34;&gt;RAG 系统&lt;/a&gt;揭示了大多数博客文章和会议演讲所掩盖的不同现实。虽然核心 RAG 概念很简单，但要使其在生产规模上可靠、高效且经济高效地工作，所需的工程挑战是巨大的，而且往往被低估。&lt;/p&gt;</description>
      <pubDate>Fri, 16 Jan 2026 20:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【IT Asset, Vulnerability, and Patch Management Best Practices】IT 资产、漏洞和补丁管理最佳实践</title>
      <link>https://dzone.com/articles/it-assets-vulnerability-management-and-patches</link>
      <description>【&lt;p&gt;The vulnerability management lifecycle is a continuous process for discovering, addressing, and prioritizing vulnerabilities in an organization&#39;s IT assets&lt;/p&gt;&#xA;&lt;p&gt;A normal round of the lifecycle has five phases:&lt;/p&gt;】&lt;p&gt;漏洞管理生命周期是一个持续的过程，用于发现、解决组织 IT 资产中的漏洞并确定其优先级&lt;/p&gt;&#xA;&lt;p&gt;正常的生命周期有五个阶段：&lt;/p&gt;</description>
      <pubDate>Fri, 16 Jan 2026 19:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Speeding Up BigQuery Reads in Apache Beam/Dataflow】加快 Apache Beam/Dataflow 中的 BigQuery 读取速度</title>
      <link>https://dzone.com/articles/bigquery-read-optimization-beam-dataflow</link>
      <description>【&lt;p&gt;Real‑time and overnight data pipelines often succeed or fail on one thing: &lt;strong&gt;Can you move enough data through BigQuery and Dataflow within your SLA window&lt;/strong&gt;?&lt;/p&gt;&#xA;&lt;p&gt;In a production &lt;a href=&#34;https://dzone.com/articles/apache-beam-working-with-files&#34;&gt;Apache Beam&lt;/a&gt;/Dataflow environment, several large jobs started to miss their daily deadlines after a Beam upgrade. All of them shared a pattern:&lt;/p&gt;】&lt;p&gt;实时和隔夜数据管道通常会在一件事上成功或失败：&lt;strong&gt;您能否在 SLA 窗口内通过 BigQuery 和 Dataflow 移动足够的数据&lt;/strong&gt;？&lt;/p&gt;&#xA;&lt;p&gt;在生产 &lt;a href=&#34;https://dzone.com/articles/apache-beam-working-with-files&#34;&gt;Apache Beam&lt;/a&gt;/Dataflow 环境中，Beam 升级后，一些大型作业开始错过每日截止日期。他们都有一个共同的模式：&lt;/p&gt;</description>
      <pubDate>Fri, 16 Jan 2026 18:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【From RAG to RAG + RAV: A Practical Pipeline for Factual LLM Responses】从 RAG 到 RAG + RAV：实际 LLM 响应的实用管道</title>
      <link>https://dzone.com/articles/rag-rag-rav-pipeline-factual-llm</link>
      <description>【&lt;p style=&#34;text-align: left;&#34;&gt;Recently, I&#39;ve been working on a project where getting the factual data right was absolutely critical. I’ll be honest, when I first wired up a &lt;a href=&#34;https://dzone.com/articles/introduction-to-retrieval-augmented-generation-rag&#34;&gt;retrieval-augmented generation&lt;/a&gt; (RAG) system, I thought I was mostly done with hallucinations. I had:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA; &lt;li style=&#34;text-align: left;&#34;&gt;A vector DB full of documents&lt;/li&gt;&#xA; &lt;li style=&#34;text-align: left;&#34;&gt;A decent embedding model&lt;/li&gt;&#xA; &lt;li style=&#34;text-align: left;&#34;&gt;A prompt that said &#34;answer only using the context above.&#34;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p style=&#34;text-align: left;&#34;&gt;And yet I still got answers that &lt;em data-end=&#34;387&#34; data-start=&#34;379&#34;&gt;looked&lt;/em&gt; grounded but contained subtle factual errors: wrong years, swapped names, invented details that weren&#39;t in any source.&lt;/p&gt;】&lt;p style=&#34;text-align: left;&#34;&gt;最近，我一直在从事一个项目，其中获取正确的事实数据绝对至关重要。老实说，当我第一次连接&lt;a href=&#34;https://dzone.com/articles/introduction-to-retrieval-augmented- Generation-rag&#34;&gt;检索增强生成&lt;/a&gt;（RAG）系统时，我以为我已经完成了幻觉。我有：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA; &lt;li style=&#34;text-align: left;&#34;&gt;充满文档的矢量数据库&lt;/li&gt;&#xA; &lt;li style=&#34;text-align: left;&#34;&gt;一个不错的嵌入模型&lt;/li&gt;&#xA; &lt;li style=&#34;text-align: left;&#34;&gt;提示“仅使用上面的上下文进行回答。”&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p style=&#34;text-align: left;&#34;&gt;然而，我仍然得到了&lt;em data-end=&#34;387&#34; data-start=&#34;379&#34;&gt;看起来&lt;/em&gt;有根据的答案，但其中包含微妙的事实错误：错误的年份、交换的名称、捏造的细节，这些细节在任何来源中都没有。&lt;/p&gt;</description>
      <pubDate>Fri, 16 Jan 2026 17:00:01 +0000</pubDate>
    </item>
    <item>
      <title>【Micro Frontends in Angular and React: A Deep Technical Guide for Scalable Front-End Architecture】Angular 和 React 中的微前端：可扩展前端架构的深入技术指南</title>
      <link>https://dzone.com/articles/micro-frontends-in-angular-and-react-a-deep-techni</link>
      <description>【&lt;p data-end=&#34;1417&#34; data-start=&#34;1362&#34;&gt;Micro-frontends allow large teams to build independent UI modules that ship autonomously. Angular and React both support micro-frontend architecture using Webpack Module Federation. Angular benefits from strong structure and RxJS-based shared services, while React provides lightweight, flexible federated components. A hybrid Angular-React MFE system typically follows a shell-and-remotes architecture, with shared libraries, version-safe dependencies, and independent deployments.&lt;/p&gt;&#xA;&lt;h2 data-end=&#34;1417&#34; data-start=&#34;1362&#34;&gt;&lt;strong data-end=&#34;1417&#34; data-start=&#34;1364&#34;&gt;What Micro Frontends Are (and Why They Matter)&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p data-end=&#34;1540&#34; data-start=&#34;1419&#34;&gt;&lt;a href=&#34;https://dzone.com/articles/micro-frontends-architecture&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;Micro frontends&lt;/a&gt; split a large UI into independently developed and deployed applications that compose together at runtime.&lt;/p&gt;】&lt;p data-end=&#34;1417&#34; data-start=&#34;1362&#34;&gt;微前端允许大型团队构建自主发布的独立 UI 模块。 Angular 和 React 都支持使用 Webpack Module Federation 的微前端架构。 Angular 受益于强大的结构和基于 RxJS 的共享服务，而 React 提供轻量级、灵活的联合组件。混合 Angular-React MFE 系统通常遵循 shell 和远程架构，具有共享库、版本安全依赖项和独立部署。&lt;/p&gt;&#xA;&lt;h2 data-end=&#34;1417&#34; data-start=&#34;1362&#34;&gt;&lt;strong data-end=&#34;1417&#34; data-start=&#34;1364&#34;&gt;什么是微前端（以及它们为何重要）&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p data-end=&#34;1540&#34; data-start=&#34;1419&#34;&gt;&lt;a href=&#34;https://dzone.com/articles/micro-frontends-architecture&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;微前端&lt;/a&gt;将大型 UI 拆分为独立开发和部署的应用程序，这些应用程序在运行时组合在一起。&lt;/p&gt;</description>
      <pubDate>Fri, 16 Jan 2026 16:00:12 +0000</pubDate>
    </item>
    <item>
      <title>【From Chatbot to Agent: Implementing the ReAct Pattern in Python】从聊天机器人到代理：在 Python 中实现 ReAct 模式</title>
      <link>https://dzone.com/articles/chatbot-to-agent-react-pattern-python</link>
      <description>【&lt;h2 data-path-to-node=&#34;5&#34;&gt;The Problem: The Limits of a Static Chatbot&lt;/h2&gt;&#xA;&lt;p data-path-to-node=&#34;6&#34;&gt;Most developers have mastered the basic LLM API call: send a prompt, get a completion. This works perfectly for summarization, sentiment analysis, or creative writing.&lt;/p&gt;&#xA;&lt;p data-path-to-node=&#34;7&#34;&gt;However, this architecture fails in real-world engineering scenarios where the application needs accurate, real-time information or needs to perform actions. If you ask a standard GPT-4 implementation: &lt;i data-index-in-node=&#34;202&#34; data-path-to-node=&#34;7&#34;&gt;&#34;What is the current stock price of Datadog multiplied by 1.5?&#34;&lt;/i&gt;, it will fail.&lt;/p&gt;】&lt;h2 data-path-to-node=&#34;5&#34;&gt;问题：静态聊天机器人的局限性&lt;/h2&gt;&#xA;&lt;p data-path-to-node=&#34;6&#34;&gt;大多数开发人员已经掌握了基本的 LLM API 调用：发送提示，获得完成。这非常适合摘要、情感分析或创意写作。&lt;/p&gt;&#xA;&lt;p data-path-to-node=&#34;7&#34;&gt;但是，这种架构在应用程序需要准确、实时信息或需要执行操作的实际工程场景中会失败。如果您询问标准 GPT-4 实现：&lt;i data-index-in-node=&#34;202&#34; data-path-to-node=&#34;7&#34;&gt;“Datadog 当前股价乘以 1.5 是多少？”&lt;/i&gt;，它将失败。&lt;/p&gt;</description>
      <pubDate>Fri, 16 Jan 2026 15:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Parallel S3 Writes for Massive Sparse DataFrames: How to Maintain Row Order Without Blowing Memory】大规模稀疏数据帧的并行 S3 写入：如何在不耗尽内存的情况下维护行顺序</title>
      <link>https://dzone.com/articles/parallel-s3-writes-massive-sparse-dataframes</link>
      <description>【&lt;div dir=&#34;ltr&#34;&gt;&#xA; &lt;p data-path-to-node=&#34;4&#34;&gt;&lt;span style=&#34;margin: 0px; padding: 0px;&#34;&gt;If you’ve worked with large-scale machine learning pipelines, you must know one of the most frustrating bottlenecks isn’t always found in the complexity of the model or the elegance of the architecture — it&#39;s writing the output efficiently.&lt;/span&gt;&lt;/p&gt;&#xA; &lt;p data-path-to-node=&#34;5&#34;&gt;Recently, I found myself navigating a complex data engineering hurdle where I needed to write a &lt;b data-index-in-node=&#34;96&#34; data-path-to-node=&#34;0&#34;&gt;massive Pandas sparse DataFrame&amp;nbsp;&lt;/b&gt;— the high-dimensional output of a CountVectorizer — directly to Amazon S3. By massive, I mean &lt;b data-index-in-node=&#34;219&#34; data-path-to-node=&#34;0&#34;&gt;tens of gigabytes&lt;/b&gt; of feature data stored in a memory-efficient sparse format that needed to be materialized as a raw CSV file. This legacy requirement existed because our downstream machine learning model was specifically built to ingest only that format, leaving us with a significant I/O challenge that threatened to derail our entire processing timeline.&lt;/p&gt;】&lt;div 目录=“ltr”&gt;&#xA; &lt;p data-path-to-node=&#34;4&#34;&gt;&lt;span style=&#34;margin: 0px; padding: 0px;&#34;&gt;如果您使用过大规模机器学习管道，您一定知道最令人沮丧的瓶颈之一并不总是存在于模型的复杂性或架构的优雅中 - 而是高效地编写输出。&lt;/span&gt;&lt;/p&gt;&#xA; &lt;p data-path-to-node=&#34;5&#34;&gt;最近，我发现自己遇到了一个复杂的数据工程障碍，需要将&lt;b data-index-in-node=&#34;96&#34; data-path-to-node=&#34;0&#34;&gt;海量 Pandas 稀疏数据帧&lt;/b&gt;（CountVectorizer 的高维输出）直接写入 Amazon S3。我所说的大规模是指以内存高效的稀疏格式存储的&lt;b data-index-in-node=&#34;219&#34; data-path-to-node=&#34;0&#34;&gt;数十 GB&lt;/b&gt; 的特征数据，需要将其具体化为原始 CSV 文件。这一遗留要求的存在是因为我们的下游机器学习模型是专门为摄取该格式而构建的，这给我们带来了重大的 I/O 挑战，可能会破坏我们的整个处理时间线。&lt;/p&gt;</description>
      <pubDate>Fri, 16 Jan 2026 14:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Integrating CUDA-Q with Amazon Bedrock AgentCore: A Technical Deep Dive】将 CUDA-Q 与 Amazon Bedrock AgentCore 集成：技术深入探讨</title>
      <link>https://dzone.com/articles/integrating-cuda-q-with-amazon-bedrock-agentcore</link>
      <description>【&lt;h2&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;The convergence of quantum computing and artificial intelligence represents one of the most exciting frontiers in modern computing. This article explores how to integrate NVIDIA&#39;s CUDA-Q framework with Amazon Bedrock AgentCore, enabling AI agents to leverage GPU-accelerated quantum circuit simulations within their operational workflows. This integration combines Amazon Braket&#39;s quantum computing capabilities with Bedrock&#39;s robust agent orchestration platform.&lt;/p&gt;&#xA;&lt;h2&gt;Understanding the Technology Stack&lt;/h2&gt;&#xA;&lt;h3&gt;CUDA-Q: GPU-Accelerated Quantum Simulation&lt;/h3&gt;&#xA;&lt;p&gt;CUDA-Q is NVIDIA&#39;s open-source platform for hybrid quantum-classical computing. It enables developers to:&lt;/p&gt;】&lt;h2&gt;简介&lt;/h2&gt;&#xA;&lt;p&gt;量子计算和人工智能的融合代表了现代计算中最令人兴奋的前沿之一。本文探讨了如何将 NVIDIA 的 CUDA-Q 框架与 Amazon Bedrock AgentCore 集成，使 AI 代理能够在其操作工作流程中利用 GPU 加速的量子电路模拟。这种集成将 Amazon Braket 的量子计算功能与 Bedrock 强大的代理编排平台相结合。&lt;/p&gt;&#xA;&lt;h2&gt;了解技术堆栈&lt;/h2&gt;&#xA;&lt;h3&gt;CUDA-Q：GPU 加速的量子模拟&lt;/h3&gt;&#xA;&lt;p&gt;CUDA-Q 是 NVIDIA 用于混合量子经典计算的开源平台。它使开发人员能够：&lt;/p&gt;</description>
      <pubDate>Fri, 16 Jan 2026 13:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【RAG on Android Done Right: Local Vector Cache Plus Cloud Retrieval Architecture】Android 上的 RAG 做得很好：本地矢量缓存加上云检索架构</title>
      <link>https://dzone.com/articles/rag-android-local-cache-cloud-retrieval</link>
      <description>【&lt;h2&gt;Why “Classic RAG” Breaks on Android&lt;/h2&gt;&#xA;&lt;p&gt;On paper, &lt;a href=&#34;https://dzone.com/articles/mastering-retrieval-augmented-generation&#34;&gt;retrieval-augmented generation&lt;/a&gt; is straightforward: embed the query, retrieve the top chunks, stuff them into a prompt, and generate an answer with citations. On Android, that “classic” flow runs into real constraints:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA; &lt;li&gt;&lt;strong&gt;Latency budgets are tight&lt;/strong&gt;. Users feel delays instantly, especially inside chat-like UIs.&lt;/li&gt;&#xA; &lt;li&gt;&lt;strong&gt;Networks are unreliable&lt;/strong&gt;. RAG becomes brittle when your retrieval depends on a perfect connection.&lt;/li&gt;&#xA; &lt;li&gt;&lt;strong&gt;Privacy expectations are higher&lt;/strong&gt;. Users assume mobile experiences are local-first, especially for enterprise or personal data.&lt;/li&gt;&#xA; &lt;li&gt;&lt;strong&gt;Resources are limited&lt;/strong&gt;. Battery, memory, and storage don’t tolerate “just cache everything.”&lt;/li&gt;&#xA; &lt;li&gt;&lt;strong&gt;Cold start is unforgiving&lt;/strong&gt;. If the first answer is slow or wrong, you lose trust quickly.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;So the goal isn’t “RAG everywhere.” The goal is first &lt;span style=&#34;margin: 0px; padding: 0px;&#34;&gt;to find a helpful answer quickly, then to&amp;nbsp;&lt;/span&gt;upgrade the grounding when the cloud is available. That’s exactly what a two-tier system provides.&lt;/p&gt;】&lt;h2&gt;为什么“Classic RAG”在 Android 上失败&lt;/h2&gt;&#xA;&lt;p&gt;在纸面上，&lt;a href=&#34;https://dzone.com/articles/mastering-retrieval-augmented- Generation&#34;&gt;检索增强生成&lt;/a&gt;很简单：嵌入查询，检索顶部块，将它们填充到提示中，并生成带有引用的答案。在 Android 上，这种“经典”流程遇到了真正的限制：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA; &lt;li&gt;&lt;strong&gt;延迟预算很紧张&lt;/strong&gt;。用户会立即感受到延迟，尤其是在类似聊天的 UI 中。&lt;/li&gt;&#xA; &lt;li&gt;&lt;strong&gt;网络不可靠&lt;/strong&gt;。当您的检索依赖于完美连接时，RAG 就会变得脆弱。&lt;/li&gt;&#xA; &lt;li&gt;&lt;strong&gt;隐私期望更高&lt;/strong&gt;。用户认为移动体验是本地优先的，尤其是对于企业或个人数据而言。&lt;/li&gt;&#xA; &lt;li&gt;&lt;strong&gt;资源有限&lt;/strong&gt;。电池、内存和存储不能容忍“只缓存所有内容”。&lt;/li&gt;&#xA; &lt;li&gt;&lt;strong&gt;冷启动是无情的&lt;/strong&gt;。如果第一个答案很慢或错误，您很快就会失去信任。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;所以我们的目标不是“让 RAG 无处不在”。我们的目标首先是&lt;span style=&#34;margin: 0px; padding: 0px;&#34;&gt;快速找到有用的答案，然后&lt;/span&gt;在云可用时升级基础。这正是两层系统所提供的。&lt;/p&gt;</description>
      <pubDate>Fri, 16 Jan 2026 12:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Securing AI-Generated Code: Preventing Phantom APIs and Invisible Vulnerabilities】保护人工智能生成的代码：防止幻像 API 和隐形漏洞</title>
      <link>https://dzone.com/articles/securing-ai-generated-code-preventing-phantom-apis</link>
      <description>【&lt;p data-end=&#34;740&#34; data-start=&#34;247&#34;&gt;The conference room went silent when the fintech&#39;s CISO pulled up the logs. There, buried in production traffic, sat an endpoint nobody had documented: &lt;code data-end=&#34;417&#34; data-start=&#34;399&#34;&gt;/api/debug/users&lt;/code&gt;. It was leaking customer data with every ping. The engineer who&#39;d committed the module swore he&#39;d only asked GitHub Copilot for a &#34;basic user lookup function.&#34; Somewhere between prompt and pull request, the AI had dreamed up an entire debugging interface — and nobody caught it until a pentester found it three months later.&lt;/p&gt;&#xA;&lt;p data-end=&#34;937&#34; data-start=&#34;742&#34;&gt;That incident, which happened at a Series B startup in Austin last spring, isn&#39;t an outlier anymore. It’s a preview of what happens when we let machines write code faster than humans can read it.&lt;/p&gt;】&lt;p data-end=&#34;740&#34; data-start=&#34;247&#34;&gt;当金融科技的 CISO 调出日志时，会议室一片寂静。在那里，埋藏在生产流量中的是一个无人记录的端点：&lt;code data-end=&#34;417&#34; data-start=&#34;399&#34;&gt;/api/debug/users&lt;/code&gt;。每次 ping 操作都会泄露客户数据。提交该模块的工程师发誓，他只是向 GitHub Copilot 询问了“基本用户查找功能”。在提示和拉取请求之间的某个地方，人工智能设想了一个完整的调试界面——直到三个月后渗透测试人员发现它为止，没有人发现它。&lt;/p&gt;&#xA;&lt;p data-end=&#34;937&#34; data-start=&#34;742&#34;&gt;去年春天在奥斯汀的一家 B 轮初创公司发生的这一事件已不再是异常情况。这是当我们让机器编写代码的速度比人类阅读代码的速度快时会发生什么的预览。&lt;/p&gt;</description>
      <pubDate>Thu, 15 Jan 2026 20:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>