<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DZone.com Feed</title>
    <link>https://feeds.dzone.com/home</link>
    <description>Recent posts on DZone.com</description>
    <item>
      <title>【From LLMs to Agents: How BigID is Enabling Secure Agentic AI for Data Governance】从法学硕士到代理：BigID 如何为数据治理实现安全的代理 AI</title>
      <link>https://dzone.com/articles/from-llms-to-agents-how-bigid-is-enabling-secure-a-1</link>
      <description>【&lt;h2 data-end=&#34;283&#34; data-start=&#34;238&#34;&gt;Understanding Large Language Models (LLMs)&lt;/h2&gt;&#xA;&lt;p data-end=&#34;783&#34; data-start=&#34;285&#34;&gt;Large Language Models (LLMs) form the foundation of &lt;a href=&#34;https://dzone.com/articles/top-generative-ai-services&#34;&gt;most generative AI innovations&lt;/a&gt;. These models are predictive engines trained on massive datasets, often spanning hundreds of billions of tokens. For example, ChatGPT was trained on nearly 56 terabytes of data, enabling it to predict the next word or token in a sequence with remarkable accuracy. The result is an AI system capable of generating human-like text, completing prompts, answering questions, and even reasoning through structured tasks.&lt;/p&gt;&#xA;&lt;p data-end=&#34;1296&#34; data-start=&#34;785&#34;&gt;At their core, LLMs are not databases of facts but statistical predictors. They excel at mimicking natural language and surfacing patterns seen in their training data. However, they are static once trained. If a model is trained on data that is five or ten years old, it cannot natively answer questions about newer developments unless it is updated or augmented with real-time sources. This limitation makes pure LLMs insufficient in enterprise contexts where accuracy, compliance, and timeliness are critical.&lt;/p&gt;】&lt;h2 data-end=&#34;283&#34; data-start=&#34;238&#34;&gt;了解大型语言模型 (LLM)&lt;/h2&gt;&#xA;&lt;p data-end=&#34;783&#34; data-start=&#34;285&#34;&gt;大型语言模型 (LLM) 构成了&lt;a href=&#34;https://dzone.com/articles/top-generative-ai-services&#34;&gt;大多数生成式 AI 创新&lt;/a&gt;的基础。这些模型是在海量数据集上训练的预测引擎，通常涵盖数千亿个代币。例如，ChatGPT 经过近 56 TB 数据的训练，使其能够以极高的准确性预测序列中的下一个单词或标记。其结果是人工智能系统能够生成类似人类的文本、完成提示、回答问题，甚至通过结构化任务进行推理。&lt;/p&gt;&#xA;&lt;p data-end=&#34;1296&#34; data-start=&#34;785&#34;&gt;从本质上讲，LLM 不是事实数据库，而是统计预测变量。他们擅长模仿自然语言和呈现训练数据中看到的模式。然而，一旦训练完毕，它们就处于静止状态。如果模型是根据五年或十年前的数据进行训练的，则它无法本地回答有关新开发的问题，除非使用实时源进行更新或增强。这种限制使得纯法学硕士在准确性、合规性和及时性至关重要的企业环境中显得不够。&lt;/p&gt;</description>
      <pubDate>Fri, 30 Jan 2026 20:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Testcontainers Explained: Bringing Real Services to Your Test Suite】测试容器解释：为您的测试套件带来真正的服务</title>
      <link>https://dzone.com/articles/testcontainers-explained-bringing-real-services-to</link>
      <description>【&lt;p data-end=&#34;709&#34; data-start=&#34;210&#34;&gt;Building robust, enterprise-grade applications requires more than just writing code — it demands reliable automated testing. These tests come in different forms, from unit tests that validate small pieces of logic to integration tests that ensure multiple components work together correctly. Integration tests can be designed as white-box (where internal workings are visible) or black-box (where only inputs and outputs matter). Regardless of style, they are a critical part of every release cycle.&lt;/p&gt;&#xA;&lt;p data-end=&#34;1008&#34; data-start=&#34;711&#34;&gt;Modern enterprise applications rarely operate in isolation. They often have to interact with external components like databases, message queues, APIs, and other services. To validate these interactions, integration tests typically rely on either real instances of components or mocked substitutes.&lt;/p&gt;】&lt;p data-end=&#34;709&#34; data-start=&#34;210&#34;&gt;构建强大的企业级应用程序需要的不仅仅是编写代码 - 它还需要可靠的自动化测试。这些测试有不同的形式，从验证小块逻辑的单元测试到确保多个组件正确协同工作的集成测试。集成测试可以设计为白盒（内部工作可见）或黑盒（仅输入和输出重要）。无论风格如何，它们都是每个发布周期的关键部分。&lt;/p&gt;&#xA;&lt;p data-end=&#34;1008&#34; data-start=&#34;711&#34;&gt;现代企业应用程序很少单独运行。它们通常必须与数据库、消息队列、API 和其他服务等外部组件进行交互。为了验证这些交互，集成测试通常依赖于组件的真实实例或模拟替代品。&lt;/p&gt;</description>
      <pubDate>Fri, 30 Jan 2026 19:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【ToolOrchestra vs Mixture of Experts: Routing Intelligence at Scale】ToolOrchestra 与混合专家：大规模路由智能</title>
      <link>https://dzone.com/articles/toolorchestra-vs-mixture-of-experts-routing-intelligence</link>
      <description>【&lt;p&gt;Last year, I came across Mixture of Experts (MoE) through this &lt;a href=&#34;https://research.ibm.com/publications/efficient-scaling-of-large-language-models-with-mixture-of-experts-and-3d-analog-in-memory-computing&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;research paper published&lt;/a&gt; in Nature. Later in 2025, Nvidia published a &lt;a href=&#34;https://research.nvidia.com/labs/lpr/ToolOrchestra/&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;research paper on ToolOrchestra&lt;/a&gt;&lt;a href=&#34;https://research.nvidia.com/labs/lpr/ToolOrchestra/&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;. While reading the paper, I kept thinking about MoE and how ToolOrchestra is similar to or different from it.&lt;/p&gt;&#xA;&lt;p&gt;In this article, you will learn about two fundamental architectural patterns reshaping how we build intelligent systems. We&#39;ll explore ToolOrchestra and Mixture of Experts (MoE), understand their inner workings, compare them with other routing-based architectures, and discover how they can work together.&lt;/p&gt;】&lt;p&gt;去年，我通过《自然》杂志上发表的这篇&lt;a href=&#34;https://research.ibm.com/publications/efficient-scaling-of-large-language-models-with-mixture-of-experts-and-3d-analog-in-memory-computing&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;研究论文&lt;/a&gt;了解到了专家混合 (MoE)。 2025 年晚些时候，Nvidia 发布了&lt;a href=&#34;https://research.nvidia.com/labs/lpr/ToolOrchestra/&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;关于 ToolOrchestra 的研究论文&lt;/a&gt;&lt;a href=&#34;https://research.nvidia.com/labs/lpr/ToolOrchestra/&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;。在阅读这篇论文的过程中，我一直在思考 MoE 以及 ToolOrchestra 与它有何相似或不同。&lt;/p&gt;&#xA;&lt;p&gt;在本文中，您将了解两种重塑我们构建智能系统方式的基本架构模式。我们将探索 ToolOrchestra 和 Mixture of Experts (MoE)，了解它们的内部工作原理，将它们与其他基于路由的架构进行比较，并发现它们如何协同工作。&lt;/p&gt;</description>
      <pubDate>Fri, 30 Jan 2026 18:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Ralph Wiggum Ships Code While You Sleep. Agile Asks: Should It?】Ralph Wiggum 在你睡觉时发送代码。敏捷问：应该吗？</title>
      <link>https://dzone.com/articles/ralph-wiggum-ships-code-agile-should-it</link>
      <description>【&lt;h2&gt;TL; DR: When Code Is Cheap, Discipline Must Come from Somewhere Else&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://dzone.com/articles/introduction-generative-ai-empowering-enterprises&#34;&gt;Generative AI&lt;/a&gt; removes the natural constraint that expensive engineers imposed on software development. When building costs almost nothing, the question shifts from “can we build it?” to “should we build it?” The Agile Manifesto’s principles provide the discipline that these costs are used to enforce. Ignore them at your peril when Ralph Wiggum meets Agile.&lt;/p&gt;&#xA;&lt;h2&gt;The Nonsense About AI and Agile&lt;/h2&gt;&#xA;&lt;p&gt;Your LinkedIn feed is full of confident nonsense about Scrum and AI.&lt;/p&gt;】&lt;h2&gt;TL; DR：当代码很便宜时，纪律必须来自其他地方&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://dzone.com/articles/introduction-generative-ai-empowering-enterprises&#34;&gt;生成式人工智能&lt;/a&gt;消除了昂贵的工程师对软件开发施加的自然限制。当建造成本几乎为零时，问题就从“我们能建造它吗？”转变为“我们能建造它吗？” “我们应该建造它吗？”敏捷宣言的原则提供了使用这些成本来执行的纪律。当 Ralph Wiggum 遇到敏捷时，忽视它们将带来危险。&lt;/p&gt;&#xA;&lt;h2&gt;关于人工智能和敏捷的废话&lt;/h2&gt;&#xA;&lt;p&gt;你的 LinkedIn feed 中充满了关于 Scrum 和 AI 的自信废话。&lt;/p&gt;</description>
      <pubDate>Fri, 30 Jan 2026 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Essential Techniques for Production Vector Search Systems, Part 3: Filterable HNSW】生产型矢量搜索系统的基本技术，第 3 部分：可过滤的 HNSW</title>
      <link>https://dzone.com/articles/filterable-hnsw-production-vector-search-part-3</link>
      <description>【&lt;p data-end=&#34;434&#34; data-start=&#34;240&#34;&gt;After implementing vector search systems at multiple companies, I wanted to document efficient techniques that can be very helpful for successful production deployments of &lt;a href=&#34;https://dzone.com/articles/vector-search-hottest-topic-in-information-retrieval&#34;&gt;vector search systems&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p data-end=&#34;741&#34; data-start=&#34;436&#34;&gt;I want to present these techniques by showcasing when to apply each one, how they complement each other, and the trade-offs they introduce. This will be a multi-part series that introduces all of the techniques one by one in each article. I have also included code snippets to quickly test each technique.&lt;/p&gt;】&lt;p data-end=&#34;434&#34; data-start=&#34;240&#34;&gt;在多家公司实施矢量搜索系统后，我想记录一些有效的技术，这些技术对于&lt;a href=&#34;https://dzone.com/articles/vector-search-hottest-topic-in-information-retrieval&#34;&gt;矢量搜索系统&lt;/a&gt;的成功生产部署非常有帮助。&lt;/p&gt;&#xA;&lt;p data-end=&#34;741&#34; data-start=&#34;436&#34;&gt;我想通过展示何时应用每种技术、它们如何相互补充以及它们引入的权衡来展示这些技术。这将是一个由多部分组成的系列，每篇文章都会一一介绍所有技术。我还提供了代码片段来快速测试每种技术。&lt;/p&gt;</description>
      <pubDate>Fri, 30 Jan 2026 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【TPU vs GPU: Real-World Performance Testing for LLM Training on Google Cloud】TPU 与 GPU：Google Cloud 上的 LLM 训练的真实性能测试</title>
      <link>https://dzone.com/articles/tpu-vs-gpu-real-world-performance-testing-for-llm</link>
      <description>【&lt;p data-selectable-paragraph=&#34;&#34;&gt;As large language models (LLMs) continue to grow in scale, the underlying hardware used for training has become the single most critical factor in a project’s success. The industry is currently locked in a fascinating architectural battle: the general-purpose power of NVIDIA’s GPUs versus the purpose-built efficiency of Google’s Tensor Processing Units (TPUs).&lt;/p&gt;&#xA;&lt;p data-selectable-paragraph=&#34;&#34;&gt;For engineers and architects building on &lt;a href=&#34;https://dzone.com/articles/google-cloud-platform-a-comprehensive-suite-of-clo?fromrel=true&#34;&gt;Google Cloud Platform&lt;/a&gt; (GCP), the choice between an A100/H100 GPU cluster and a TPU v4/v5p pod is not merely a matter of cost — it is a decision that impacts software architecture, data pipelines, and convergence speed. This article provides a deep-dive technical analysis of these two architectures through the lens of real-world LLM training performance.&lt;/p&gt;】&lt;p data-selectable-paragraph=&#34;&#34;&gt;随着大型语言模型 (LLM) 规模的不断扩大，用于训练的底层硬件已成为项目成功的最关键因素。该行业目前陷入了一场引人入胜的架构大战：NVIDIA GPU 的通用能力与 Google 张量处理单元 (TPU) 的专用效率。&lt;/p&gt;&#xA;&lt;p data-selectable-paragraph=&#34;&#34;&gt;对于在 &lt;a href=&#34;https://dzone.com/articles/google-cloud-platform-a-compressive-suite-of-clo?fromrel=true&#34;&gt;Google Cloud Platform&lt;/a&gt; (GCP) 上构建的工程师和架构师来说，A100/H100 GPU 集群和 TPU v4/v5p pod 之间的选择不仅仅是成本问题，而是影响软件架构、数据管道和融合的决策速度。本文通过现实世界的 LLM 训练性能的视角，对这两种架构进行了深入的技术分析。&lt;/p&gt;</description>
      <pubDate>Fri, 30 Jan 2026 15:00:03 +0000</pubDate>
    </item>
    <item>
      <title>【Automating TDD: Using AI to Generate Edge-Case Unit Tests】自动化 TDD：使用 AI 生成边缘情况单元测试</title>
      <link>https://dzone.com/articles/automating-tdd-ai-edge-case-tests</link>
      <description>【&lt;h2 data-path-to-node=&#34;5&#34;&gt;The Problem: The &#34;Happy Path&#34; Trap in TDD&lt;/h2&gt;&#xA;&lt;p data-path-to-node=&#34;6&#34;&gt;&lt;a href=&#34;https://dzone.com/articles/the-importance-of-test-driven-development-in-softw&#34;&gt;Test-driven development&lt;/a&gt; (Red-Green-Refactor) is the gold standard for reliable software. However, it has a flaw: The quality of your code is capped by the imagination of your test cases.&lt;/p&gt;&#xA;&lt;p data-path-to-node=&#34;7&#34;&gt;If you are building a payment processing function, you will naturally write a test for &#34;valid payment.&#34; You might even remember &#34;insufficient funds.&#34; But will you remember to test for:&lt;/p&gt;】&lt;h2 data-path-to-node=&#34;5&#34;&gt;问题：TDD 中的“幸福之路”陷阱&lt;/h2&gt;&#xA;&lt;p data-path-to-node=&#34;6&#34;&gt;&lt;a href=&#34;https://dzone.com/articles/the-importance-of-test-driven-development-in-softw&#34;&gt;测试驱动开发&lt;/a&gt;（红绿重构）是可靠软件的黄金标准。然而，它有一个缺陷：代码的质量受到测试用例的想象力的限制。&lt;/p&gt;&#xA;&lt;p data-path-to-node=&#34;7&#34;&gt;如果您正在构建支付处理功能，您自然会编写“有效支付”的测试。您甚至可能还记得“资金不足”。但您会记得测试一下：&lt;/p&gt;</description>
      <pubDate>Fri, 30 Jan 2026 14:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Designing Irreversible Security Release at Hyper-Scale: Lessons Learned From Things You Can’t Undo】设计超大规模的不可逆安全发布：从无法撤销的事情中吸取的教训</title>
      <link>https://dzone.com/articles/designing-irreversible-security-releases-hyperscale</link>
      <description>【&lt;h2 dir=&#34;ltr&#34;&gt;&lt;strong&gt;What Makes a Change Irreversible?&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;Reverting a line of code is easy, and most of the time, firmware is backward-compatible. But what if a piece of hardware is specifically designed not to take older firmware, and the only option is to fix it with a new version?&amp;nbsp;&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;You could argue: Why design the hardware in such a manner? Well, it could be for a myriad of reasons, including a hardware design bug, a security hash algorithm that was a one-way function, or an older firmware bug that&#39;s being fixed in the newer release. It&#39;s easy to update the software behavior if needed, but it&#39;s not possible to change any hardware behavior. So we go to the next best option — mimic software to accept the hardware flaw and invert the operation on the software side.&amp;nbsp;&lt;/p&gt;】&lt;h2 dir=&#34;ltr&#34;&gt;&lt;strong&gt;什么使更改不可逆转？&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;恢复一行代码很容易，而且大多数时候，固件是向后兼容的。但是，如果某个硬件专门设计为不采用旧固件，而唯一的选择是使用新版本修复它，该怎么办？ &lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;您可能会争论：为什么要以这种方式设计硬件？好吧，这可能有多种原因，包括硬件设计错误、单向函数的安全哈希算法，或者在新版本中修复的旧固件错误。如果需要，更新软件行为很容易，但不可能更改任何硬件行为。因此，我们采取下一个最佳选择——模仿软件来接受硬件缺陷并反转软件方面的操作。 &lt;/p&gt;</description>
      <pubDate>Fri, 30 Jan 2026 13:00:01 +0000</pubDate>
    </item>
    <item>
      <title>【Mentorship in Modern Engineering Teams: The ROI Question in the Age of AI】现代工程团队的指导：人工智能时代的投资回报率问题</title>
      <link>https://dzone.com/articles/mentorship-roi-modern-engineering-ai</link>
      <description>【&lt;h2 dir=&#34;ltr&#34;&gt;The Uncomfortable Question&lt;/h2&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;As an engineer, I often ask myself whether mentoring junior engineers still makes economic sense. A few years ago, the path was predictable: juniors handled basic tasks, learned the codebase, and became reliable contributors within 6–12 months. The early period required guidance, but the return was clear and arrived within a predictable window.&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;AI tools changed that structure. Much of the work that historically built junior competence, such as small features, refactoring tasks, and routine implementation, can now be produced quickly through &lt;a href=&#34;https://dzone.com/articles/use-anthropic-claude-3-models-to-build-generative&#34;&gt;Claude&lt;/a&gt;, ChatGPT, or Copilot. This reshapes team expectations about where early productivity should come from.&lt;/p&gt;】&lt;h2 dir=&#34;ltr&#34;&gt;令人不舒服的问题&lt;/h2&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;作为一名工程师，我经常问自己，指导初级工程师是否仍然具有经济意义。几年前，这条路是可以预测的：初级人员处理基本任务，学习代码库，并在 6 到 12 个月内成为可靠的贡献者。早期需要指导，但回报是明确的，并且在可预测的窗口内到达。&lt;/p&gt;&#xA;&lt;p dir=&#34;ltr&#34;&gt;人工智能工具改变了这种结构。历史上培养初级能力的许多工作，例如小功能、重构任务和例行实施，现在可以通过 &lt;a href=&#34;https://dzone.com/articles/use-anthropic-claude-3-models-to-build-generative&#34;&gt;Claude&lt;/a&gt;、ChatGPT 或 Copilot 快速完成。这重塑了团队对于早期生产力应该来自何处的期望。&lt;/p&gt;</description>
      <pubDate>Fri, 30 Jan 2026 12:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Modernizing Applications with the 7 Rs Strategy – A CTO&#39;s Guide】通过 7 Rs 策略实现应用程序现代化 – 首席技术官指南</title>
      <link>https://dzone.com/articles/modernizing-applications-with-the-7-r-strategy</link>
      <description>【&lt;p&gt;Think about the time CTOs spent most of their time fixing old systems. Updates were slow, servers were expensive, and adding new features took time.&lt;/p&gt;&#xA;&lt;p&gt;Now, things have changed. Cloud technology applications can grow fast, collaborate, and meet business demands quickly.&lt;/p&gt;】&lt;p&gt;想想 CTO 大部分时间都花在修复旧系统上。更新缓慢，服务器昂贵，添加新功能需要时间。&lt;/p&gt;&#xA;&lt;p&gt;现在，情况发生了变化。云技术应用可以快速增长、协作并快速满足业务需求。&lt;/p&gt;</description>
      <pubDate>Thu, 29 Jan 2026 20:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>