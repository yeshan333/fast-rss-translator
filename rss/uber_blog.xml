<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Uber Engineering Blog</title>
    <link>http://rsshub.rssforever.com/uber/blog</link>
    <description>The technology behind Uber Engineering - Powered by RSSHub</description>
    <managingEditor>contact@rsshub.app (RSSHub)</managingEditor>
    <item>
      <title>【Pickup in 3 minutes: Uber’s implementation of Live Activity on iOS】3 分钟内接载：Uber 在 iOS 上实现 Live Activity</title>
      <link>https://www.uber.com/blog/live-activity-on-ios/</link>
      <description>【&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;d8d5a54c-aece-4e72-a8ee-4b0719264e19&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a21eee85-8e66-4b3f-9bbe-f9d6fac23655&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The 2022 WWDC keynote brought an unexpected surprise when Apple™ unveiled the new Live Activities feature, using Uber’s Rider app as a prominent example. This announcement generated excitement for the feature to come and set the stage for an exhilarating journey for our team.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0acb5d83-99cb-4643-9aae-959dc48f59a6&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;What follows is the story of how we started designing for surfaces outside the app, the engineering problems we had to solve along the way, and ultimately how we measurably improved the experience of riders and drivers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fdbf46e9-3a89-4d91-b5cf-b2d21a6a0728&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;584c60c8-0e10-4ce8-99b7-d11e81af48b6&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-planning-the-work-ahead&#34;&gt;Planning the Work Ahead&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;48cba36c-9ab1-4cb3-a7f3-f058e5ef4f3e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Given the secretive nature of the announcement,&amp;nbsp; only a few directors on our side were in the loop. For the rest of the engineering and design team, it was an exhilarating surprise, immediately followed by the realization that we needed to move quickly.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;630d3cda-62e6-463e-b161-71f6818da440&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;At a company the size of Uber, work is usually planned on a half-year schedule and resources and timelines are always tight.&amp;nbsp; So, while considering the prioritization of this feature, we recognized the potential upside of being in the App Store® on day one, to be a champion of the platform.&amp;nbsp; We also believed that this feature would be of great benefit to our users, as now they would be able to observe trip updates from outside of the app in a granular way.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3356d1f0-c441-4d92-98b7-a9da8eed85b0&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We got into a room and began redrawing our plans, reallocating resources, starting from a minimal team that could work on this as their main project. We then recruited additional resources on a side project basis, making sure to plan their contributions to avoid too much context switching.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8d58daf1-1c9b-4423-914b-237970e72c48&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We began by tech-proofing the concept as much as possible up front, working closely with the UX and design team. We set specific guardrails to keep the scope manageable, knowing that we needed to construct a space for flexible thinking. Everyone on the team had enough know-how and authority to allow for plan changes based on product iterations, newly discovered engineering limitations, or staffing conflicts. The autonomy given to each member of the team was a crucial aspect, meant to minimize the turnaround time of changes in scope and requirements while navigating the uncharted waters of developing for a new app surface.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6cbee0ca-c76d-4d97-8158-efbe72e5a662&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e9ecafd5-4990-4b89-89f9-a2dea56c85ba&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-ui-at-a-glance&#34;&gt;UI at a Glance&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;eeae5d98-28d3-4108-bebc-6a82dea8f7b7&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;From a UX perspective, we quickly recognized the unique challenge posed by Live Activities. Information on this surface is only glanced at for a few seconds at a time, and it extends beyond the boundaries of the main app. Attempting to cram the same information from our app into the Live Activity view would have been a mistake. Users would only look at it briefly, so we categorized the information based on what we believe users’ priorities would be during a trip:&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c1b61d5e-2b15-4c06-a6bf-9c7680a7c60b&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;P0. time to pick up/destination. &lt;/strong&gt;Important information during the waiting-for-pickup phase, the user wants to know how long before being picked up, so they can get ready and still use their phone without constantly hopping into the app to check.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;P1. license number, and picture of the vehicle.&lt;/strong&gt; At pickup time it helps the user identify the location of the vehicle in the real world.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;P1. name and picture of the driver. &lt;/strong&gt;This adds safety context and helps the user ensure they are getting into the right vehicle.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;P2. overall progress of the trip.&lt;/strong&gt; Brings the experience all together. It has been designed with a dynamic display curve, where the last 20% of the progress bar represents the last 2 min of the progress no matter how long the whole waiting time is.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;dd5dd607-b688-4cfe-9347-14e25b04da37&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;This prioritization not only informed the feature design, but also guided us whenever we encountered engineering issues. The information priority was always the main anchor point when evaluating and justifying the scope of a change.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5d3238c9-235e-4005-a235-a53678b5d069&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;48fbba51-0143-485b-85d9-99891d53f77e&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXfnNfm5WFNr_nxKNiRnUfm365F_py2lz4MjHL7SSdCM4DhNliUd83sfhJHg1gKDhSt_CPcyeMoDHyquxmzWhcmKRzKLQHYlwUeUQVw-TB-0XrJi4O97G4O2R-J8TlI1jOEhDG66JT4rylDmiTI11MV0hwA?key=rkl1fUrFyfE6SsnQYMStNA&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 1: Visual representation of the hierarchy of information on the live activity.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0b6e161c-a75a-439f-9d87-84ee9846e681&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;850bc16f-fbde-471b-a027-bbc943d7c502&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Working with the Dynamic Island, the pill-shaped cutout that shows app information on top of the screen, took this concept to an extreme, due to its minuscule amount of space. Unlike a standalone Live Activity, the Dynamic Island shares its limited real estate with other apps and system information. This meant that our design had to be even more concise and efficient. We had to ensure that our information was clear and useful within the tiny space, without overwhelming the user or clashing with other data. For example, during the on-trip phase, we had several iterations to make sure the drop-off time wasn’t going to be mistaken with the system clock. Every pixel counted, and we had to be exceptionally creative to maximize the utility of this feature.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;80db5ef0-fb5e-46e5-98c6-316697cfbd6d&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;5dba3263-5965-4b92-9c5b-c0974a4d0ed7&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXcJ6ljMLzX0eQB8NpOhehU_arCfn53RdDCuJ37Qm9bcRWqKlBL9Z7w5FhYxkD7YrBMSI794qrnEQtM1Ir9_lg0HFX7IUX_rkUnKXiSsn8zIe5vQtWRw6X2lahNcbySZpDr5X3LPceyZtT4dOcI7U_GYFlD3?key=rkl1fUrFyfE6SsnQYMStNA&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 2: All the phases of a trip expressed in the Dynamic Island.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d38c74f9-da14-4861-b302-9e1a378e48e5&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7ec25563-ca41-42ac-8100-018f7c14ad49&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-overcoming-technical-challenges&#34;&gt;Overcoming Technical Challenges&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6ff1c177-70bb-40a1-a455-4eff1d095bae&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Live Activities are designed as a separate target from the main application, with no access to networking capabilities and no state. They are updated through the main app or push notifications. Once invoked, the Live Activity constructs its own SwiftUI® view and exists without the ability to directly communicate back to the main target. This setup presented a major challenge: how to handle images dynamically needed by the Live Activity, such as the driver and vehicle’s&amp;nbsp; picture.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;887d7962-8772-40c8-8cd3-8238eac23125&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To solve this, we leveraged two technical characteristics of the app: the main target being in the background during the trip request and App Groups, which allows targets to read and write into a shared directory. Upon a payload update, we process any image URLs found and serialize them to disk. The Live Activity then reads those same URLs from disk.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fd4a0b78-7ba6-43b5-85b6-9844b42e0d98&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;2e2f3801-246c-4c2d-842a-bf9fbadbc11e&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXeMRO_W2T4rGjwycnXIGluXrdrMbIf0OYp_WFp8cH5CU7HHZ82EHLuDI8xHHnlj3b3H0jEALsmcsNWxXOfo_cPU9PDV8ji47KX9HUMRhGtyGCfT4VYgGw5x84Xk00MxZFsg3_RgYBNQsKKzR5cWWrwI-jLE?key=rkl1fUrFyfE6SsnQYMStNA&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 3: Timeline of Live Activity update and asset caching strategy.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;093216e5-b2bc-49d6-8ec3-1f40b2b09545&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;78f18350-259b-45be-abff-c247c5f4ccd1&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;What is usually the simple task of downloading assets and presenting them on screen, becomes, with Live Activities, a more complicated process that adds several possible failure points. The only silver lining, in our situation,&amp;nbsp; is that since assets rarely change during a trip, we have the opportunity to retry at every payload update if necessary.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;dbc5915a-63ff-4885-bcc7-2094023de069&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Later in this article we will discuss possible alternative solutions, but there is one main aspect that is worth noting up front: the app is not guaranteed to be running in the background during a trip. The Rider app uses an entitlement to run in the background, so we can update the user location and guarantee precise pickup information, but the user or the OS can still kill the app, stopping the stream of updates.&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2280f88d-0b85-4b8d-99af-ef7fe91fc1cc&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In our first design, we relied exclusively on the app running in the background to update the live activity, but during beta testing we received reports from users lamenting that the Live Activity stopped updating or it was not dismissed after trip completion. We consequently opted for a different design that leverages a backend service and Push Notifications (more in the OOA Service section).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b4838fb0-80f5-49a4-aa4a-45dd71bc9475&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;A major part of the first solution was to estimate the impact of this feature on the backend API.&amp;nbsp; While this feature would add traffic to our API services, we believed users would spend less time inside the application, reducing traffic at the same time (the app needs to pull data frequently while in the foreground). We could not guess how our users’ behavior would change, but we estimated the load transfer from foreground to background use would not be impactful. Additionally, as an initial safeguard, a feature flag was added to control the rate of pulling while in the background. However, after moving to the aforementioned OOA Service, the rate of updates is already controlled by the backend, which has a more sophisticated load balancing and throttling logic.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;19436a26-34b7-4c92-86a8-b30aaee10551&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c8d6621d-1612-4011-b4ec-95d339b65083&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-measuring-impact-and-collecting-data&#34;&gt;Measuring Impact and Collecting Data&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9dfe427e-8bb8-46ec-a555-7acb9d1651af&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Measuring the impact of new features is a standard practice at Uber. We anticipated that Live Activities would change how users interacted with our app, leading to significant improvements in metrics such as reduced cancellations and waiting times at pickup. The figures below may seem small, but they are in fact huge wins at the scale we operate. Based on the results, we believe riders are more aware of when they are getting picked up, resulting in more completed trips (and earnings for drivers).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c89351e4-6746-48a7-ba52-663a2065b074&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;2.26% &lt;/strong&gt;reduction in driver cancellations at pickup&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;2.13% &lt;/strong&gt;reduction in rider cancellation at pickup&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;1.06% &lt;/strong&gt;reduction in pickup defects per request&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;43903745-6d9a-43b0-95c5-4744b7c1d72d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The metrics above are a measure of user behavior which are measured independently by features that are added to the app. When it comes to collecting engineering metrics for Live Activities, their unique behavior made the task more challenging. Unlike a normal app, Live Activities don’t offer easy ways to gather metrics without significant gaps. We had to make assumptions based on incomplete data, such as monitoring the sequence of &lt;strong&gt;&lt;em&gt;activityStateUpdates&lt;/em&gt;&lt;/strong&gt; to see if Live Activities were correctly cleared at the end of a trip. However this sequence only emits if the app is running in the background, which is not guaranteed, meaning that a significant amount of events would be missing.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a3b3ca98-e5d4-4f17-8059-02618ee0592d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To mitigate the issue we had to join data about foreground events where a Live Activity is present without a trip in progress. This method does not guarantee 100% coverage of the cases, but it allows us to set an acceptable baseline and create regression metrics and dashboards that, while they don’t represent a complete picture, are still useful to catch regressions in production or during development.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a51bd47b-2864-49b0-9706-ec7976b60310&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;334591db-c22f-4942-a6e8-976a7a645b60&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-implementing-feature-flags&#34;&gt;Implementing Feature Flags&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d254d666-52ca-4769-bc1f-ea32d1a5279c&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Another standard Uber practice is to flag new features or sizable iteration changes. With Live Activities, this was tricky due to the lack of network access and bilateral communication with the main target. We use the same trick as with image loading: at the startup of the main target, we read the feature flags from our standard pipeline and write their values to disk. The Live Activity then reads these values from disk when invoked.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cca24f0a-abe0-45ec-99db-1fb195ddccf4&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;id&amp;quot;:1093155,&amp;quot;sizeSlug&amp;quot;:&amp;quot;large&amp;quot;,&amp;quot;linkDestination&amp;quot;:&amp;quot;none&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;19c012b7-97a2-4c31-868c-077278b52895&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter size-large&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1024&#34; height=&#34;673&#34; src=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure_4-1024x673.png&#34; alt=&#34;&#34; class=&#34;wp-image-1093155&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure_4.png 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure_4.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure_4.png 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1536,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure_4.png 1536w, https://blog.uber-cdn.com/cdn-cgi/image/width=2048,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure_4.png 2048w&#34; sizes=&#34;(max-width: 1024px) 100vw, 1024px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 4: Diagram describing the passthrough of Feature flags to the Live Activity.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4fc898b4-1f9b-4b46-a83e-b8ccc3816348&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;259718db-b6a1-44c4-a575-b1bcc18eee74&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-dsl-android-and-the-ooa-service&#34;&gt;DSL, Android™, and the OOA Service&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d3f56171-ce5b-4fcf-b17b-0879dd541e2e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;While Android doesn’t have a direct counterpart to Live Activities, we could still enhance the push notification strategy by updating push notifications, as opposed to sending new ones, adding the same visual element used in the Live Activity to achieve feature parity.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;07aad116-6775-46cc-9ca2-bd25434510fc&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To streamline this process and ensure consistency across iOS™ and Android, we developed a simple server-driven language. This language allows us to easily modify the content presented in both the Live Activity and Android Push Notifications. By centralizing the content logic on the server, we can dynamically update and tailor the user experience without requiring app updates. This system provides flexibility and ensures that any changes or new features can quickly be implemented across both platforms, maintaining a unified experience for all our users.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9b69d100-3fed-475f-afdb-03d7c707d53e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We needed to design a domain specific language (DSL) that had a very small implementation footprint and few external dependencies. Uber already has a Server Driven UI system, but it is pretty extensive and as a result it cannot be imported to an external target without impact on the binary size. Furthermore keeping the capabilities to a minimum also allows to reduce the maintenance cost in the future.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e42c67ef-657d-4313-9398-f883de028b97&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;br&gt;Our solution was to build a very opinionated, semi-descriptive DSL. Only UI elements that can be present in Live Activities are included (no text fields or segmented controllers for example) and no extensive styling was provided. For views that need styling, we provide an array of tags that is then processed at the client level. For example, the title label could be represented by a component of type &lt;em&gt;Label&lt;/em&gt;, along with the text and a tag of type &lt;em&gt;title&lt;/em&gt;. Then the client applies the font, color, number of lines, alignment, etc. for the specific style &lt;em&gt;title&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;619096ff-7460-4c11-8324-b4bcc1860df4&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;950fa69b-2d4b-4e55-8d75-b059467bd53e&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdsNu5jyXsfpaTI9Vg-oHlbNbNdMHh9Idpf30MDWoRihwehyVMrjCdIn0CM6qhbfwG9uYmXvj0ix5Or984gWE7ADWXuAYEgIOzwgEQT0PRdKv01TZ384gQ8-Jm6gyVLby8pQtNqLa4ABIhQfBaZvKMwMl8?key=rkl1fUrFyfE6SsnQYMStNA&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 5: Exemplification of a DSL payload that hydrates the Live Activity.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;308b68d5-cb9c-4a9a-9caa-3831abbae52c&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e8ef1a57-fd18-40bc-849e-803fa8a1e81d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Lastly the DSL would not represent any framing or anchoring information. A different view could be provided for the header or the progress bar, but the location of each view is set by the client and cannot change (unless for instructions that are applied with the aforementioned tags).&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c7252015-a88b-44e8-9a77-4a9a4df67519&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;id&amp;quot;:1093156,&amp;quot;sizeSlug&amp;quot;:&amp;quot;large&amp;quot;,&amp;quot;linkDestination&amp;quot;:&amp;quot;none&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;d033db0f-5c7e-4ce8-9ce5-91ffb62239f7&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter size-large&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1024&#34; height=&#34;220&#34; src=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure_6-1024x220.jpg&#34; alt=&#34;&#34; class=&#34;wp-image-1093156&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure_6.jpg 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure_6.jpg 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure_6.jpg 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1521,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure_6.jpg 1521w&#34; sizes=&#34;(max-width: 1024px) 100vw, 1024px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 6: Demonstration of how the templating works on the Live Activity and Android Push Notification.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;29e191ed-f97e-412e-8c92-f2d44abda49c&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8ce1fd0b-1827-46de-be68-99978022606e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Working in tandem with this new DSL system we developed a backend service specifically designed for surfaces that live outside of the app; we call it the OOA Service (Out Of App). The OOA Service is responsible for the logic to balance the amount of updates delivered to the Apple Push Notifications Service. It evaluates whether changes to the application state are important enough to be delivered, as well as debouncing state changes that happen in rapid succession. Because of the need to evaluate and debounce, this service has to cache previous states for all concurrent trips on the platform, which is a substantial scaling effort.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f1c38265-62e8-44a2-ac71-622ac7b4a94d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Building the DSL and the OOA Service was a significant step forward. It didn’t only simplify the development process, but also opened the door to integrate changes without having to duplicate the decision tree logic&amp;nbsp; and deploy code on multiple platforms. By building this generic solution, we avoid solving complex problems multiple times on different teams. This, in particular, is a decision that we believe will pay dividends in the future as more and more vertical teams at Uber utilize the Live Activity flow.&lt;br&gt;Even the Eats platform is already using a good part of the mobile framework that deals with the Live Activity life cycle, the image caching and the feature flag injection. They are also evaluating onboarding the DSL and the OOA Service after seeing ‌the positive results of the Rider integration.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7c605f28-0f27-430e-aa1b-856fb0a30877&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;456dd394-451c-43ec-8264-99820ff236a2&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-conclusion&#34;&gt;Conclusion&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8fdbe365-654c-49bf-becc-de4ebf7d98b6&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Building the Rider iOS Live Activity was an intense, but rewarding journey. From the surprise of the WWDC announcement to the challenges of developing and implementing a new technology on a tight timeline, the experience showcased the resilience, adaptability, and creativity of our team. We navigated technical hurdles, redefined our UX approach, and ultimately delivered a feature that we believe improved the rider and driver experience. I hope the pieces of ingenuity we have shown in this article may inspire any developer working on Live Activities, to help them overcome similar scenarios and generally take a pragmatic approach for experiences that live outside of the main app.&lt;br&gt;At a personal level, it was a great experience and opportunity: developing an early adopter product at a company of our scale is challenging, but insanely fulfilling.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;73a51cf5-4e2c-4d8a-8ca9-51e142dde84b&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cfcd3e00-4b2b-4695-be8f-619d8c2dc7f9&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;A special thanks to the whole team. This project not only demonstrated everyone’s technical prowess, but also highlighted the collaborative spirit that drives innovation at Uber. Kyle Gabriel, Ken Nguyen, Tiffany Chang, Hyewon Son, Radhika Gemawat, Maxim Bogatov, Yifan Ding, Evan Cooper&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;0d80d3ae-d935-48d4-823a-9768c64618bd&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Apple, App Store, Swift, and SwiftUI are trademarks of Apple Inc., registered in the U.S. and other countries and regions.&lt;br&gt;iOS is a trademark or registered trademark of Cisco in the U.S. and other countries and is used by Apple under license.&lt;br&gt;Android is a trademark of Google LLC, registered in the U.S. and other countries and regions.&lt;/p&gt;】&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;d8d5a54c-aece-4e72-a8ee-4b0719264e19&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-introduction&#34;&gt;简介&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a21eee85-8e66-4b3f-9bbe-f9d6fac23655&#34;,&#34;dropCap&#34;:false}&#34;&gt;2022 年 WWDC当 Apple™ 以 Uber 的 Rider 应用程序作为突出示例推出新的实时活动功能时，主题演讲带来了意想不到的惊喜。这一公告让我们对即将推出的功能感到兴奋，并为我们团队的激动人心的旅程奠定了基础。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0acb5d83-99cb-4643-9aae-959dc48f59a6&#34;,&#34;dropCap&#34;:false}&#34;&gt;接下来是我们如何开始设计应用程序之外的表面，一路上必须解决的工程问题，以及最终我们如何显着改善乘客和驾驶员的体验。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fdbf46e9-3a89-4d91-b5cf-b2d21a6a0728&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;584c60c8-0e10-4ce8-99b7-d11e81af48b6&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-planning-the-work-ahead&#34;&gt;规划未来的工作&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;48cba36c-9ab1-4cb3-a7f3-f058e5ef4f3e&#34;,&#34;dropCap&#34;:false}&#34;&gt;鉴于秘密由于该公告的性质，我们这边只有少数董事了解情况。对于工程和设计团队的其他成员来说，这是一个令人兴奋的惊喜，随后我们立即意识到我们需要迅速采取行动。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;630d3cda-62e6-463e-b161-71f6818da440&#34;,&#34;dropCap&#34;:false}&#34;&gt;在一家公司就 Uber 的规模而言，工作通常按半年计划一次，资源和时间安排总是很紧张。  因此，在考虑此功能的优先级时，我们认识到第一天进入 App Store® 的潜在优势，即成为该平台的冠军。  我们还相信，此功能将为我们的用户带来很大好处，因为现在他们将能够从应用程序外部以精细的方式观察行程更新。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3356d1f0-c441-4d92-98b7-a9da8eed85b0&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们进入一个房间，并开始重新制定我们的计划，重新分配资源，从一个可以将其作为主要项目的最小团队开始。然后，我们在业余项目的基础上招募了额外的资源，确保计划他们的贡献以避免过多的上下文切换。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;核心/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8d58daf1-1c9b-4423-914b-237970e72c48&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们首先尽可能地预先对这个概念进行技术验证，工作我们与用户体验和设计团队密切合作，设置了特定的护栏以保持范围的可控性，因为我们知道我们需要构建一个灵活思考的空间，以便根据产品迭代进行计划变更。 、新发现的工程限制或人员冲突，给予团队每个成员的自主权是一个至关重要的方面，这意味着在探索新应用程序界面开发的未知领域时，可以最大限度地缩短范围和需求变化的周转时间。&lt; /p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6cbee0ca-c76d-4d97-8158-efbe72e5a662&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e9ecafd5-4990-4b89-89f9-a2dea56c85ba&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-ui-at-a-glance&#34;&gt;用户界面概览&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;eeae5d98-28d3-4108-bebc-6a82dea8f7b7&#34;,&#34;dropCap&#34;:false}&#34;&gt;来自用户体验从角度来看，我们很快认识到现场活动带来的独特挑战。一次只能浏览该表面上的信息几秒钟，并且它超出了主应用程序的边界。试图将我们应用程序中的相同信息塞入“实时活动”视图将是一个错误。用户只会简单地查看它，因此我们根据我们认为用户在旅行期间的优先事项对信息进行了分类：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c1b61d5e-2b15-4c06-a6bf-9c7680a7c60b&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;P0。接车时间/目的地。 &lt;/strong&gt;等待接载阶段的重要信息，用户想知道距离接载还有多长时间，这样他们就可以做好准备并继续使用手机，而无需不断地跳入应用程序进行检查。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;P1。车牌号码和车辆图片。&lt;/strong&gt;在取车时，它可以帮助用户识别车辆在现实世界中的位置。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;P1。司机的姓名和照片。 &lt;/strong&gt;这增加了安全背景，并帮助用户确保他们进入正确的车辆。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;P2。旅行的整体进度。&lt;/strong&gt;将所有体验汇集在一起​​。采用动态显示曲线设计，最后20%进度为r代表进度的最后2分钟，无论整个等待时间有多长。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;dd5dd607-b688-4cfe-9347-14e25b04da37&#34;,&#34;dropCap&#34;:false}&#34;&gt;此优先级不不仅为功能设计提供了信息，而且在我们遇到工程问题时也为我们提供了指导。在评估和证明变更范围的合理性时，信息优先级始终是主要锚点。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5d3238c9-235e-4005-a235-a53678b5d069&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;48fbba51-0143-485b-85d9-99891d53f77e&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXfnNfm5WFNr_nxKNiRnUfm365F_py2lz4MjHL7SSdCM4DhNliUd83sfhJHg1gKDhSt_CPcyeMoDHyquxmzWhcmKRzKLQHYl wUeUQVw-TB-0XrJi4O97G4O2R-J8TlI1jOEhDG66JT4rylDmitI11MV0hwA?key=rkl1fUrFyfE6SsnQYMStNA&#34; alt=&#34; &#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 1：实时活动信息层次结构的直观表示。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0b6e161c-a75a-439f-9d87-84ee9846e681&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;850bc16f-fbde-471b-a027-bbc943d7c502&#34;,&#34;dropCap&#34;:false}&#34;&gt;使用动态岛是一种在屏幕顶部显示应用程序信息的药丸状切口，由于其空间极小，因此将这一概念发挥到了极致。与独立的实时活动不同，动态岛与其他应用程序和系统信息共享其有限的空间。这意味着我们的设计必须更加简洁和高效。我们必须确保我们的信息在狭小的空间内清晰有用，而不会让用户感到不知所措或与其他数据发生冲突。例如，在行程阶段，我们进行了多次迭代，以确保还车时间不会与系统时钟发生错误。每个像素都很重要，我们必须具有非凡的创造力才能最大限度地利用此功能。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;80db5ef0-fb5e-46e5-98c6-316697cfbd6d&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;5dba3263-5965-4b92-9c5b-c0974a4d0ed7&#34;,&#34;alt&#34; ：“”;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXcJ6ljMLzX0eQB8NpOhehU_arCfn53RdDCuJ37Qm9bcRWqKlBL9Z7w5FhYxkD7YrBMSI794qrnEQtM1Ir9_lg0HFX7IUX _rkUnKXiSsn8zIe5vQtWRw6X2lahNcbySZpDr5X3LPceyZtT4dOcI7U_GYFlD3?key=rkl1fUrFyfE6SsnQYMStNA&#34; alt=&#34;&#34;referrerpolicy=&#34;无引荐来源&#34; &gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 2：动态岛中表达的旅行的所有阶段。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d38c74f9-da14-4861-b302-9e1a378e48e5&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7ec25563-ca41-42ac-8100-018f7c14ad49&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-overcoming-technical-challenges&#34;&gt;克服技术挑战&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6ff1c177-70bb-40a1-a455-4eff1d095bae&#34;,&#34;dropCap&#34;:false}&#34;&gt;实时活动是设计为与主应用程序分离的目标，无法访问网络功能，也没有状态。它们通过主应用程序或推送通知进行更新。一旦被调用，Live Activity 就会构建自己的 SwiftUI® 视图，并且存在时无法直接与主要目标进行通信。此设置提出了一个重大挑战：如何动态处理实时活动所需的图像，例如驾驶员和车辆的图片。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;887d7962-8772-40c8-8cd3-8238eac23125&#34;,&#34;dropCap&#34;:false}&#34;&gt;解决此问题，我们利用了该应用程序的两个技术特征：主要目标在行程请求期间位于后台，以及应用程序组，允许目标读取和写入共享目录。有效负载更新后，我们会处理找到的所有图像 URL 并将它们序列化到磁盘。然后，实时活动从磁盘读取这些相同的 URL。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fd4a0b78-7ba6-43b5-85b6-9844b42e0d98&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;2e2f3801-246c-4c2d-842a-bf9fbadbc11e&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXeMRO_W2T4rGjwycnXIGluXrdrMbIf0OYp_WFp8cH5CU7HHZ82EHLuDI8xHHnlj3b3H0jEALsmcsNWxXOfo_cPU9PDV 8ji47KX9HUMRhGtyGCfT4VYgGw5x84Xk00MxZFsg3_RgYBNQsKKzR5cWWrwI-jLE?key=rkl1fUrFyfE6SsnQYMStNA&#34; alt=&#34;&#34;referrerpolicy=&#34; no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 3：实时活动更新时间线和资产缓存策略。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;小时数据-wp-block-name=&#34;核心/分隔符&#34; data-wp-block=&#34;{&#34;散列&#34;:&#34;093216e5-b2bc-49d6-8ec3-1f40b2b09545&#34;,&#34;不透明度&#34;:&#34;alpha通道&#34;}&#34; class=&#34; wp-块分隔符具有-alpha-通道-不透明度&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;78f18350-259b-45be-abff-c247c5f4ccd1&#34;,&#34;dropCap&#34;:false}&#34;&gt;通常是什么下载资产并将其显示在屏幕上的简单任务，通过实时活动变成了一个更复杂的过程，增加了几个可能的故障点。在我们的情况下，唯一的一线希望是，由于资产在旅行期间很少发生变化，因此我们有机会在必要时重试每次有效负载更新。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;dbc5915a-63ff-4885-bcc7-2094023de069&#34;,&#34;dropCap&#34;:false}&#34;&gt;稍后在本文中，我们将讨论可能的替代解决方案，但有一个主要方面值得注意：不保证该应用程序在旅行期间在后台运行。 Rider 应用程序使用在后台运行的权限，因此我们可以更新用户位置并保证精确的接送信息，但用户或操作系统仍然可以终止该应用程序，从而停止更新流。&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2280f88d-0b85-4b8d-99af-ef7fe91fc1cc&#34;,&#34;dropCap&#34;:false}&#34;&gt;在我们的第一个在设计上，我们完全依赖在后台运行的应用程序来更新实时活动，但在 Beta 测试期间，我们收到了用户的报告，抱怨实时活动停止更新，或者在旅行完成后没有取消。因此，我们选择了利用后端服务和推送通知的不同设计（更多内容请参见 OOA 服务部分）。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b4838fb0-80f5-49a4-aa4a-45dd71bc9475&#34;,&#34;dropCap&#34;:false}&#34;&gt;主要部分第一个解决方案的目的是估计此功能对后端 API 的影响。  虽然此功能会增加我们 API 服务的流量，但我们相信用户会在应用程序内花费更少的时间，同时减少流量（应用程序需要在前台频繁提取数据）。我们无法猜测用户的行为会如何变化，但我们估计从前台到后台使用的负载转移不会产生影响。此外，作为初始保护措施，添加了一个功能标志来控制后台拉动的速率。然而，在迁移到前面提到的 OOA 服务后，更新速率已经由后端控制，后端具有更复杂的负载平衡和节流逻辑。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;19436a26-34b7-4c92-86a8-b30aaee10551&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp块分隔符”具有 alpha 通道不透明度&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c8d6621d-1612-4011-b4ec-95d339b65083&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-measuring-impact-and-collecting-data&#34;&gt;衡量影响和收集数据&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9dfe427e-8bb8-46ec-a555-7acb9d1651af&#34;,&#34;dropCap&#34;:false}&#34;&gt;衡量影响推出新功能是 Uber 的标准做法。我们预计实时活动将改变用户与我们的应用程序交互的方式，从而显着改进指标，例如减少取消和取货等待时间。下面的数字可能看起来很小，但实际上就我们的运营规模而言，它们是巨大的胜利。根据结果​​，我们相信乘客会更清楚自己何时被接载，从而完成更多行程（以及司机的收入）。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c89351e4-6746-48a7-ba52-663a2065b074&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;取车时取消司机数量减少 2.26%&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;取车时取消乘客数量减少 2.13%&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;每个请求的取件缺陷减少 1.06%&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;43903745-6d9a-43b0-95c5-4744b7c1d72d&#34;,&#34;dropCap&#34;:false}&#34;&gt;上述指标是对用户行为的衡量，通过添加到应用程序的功能独立衡量。在收集实时活动的工程指标时，它们独特的行为使任务更具挑战性。与普通应用程序不同，实时活动不提供简单的方法来收集没有明显差距的指标。我们必须根据不完整的数据做出假设，例如监控&lt;strong&gt;&lt;em&gt;activityStateUpdates&lt;/em&gt;&lt;/strong&gt;的顺序，以查看实时活动是否在行程结束时正确清除。但是，此序列仅在应用程序在后台运行时才会发出，这是无法保证的，这意味着会丢失大量事件。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a3b3ca98-e5d4-4f17-8059-02618ee0592d&#34;,&#34;dropCap&#34;:false}&#34;&gt;为了缓解问题是，我们必须加入有关前台事件的数据，其中存在实时活动而没有正在进行的行程。此方法不能保证 100% 覆盖案例，但它允许我们设置可接受的基线并创建回归指标和仪表板，虽然它们不能代表完整的情况，但对于捕获生产或开发过程中的回归仍然有用.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a51bd47b-2864-49b0-9706-ec7976b60310&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34; class=&#34;wp-block-separator has-alpha-channel -不透明度&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;334591db-c22f-4942-a6e8-976a7a645b60&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-implementing-feature-flags&#34;&gt;实现功能标志&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d254d666-52ca-4769-bc1f-ea32d1a5279c&#34;,&#34;dropCap&#34;:false}&#34;&gt;另一个标准 Uber实践是标记新功能或相当大的迭代更改。对于实时活动，由于缺乏网络访问以及与主要目标的双边通信，这很棘手。我们使用与图像加载相同的技巧：在主目标启动时，我们从标准管道中读取功能标志并将其值写入磁盘。然后，实时活动在调用时从磁盘读取这些值。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cca24f0a-abe0-45ec-99db-1fb195ddccf4&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;id&#34;:1093155,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;align&#34;:&#34;中心”，“散列”：“19c012b7-97a2-4c31-868c-077278b52895”，“alt”：“”}“类=“aligncenter size-large”&gt; &lt;img加载=“惰性”解码=“异步”宽度= “1024”高度=“673”src=“https://blog.uber-cdn.com/cdn-cgi/image/width=2160，质量= 80，onerror=重定向，format=auto/wp-content/uploads /2024/07/Figure_4-1024x673.png&#34; alt=&#34;&#34; class=&#34;wp-image-1093155&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality =80，onerror=重定向，format=auto/wp-content/uploads/2024/07/Figure_4.png 1024w，https://blog.uber-cdn.com/cdn-cgi/image/width=300，quality= 80，onerror=重定向，format=auto/wp-content/uploads/2024/07/Figure_4.png 300w，https://blog.uber-cdn.com/cdn-cgi/image/width=768，quality=80 ,onerror=重定向,format=auto/wp-content/uploads/2024/07/Figure_4.png 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1536,quality=80, onerror=重定向，format=auto/wp-content/uploads/2024/07/Figure_4.png 1536w，https://blog.uber-cdn.com/cdn-cgi/image/width=2048，quality=80，onerror =重定向，格式=自动/wp-content/uploads/2024/07/Figure_4.png 2048w“尺寸=”（最大宽度：1024px）100vw，1024px“referrerpolicy=“no-referrer”&gt;&lt;figcaption class=“wp” -element-caption&#34;&gt;图 4：描述功能标志传递到实时活动的图表。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4fc898b4-1f9b-4b46-a83e-b8ccc3816348&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 数据a-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;259718db-b6a1-44c4-a575-b1bcc18eee74&#34;,&#34;level&#34;:2}&#34; class=&#34;wp-block -heading&#34; id=&#34;h-dsl-android-and-the-ooa-service&#34;&gt;DSL、Android™ 和 OOA 服务&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d3f56171-ce5b-4fcf-b17b-0879dd541e2e&#34;,&#34;dropCap&#34;:false}&#34;&gt;虽然 Android 没有虽然没有与实时活动直接对应的内容，但我们仍然可以通过更新推送通知来增强推送通知策略，而不是发送新通知，添加与实时活动中使用的相同视觉元素以实现功能对等。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;07aad116-6775-46cc-9ca2-bd25434510fc&#34;,&#34;dropCap&#34;:false}&#34;&gt;简化此操作为了处理并确保 iOS™ 和 Android 之间的一致性，我们开发了一种简单的服务器驱动语言。这种语言使我们能够轻松修改实时活动和 Android 推送通知中呈现的内容。通过将内容逻辑集中在服务器上，我们可以动态更新和定制用户体验，而无需更新应用程序。该系统提供了灵活性，并确保任何更改或新功能都可以在两个平台上快速实施，从而为所有用户保持统一的体验。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9b69d100-3fed-475f-afdb-03d7c707d53e&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们需要设计一种具有非常小的实现占用空间和很少的外部依赖项的领域特定语言 (DSL)。 Uber 已经有一个服务器驱动的 UI 系统，但它相当广泛，因此无法在不影响二进制大小的情况下导入到外部目标。此外，将功能保持在最低限度还可以降低未来的维护成本。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e42c67ef-657d-4313-9398-f883de028b97&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;br&gt;我们的解决方案是构建一个非常固执己见、半描述性的 DSL。仅包含可以在实时活动中出现的 UI 元素（例如，没有文本字段或分段控制器），并且没有提供广泛的样式。对于需要样式的视图，我们提供了一组标签，然后在客户端级别进行处理。例如，标题标签可以由 &lt;em&gt;Label&lt;/em&gt; 类型的组件以及文本和 &lt;em&gt;title&lt;/em&gt; 类型的标签来表示。然后客户端应用特定样式&lt;em&gt;标题&lt;/em&gt;的字体、颜色、行数、对齐方式等。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;619096ff-7460-4c11-8324-b4bcc1860df4&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;950fa69b-2d4b-4e55-8d75-b059467bd53e&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img解码=&#34;async&#34; src =&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdsNu5jyXsfpaTI9Vg-oHlbNbNdMHh9Idpf30MDWoRihwehyVMrjCdIn0CM6qhbfwG9uYmXvj0ix5Or984gWE7ADWXuAYEgIOzwgEQT0PRdKv01TZ384gQ8- Jm6gyVLby8pQtNqLa4ABIhQfBaZvKMwMl8?key=rkl1fUrFyfE6SsnQYMStNA&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 5：水合实时活动的 DSL 有效负载示例。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;308b68d5-cb9c-4a9a-9caa-3831abbae52c&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e8ef1a57-fd18-40bc-849e-803fa8a1e81d&#34;,&#34;dropCap&#34;:false}&#34;&gt;最后是 DSL不代表任何框架或锚定信息。可以为标题或进度条提供不同的视图，但每个视图的位置由客户端设置并且不能更改（除非与上述标签一起应用的指令）。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c7252015-a88b-44e8-9a77-4a9a4df67519&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;id&#34;:1093156,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;align&#34;:&#34;中心”，“散列”：“d033db0f-5c7e-4ce8-9ce5-91ffb62239f7”，“alt”：“”}“类=“aligncenter size-large”&gt;&lt;img加载=“惰性”解码=“异步”宽度= “1024”高度=“220”src=“https://blog.uber-cdn.com/cdn-cgi/image/width=2160，质量= 80，onerror=重定向，format=auto/wp-content/uploads /2024/07/Figure_6-1024x220.jpg&#34; alt=&#34;&#34; class=&#34;wp-image-1093156&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality =80，onerror=重定向，format=auto/wp-content/uploads/2024/07/Figure_6.jpg 1024w，https://blog.uber-cdn.com/cdn-cgi/image/width=300，quality= 80，onerror=重定向，format=auto/wp-content/uploads/2024/07/Figure_6.jpg 300w，https://blog.uber-cdn.com/cdn-cgi/image/width=768，quality=80 ,onerror=重定向,format=auto/wp-content/uploads/2024/07/Figure_6.jpg 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1521,quality=80, onerror=redirect，format=auto/wp-content/uploads/2024/07/Figure_6.jpg 1521w&#34; 尺寸=&#34;(最大宽度：1024px) 100vw，1024px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34; wp-element-caption&#34;&gt;图 6：演示模板如何在实时活动和 Android 推送通知上工作。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;29e191ed-f97e-412e-8c92-f2d44abda49c&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8ce1fd0b-1827-46de-be68-99978022606e&#34;,&#34;dropCap&#34;:false}&#34;&gt;协同工作通过这个新的 DSL 系统，我们开发了一项专门为应用程序外部的界面设计的后端服务；我们称之为 OOA 服务（Out Of App）。 OOA 服务负责平衡传送到 Apple 推送通知服务的更新量的逻辑。它评估应用程序状态的更改是否重要到需要交付，以及消除快速连续发生的状态更改。由于需要评估和反跳，该服务必须缓存平台上所有并发行程的先前状态，这是一项巨大的扩展工作。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f1c38265-62e8-44a2-ac71-622ac7b4a94d&#34;,&#34;dropCap&#34;:false}&#34;&gt;构建 DSL OOA 服务是向前迈出的重要一步。它不仅简化了开发过程，而且还为集成更改打开了大门，而无需复制决策树逻辑并在多个平台上部署代码。通过构建这个通用解决方案，我们可以避免在不同的团队中多次解决复杂的问题。尤其是，我们相信这一决定将在未来带来红利，因为 Uber 越来越多的垂直团队利用实时活动流程。&lt;br&gt;甚至 Eats 平台也已经使用了处理移动框架的很大一部分。与实时活动生命周期、图像缓存和功能标志注入。在看到 Rider 集成的积极成果后，他们还在评估 DSL 和 OOA 服务的上线。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7c605f28-0f27-430e-aa1b-856fb0a30877&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;456dd394-451c-43ec-8264-99820ff236a2&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-conclusion&#34;&gt;结论&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8fdbe365-654c-49bf-becc-de4ebf7d98b6&#34;,&#34;dropCap&#34;:false}&#34;&gt;构建 Rider iOS Live Activity 是一次紧张但收获颇丰的旅程。从 WWDC 宣布的惊喜到在紧迫的时间内开发和实施新技术的挑战，这些经历展示了我们团队的韧性、适应性和创造力。我们克服了技术障碍，重新定义了我们的用户体验方法，并最终提供了一项我们认为改善了骑手和驾驶员体验的功能。我希望我们在本文中所展示的独创性可以启发对于任何从事实时活动的开发人员来说，帮助他们克服类似的场景，并通常对主应用程序之外的体验采取务实的方法。&lt;br&gt;在个人层面上，这是一次很好的经历和机会：培养早期采用者对于我们这样规模的公司来说，开发产品具有挑战性，但却非常令人满意。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;73a51cf5-4e2c-4d8a-8ca9-51e142dde84b&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-acknowledgments&#34;&gt;致谢&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cfcd3e00-4b2b-4695-be8f-619d8c2dc7f9&#34;,&#34;dropCap&#34;:false}&#34;&gt;特别感谢给整个团队。这个项目不仅展示了每个人的技术实力，还凸显了 Uber 推动创新的协作精神。凯尔·加布里埃尔、Ken Nguyen、Tiffany Chang、Hyewon Son、Radhika Gemawat、Maxim Bogatov、丁一凡、Evan Cooper&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;0d80d3ae-d935-48d4-823a-9768c64618bd&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;Apple、App Store、Swift 和 SwiftUI 是 Apple Inc. 在美国和其他国家和地区注册的商标。&lt;br&gt;iOS 是商标或是 Cisco 在美国和其他国家/地区的注册商标，Apple 经许可使用。&lt;br&gt;Android 是 Google LLC 在美国和其他国家/地区注册的商标。&lt;/p&gt;</description>
      <pubDate>Thu, 25 Jul 2024 07:32:52 +0000</pubDate>
    </item>
    <item>
      <title>【Sparkle: Standardizing Modular ETL at Uber】Sparkle：Uber 的模块化 ETL 标准化</title>
      <link>https://www.uber.com/blog/sparkle-modular-etl/</link>
      <description>【&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;9a474da8-2e63-43fb-b245-d45ac76132dc&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-background&#34;&gt;Background&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;dc36e5e7-858b-4f20-8e29-53e6c309fdf7&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Uber’s data ecosystem comprises a complex and diverse big data landscape, operating at exabyte-scale and composed of a wide variety of tools to cater to each need such as ingestion layer (Apache Kafka®) and real-time compute (Apache Flink®), real-time analytics (Apache Pinot™), batch compute and aggregation layer (Spark ETL, Presto ETL, &lt;a href=&#34;https://www.uber.com/en-IN/blog/no-code-workflow-orchestrator/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;uWorc&lt;/a&gt;), batch analytics (Query Builder), ML studio (For building ML models), visualization (Tableau, Google Studio), different types of data stores (DocStore, MySQL™, Apache Hive™, Apache Hudi, TerraBlob), etc.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;bca2d727-f2cc-4f96-86a7-9129caa4bc93&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In 2023, the Uber Data platform migrated all batch workloads to Apache Spark™-based computation. Around 20,000+ critical pipelines and datasets are used to power the batch workloads and more than 3,000+ engineers are responsible for creating pipelines and owning datasets.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f7bfb52b-ffcf-4e49-81d3-c42c40256dd7&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;efad5664-0996-4562-9dd3-e7207eddd4c7&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdE0HNqTI2VLT-Ulz9HqL20RZYHIGNC9XLQ_ssQD1twN2fgcSxR7CxrnkwakmtmKy5K6O6FhVec0QVyldyxc2qamIUMN_9EK-wMxCSNV5SPsqUv01kpNWiEm-g3IsBpwDrR4zIItw7O2afvBL2p45ewoFhj?key=Xnzw8ihRQ2AsRE5-ZhryOA&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Data Technology Stack At Uber.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ff9190cf-14ed-4a9e-b485-890c37ff20c2&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;62cecafa-d9a0-4363-8af5-104825bf312d&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-why-an-etl-framework&#34;&gt;Why an ETL Framework&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4936b526-48ff-4df5-9abe-3c5f54a3e9cd&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Uber has standardized the backend development flow where 5,000+ services are being built and managed by thousands of backend engineers. JFX is the application framework built on top of Java Spring Boot service and &lt;a href=&#34;https://uber-go.github.io/fx/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;UberFx&lt;/a&gt; is the framework built for GO language-based service to assist developers in improving productivity. These frameworks make it easy for developers to write composable, testable apps using dependency injection. It removes boilerplate, global state, and package-level init functions. This also eliminates the need for service owners to install and manage individual libraries manually and provides multiple components as a package out of the box during service bootstrapping.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a86ae31b-c345-4719-b62c-2298a1e470d4&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Similar complexities exist in the data world, but the data development cycle is not standardized, unlike backend or mobile development. Also, the concept of test-driven development in ETL is non-existent. More than 90% of the pipelines do not have any unit test cases available during development and the testing usually happens in the staging layer.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e301926b-0564-4066-b7b6-27cb6c4322ad&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We wanted to implement a framework model similar to the one used in the backend into the data development lifecycle. This would allow developers to focus solely on writing business logic while eliminating the need for repetitive code common in pipeline writing. Additionally, we aim to provide a unit testing framework that requires minimal configuration, allowing for better pipeline test coverage.&amp;nbsp;&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;511e8300-7565-46fe-9572-ed3510a76da7&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;80fe7a42-765e-48b7-9539-3ecd4e44b13f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;em&gt;The below diagram depicts the components that are expected to be packaged as part of an ETL tool:&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;10ad83fe-4c6e-4f61-887e-e0018ed616dc&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfImgYCM6udr7xsoeQp3CpPxaW6kSq8fcirevVZAlnrgY7NgL17w8dHj0Astlm47z1rtuvqIanfiEYA5w0Ct-YNirgl_5cSlynxe-9WKywu9WHd9fA19xszoW9LJMO6Z1CeWQv-ApzqinD0aGEovguHEwIL?key=Xnzw8ihRQ2AsRE5-ZhryOA&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; Components that are expected to be packaged as part of an ETL tool.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;04c09293-e1e2-4803-96ae-be7cde7e44b3&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1cea999b-9712-49fd-a1ff-6a8a01d4d832&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-industry-trends&#34;&gt;Industry Trends&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;643e6cd7-7a54-4b67-be0d-1fa9dfc68f4e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The industry is also moving towards writing ETL jobs similarly to any other software development practice, with features such as modular ETL, test-driven development, data quality checks, observability, version control, etc. One such tool that is popular among the data community is &lt;a href=&#34;https://www.getdbt.com/blog/what-exactly-is-dbt&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;DBT&lt;/a&gt;. However, introducing a new ETL tool other than Spark for this problem would be challenging due to the complex Uber data ecosystem, scale of the data, developer language preference, and an increased developer learning curve. The challenge is to add new features that meet industry standards without disrupting the existing Uber developer experience.&lt;br&gt;&lt;br&gt;To address these challenges, the &lt;strong&gt;&lt;em&gt;Sparkle&lt;/em&gt;&lt;/strong&gt; framework was developed. This framework was written on top of native Apache Spark™, simplifying Spark pipeline development and testing, while still making the best use of Spark’s capabilities. The framework supports writing configuration-based modular ETL jobs and incorporates test-driven ETL development, which aligns with current industry trends.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c2eb9f23-dd8c-4aa4-99f6-be48393f1f69&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Sparkle provides boilerplate code and various source and sink integrations out of the box so that the ETL developer can just focus on writing the business logic expressed in either SQL or Java/Scala/Python-based procedural blocks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b2697c68-088d-4b84-9686-45891cb95da6&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ebecd4c5-f458-4dbd-9e47-50924f8458ac&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-architecture&#34;&gt;Architecture&lt;/h2&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;id&amp;quot;:1094254,&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;sizeSlug&amp;quot;:&amp;quot;large&amp;quot;,&amp;quot;linkDestination&amp;quot;:&amp;quot;none&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;bcb04643-d2ff-4b38-be7f-a9a9c031814a&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter size-large is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;936&#34; height=&#34;1024&#34; src=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/sparkle-layered-architecture-1-17236489174498-936x1024.jpeg&#34; alt=&#34;&#34; class=&#34;wp-image-1094254&#34; style=&#34;width:700px;height:auto&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=936,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/sparkle-layered-architecture-1-17236489174498.jpeg 936w, https://blog.uber-cdn.com/cdn-cgi/image/width=274,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/sparkle-layered-architecture-1-17236489174498.jpeg 274w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/sparkle-layered-architecture-1-17236489174498.jpeg 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1405,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/sparkle-layered-architecture-1-17236489174498.jpeg 1405w, https://blog.uber-cdn.com/cdn-cgi/image/width=1873,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/sparkle-layered-architecture-1-17236489174498.jpeg 1873w&#34; sizes=&#34;(max-width: 936px) 100vw, 936px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; High-level flow of the sparkle framework.&lt;br&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cae90125-0f99-4630-8f6a-c44e7421d82e&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;id&amp;quot;:1094216,&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;sizeSlug&amp;quot;:&amp;quot;full&amp;quot;,&amp;quot;linkDestination&amp;quot;:&amp;quot;none&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;bd373a21-a294-47e5-a45e-2739d1eedeb5&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter size-full is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;724&#34; height=&#34;757&#34; src=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/sparkle-components-17236416505263.jpeg&#34; alt=&#34;&#34; class=&#34;wp-image-1094216&#34; style=&#34;width:700px;height:auto&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=724,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/sparkle-components-17236416505263.jpeg 724w, https://blog.uber-cdn.com/cdn-cgi/image/width=287,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/sparkle-components-17236416505263.jpeg 287w&#34; sizes=&#34;(max-width: 724px) 100vw, 724px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Figure 4:&lt;/strong&gt; Details of different Technical Components used in Sparkle.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;262955c6-6f24-4c4d-b4d7-6cd430ad1ff9&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-introduction-to-modular-etl&#34;&gt;Introduction to Modular ETL&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;449d3663-03c5-459e-8c6d-a4932286cb7b&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;391ace85-1891-4206-8649-a53f07b2d462&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The core concept behind Sparkle architecture is the ability for users to express business logic as a sequence of modules. Each module in a Sparkle framework is a unit of transformation that can be expressed either as SQL, procedural code, or data extracted from any external data source.&lt;br&gt;&lt;br&gt;&lt;em&gt;The below snippet depicts the sample module configuration defined in Sparkle base YAML:&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;cb407545-14a7-42fa-90ae-6f601cd6779f&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXc1LUP_hKly3TsaPRycDhoEx1mZ5FkHkrKyawiNSRO3szPKoU76inm2C1rPtjZhDazVSE7ku74F8V_GNjICvXP8afGDiUduBBfpOCm0RwdPdmotC3KM1EyThjQy8jBY1RGKhQl3Lvaeev2Oc66CGXcH2Wio?key=Xnzw8ihRQ2AsRE5-ZhryOA&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&amp;nbsp;&lt;strong&gt;Figure 5:&lt;/strong&gt; Configuring workflow in Base YAML, defining relationships between the modules.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;52a00f85-8121-4333-aa28-e094d7f3eda8&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1a4b390f-b5da-4cb3-84f9-092f248d6dd6&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;All the modules are defined as sequences under workflow config. In the sqlFile module (source: sqlFile), the SQLs are expressed as &lt;a href=&#34;https://jinja.palletsprojects.com/en/3.1.x/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Jinja&lt;/a&gt; templates, and the properties loaded in the application context are accessible either as template variables in the SQLs or accessed as Environment variables in the procedural module (source: classTransformation). The module output gets stored as a Spark temp table using the variable &lt;em&gt;outputTableName,&lt;/em&gt; which gets referred to in subsequent modules. Configs defined under &lt;em&gt;UDFs&lt;/em&gt; are used to register any Spark or Hive UDFs in the Spark context.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f79daadf-417c-4050-8728-ce08fd383361&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;890d2fd2-2e3e-45dd-957f-0b3dfb214bf6&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;em&gt;The below snippet depicts the Jinja template-based sample SQL file:&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;640px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;ff9de40a-378f-4378-b50f-1a3e931e826a&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;wp-block-image is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeH91HHm36vVz278-0hoW0VJ9sa1JUT6pkFggrGLOwq8tMXNp1HmG5VP8ekKjFBrzEDahgxmXUG4ghNZgx3mjg46vFBYvIi__WQ87KAO0eR055SX2Y4lEsPQDBZbcqQ98asPzTQe11sUxCeS0Av-vUp2z4?key=Xnzw8ihRQ2AsRE5-ZhryOA&#34; alt=&#34;&#34; style=&#34;width:640px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;&amp;nbsp;Figure 6:&lt;/strong&gt; SQL transformation, reading from the source tables with the required filters defined as Jinja template variables.&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0c5afdae-9aeb-4f56-91c4-20a2ecafe3e7&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;476888e5-46ca-4ec7-97f4-722cae16af9f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;em&gt;The below snippet depicts the sample procedural transformation block in Java&lt;/em&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;id&amp;quot;:1094194,&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;sizeSlug&amp;quot;:&amp;quot;full&amp;quot;,&amp;quot;linkDestination&amp;quot;:&amp;quot;none&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;ae520d69-e941-4779-a9ce-8d6486fef20f&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;wp-block-image size-full is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;641&#34; height=&#34;314&#34; src=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/figure-3-17235599374318.jpeg&#34; alt=&#34;&#34; class=&#34;wp-image-1094194&#34; style=&#34;width:700px;height:auto&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=641,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/figure-3-17235599374318.jpeg 641w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/figure-3-17235599374318.jpeg 300w&#34; sizes=&#34;(max-width: 641px) 100vw, 641px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Figure 7:&lt;/strong&gt; Class Transformation, implementing ITransform interface method apply().&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;82091ae7-44a0-4d32-9d46-f36406161041&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cd5ff578-57fc-47f5-ae2c-e2c17c6c65a5&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;As the final step, the defined module outputs, which are stored as Spark Temp-table, can be persisted to different target sinks by defining the required write configs via Env YAML.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b75db90b-81ec-4af5-a4a7-51f343e9c873&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cd5ff578-57fc-47f5-ae2c-e2c17c6c65a5&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;em&gt;The below snippet depicts the sample Env YAML:&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;916a6d3a-9466-4df7-a960-9d24c24f4e1e&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeoUnQQAd7f0OhoGfX4JeUqFfIHUi6BtO4aMFG2JgNXSVs04sCYYE11vv4vVMArCe9TujJY0kHHN7yYbQepLqEd7JlN91pQ97GBXRwRHb6yVELiQxHKKYwQka7l51lcgUual4GYVS3-dSHNI7MX6XSybLET?key=Xnzw8ihRQ2AsRE5-ZhryOA&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Figure 8:&lt;/strong&gt;&amp;nbsp; Configuring applicationConfigMap, writeConfigs, and connector configs in Env YAML (prod, dev, staging).&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;74283e9f-5f65-49a1-b804-eff064cc26bc&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c0f92d9c-f540-46f3-9e84-796dd40cfb75&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;After the creation of the pipeline, Sparkle provides an option for users to test the pipeline locally both at the level of each module and of the entire pipeline. Users can test their pipeline by providing the required mock data as table inputs, any test-specific configs, and the necessary assertions defined as SQL queries.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fd98d99b-eae4-407c-9e27-629164e456c4&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c0f92d9c-f540-46f3-9e84-796dd40cfb75&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;em&gt;The below snippet depicts the sample SQL assertions used in unit testing&lt;/em&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;2ddd0809-86b8-41be-8f8d-cec6ffe321d9&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdomJNYkxoNH4LOrrqKcsWt2bsorqtIjjQaTiSpOv55JJqM_jYtkZEMLwAsklsLE1vMPKbJZiM7q7LRBBJXhkbiwyePuB8fpyetRhMbE2DKtEhpvN80Pqs80JAIEQ5ENSAn0nsxexKuRRkHzQfXyJtPzSte?key=Xnzw8ihRQ2AsRE5-ZhryOA&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Figure 9: &lt;/strong&gt;SQL Validation queries which evaluate to&amp;nbsp; Boolean. Unit test is considered to have passed if all the test cases ( validation SQLs) assert TRUE.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;34c3762a-236c-4ae2-acb8-ac1a1d112b72&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ecbbff39-03a0-4d92-8651-6df3c988663d&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-before-vs-after-sparkle-flow&#34;&gt;Before vs After Sparkle Flow&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;04066f12-93b0-4e65-a711-98f54fdbcea9&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;880a5ab8-c55d-4c74-a3a1-57e16ece5198&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXd9ft0ysZTVsos4nwfB6g7C32C9aIYqWJx4h09tXOUGPAg1-0xpfDBHJ_8Ia66NpMCB1O9rGJj55jnJll1vKlH1LWyW1w3Qq5RyqJIYyQzdLjdQRUmJ_AzJAE2cyU3-DHM73i0RLgDk6MKpfjHjqczRyp-p?key=Xnzw8ihRQ2AsRE5-ZhryOA&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;Figure 10:&lt;/strong&gt; Streamlining ETL: From complexity to simplicity with Sparkle Framework.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e28aeb6e-3918-4227-9daf-fd9dea2adc20&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9d569688-797b-4414-b31f-6423cefa650f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The above diagram depicts a flow comparison for the user writing ETL before and after using Sparkle. We can notice that all the sub-steps in each step would be defined and re-defined in each new pipeline in the BEFORE flow. At the same time, users only write the business logic in the AFTER flow and every other step is managed via configuration.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;86bf4a4a-6b01-4d72-9810-acd17eba36db&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0c7cbd95-b29f-496e-9395-804d885e4f73&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-benefits-and-impact&#34;&gt;Benefits and Impact&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3d8c4cc6-1dee-4378-9d32-f96331e4910d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Following are some benefits and impacts realized after adopting the Sparkle framework.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;672a89fb-dc5c-441f-b231-008bda42fdea&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Code Reusability&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Support for custom implementation of readers, writers, connectors, and lookup support&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;User-provided environment configuration, native integration with dynamic config, and secret management tools&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Developer productivity improvement by at least 30%&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Easy way to write a mix of SQL and procedural-based ETL by helping developers focus only on writing business logic&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Simplify the complexity of writing Spark jobs with a minimal understanding of Spark internals&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Standardized ETL creation @ Uber&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;A single framework that can be used across different languages like Java, Scala, and Python&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Improved Data Quality with 100% test coverage&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Support to create multiple test suites to test each transformation module&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Support to create multiple test suites to test pipelines end-to-end in local mode&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Optimized Performance with a minimum of 5x improvement&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;By migrating the existing Hive-based ETL to Sparkle-based ETL, there was a performance improvement of a minimum of 5x in each pipeline in execution time and resource utilization. Hive to Spark migration brings inherent performance benefits. Additional performance gains were achieved due to:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;825615c9-37eb-4019-a416-29cecde970fd&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Previously multiple Hive SQLs in a single ETL job were executed by the DAG defined by ETL developers via an orchestrator tool. This often resulted in unoptimized execution. In contrast, Sparkle uses the Spark SQL query planner and automatic DAG generation for the multiple Spark SQLs defined for a single ETL job. This ensures that optimal resources are used for computation.&amp;nbsp;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Each hive SQL gets executed in a separate hive context and intermediate SQL outputs get persisted in a hive table. Whereas Sparkle executes all the SQL in a single SparkContext and registers each of this intermediate SQL output as Spark temp tables and it has support for enabling cache to reuse DAGs on multiple target writes. Due to the in-memory stage compute and reused DAG, executions resulted in better performance as compared to the previous approach.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;274c6bd5-776d-4950-a1c0-df2ec8dc41a3&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;5c0445b3-768e-446d-9d7c-469bdfb2abfb&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-way-forward&#34;&gt;Way Forward&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6be560f5-0295-4575-9033-3615312b07f8&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In the future, we aim to improve the developer experience in ETL creation by integrating Sparkle with &lt;a href=&#34;https://www.uber.com/en-IN/blog/no-code-workflow-orchestrator/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;uWorc&lt;/a&gt;, extending support for various sources and sinks including Cassandra, DocStore, and custom cloud connectors, providing Hudi incremental read support for Hudi-based sources, and migrating legacy frameworks to Sparkle-based pipelines for batch ETL standardization at Uber.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4d99d613-2f47-4205-a25f-9fc12754de3e&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a73911e3-8bca-49e7-9123-718f8e9a44bf&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9d6509f0-3fe2-4848-8c8a-576e34c02916&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Many thanks to the team members of Uber Data Intelligence and the Uber GDW team for the contribution of the many Sparkle features.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;bc8c06ac-d6d4-4781-81db-fd328f571993&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Apache®, Apache Kafka®, Apache Flink®, Apache Pinot™, Apache Spark™, and the Apache Hive™ logo are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries. No endorsement by The Apache Software Foundation is implied by the use of these marks.&lt;br&gt;MySQL™ are registered trademarks of Oracle and/or its affiliates. No endorsement by Oracle is implied by the use of these marks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;c37e7076-1829-4eea-bf54-d1b6d3462873&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;“Steel Wool Sparks on the Beach” by Photo Extremist is licensed under CC BY-ND 2.0. To view a copy of this license, visit &lt;a href=&#34;https://creativecommons.org/licenses/by-nd/2.0/?ref=openverse&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;https://creativecommons.org/licenses/by-nd/2.0/?ref=openverse&lt;/a&gt;. No Changes were made.&lt;/p&gt;】&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;9a474da8-2e63-43fb-b245-d45ac76132dc&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-background&#34;&gt;​​背景&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;dc36e5e7-858b-4f20-8e29-53e6c309fdf7&#34;,&#34;dropCap&#34;:false}&#34;&gt;Uber 的数据生态系统包含复杂多样的大数据环境，以 EB 级运行，并由各种工具组成，以满足各种需求，例如摄取层 (Apache Kafka®) 和实时计算 (Apache Flink®)、实时分析 (Apache Pinot™)、批量计算和聚合层（Spark ETL、Presto ETL、&lt;a href=&#34;https://www.uber.com/en-IN/blog/no-code-workflow-orchestrator/&#34; 目标=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;uWorc&lt;/a&gt;)、批量分析（查询生成器）、ML studio（用于构建 ML 模型）、可视化（Tableau、Google Studio）、不同类型的数据存储（DocStore、 MySQL™、Apache Hive™、Apache Hudi、TerraBlob) 等&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;bca2d727-f2cc-4f96-86a7-9129caa4bc93&#34;,&#34;dropCap&#34;:false}&#34;&gt;2023 年， Uber Data 平台将所有批处理工作负载迁移到基于 Apache Spark™ 的计算。大约 20,000 多个关键管道和数据集用于为批处理工作负载提供支持，超过 3,000 多名工程师负责创建管道和拥有数据集。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f7bfb52b-ffcf-4e49-81d3-c42c40256dd7&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34; :&#34;efad5664-0996-4562-9dd3-e7207eddd4c7&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent. com/docsz/AD_4nXdE0HNqTI2VLT-Ulz9HqL20RZYHIGNC9XLQ_ssQD1twN2fgcSxR7CxrnkwakmtmKy5K6O6FhVec0QVyldyxc2qamIUMN_9EK-wMxCSNV5SPsqUv01kpNWiEm-g3IsBpwDrR4 zIItw7O2afvBL2p45ewoFhj?key=Xnzw8ihRQ2AsRE5-ZhryOA&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element- Caption&#34;&gt;&lt;strong&gt;图 1：&lt;/strong&gt; Uber 的数据技术堆栈。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ff9190cf-14ed-4a9e-b485-890c37ff20c2&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;62cecafa-d9a0-4363-8af5-104825bf312d&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-why-an-etl-framework&#34;&gt;为什么选择 ETL 框架&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;哈希&#34;:&#34;4936b526-48ff-4df5-9abe-3c5f54a3e9cd&#34;,&#34;dropCap&#34;:false}&#34;&gt;Uber 已经标准化了后端开发流程，其中数千名后端工程师正在构建和管理 5,000 多项服务。 JFX 是构建在 Java Spring Boot 服务和 &lt;a href=&#34;https://uber-go.github.io/fx/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;UberFx&lt;/a 之上的应用程序框架&gt; 是为基于GO语言的服务构建的框架，旨在帮助开发者提高生产力。这些框架使开发人员可以轻松地使用依赖项注入编写可组合、可测试的应用程序。它删除了样板文件、全局状态和包级初始化函数。这也消除了服务所有者手动安装和管理各个库的需要，并在服务引导期间将多个组件作为开箱即用的包提供。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a86ae31b-c345-4719-b62c-2298a1e470d4&#34;,&#34;dropCap&#34;:false}&#34;&gt;存在类似的复杂性在数据世界中，但数据开发周期并不标准化，这与后端或移动开发不同。而且，ETL 中不存在测试驱动开发的概念。超过 90% 的管道在开发过程中没有任何可用的单元测试用例，测试通常发生在登台层。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e301926b-0564-4066-b7b6-27cb6c4322ad&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们想要在数据开发生命周期中实施与后端使用的框架模型类似的框架模型。这将使开发人员能够专注于编写业务逻辑，同时消除管道编写中常见的重复代码的需要。此外，我们的目标是提供一个需要最少配置的单元测试框架，从而实现更好的管道测试覆盖率。  &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;511e8300-7565-46fe-9572-ed3510a76da7&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;80fe7a42-765e-48b7-9539-3ecd4e44b13f&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;em&gt;下图描述了预期打包为 ETL 工具一部分的组件：&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;10ad83fe-4c6e-4f61-887e-e0018ed616dc&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfImgYCM6udr7xsoeQp3CpPxaW6kSq8fcirevVZAlnrgY7NgL17w8dHj0Astlm47z1rtuvqIanfiEYA5w0Ct-YNirgl_ 5cSlynxe-9WKywu9WHd9fA19xszoW9LJMO6Z1CeWQv-ApzqinD0aGEovguHEwIL?key=Xnzw8ihRQ2AsRE5-ZhryOA&#34; alt =&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;图 2：&lt;/strong&gt;预计将打包为 ETL 工具一部分的组件。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;04c09293-e1e2-4803-96ae-be7cde7e44b3&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1cea999b-9712-49fd-a1ff-6a8a01d4d832&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-industry-trends&#34;&gt;行业趋势&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;643e6cd7-7a54-4b67-be0d-1fa9dfc68f4e&#34;,&#34;dropCap&#34;:false}&#34;&gt;该行业是还朝着与任何其他软件开发实践类似的方式编写 ETL 作业，具有模块化 ETL、测试驱动开发、数据质量检查、可观察性、版本控制等功能。在数据社区中流行的一个这样的工具是&lt;a href=&#34;https://www.getdbt.com/blog/what-exactly-is-dbt&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;DBT&lt;/a&gt;。然而，由于 Uber 数据生态系统复杂、数据规模大、开发人员语言偏好以及开发人员学习曲线增加，引入 Spark 以外的新 ETL 工具来解决此问题将具有挑战性。我们面临的挑战是在不影响现有 Uber 开发者体验的情况下添加符合行业标准的新功能。&lt;br&gt;&lt;br&gt;为了应对这些挑战，我们开发了 &lt;strong&gt;&lt;em&gt;Sparkle&lt;/em&gt;&lt;/strong&gt; 框架。该框架是在本机 Apache Spark™ 之上编写的，简化了 Spark 管道开发和测试，同时仍然充分利用 Spark 的功能。该框架支持编写基于配置的模块化 ETL 作业，并结合了测试驱动的 ETL 开发，这符合当前的行业趋势。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c2eb9f23-dd8c-4aa4-99f6-be48393f1f69&#34;,&#34;dropCap&#34;:false}&#34;&gt;Sparkle 提供样板开箱即用的代码以及各种源和接收器集成，以便 ETL 开发人员可以专注于编写以 SQL 或基于 Java/Scala/Python 的过程块表示的业务逻辑。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b2697c68-088d-4b84-9686-45891cb95da6&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ebecd4c5-f458-4dbd-9e47-50924f8458ac&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-architecture&#34;&gt;架构&lt;/h2&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;id&#34;:1094254,&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;sizeSlug&#34;:&#34;大&#34;,&#34;linkDestination&#34;:&#34;无&#34;,&#34;对齐n&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;bcb04643-d2ff-4b38-be7f-a9a9c031814a&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter size-large is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34;解码=“异步”宽度=“936”高度=“1024”src=“https://blog.uber-cdn.com/cdn-cgi/image/width=2160，质量= 80，onerror =重定向，格式=自动/wp-content/uploads/2024/08/sparkle-layered-architecture-1-17236489174498-936x1024.jpeg&#34; alt=&#34;&#34; class=&#34;wp-image-1094254&#34; style=&#34;width:700px;height:auto&#34; srcset =“https://blog.uber-cdn.com/cdn-cgi/image/width=936，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/08/sparkle-layered -architecture-1-17236489174498.jpeg 936w，https://blog.uber-cdn.com/cdn-cgi/image/width=274，quality=80，onerror=redirect，format=auto/wp-content/uploads/ 2024/08/sparkle-layered-architecture-1-17236489174498.jpeg 274w，https://blog.uber-cdn.com/cdn-cgi/image/width=768，quality=80，onerror=redirect，format=auto /wp-content/uploads/2024/08/sparkle-layered-architecture-1-17236489174498.jpeg 768w，https://blog.uber-cdn.com/cdn-cgi/image/width=1405，quality=80， onerror=redirect，format=auto/wp-content/uploads/2024/08/sparkle-layered-architecture-1-17236489174498.jpeg 1405w，https://blog.uber-cdn.com/cdn-cgi/image/width =1873，质量=80，onerror=重定向，格式=自动/wp-content/uploads/2024/08/sparkle-layered-architecture-1-17236489174498.jpeg 1873w“尺寸=”（最大宽度：936px）100vw， 936px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;图 3：&lt;/strong&gt; Sparkle 框架的高级流程。&lt;br&gt;&lt;/figcaption&gt;&lt;/图&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cae90125-0f99-4630-8f6a-c44e7421d82e&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;id&#34;:1094216,&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;sizeSlug&#34;:&#34; full&#34;,&#34;linkDestination&#34;:&#34;无&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;bd373a21-a294-47e5-a45e-2739d1eedeb5&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter size-full is-resized&#34;&gt;&lt;img 加载=“惰性”解码=“异步”宽度=“724”高度=“757”src=“https://blog.uber-cdn.com/cdn-cgi/image/width= 2160，质量= 80，onerror =重定向，格式=自动/wp-content/uploads/2024/08/sparkle-components-17236416505263.jpeg“ alt =“”类=“ wp-image-1094216”样式=“宽度： 700px;高度：自动” srcset =“https://blog.uber-cdn.com/cdn-cgi/image/width=724，质量= 80，onerror =重定向，format=auto/wp-content/uploads/2024 /08/sparkle-components-17236416505263.jpeg 724w，https://blog.uber-cdn.com/cdn-cgi/image/width=287，quality=80，onerror=redirect，format=auto/wp-content/ uploads/2024/08/sparkle-components-17236416505263.jpeg 287w&#34;sizes=&#34;(max-width: 724px) 100vw, 724px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt; strong&gt;图4：&lt;/strong&gt;详细信息Sparkle 中使用的不同技术组件。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;262955c6-6f24-4c4d-b4d7-6cd430ad1ff9&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-introduction-to-modular-etl&#34;&gt;模块化 ETL 简介&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;449d3663-03c5-459e-8c6d-a4932286cb7b&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;391ace85-1891-4206-8649-a53f07b2d462&#34;,&#34;dropCap&#34;:false}&#34;&gt;核心概念Sparkle 架构背后是用户将业务逻辑表达为一系列模块的能力。 Sparkle 框架中的每个模块都是一个转换单元，可以表示为 SQL、过程代码或从任何外部数据源提取的数据。&lt;br&gt;&lt;br&gt;&lt;em&gt;下面的代码片段描述了在Sparkle 基础 YAML：&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34; :&#34;cb407545-14a7-42fa-90ae-6f601cd6779f&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent. com/docsz/AD_4nXc1LUP_hKly3TsaPRycDhoEx1mZ5FkHkrKyawiNSRO3szPKoU76inm2C1rPtjZhDazVSE7ku74F8V_GNjICvXP8afGDiUduBBfpOCm0RwdPdmotC3KM1EyThjQy8jBY1RGKhQl3Lvaeev 2Oc66CGXcH2Wio?key=Xnzw8ihRQ2AsRE5-ZhryOA&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt; &lt;strong&gt;图 5：&lt;/strong&gt; 在 Base YAML 中配置工作流程，定义模块之间的关系。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;52a00f85-8121-4333-aa28-e094d7f3eda8&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1a4b390f-b5da-4cb3-84f9-092f248d6dd6&#34;,&#34;dropCap&#34;:false}&#34;&gt;所有模块在工作流配置下定义为序列。在 sqlFile 模块（来源：sqlFile）中，SQL 表示为 &lt;a href=&#34;https://jinja.palletsprojects.com/en/3.1.x/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Jinja &lt;/a&gt; 模板，以及在应用程序上下文中加载的属性可以作为 SQL 中的模板变量进行访问，也可以作为过程模块中的环境变量进行访问（来源：classTransformation）。模块输出使用变量 &lt;em&gt;outputTableName&lt;/em&gt; 存储为 Spark 临时表，该变量在后续模块中引用。 UDF 下定义的配置用于在 Spark 上下文中注册任何 Spark 或 Hive UDF。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f79daadf-417c-4050-8728-ce08fd383361&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34; 类=&#34;wp-block-separator 有-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;890d2fd2-2e3e-45dd-957f-0b3dfb214bf6&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;em&gt;下面的代码片段描述了基于 Jinja 模板的示例 SQL 文件：&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;640px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;hash&#34;:&#34;ff9de40a-378f-4378- b50f-1a3e931e826a&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;wp-block-image is-resized&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/ AD_4nXeH91HHm36vVz278-0hoW0VJ9sa1JUT6pkFggrGLOwq8tMXNp1HmG5VP8ekKjFBrzEDahgxmXUG4ghNZgx3mjg46vFBYvI__WQ87KAO0eR055SX2Y4lEsPQDBZbcqQ98asPzTQe11sU xCeS0Av-vUp2z4?key=Xnzw8ihRQ2AsRE5-ZhryOA&#34; alt=&#34;&#34; style=&#34;width:640px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;图 6：&lt;/strong&gt; SQL 转换，使用定义为 Jinja 模板变量的所需过滤器从源表中读取数据。&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0c5afdae-9aeb-4f56-91c4-20a2ecafe3e7&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;476888e5-46ca-4ec7-97f4-722cae16af9f&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;em&gt;下面的代码片段描述了 Java 中的示例过程转换块&lt;/em&gt;：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;id&#34;:1094194,&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;sizeSlug&#34;:&#34; full&#34;,&#34;linkDestination&#34;:&#34;无&#34;,&#34;hash&#34;:&#34;ae520d69-e941-4779-a9ce-8d6486fef20f&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;wp-block-image size-full is-resized&#34; &gt;&lt;img 加载=“惰性”解码=“异步”宽度=“641”高度=“314”src=“https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality= 80,onerror=重定向，格式=自动/wp-content/uploads/2024/08/figure-3-17235599374318.jpeg&#34; alt=&#34;&#34; class=&#34;wp-image-1094194&#34; style=&#34;width:700px;height:自动” srcset =“https://blog.uber-cdn.com/cdn-cgi/image/width=641，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/08/figure -3-17235599374318.jpeg 641w，https://blog.uber-cdn.com/cdn-cgi/image/width=300，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/ 08/figure-3-17235599374318.jpeg 300w&#34;sizes=&#34;(max-width: 641px) 100vw, 641px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;图 7 ：&lt;/strong&gt;类Transformation，实现ITransform接口方法apply()。&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&amp;quot;82091ae7-44a0-4d32-9d46-f36406161041&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cd5ff578-57fc-47f5-ae2c-e2c17c6c65a5&#34;,&#34;dropCap&#34;:false}&#34;&gt;作为最终步骤，定义的模块输出（存储为 Spark 临时表）可以通过 Env YAML 定义所需的写入配置来持久化到不同的目标接收器。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b75db90b-81ec-4af5-a4a7-51f343e9c873&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cd5ff578-57fc-47f5-ae2c-e2c17c6c65a5&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;em&gt;下面的代码片段描述了示例 Env YAML：&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;916a6d3a-9466-4df7-a960-9d24c24f4e1e&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeoUnQQAd7f0OhoGfX4JeUqFfIHUi6BtO4aMFG2JgNXSVs04sCYYE11vv4vVMArCe9TujJY0kHHN7yYbQepLqEd7JlN 91pQ97GBXRwRHb6yVELiQxHKKYwQka7l51lcgUual4GYVS3-dSHNI7MX6XSybLET?key=Xnzw8ihRQ2AsRE5-ZhryOA&#34; alt=&#34;&#34;referrerpolicy =&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;图 8：&lt;/strong&gt; 在 Env YAML（prod、dev、staging）中配置 applicationConfigMap、writeConfigs 和连接器配置。&lt;/图标题&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;74283e9f-5f65-49a1-b804-eff064cc26bc&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c0f92d9c-f540-46f3-9e84-796dd40cfb75&#34;,&#34;dropCap&#34;:false}&#34;&gt;创建后对于管道，Sparkle 为用户提供了一个选项，可以在每个模块级别和整个管道级别本地测试管道。用户可以通过提供所需的模拟数据作为表输入、任何特定于测试的配置以及定义为 SQL 查询的必要断言来测试其管道。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fd98d99b-eae4-407c-9e27-629164e456c4&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c0f92d9c-f540-46f3-9e84-796dd40cfb75&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;em&gt;下面的代码片段描述了单元测试中使用的示例 SQL 断言&lt;/em&gt;：&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;2ddd0809-86b8-41be-8f8d-cec6ffe321d9&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdomJNYkxoNH4LOrrqKcsWt2bsorqtIjjQaTiSpOv55JJqM_jYtkZEMLwAsklsLE1vMP KbJZiM7q7LRBBJXhkbiwyePuB8fpyetRhMbE2DKtEhpvN80Pqs80JAIEQ5ENSAn0nsxexKuRRkHzQfXyJtPzSte?key=Xnzw8ihRQ2AsRE5 -ZhryOA&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;图 9：&lt;/strong&gt;计算结果为布尔值的 SQL 验证查询。如果所有测试用例（验证 SQL）都断言 TRUE，则认为单元测试已通过。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;34c3762a-236c-4ae2-acb8-ac1a1d112b72&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ecbbff39-03a0-4d92-8651-6df3c988663d&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-before-vs-after-sparkle-flow&#34;&gt;Sparkle Flow 之前与之后&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;04066f12-93b0-4e65-a711-98f54fdbcea9&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;880a5ab8-c55d-4c74-a3a1-57e16ece5198&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXd9ft0ysZTVsos4nwfB6g7C32C9aIYqWJx4h09tXOUGPAg1-0xpfDBHJ_8Ia66NpMCB1O9rGJj55jnJll1vKlH 1LWyW1w3Qq5RyqJIYyQzdLjdQRUmJ_AzJAE2cyU3-DHM73i0RLgDk6MKpfjHjqczRyp-p?key=Xnzw8ihRQ2AsRE5-ZhryOA&#34;替代=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;&lt;strong&gt;图 10：&lt;/strong&gt; 简化 ETL：使用 Sparkle Framework 从复杂性变为简单性。&lt;/figcaption&gt;&lt;/figcaption&gt;&lt;/图&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e28aeb6e-3918-4227-9daf-fd9dea2adc20&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9d569688-797b-4414-b31f-6423cefa650f&#34;,&#34;dropCap&#34;:false}&#34;&gt;上图描绘了用户使用 Sparkle 之前和之后编写 ETL 的流程比较。我们可以注意到，每个步骤中的所有子步骤都将在 BEFORE 流程中的每个新管道中定义和重新定义。同时，用户只需在 AFTER 流程中编写业务逻辑，其他所有步骤都通过配置进行管理。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;86bf4a4a-6b01-4d72-9810-acd17eba36db&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 数据可湿性粉剂块名称=“核心/标题”data-wp-block =“{”散列“：”0c7cbd95-b29f-496e-9395-804d885e4f73“，”level“：2}”class =“wp-block-heading”id =“h- Benefits-and-impact&#34;&gt;好处和影响&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3d8c4cc6-1dee-4378-9d32-f96331e4910d&#34;,&#34;dropCap&#34;:false}&#34;&gt;以下是一些采用 Sparkle 框架后实现的效益和影响。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;672a89fb-dc5c-441f-b231-008bda42fdea&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;代码可重用性&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:false,&#34;values&#34;:&#34;&#34;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;支持读取器、写入器、连接器和查找支持的自定义实现&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;用户提供的环境配置、与动态配置的原生集成以及秘密管理工具&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;开发人员生产力提高至少 30%&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:false,&#34;values&#34;:&#34;&#34;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;通过帮助开发人员专注于编写业务逻辑，轻松编写 SQL 和基于过程的 ETL 组合&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;通过对 Spark 内部结构有最少的了解，简化编写 Spark 作业的复杂性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;标准化 ETL 创建 @ Uber&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:false,&#34;values&#34;:&#34;&#34;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;可跨 Java、Scala 和 Python 等不同语言使用的单一框架&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;通过 100% 测试覆盖率提高数据质量&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:false,&#34;values&#34;:&#34;&#34;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;支持创建多个测试套件来测试每个转换模块&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;支持创建多个测试套件，以本地模式端到端测试管道&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;优化性能，至少提升 5 倍&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:false,&#34;values&#34;:&#34;&#34;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;通过将现有的基于 Hive 的 ETL 迁移到基于 Sparkle 的 ETL，性能得到了提升每个管道的执行时间和资源利用率至少提高 5 倍。 Hive 到 Spark 的迁移带来了固有的性能优势。获得额外的性能提升是由于：&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;825615c9-37eb-4019-a416-29cecde970fd&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;以前，单个 ETL 作业中的多个 Hive SQL 是由 ETL 开发人员通过协调器工具定义的 DAG 执行的。这通常会导致执行未优化。相比之下，Sparkle 使用 Spark SQL 查询规划器和自动 DAG 生成来生成为单个 ETL 作业定义的多个 Spark SQL。这确保了计算使用最佳资源。 &lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;每个 Hive SQL 都在单独的 Hive 上下文中执行，中间 SQL 输出保留在 Hive 表中。而 Sparkle 在单个 SparkContext 中执行所有 SQL，并将每个中间 SQL 输出注册为 Spark 临时表，并且它支持缓存在多个目标写入上重用 DAG。由于内存阶段计算和重用 DAG，与之前的方法相比，执行结果具有更好的性能。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;274c6bd5-776d-4950-a1c0-df2ec8dc41a3&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;5c0445b3-768e-446d-9d7c-469bdfb2abfb&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-way-forward&#34;&gt;前进的方向&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6be560f5-0295-4575-9033-3615312b07f8&#34;,&#34;dropCap&#34;:false}&#34;&gt;将来，我们的目标是通过将 Sparkle 与 &lt;a href=&#34;https://www.uber.com/en-IN/blog/no-code-workflow-orchestrator/&#34; target=&#34;_blank&#34; 集成来改善 ETL 创建中的开发人员体验rel=&#34;noreferrer noopener&#34;&gt;uWorc&lt;/a&gt;，扩展对各种源和接收器的支持，包括 Cassandra、DocStore 和自定义云连接器，为基于 Hudi 的源提供 Hudi 增量读取支持，并将旧框架迁移到基于 Sparkle 的管道用于 Uber 的批量 ETL 标准化。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4d99d613-2f47-4205-a25f-9fc12754de3e&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a73911e3-8bca-49e7-9123-718f8e9a44bf&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-acknowledgments&#34;&gt;致谢&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9d6509f0-3fe2-4848-8c8a-576e34c02916&#34;,&#34;dropCap&#34;:false}&#34;&gt;非常感谢 Uber Data Intelligence 和 Uber GDW 团队的团队成员对许多 Sparkle 功能的贡献。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;bc8c06ac-d6d4-4781-81db-fd328f571993&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;Apache®、Apache Kafka®、Apache Flink®、Apache Pinot™、Apache Spark™ 和 Apache Hive™ 徽标是 Apache 的注册商标或商标美国和/或其他国家的软件基金会。使用这些标记并不暗示 Apache 软件基金会的认可。&lt;br&gt;MySQL™ 是 Oracle 和/或其附属公司的注册商标。使用这些标记并不暗示 Oracle 的认可。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;c37e7076-1829-4eea-bf54-d1b6d3462873&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;Photo Extremist 的“海滩上的钢丝绒火花”已获得 CC BY-ND 2.0 许可。要查看此许可证的副本，请访问 &lt;a href=&#34;https://creativecommons.org/licenses/by-nd/2.0/?ref=openverse&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;https:// /creativecommons.org/licenses/by-nd/2.0/?ref=openverse&lt;/a&gt;。未进行任何更改。&lt;/p&gt;</description>
      <pubDate>Thu, 15 Aug 2024 07:13:22 +0000</pubDate>
    </item>
    <item>
      <title>【Odin: Uber’s Stateful Platform】Odin：Uber 的有状态平台</title>
      <link>https://www.uber.com/blog/odin-stateful-platform/</link>
      <description>【&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;90695143-31cf-483d-b85f-7ce1334bb6f8&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;548c2e1c-75fa-4ffa-905e-ca342260c0d0&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Uber employs various technologies for data storage, including well-known open-source products such as Kafka, Cassandra, and MySQL, alongside internally developed solutions. In 2014, Uber underwent rapid expansion. Like many startups, the technology teams manually performed provisioning and maintenance operations using runbooks. This approach led to operational toil as storage demands rapidly increased. Uber created a technology-agnostic management platform called Odin to uplevel operational throughput through automation and allow the teams to manage thousands of databases effortlessly.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8cf61d25-9d6d-40de-a187-c932da9d9811&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The Odin platform aims to provide a unified operational experience by encompassing all aspects of managing stateful workloads. These aspects include host lifecycle, workload scheduling, cluster management, monitoring, state propagation, operational user interfaces, alerting, auto-scaling, and automation. Uber deploys stateful systems at global, regional, and zonal levels, and Odin is designed to manage these systems consistently and in a technology-agnostic manner. Moreover, Odin supports co-location to increase hardware cost efficiency. All stateful workloads must be fully containerized, a relatively novel and &lt;a href=&#34;https://news.ycombinator.com/item?id=13054793&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;controversial&lt;/a&gt; concept when the platform was created.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6f97caa1-0a1f-46ab-8261-61b142951e9f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;This blog post is the first of a series on Uber’s stateful platform. The series aims to be accessible and engaging for readers with no prior knowledge of building container platforms and those with extensive expertise. This post provides an overview of Odin’s origins, the fundamental principles, and the challenges encountered early on. The next post will explore how we have safely scaled operational throughput, significantly improving our handling of large-scale, fleet-wide operations and up-leveling runbooks through workflows. Stay tuned for more posts in the series.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5d8805b0-a807-4ce6-b495-7e2138a76eba&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0e529716-5369-49b9-a516-66e71fa92691&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-the-scale-of-the-platform&#34;&gt;The Scale of the Platform&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c7275b4e-2c3a-465d-ae42-598f1e28f875&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Since 2014, Uber’s storage and data infrastructure has grown dramatically, from a few hundred hosts to over 100,000 today. These hosts support the operation of 300,000 workloads across various storage clusters. Each workload on the Odin platform is similar to a Kubernetes® pod, comprising a collection of containers. Currently, the platform manages 3.8 million individual containers. The fleet of hosts collectively boasts a storage capacity of multiple exbibytes and millions of compute cores.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;70705850-6138-44c0-b04f-cfc5d407b5a5&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The platform supports 23 technologies, ranging from traditional online databases such as MySQL® and Cassandra® to advanced data platform technologies, including HDFS™, Presto™, and Kafka®. It also integrates resource scheduling frameworks like Yarn™ and Buildkite™, primarily due to the platform’s robust capacity management and scaling solutions.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;bc701c08-c47b-4395-bee9-06a97e004924&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The platform is heavily colocated to leverage the resources available on the hosts best. Uber uses locally attached drives for all stateful workloads for performance. Our scheduler, therefore, has to optimize to keep allocation rates high across all three dimensions (i.e., CPU, memory, and disk) to be efficient. We boast a very high allocation rate of +95% on the bottlenecked resource, and we have hosts with +100 databases.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a952ccc3-67f4-44c4-8ebd-c3867d80f3cd&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The Odin platform is both technology-agnostic and cloud-agnostic through &lt;a href=&#34;https://www.uber.com/en-SE/blog/crane-ubers-next-gen-infrastructure-stack/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Uber’s cloud abstraction&lt;/a&gt; and currently runs across OCI, GCP, and Uber’s on-prem fleet.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0aba77a1-cb2f-4642-93ee-48c85afed8ba&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;49ba8eed-f6b5-48d7-9472-a225618b4db4&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-a-self-healing-platform&#34;&gt;A Self-Healing Platform&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;578c4714-ae37-4b95-94aa-7902076a06d7&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Odin is built to be fully declarative and intent-based. This results in properties such as failure tolerance, scalability, and self-healing. The desired state is expressed as the goal state, which the platform uses to converge the actual state automatically. We accomplish this by using an extensive collection of small, autonomous remediation loops designed to ensure the system’s actual state converges with the declared goal state by continuously nudging the system in the “right direction.”&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7df9c643-7221-4e50-99b6-e7223356e28f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Today, Odin and Kubernetes® share many similarities. This mental model will be helpful as you read through the blog series.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fe74b631-4317-499c-a507-32164ec308bc&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The example below illustrates the anatomy of a remediation loop. A remediation loop starts by inspecting the goal state. It then collects the actual state, identifies discrepancies between the two, and uses &lt;a href=&#34;https://cadenceworkflow.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Cadence™ workflows&lt;/a&gt; to adjust the system state to close the gap. This is similar to how Kubernetes controllers work–except they directly manipulate the state through the APIServer.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f6ae9904-86ac-423f-95e2-a04e0f831377&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;4743a97f-d970-4b9e-bcc7-60029013f43b&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXfrbiVvJh6xCuhP9K9RQRFzpiFd5uwiRBFOOtPiMfB7iRYc_z-MMSXNB34iLdfE_-mIIx2HbQz08pY1IRYeuaaa_W6JlTy2IvK3owVsrkDSPqnwXAXAvy9LX3zySlsEbvmUXL3AcAaXGIMgzpvf0RwXAeP-?key=0kVK3UHY5mqh0YpEHEfxHw&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 1: High-level overview of an Odin remediation loop.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;79575b6a-b658-4b7e-adff-72767d32195f&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8b602db4-cbe7-46fb-8ce3-a60810a26396&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Our remediation loops prioritize modularity and loose coupling. This facilitates horizontal scaling and ease of development so that each loop can be developed as an independent microservice.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2397929c-c8f4-432b-ba9d-96439c194265&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;On the Odin platform, we have streamlined the development of these loops by providing an up-to-date view of the global system state through our data integration platform, &lt;a href=&#34;https://eng.uber.com/grail&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Grail&lt;/a&gt;. Grail delivers a unified data model that spans all technologies and provides an up-to-date view of the platform’s goal and actual states, including hosts, containers, and even the technology-specific internal state. Our Grail setup also allows performing queries with a global scope, fetching data provided in all the data centers and regions around the globe where Uber operates. In essence, think of Grail as the resource model in the Kubernetes APIServer on steroids.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e0b4f9d7-27e9-4f5a-b0be-32ff310cbbe9&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The platform’s control plane focuses solely on high-level decisions, such as scheduling workloads and managing cluster topology.&amp;nbsp; The API for manipulating the goal state is the execution of a workflow. Both remediation loops and human operators start workflows. The platform provides a UI that allows operators to quickly overview the state of their databases and apply operations when needed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;141a1dce-0b34-4801-b240-c434901bc5ec&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Workflows are incredibly valuable because of the insights they provide. A workflow for updating goal state usually consists of two steps. First, the goal state is updated, and then the workflow will monitor the actual state until it converges on the specified goal state. So when it doesn’t, it is clear which operations didn’t converge, including the context of why that operation was performed in the first place. As the actual state is not expected to converge immediately, the system must prevent the remediation loops from continuously starting workflows that attempt to fix the same issues.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cf949352-02eb-4d3b-a249-178fe472a2a3&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;271afd88-96a7-4bd9-849c-7e5886fc5d50&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXfkpBCcYU1h2PhNjamcT3aGcIQWIAr7wn-77iP7vY861TcFRrSJ9b4oayn5Brm7MgSzS3nqJqB0J9Qys0t6CnYtHWgIYckWP64HUxy-8LKxmnHBgC-DJU2zkpCZpmTZiLEJ4GP08aLD5yqOgOuk6S4KDsc4?key=0kVK3UHY5mqh0YpEHEfxHw&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 2: A 10.000ft view of Odin’s architecture.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;56db4950-7d76-45b8-819e-0dd0fa247fdd&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2acecab1-597e-464e-a287-66bd80786482&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-managing-generic-storage-clusters&#34;&gt;Managing Generic Storage Clusters&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d0ffd8b7-98aa-4deb-94ae-15aee37e8492&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Odin is built on a technology-agnostic cluster model that provides generic operations for managing clusters of workloads of any kind. This model supports generic operations such as adding or removing workloads, upgrading container images, and handling other cluster-related tasks uniformly across technologies. As the model and operations are technology-agnostic, the technology is required to extend the platform through a collection of plugins to tailor various aspects of cluster management according to their specific needs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;cda4e095-7482-482f-b6b2-a05a92a267d6&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-two-host-agents&#34;&gt;Two Host Agents&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;dd82e5e7-cf72-4e2e-ab87-2d536dd42dbe&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The Odin control plane is divided into two parts. So far, we have only discussed the global part. At the host level, there are two agents. One is shared among all workloads on the host and is technology-agnostic. We simply refer to this as the &lt;em&gt;odin-agent&lt;/em&gt;. The other agent is technology-specific and runs containerized as part of the workload. These two agents, when combined, serve the same functionality as the &lt;em&gt;kubelet&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;65a90e51-2220-4720-94ac-84668c27dbbd&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;a12eed6c-d521-456b-8c52-de1b38e4bec2&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdSePHcS8Ijkt17F1dBAJtAyQ4DANP0C3CoVQdZFqpTDNdMg6oUZLrPRUgoTrTUC_3ukppykFnZiFLkJkfTrH_iV_OW5XdhajSi6fKC9z7fSq9Yw6x8l0XgDvomdfCgZqe5Z5ZZYeHvmvAwoeaMPNZEyoHs?key=0kVK3UHY5mqh0YpEHEfxHw&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 3: The anatomy of an Odin host with the two host-level agents communicating with the global control plane.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e0817d94-ec97-4063-9a8c-4e9904175475&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1621dd43-a4be-45da-b5d6-0d17c0c5bb08&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The host agent communicates the latest goal and actual states with the global control plane. Its primary responsibility is managing host-level resource allocation, such as scheduling cores for workloads, creating and managing disk volumes/cgroups, and bootstrapping the workloads when assigned to the host.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8fa5a88e-540a-4c48-b1cc-ed7df86a3a00&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The technology-specific agent is known as the &lt;em&gt;worker&lt;/em&gt;. The worker is a supervisor running alongside each workload in a side container. Its primary role is to ensure that the running containers align with the goal state and to collect health and other relevant data for reporting to the global part of the control plane. The worker’s generic functionality includes starting and stopping containers. You can think of the worker as the translator between the generic cluster state the platform understands and the running database containers in the workload.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9965fa47-2eb1-4626-b1d3-dc518449f76a&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The technology team can customize the worker to collect the technology-specific actual state and apply the goal state to the workload. For instance, the MySQL worker is tasked with ensuring that MySQL masters and replicas maintain their roles and report the current state for consumption to the global control plane. This allows the team to write new workflows for operating the cluster based on the actual state of the cluster. Many technologies have extended their workers to perform self-healing cluster operations in their workers. Examples could be coordinating leader elections or data replication when the cluster topology changes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;5598ca20-758c-4080-b97b-81afc4cffece&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-optimizing-for-robustness-and-resilience&#34;&gt;Optimizing for Robustness and Resilience&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;26972dfb-89dd-40ce-906d-ca16199626d9&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Separating the control plane into a host-local and a global component allows us to deploy worker changes per workload, limiting the potential blast radius of bad changes to the smallest possible unit. The same applies to each container in a workload, as the platform allows in-place upgrading containers individually.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;138c489d-3c9f-4892-84e1-6387f9265a64&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;For both agents, a rule is that the goal state must first be written to disk before any changes to the running workloads are made. This approach guarantees self-sufficient clusters that can be initialized without an active control plane, effectively addressing the bootstrap issue. Furthermore, it offers robust failure resilience, allowing workloads to function during control plane degradations and simplifying writing emergency tooling.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;f07917a0-558e-42cb-b46f-c7a20196c247&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-transitioning-identities&#34;&gt;Transitioning Identities&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1eef1f47-896d-478c-9dfe-b43cd5b83885&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;As we continuously optimize for efficiency and modernize our host fleet, moving a workload from one host to another has become one of the most common operations on the platform. To date, we reschedule up to 60% of our stateful workloads per month. A crucial requirement of stateful workloads is maintaining the workload identity across rescheduling. This capability allows, for example, the replacement of one workload serving a shard with another serving the same shard. In Kubernetes, this is managed through StatefulSets by assigning a stable pod identity that persists across rescheduling. When a pod is deleted, a new one is provisioned with the same identity. Let us explore why this model does not suit Uber’s use case well.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1bbb811d-86a9-4cd2-b96b-21285d91dabe&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;As mentioned, Uber uses locally attached drives for all stateful workloads for performance. This setup requires that data on a host be replicated and made available elsewhere before the corresponding workload can be terminated. The cluster becomes temporarily under-provisioned if a workload is shut down before its replacement is fully ready. This delay, which is not negligible with a median time of 1 hour, exposes the cluster to an increased risk of data loss or availability issues. Moreover, this delay also diminishes the cluster’s overall capacity to handle requests.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;3b6e5f94-d8c1-4822-aff4-a93c79b744c9&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-replacing-workloads&#34;&gt;“Replacing” Workloads&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;aa7ad1d4-ea62-4c1c-910d-8286f06a3711&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Consequently, the most essential operation on Odin is migrating a workload from one host to another–or, in Odin terms, “replacing” the workload. A replace operation is purposely designed to &lt;em&gt;make-before-break&lt;/em&gt;. This means it creates a new workload that can carry the identity and data of the workload it replaces &lt;em&gt;before&lt;/em&gt; the old workload is shut down, wherever possible. The technology integrations can choose which parts of the goal state to propagate between workloads. The goal state model makes the two workloads explicitly aware of each other’s existence. This is essential because it allows the respective workers to coordinate the process for data replication and other things. In rare cases, when &lt;em&gt;make-before-break&lt;/em&gt; semantics are impossible (e.g., the old workload is on a failed host), the old workload is deleted simultaneously with the replacement workload being created.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f2c8a1a1-fe85-4656-bbd8-381c24da7641&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Using make-before-break also has an efficiency benefit, as the alternative would require us to run the clusters overprovisioned continuously. In our model, we only temporarily overprovision when we have to.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;de1ed6fc-1a1e-4666-b2e9-c58224a32d9e&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c29e483c-a48a-4924-a033-ef8503c36fe1&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-growing-pains&#34;&gt;Growing Pains&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;dad7f7a3-0ea3-479b-a1fe-9bfa5af38d18&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The first iteration of the Odin platform was a giant leap forward concerning the teams’ ability to operate their storage clusters. Storage teams could converge all their operations and insights on a single platform with a shared operational model. However, the platform was still very much human-centered. Humans were the principal actors initiating changes on the platform, and, crucially, they were the ones vouching for the safety and the fit of the operations performed. The role of the remediation loops was confined to operations with a relatively small scope, such as a single replace workflow&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fad8c154-683f-4e0d-ba9b-d1d4b7157cb8&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Operational overhead started to grow as the business continued to scale to the point where fleet-wide operations spanning technologies were not practically possible for human operators to do. Examples of complex, fleet-wide, cross-technology operations that require us to move workloads are:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0c6b29b5-a3b0-4770-b81e-5374de15e4b5&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Optimizing workload placement for resiliency against infrastructural failures like rack or zone failures&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Host bin-packing to improve cost-efficiency&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Fleet-wide host upgrades like new kernels, firmware, or functionality like LVM/cgroupv2&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Facilitating the bringing up of new data centers or decommissioning existing data centers&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;668aa557-dc41-41b6-a40a-29371389096e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Furthermore, the large number of databases on a single host makes handling host failures daunting. These limitations indicated that we had to double down on automation by developing new remediation loops with much broader scopes than we used to.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;56f8fe34-2ef2-4a3f-bb0d-4f69c90b960c&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-coordination-required&#34;&gt;Coordination Required&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8036e749-8a7a-4c7f-b617-1321eac2cf8f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The increased operational load could quickly result in outages if not carefully coordinated. Imagine a situation where a workload in a 3-node consensus-based storage cluster is taken down to perform a container upgrade. Now imagine another remediation loop that has identified that one of the other workloads in the cluster would benefit from being moved for efficiency reasons. If allowed, this would result in the storage cluster losing quorum, impacting the write availability.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;44963a9c-9e27-4921-91da-42d031dd81be&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Initially, we started augmenting the remediation loops to perform concurrency control internally (Figure 3). This way, they could prevent operations of the same type from creating problems for the storage clusters. For instance, the container upgrade loop would block overlapping upgrades. However, this approach fell short as it didn’t solve the issue of conflicting operations started by different remediation loops. Figure 3 shows a case where two independent operations can operate two workloads in the same storage cluster, resulting in an availability loss.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;37267e15-3898-4883-b5d5-6617d00f67ce&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;d131b897-0c2b-4684-8052-d5db42a85208&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXd4i6HPr3ZkL1WFmR1Kd-mkiLceVO0Liik1jb6NR51eaucwSAjFss_8QW_m-doDIHsbCW9kv5TJwgPukhH3rPiaE1o0lWbpvtlwSliGofFpGiD68jS5NGuxR1V3HZs6HxnZYsScd9vpweh0ouKSHRF0RlbN?key=0kVK3UHY5mqh0YpEHEfxHw&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 4: Internal concurrency control does not protect storage clusters against overlapping operations and can compromise availability.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4ab49524-3b00-4ca5-b966-d2a03569fdc2&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5304b888-d27b-4d5c-8703-a262a8183b7f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Different technologies exhibit widely varying tolerances for safe cluster operations. For some technologies, the specific workloads being operated are crucial, while all workloads are treated equally for others. Some technologies focus solely on ensuring that a certain number of workloads are always available within the cluster. This diversity in requirements made it such that we had to support customizing cluster management strategies to the specific needs of each technology.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d2d9eb70-3d33-4f06-a09d-71c7ba530c6c&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We needed to ensure global, cross-type coordination of operations using a system that could guarantee to preserve the cluster availability by leveraging technology-specific limits based on the current workload health and disruption budget. Furthermore, it should protect against engineering mistakes and overload of the internal platform systems by enforcing platform-wide global concurrency and rate limits.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;573fa4c8-a71a-4934-bdd2-6e76c64ce602&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;4b05fc1f-55a2-498f-8240-0bbc9d986ea0&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-summary&#34;&gt;Summary&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cd8543d3-0d20-4857-97d7-92d0f10392a5&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In this introductory post of our series, we explored the background and foundational principles of Uber’s stateful platform, Odin. Odin stands out as a generic, technology-agnostic platform capable of managing various technologies with special demands on how databases are operated.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b162d0bd-d1af-4e18-bad6-01cf9829155e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We discussed how Uber’s databases use locally attached disks for enhanced performance and cost-efficiency and the challenges this presents, as the stored data must follow the workload identity.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a6a3e2b0-f102-45ac-8dab-4f45887f56c1&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We also examined how the platform is inherently intent-based, with disparate remediation loops continuously striving to align the actual state of the managed workloads with their intended goal states. These loops initiate Cadence workflows to update the goal state and wait for convergence—a process that, at scale, requires careful coordination to safely manage without compromising the availability of the managed databases.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;89518bc2-03a8-452f-a779-fab4eb21f534&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In the next blog post, we will discuss how we centralized the coordination of all operations and leveraged it to significantly improve Uber’s ability to operate the fleet at scale.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;74b918d1-e0a6-4dd8-adb0-261d96dcccdb&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;114640af-7176-4346-87ab-dd430c350a03&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The Odin platform was only possible with the effort of many contributors through the years. The authors would like to thank all the teams working on the platform or contributing integrations and to previous team members who helped make the platform great.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;5ad1e8e8-0a5d-4299-8c12-36015f6ef911&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;The cover photo was generated using OpenAI’s ChatCPT Enterprise and edited using Pixlr.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;44181acd-1a06-48b7-97b3-2538dfbffe59&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Kubernetes® is a registered trademark of the Linux Foundation in the United States and other countries.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;a3a471c0-0118-4ea6-840b-90e764c1ea0e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;MySQL® is a registered trademark of Oracle Corporation and/or its affiliates.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;3827feed-6951-4e20-bd15-7fb796002361&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Apache Cassandra®, Apache HDFS™, Apache Kafka®, Apache Yarn™, and Apache Presto™ are registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries. The use of these marks does not imply endorsement by the Apache Software Foundation.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;06d69f2b-38ee-4898-946e-02588d178a99&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Buildkite™ is a trademark of Buildkite Pty Ltd.&lt;/p&gt;】&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;90695143-31cf-483d-b85f-7ce1334bb6f8&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-introduction&#34;&gt;简介&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;548c2e1c-75fa-4ffa-905e-ca342260c0d0&#34;,&#34;dropCap&#34;:false}&#34;&gt;Uber 使用各种数据存储技术，包括Kafka、Cassandra、MySQL等知名开源产品以及内部开发的解决方案。 2014年，Uber经历了快速扩张。与许多初创公司一样，技术团队使用运行手册手动执行配置和维护操作。随着存储需求的迅速增加，这种方法导致了运营工作的繁重。 Uber 创建了一个名为 Odin 的技术无关管理平台，通过自动化提高运营吞吐量，并允许团队轻松管理数千个数据库。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8cf61d25-9d6d-40de-a187-c932da9d9811&#34;,&#34;dropCap&#34;:false}&#34;&gt;Odin 平台旨在通过涵盖管理有状态工作负载的各个方面来提供统一的操作体验。这些方面包括主机生命周期、工作负载调度、集群管理、监控、状态传播、操作用户界面、警报、自动扩展和自动化。 Uber 在全球、区域和区域级别部署有状态系统，而 Odin 旨在以与技术无关的方式一致地管理这些系统。此外，Odin 支持主机托管以提高硬件成本效率。所有有状态工作负载都必须完全容器化，这是一个相对新颖且&lt;a href=&#34;https://news.ycombinator.com/item?id=13054793&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;有争议的&lt;/a&gt;平台创建时的概念。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6f97caa1-0a1f-46ab-8261-61b142951e9f&#34;,&#34;dropCap&#34;:false}&#34;&gt;这篇博文是 Uber 有状态平台系列文章的第一篇。该系列旨在让那些没有构建容器平台知识的读者和具有广泛专业知识的读者能够理解并参与其中。这篇文章概述了 Odin 的起源、基本原理以及早期遇到的挑战。下一篇文章将探讨我们如何安全地扩展运营吞吐量，显着改善我们对大规模、整个车队运营的处理，并通过工作流程升级运行手册。请继续关注该系列的更多帖子。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5d8805b0-a807-4ce6-b495-7e2138a76eba&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0e529716-5369-49b9-a516-66e71fa92691&#34;,&#34;level&#34;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-the-scale-of-the-platform&#34;&gt;平台规模&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c7275b4e-2c3a-465d-ae42-598f1e28f875&#34;,&#34;dropCap&#34;:false}&#34;&gt;自 2014 年以来， Uber 的存储和数据基础设施急剧增长，从几百台主机增加到如今的 100,000 多个主机。这些主机支持跨各种存储集群的 300,000 个工作负载的运行。 Odin 平台上的每个工作负载都类似于 Kubernetes® Pod，由容器集合组成。目前，该平台管理着 380 万个独立容器。主机群总共拥有数艾字节的存储容量和数百万个计算核心。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;70705850-6138-44c0-b04f-cfc5d407b5a5&#34;,&#34;dropCap&#34;:false}&#34;&gt;平台支持23种技术，从传统的在线数据库（如MySQL®和Cassandra®）到先进的数据平台技术（包括HDFS™、Presto™和Kafka®）。它还集成了 Yarn™ 和 Buildkite™ 等资源调度框架，这主要归功于该平台强大的容量管理和扩展解决方案。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;bc701c08-c47b-4395-bee9-06a97e004924&#34;,&#34;dropCap&#34;:false}&#34;&gt;该平台是大量集中在一起，以最好地利用主机上的可用资源。 Uber 使用本地连接的驱动器来处理所有有状态工作负载，以提高性能。因此，我们的调度程序必须进行优化，以在所有三个维度（即 CPU、内存和磁盘）上保持较高的分配率，从而提高效率。我们拥有高达+95%的瓶颈资源分配率，并且我们拥有拥有+100个数据库的主机。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a952ccc3-67f4-44c4-8ebd-c3867d80f3cd&#34;,&#34;dropCap&#34;:false}&#34;&gt;Odin 平台通过 &lt;a href=&#34;https://www.uber.com/en-SE/blog/crane-ubers-next-gen-infrastruct-stack/&#34; target=&#34;_blank&#34; rel 既与技术无关，又与云无关=&#34;noreferrer noopener&#34;&gt;Uber 的云抽象&lt;/a&gt;，目前在 OCI、GCP 和 Uber 的本地部署中运行。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0aba77a1-cb2f-4642-93ee-48c85afed8ba&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;49ba8eed-f6b5-48d7-9472-a225618b4db4&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-a-self-healing-platform&#34;&gt;自我修复平台&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;578c4714-ae37-4b95-94aa-7902076a06d7&#34;,&#34;dropCap&#34;:false}&#34;&gt;Odin 已构建完全声明性且基于意图。这导致诸如容错能力之类的属性性、可扩展性和自我修复。期望状态表示为目标状态，平台使用目标状态自动收敛实际状态。我们通过使用大量小型自主修复循环来实现这一目标，这些循环旨在通过不断推动系统朝“正确的方向”推进，以确保系统的实际状态与声明的目标状态一致。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7df9c643-7221-4e50-99b6-e7223356e28f&#34;,&#34;dropCap&#34;:false}&#34;&gt;今天，奥丁和 Kubernetes® 有许多相似之处。当您阅读本博客系列时，这种思维模型将会很有帮助。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fe74b631-4317-499c-a507-32164ec308bc&#34;,&#34;dropCap&#34;:false}&#34;&gt;下面的示例说明了修复循环的剖析。补救循环从检查目标状态开始。然后，它会收集实际状态，识别两者之间的差异，并使用 &lt;a href=&#34;https://cadenceworkflow.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Cadence™ 工作流程&lt;/a&gt;进行调整系统状态来缩小差距。这与 Kubernetes 控制器的工作方式类似，只不过它们通过 APIServer 直接操纵状态。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f6ae9904-86ac-423f-95e2-a04e0f831377&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;4743a97f-d970-4b9e-bcc7-60029013f43b&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXfrbiVvJh6xCuhP9K9RQRFzpiFd5uwiRBFOOtPiMfB7iRYc_z-MMSXNB34iLdfE_-mIIx2HbQz08pY1IRYeuaaa_W6JlTy 2IvK3owVsrkDSPqnwXAXAvy9LX3zySlsEbvmUXL3AcAaXGIMgzpvf0RwXAeP-?key=0kVK3UHY5mqh0YpEHEfxHw&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 1：Odin 修复循环的高级概述。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;79575b6a-b658-4b7e-adff-72767d32195f&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8b602db4-cbe7-46fb-8ce3-a60810a26396&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们的修复循环优先考虑模块化和松耦合。这有利于水平扩展和易于开发，以便每个循环都可以开发为独立的微服务。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2397929c-c8f4-432b-ba9d-96439c194265&#34;,&#34;dropCap&#34;:false}&#34;&gt;关于奥丁平台，我们简化了开发通过我们的数据集成平台 &lt;a href=&#34;https://eng.uber.com/grail&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; 提供全局系统状态的最新视图来实现这些循环&gt;圣杯&lt;/a&gt;。 Grail 提供了一个涵盖所有技术的统一数据模型，并提供了平台目标和实际状态的最新视图，包括主机、容器，甚至特定于技术的内部状态。我们的 Grail 设置还允许在全球范围内执行查询，获取 Uber 运营的全球所有数据中心和地区提供的数据。本质上，可以将 Grail 视为 Kubernetes APIServer 中的资源模型。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e0b4f9d7-27e9-4f5a-b0be-32ff310cbbe9&#34;,&#34;dropCap&#34;:false}&#34;&gt;平台的控制plane 仅专注于高层决策，例如调度工作负载和管理集群拓扑。  用于操作目标状态的 API 是工作流程的执行。修复循环和人工操作员都会启动工作流程。该平台提供的 UI 允许操作员快速概览数据库的状态并在需要时应用操作。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;141a1dce-0b34-4801-b240-c434901bc5ec&#34;,&#34;dropCap&#34;:false}&#34;&gt;工作流程令人难以置信由于他们提供的见解而有价值。更新目标状态的工作流程通常包含两个步骤。首先，更新目标状态，然后工作流将监视实际状态，直到收敛于指定的目标状态。因此，当没有收敛时，哪些操作没有收敛就一目了然，包括为什么首先执行该操作的上下文。由于实际状态预计不会立即收敛，因此系统必须防止修复循环不断启动尝试修复相同问题的工作流程。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cf949352-02eb-4d3b-a249-178fe472a2a3&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;271afd88-96a7-4bd9-849c-7e5886fc5d50&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXfkpBCcYU1h2PhNjamcT3aGcIQWIAr7wn-77iP7vY861TcFRrSJ9b4oayn5Brm7MgSzS3nqJqB0J9Qys0t6CnYtHW gIYckWP64HUxy-8LKxmnHBgC-DJU2zkpCZpmTZiLEJ4GP08aLD5yqOgOuk6S4KDsc4?key=0kVK3UHY5mqh0YpEHEfxHw&#34; alt=&#34; &#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 2：Odin 建筑的 10.000 英尺视图。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;56db4950-7d76-45b8-819e-0dd0fa247fdd&amp;quot;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2acecab1-597e-464e-a287-66bd80786482&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-managing-generic-storage-clusters&#34;&gt;管理通用存储集群&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d0ffd8b7-98aa-4deb-94ae-15aee37e8492&#34;,&#34;dropCap&#34;:false}&#34;&gt;Odin 已构建基于与技术无关的集群模型，该模型提供用于管理任何类型工作负载集群的通用操作。该模型支持通用操作，例如添加或删除工作负载、升级容器映像以及跨技术统一处理其他与集群相关的任务。由于模型和操作与技术无关，因此需要通过一系列插件来扩展平台，以根据其特定需求定制集群管理的各个方面。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;cda4e095-7482-482f-b6b2-a05a92a267d6&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-two-host-agents&#34;&gt;两个主机代理&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;dd82e5e7-cf72-4e2e-ab87-2d536dd42dbe&#34;,&#34;dropCap&#34;:false}&#34;&gt;Odin 控件平面分为两部分。到目前为止，我们只讨论了全局部分。在主机级别，有两个代理。一种是在主机上的所有工作负载之间共享的，并且与技术无关。我们简称为&lt;em&gt;odin-agent&lt;/em&gt;。另一个代理是特定于技术的，并作为工作负载的一部分运行容器化。这两个代理结合起来可提供与 kubelet 相同的功能。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;65a90e51-2220-4720-94ac-84668c27dbbd&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;a12eed6c-d521-456b-8c52-de1b38e4bec2&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdSePHcS8Ijkt17F1dBAJtAyQ4DANP0C3CoVQdZFqpTDNdMg6oUZLrPRUgoTrTUC_3ukppykFnZiFLkJkfTrH_iV_OW5Xdhaj Si6fKC9z7fSq9Yw6x8l0XgDvomdfCgZqe5Z5ZZYeHvmvAwoeaMPNZEyoHs?key=0kVK3UHY5mqh0YpEHEfxHw&#34; alt=&#34;&#34;referrerpolicy=&#34;no- referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 3：Odin 主机的剖析，其中两个主机级代理与全局控制平面进行通信。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e0817d94-ec97-4063-9a8c-4e9904175475&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator”有-alpha-通道不透明度&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1621dd43-a4be-45da-b5d6-0d17c0c5bb08&#34;,&#34;dropCap&#34;:false}&#34;&gt;主机代理与全局控制平面传达最新目标和实际状态。它的主要职责是管理主机级资源分配，例如为工作负载调度核心、创建和管理磁盘卷/cgroup，以及在分配给主机时引导工作负载。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8fa5a88e-540a-4c48-b1cc-ed7df86a3a00&#34;,&#34;dropCap&#34;:false}&#34;&gt;技术-特定代理称为&lt;em&gt;工作人员&lt;/em&gt;。工作人员是一个主管，与侧容器中的每个工作负载一起运行。其主要作用是确保正在运行的容器与目标状态保持一致，并收集运行状况和其他相关数据以向控制平面的全局部分报告。 Worker 的通用功能包括启动和停止容器。您可以将工作线程视为平台理解的通用集群状态与工作负载中正在运行的数据库容器之间的转换器。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9965fa47-2eb1-4626-b1d3-dc518449f76a&#34;,&#34;dropCap&#34;:false}&#34;&gt;技术团队可以定制worker来收集特定于技术的实际状态并将目标状态应用于工作负载。例如，MySQL 工作线程的任务是确保 MySQL 主服务器和副本服务器保持其角色并向全局控制平面报告当前的消费状态。这使得团队可以根据集群的实际状态编写新的工作流程来操作集群。许多技术已经扩展了它们的工作线程，使其能够在工作线程中执行自我修复集群操作。例如，当集群拓扑发生变化时协调领导者选举或数据复制。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;5598ca20-758c-4080-b97b-81afc4cffece&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-optimizing-for-robustness-and-resilience&#34;&gt;优化鲁棒性和弹性&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;26972dfb-89dd-40ce-906d-ca16199626d9&#34;,&#34;dropCap&#34;:false}&#34;&gt;分离控件平面到主机本地和全局组件允许我们根据工作负载部署工作更改，将不良更改的潜在爆炸半径限制在尽可能小的单位。这同样适用于工作负载中的每个容器，因为该平台允许单独就地升级容器。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;138c489d-3c9f-4892-84e1-6387f9265a64&#34;,&#34;dropCap&#34;:false}&#34;&gt;对于两个代理，一个规则是，在对运行中的任何更改之前，必须首先将目标状态写入磁盘工作量已完成。这种方法保证了自给自足的集群可以在没有活动控制平面的情况下进行初始化，从而有效地解决了引导问题。此外，它还提供强大的故障恢复能力，允许工作负载在控制平面降级期间运行并简化紧急工具的编写。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;f07917a0-558e-42cb-b46f-c7a20196c247&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-transitioning-identities&#34;&gt;转换身份&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1eef1f47-896d-478c-9dfe-b43cd5b83885&#34;,&#34;dropCap&#34;:false}&#34;&gt;随着我们不断优化效率并使我们的主机群现代化，将工作负载从一台主机转移到另一台主机已成为平台上最常见的操作之一。迄今为止，我们每月重新安排多达 60% 的有状态工作负载。有状态工作负载的一个关键要求是在重新调度期间保持工作负载身份。例如，此功能允许将服务于某个分片的一个工作负载替换为服务于同一分片的另一个工作负载。在 Kubernetes 中，这是通过 StatefulSets 进行管理的，方法是分配一个在重新调度过程中持续存在的稳定 Pod 身份。删除 pod 后，会使用相同的身份配置一个新的 pod。让我们探讨一下为什么这个模型不太适合 Uber 的用例。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1bbb811d-86a9-4cd2-b96b-21285d91dabe&#34;,&#34;dropCap&#34;:false}&#34;&gt;如上所述， Uber 使用本地连接的驱动器来处理所有有状态工作负载，以提高性能。此设置要求在终止相应的工作负载之前复制主机上的数据并使其在其他地方可用。如果在替换工作负载完全准备好之前关闭工作负载，则集群会暂时出现配置不足的情况。这种延迟对于 1 小时的中位时间来说是不可忽略的，它使集群面临更大的数据丢失或可用性问题的风险。此外，这种延迟还会降低集群处理请求的整体能力。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;3b6e5f94-d8c1-4822-aff4-a93c79b744c9&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-replacing-workloads&#34;&gt;“替换”工作负载&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;aa7ad1d4-ea62-4c1c-910d-8286f06a3711&#34;,&#34;dropCap&#34;:false}&#34;&gt;因此， Odin 上最重要的操作是将工作负载从一台主机迁移到另一台主机，或者用 Odin 的术语来说，“替换”工作负载。替换操作是专门为&lt;em&gt;make-before-break&lt;/em&gt;而设计的。这意味着它会创建一个新的工作负载，只要可能，在旧工作负载关闭之前，该新工作负载可以承载其所替换的工作负载的身份和数据。技术集成可以选择目标状态的哪些部分要在工作负载之间传播。目标状态模型使两个工作负载明确意识到彼此的存在。这是至关重要的，因为它允许各个工作人员协调数据复制和其他事情的过程。在极少数情况下，当&lt;em&gt;make-before-break&lt;/em&gt;语义不可能时（例如，旧工作负载位于故障主机上），旧工作负载将在创建替换工作负载的同时被删除。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f2c8a1a1-fe85-4656-bbd8-381c24da7641&#34;,&#34;dropCap&#34;:false}&#34;&gt;使用 make- before-break 还具有效率优势，因为替代方案将要求我们持续运行过度配置的集群。在我们的模型中，我们只会在必要时暂时过度配置。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;de1ed6fc-1a1e-4666-b2e9-c58224a32d9e&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c29e483c-a48a-4924-a033-ef8503c36fe1&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-forming-pains&#34;&gt;成长的烦恼&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;dad7f7a3-0ea3-479b-a1fe-9bfa5af38d18&#34;,&#34;dropCap&#34;:false}&#34;&gt;第一次迭代Odin 平台的使用对于团队操作存储集群的能力来说是一个巨大的飞跃。存储团队可以通过共享运营模型将所有运营和见解融合在一个平台上。然而，该平台仍然非常以人为本。人类是在平台上发起变革的主要参与者，而且最重要的是，他们是所执行操作的安全性和适合性的保证者。修复循环的作用仅限于范围相对较小的操作，例如单个替换工作流程&lt;em&gt;。&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fad8c154-683f-4e0d-ba9b-d1d4b7157cb8&#34;,&#34;dropCap&#34;:false}&#34;&gt;运营开销已开始随着业务不断扩展，跨车队的跨技术操作实际上是人类操作员无法完成的。需要我们转移工作负载的复杂、跨车队、跨技术的操作示例包括：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0c6b29b5-a3b0-4770-b81e-5374de15e4b5&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;优化工作负载放置，以提高抵御机架或区域故障等基础设施故障的能力&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;主机装箱以提高成本效率&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;核心/list-item&#34; data-wp-block=&#34;[]&#34;&gt;整个队列的主机升级，例如新内核、固件或 LVM/cgroupv2 等功能&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;促进新数据中心的建立或现有数据中心的退役&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;668aa557-dc41-41b6-a40a-29371389096e&#34;,&#34;dropCap&#34;:false}&#34;&gt;此外，单个主机上的大量数据库使得处理主机故障变得令人畏惧。这些限制表明，我们必须通过开发范围比以前更广泛的新修复循环来加倍自动化。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;56f8fe34-2ef2-4a3f-bb0d-4f69c90b960c&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-coordination-required&#34;&gt;需要协调&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8036e749-8a7a-4c7f-b617-1321eac2cf8f&#34;,&#34;dropCap&#34;:false}&#34;&gt;增加了可操作性如果不仔细协调，负载可能很快就会导致中断。想象一下这样一种情况：删除基于共识的 3 节点存储集群中的工作负载以执行容器升级。现在想象另一个修复循环，该循环已确定集群中的其他工作负载之一将受益于出于效率原因而被移动。如果允许，这将导致存储集群失去仲裁，从而影响写入可用性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;44963a9c-9e27-4921-91da-42d031dd81be&#34;,&#34;dropCap&#34;:false}&#34;&gt;最初，我们开始增强修复循环以在内部执行并发控制（图 3）。这样，他们可以防止相同类型的操作给存储集群带来问题。例如，容器升级循环会阻止重叠升级。然而，这种方法存在缺陷，因为它没有解决不同修复循环引发的操作冲突问题。图 3 显示了两个独立操作可以在同一存储集群中操作两个工作负载，从而导致可用性损失的情况。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;37267e15-3898-4883-b5d5-6617d00f67ce&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;d131b897-0c2b-4684-8052-d5db42a85208&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXd4i6HPr3ZkL1WFmR1Kd-mkiLceVO0Liik1jb6NR51eaucwSAjFss_8QW_m-doDIHsbCW9kv5TJwgPukhH3rPiaE1o0lWbp vtlwSliGofFpGiD68jS5NGuxR1V3HZs6HxnZYsScd9vpweh0ouKSHRF0RlbN?key=0kVK3UHY5mqh0YpEHEfxHw&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 4：内部并发控制不能保护存储集群免受重叠操作的影响，并且可能会损害可用性。&lt;/figcaption&gt;&lt; /图&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4ab49524-3b00-4ca5-b966-d2a03569fdc2&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5304b888-d27b-4d5c-8703-a262a8183b7f&#34;,&#34;dropCap&#34;:false}&#34;&gt;展示不同的技术安全集群操作的容差差异很大。对于某些技术来说，正在运行的特定工作负载至关重要，而所有工作负载对于其他技术都一视同仁。某些技术仅专注于确保集群内始终可用一定数量的工作负载。这种需求的多样性使得我们必须支持根据每种技术的特定需求定制集群管理策略。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d2d9eb70-3d33-4f06-a09d-71c7ba530c6c&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们需要确保使用一个系统进行全局、跨类型的操作协调，该系统可以通过利用基于当前工作负载健康状况和中断预算的特定技术限制来保证保持集群可用性。此外，它应该通过强制执行平台范围内的全局并发和速率限制来防止工程错误和内部平台系统过载。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;573fa4c8-a71a-4934-bdd2-6e76c64ce602&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;4b05fc1f-55a2-498f-8240-0bbc9d986ea0&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-summary&#34;&gt;摘要&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cd8543d3-0d20-4857-97d7-92d0f10392a5&#34;,&#34;dropCap&#34;:false}&#34;&gt;在此介绍性内容中在我们的系列文章中，我们探讨了 Uber 有状态平台 Odin 的背景和基本原则。 Odin 是一个与技术无关的通用平台，能够管理对数据库操作方式有特殊要求的各种技术。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b162d0bd-d1af-4e18-bad6-01cf9829155e&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们讨论了如何Uber 的数据库使用本地连接的磁盘来提高性能和成本效率，并解决由此带来的挑战，因为存储的数据必须遵循工作负载身份。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;核心/段落&#34; data-wp-block=&#34;{&#34;哈希&#34;:&#34;a6a3e2b0-f102-45ac-8dab-4f45887f56c1&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们还检查了该平台本质上是如何基于意图的，通过不同的补救循环不断努力使托管工作负载的实际状态与其预期目标状态保持一致。这些循环启动 Cadence 工作流程来更新目标状态并等待收敛，这个过程在规模上需要仔细协调才能安全管理，而不影响托管数据库的可用性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;89518bc2-03a8-452f-a779-fab4eb21f534&#34;,&#34;dropCap&#34;:false}&#34;&gt;在下一个在博客文章中，我们将讨论如何集中协调所有运营，并利用它来显着提高 Uber 大规模运营车队的能力。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;74b918d1-e0a6-4dd8-adb0-261d96dcccdb&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-acknowledgments&#34;&gt;致谢&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;114640af-7176-4346-87ab-dd430c350a03&#34;,&#34;dropCap&#34;:false}&#34;&gt;Odin 平台多年来，只有在许多贡献者的努力下才可能实现。作者要感谢所有在该平台上工作或贡献集成的团队，以及帮助使该平台变得更加出色的以前的团队成员。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;5ad1e8e8-0a5d-4299-8c12-36015f6ef911&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;封面照片是使用 OpenAI 的 ChatCPT Enterprise 生成并使用 Pixlr 编辑的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;44181acd-1a06-48b7-97b3-2538dfbffe59&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;Kubernetes® 是 Linux 基金会在美国和其他国家/地区的注册商标。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;a3a471c0-0118-4ea6-840b-90e764c1ea0e&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;MySQL® 是 Oracle Corporation 和/或其附属公司的注册商标。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;3827feed-6951-4e20-bd15-7fb796002361&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;Apache Cassandra®、Apache HDFS™、Apache Kafka®、Apache Yarn™ 和 Apache Presto™ 是 Apache Software Foundation 在美国的注册商标或商标和/或其他国家。使用这些标记并不意味着 Apache 软件基金会的认可。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;small&#34;,&amp;quot;hash&#34;:&#34;06d69f2b-38ee-4898-946e-02588d178a99&#34;,&#34;dropCap&#34;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Buildkite™ 是 Buildkite Pty Ltd 的商标。&lt;/p&gt;</description>
      <pubDate>Thu, 18 Jul 2024 06:12:33 +0000</pubDate>
    </item>
    <item>
      <title>【Differential Backups in MyRocks Based Distributed Databases at Uber】Uber 基于 MyRocks 的分布式数据库中的差异备份</title>
      <link>https://www.uber.com/blog/differential-backups-on-myrocks/</link>
      <description>【&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;cb351f55-04f3-4e29-a8f1-bda473e92c48&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d1e54aba-4df5-4373-876c-540d6c7af934&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Uber uses MySQL® as the underlying storage layer for &lt;a href=&#34;https://www.uber.com/blog/schemaless-part-one-mysql-datastore/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Schemaless&lt;/a&gt; and &lt;a href=&#34;https://www.uber.com/blog/schemaless-sql-database/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Docstore&lt;/a&gt;, Uber’s in-house distributed databases. Storing tens of petabytes of operational data and serving tens of millions of requests/second, it is one of the largest database services at Uber, used by microservices from all business verticals and serves critical Uber use cases worldwide.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4170752c-241e-42f3-a98a-e0c17f923e66&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The Uber Storage Platform team &lt;a href=&#34;https://www.uber.com/blog/mysql-to-myrocks-migration-in-uber-distributed-datastores/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;migrated&lt;/a&gt; all Schemaless and Docstore instances to &lt;a href=&#34;http://myrocks.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;MyRocks&lt;/a&gt;, a &lt;a href=&#34;http://rocksdb.org/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;RocksDB&lt;/a&gt; storage engine based MySQL® version. The primary motivation was that RocksDB engine is optimized for write and outstanding storage space efficiency.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5609ac99-c108-4293-9cc7-5ac548c171fc&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In this post, we are going to talk about our journey of how we solved the subsequent issues with a lack of incremental backups for MyRocks, and the interesting technical challenges that we faced along the way.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5bfa7005-88b7-4a0d-808d-4a1390c251bc&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;edfc2d5d-f55f-4209-b096-b4ac4301fe24&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-motivation-and-background-behind-project&#34;&gt;Motivation and Background Behind Project&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ee48dc21-1f00-4f36-80f9-5221c7894a04&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Uber’s operation, bridging the physical and digital realms, relies heavily on robust, distributed databases, which are essential for a range of critical functions from disaster recovery to business continuity and compliance. These databases also support vital auditing and data analysis efforts across the company.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a439f569-a1c9-4145-808b-43551e7bbd6b&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Schemaless and Docstore have a layered architecture and a Docstore deployment is called an instance. In the architecture of a single Schemaless or Docstore instance, data is organized into shards. These shards are further distributed across multiple physical partitions to ensure scalability and manageability. Each partition houses redundant copies of the data, all located within a single region on individual MySQL® servers. Notably, each server is configured to store a single RocksDB column family, optimizing data handling and storage efficiency.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b867f543-c38a-4b0e-a7f4-d06156cca200&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To bolster data availability and safeguard against regional outages, each partition is also replicated across various geographical regions. This replication strategy not only enhances the resilience of Uber’s data infrastructure but also supports the company’s global operations, ensuring data integrity and accessibility across different markets.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c2434482-6809-44e2-b4e5-44ef6fb7eec1&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;b2bbc28c-baad-4e8b-abb1-dc8d36e3f106&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXet4ixteU8WtBdOdSDlpa7d86iT5RDse1PD4J_47W_Hqx4fGG2CV9py7A5qDW0NVIVKRQhbxshdrT8bm91wJB6Dyr6e9225iU2tAzh-vObD1vr9eTOTytetWvX2fDeN0QsvYc7ZdUcrru4PGW06Zb37EOo?key=5NK_RoiIfdfw2rcH98YHAg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 1: Docstore Arch.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d64fa94d-597b-4ef6-a8dd-fff80caa9fa8&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cba0615a-6621-452a-af91-bdc7bf2bfc45&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Backup process for a single Schemaless and Docstore instance traditionally involves using the &lt;a href=&#34;https://www.percona.com/mysql/software/percona-xtrabackup&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Percona Xtrabackup&lt;/a&gt;™&amp;nbsp; tool to conduct periodic backups across all partitions and storing these in a blob store for a predetermined retention period. However, with the transition to the MyRocks engine, the lack of support for incremental backups has significantly increased the cost and complexity of maintaining these backups. At Uber’s extensive scale, this change has led to storing hundreds of petabytes of full backups, which incurs millions in blob store expenses. Our motivation was to tackle two important pain points for our ever growing instances: escalating cost and speed of backups.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;28441045-989d-4acf-9aaa-4e4ee1789833&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Cost:&lt;/strong&gt;&amp;nbsp; The storage of backups results in a substantial monthly cost in blob store usage; this problem is expected to grow alongside our business, further inflating expenses. The costs associated with our backup strategy are largely due to the lack of support for &lt;a href=&#34;https://docs.percona.com/percona-xtrabackup/8.0/create-incremental-backup.html&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;incremental backups&lt;/a&gt; or &lt;a href=&#34;https://docs.percona.com/percona-xtrabackup/innovation-release/create-partial-backup.html&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;partial backups&lt;/a&gt; by Percona Xtrabackup for MyRocks engine, necessitating full backups for each partition every time a backup is taken.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Another critical issue, as instance partition sizes increase—varied by the sharding of customer data—the time required to complete each full backup also increases. This variation puts additional pressure on our backup infrastructure to keep pace with growing data volumes, challenging our ability to scale efficiently.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1c3d3e06-8559-470d-99a6-bc4126c15be0&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fe77267a-a983-4580-95dd-e4b9520dde3f&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-challenges&#34;&gt;Challenges&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;66f2a4e9-3476-4a75-9036-e8daff7a1faf&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In the pursuit of a solution, we set ourselves guidelines principles that were challenging yet measurable. Our solution should be:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e6475110-853b-4f0e-bead-10b5f6596e33&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Scalable to thousands of partitions across instances and also scale well with the growth of the partition size&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Have predictable Service Level Agreement (SLA) time for backup completion per terabyte of partition data&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Compatible with the current ecosystem built for backups and not boiling the ocean to have a drastically different approach, which would drag out the project timeline&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5ec0cb51-5122-4e2e-a6d8-258e8502c1b6&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We explored multiple options to address the pain points, such as:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1bdd8bb1-dd1a-45be-938c-3359cfcfe8ef&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Improving efficiency of the backup scheduler, which intelligently deferred the backups to a later time, but did not address the root cause&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Solving a few latent bugs with the backup process, which helped reduce cost, but was a one-time medication&amp;nbsp;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Changing tiering options in our blob store, again being a one-off solution&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Evaluating logical backups (with compression), which did not yield much savings compared to the already-efficient physical backup (majorly due to MyRocks already being efficient in storing data)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Logical backups paired with Binlog backup, which did not yield much in terms of savings and also increased the cost for separate binlogs&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5038ca91-bf7f-40fd-ae54-2735e086f6f3&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;All these options were one-off solutions that helped reduce about 17-20% in blob store usage, but did really provide a viable long-term solution to keep the backup footprint low.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d29cd7dd-28c0-4c1b-8da9-c5f93e64463d&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;162d8894-9eb2-4137-87de-d00ff4bbbcfa&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-differential-backups-to-rescue-nbsp&#34;&gt;Differential Backups To Rescue&amp;nbsp;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;865dee3b-9230-4274-92ec-4ae4c47d72cb&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-context&#34;&gt;Context&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;21ba3183-4577-4326-809c-7b437c54ca32&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;During the evaluation to enhance our backup strategies, we explored various options including logical backups (with tools like &lt;a href=&#34;https://github.com/mydumper/mydumper&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Mydumper&lt;/a&gt;) and experimented with different compression methods for both physical and logical backups. We also considered designs involving physical backups with random partition sharing. Through these explorations, we identified that the Percona XtraBackup tool was the most effective for capturing a consistent snapshot of the database at a specific point in time (corresponding to a specific Log Sequence Number or LSN). It proved superior in terms of memory usage and speed compared to other methods.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1d72be60-76ff-4888-9e01-f2c52ce6de5e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Percona XtraBackup operates directly on live databases, copying both the data files and, when configured, the binary log files of the MyRocks server. It ensures data consistency and guards against corruption from ongoing transactions. Additionally, it can prepare backup files for restoration, apply incremental backups, and stream them to different storage devices. Given these robust capabilities, we decided to continue using XtraBackup for our backup and restoration processes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cea2ed85-5c7f-41e2-8835-46c316037d9a&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;However, a significant challenge remained: XtraBackup does not support incremental or partial backups for the MyRocks engine. As a result, each backup was a full backup, often containing up to 95% duplicate data from previous backups. To address this inefficiency, we delved into the backup file structure to identify deduplication opportunities. This deep dive revealed potential strategies to optimize storage and reduce redundancy in our backup process.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a493523e-97c4-48f0-8911-6b6d22a1a978&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We found that XtraBackup backup and RocksDB file structures shared a lot of similarities:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;20c16f67-1f41-4429-b511-61d4fb01cd9f&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;All the data for a MyRocks database is stored in multiple files called SSTable files (*.sst). The data in each SSTable file is immutable, thus each time more data needs to be written to the database a new SSTable file is created and flushed to disk.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3ea1c9af-e1fa-426d-9448-59d1d2131d6b&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;XtraBackup takes physical backups, which involves backup of database directories and physical files. For MyRocks, a backup is a copy of all relevant SSTable files on disk,&amp;nbsp; when the backup is initiated, and database metadata such as the database schema (*.sdi), ibdata1, .log, checkpoint files and a few small metadata files needed for recovery. The SSTable files contain the actual database data and account for more than 95% of the backup size.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Every incremental backup on the MyRocks storage engine does not determine if an earlier full or incremental backup contains the same files. XtraBackup copies all the SSTable files each time it takes a backup.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;192c0f7e-368d-43c7-8391-22311265fe4a&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;40c8792d-e87d-4852-96d1-1d81074ff884&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdsNC4wSooJDb71D6WbpAnC19sxXPB2P95sIn78ppwweBduT2-BSZsRhnzvwu9QVf7Q4njUNg4uYf94Nyncfm38A4gGDCLk6aQvPypu2bb9xqnf2qWJGW4qzQbsnrSdyVjtMxj88NYpP4mfv1C1HXiUa9zK?key=5NK_RoiIfdfw2rcH98YHAg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 2: MyRocks File Structure.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e9e05ce5-b4bf-4253-a289-b0422573e142&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;3af8a8c9-cc84-4578-b746-9e8bd439f57d&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-solution&#34;&gt;Solution&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0c144131-e76e-4535-8e1e-dd5926641f30&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Our backup strategy capitalizes on the immutability of SSTable files, which often remain unchanged between consecutive backups. This insight allows us to adopt a more efficient differential backup approach, reducing unnecessary data duplication by sharing SSTable files across backups. Here’s how this streamlined process is implemented:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:true,&amp;quot;hash&amp;quot;:&amp;quot;f880f2a1-9bde-4fdb-9ffd-c8f2ef26b29d&amp;quot;,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Initial Full Backup&lt;/strong&gt;: We start with a full backup, during which all metadata and SSTable files are stored in a shared “&lt;strong&gt;pool of SSTable files&lt;/strong&gt;” within the blob store. This acts as a foundational data repository for all future backups.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Differential Backups&lt;/strong&gt;: Subsequent backups only add newly created SSTable files to the pool. Existing SSTable files from previous backups are reused, minimizing data redundancy. Each backup is defined by a simple &lt;strong&gt;backup manifest file&lt;/strong&gt;, which records the specific files included in that backup.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Restoration&lt;/strong&gt;: When restoring data from a specific backup, the backup manifest file guides the process. It serves as the definitive reference, dictating which files to retrieve from the pool to accurately reconstruct the data.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Cleanup&lt;/strong&gt;: SSTable files that are no longer referenced by any backup manifest are eventually purged from the system through a dedicated TTL (Time To Live) process, ensuring efficient use of storage space.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b430dbf7-1fce-4f6b-bff9-7def645e4a64&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;This approach not only enhances backup efficiency by reducing the volume of data stored, but also speeds up both the backup and restore processes by eliminating redundant data handling.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b9fee78a-2f4b-406c-916d-6db63a8115a5&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;edebd41a-203a-45a7-aeb9-5da2942876ac&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfCb9fhrKQmz_E-om9aLszMnP8rwiDhPJ3IbLdv7snvE5I3BpmXVJpMhVz9ZvPyvKsxc-PgGbw8bTlYbtX75kJvX-6KuRW9K8AHfM8lXpgAMpizpvXZg-Uq1T859HvLe5JPRn4RNTmH27ZjvYT3ryXluU9R?key=5NK_RoiIfdfw2rcH98YHAg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 3: Algorithm flowchart example.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2a5ddde0-4a5e-4bd3-8a2e-089b3b5cd75b&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;0d521f99-e0c7-4210-8ef0-ee8e452c35e4&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-backup-manifest-file&#34;&gt;Backup Manifest File&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1dc00266-87e1-4ab9-870f-6d8fcecd5d99&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Next, let’s delve into the manifest file internals. The core concept of a differential backup is sharing the SSTable files with previous backups. Hence it’s important for each differential backup to have the backup manifest file, which keeps track of what files are ultimately required for restoring that specific backup during the recovery process.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;aa219709-14a0-4fd2-a956-fe0957c5e1bd&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The manifest is a simple json file. Each backup will have its own manifest file associated with it and is used to quickly index the individual files in the backup. The backup manifest file consists of different properties.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;930a4598-a11b-4a18-b1c5-72ba6012a757&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The manifest file firstly records the type of backup and its success status (success/failure). For each backup operation, it records the start and end time along with the actual backup size and full data set size. It also stores information of the instance name, the physical node the backup was taken from and the MySQL version. The core of the manifest file is the set of manifest records (one per SSTable file) which define for a given SSTable file—including file name, storage path, file size, and SHA256 checksum for integrity checks. Finally, it records the blob store path of the metadata tarball containing the database files(*.sdi), ibdata1, .log, checkpoint files, xtrabackup_info, etc. This information is the starting point for the next backup that gets triggered, building on the existing SSTable file pool.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;0741f261-6e09-4000-b490-d6931bc68204&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-backup-architecture&#34;&gt;Backup Architecture&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;53656ec6-c7b2-4682-aeb9-080cce3439ef&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Now that we have introduced the core components of the differential backup, let’s take a detailed look at how backups are orchestrated. Backups are taken at a partition level from one of the MyRocks containers on one of the 2 follower nodes of the partition and are triggered by a stateless service called Backup Scheduler. It takes care of the frequency and the timing of the backups depending on the partition’s backup state (stored separately in a state machine).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6b94cb64-b72f-41c5-a00a-7025fed1cedd&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Once triggered, the actual backup is taken by the individual backup containers in the partition node. Backup container is an ephemeral container, which boots up when required to take a backup and runs the XtraBackup tool to extract the backup, persists the backup in the blob store, persists the state and goes back to sleep.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;524cff10-2d66-48b7-9ba7-6cdac23591dd&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;3df1e79b-0ec2-43a8-9dbf-c06a5c60db3b&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdABiGaoJnGyPBMqOAb3IA8tG4c-yDW3p7Z0y3fTwF9NGLW6FB5FJCXGOwBF5_qcymJOqeQeY6rHnOsXK5qvFbv2K6trWTimxTinZJ4vhoaKLXVSr0BbP3O9XGOJtOl02RuGF2WSAoF50er63JOVtlIVHdg?key=5NK_RoiIfdfw2rcH98YHAg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 4: Generic Backup Control Flow.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;54fede1e-04e4-44a5-9c22-cff3288830ec&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7b057e30-c38d-4519-964b-695d00af782e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Before we get into the backup container process level, here are the terminology definitions:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ba8a98b4-12d3-4690-b59e-fdbd8f633a01&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;SSTFile Pool Location:&lt;/strong&gt; This is the location in the blob store where all the SSTable files are pooled together to be shared/referenced by the differential backups.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;99e60407-d915-45bd-a26f-2fde13ab876c&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Individual Backup Location:&lt;/strong&gt; This is the location in the blob store specific to a backup and will contain the tarballed metadata files and backup manifest file. This is not shared with any other backup and is unique to the specific backup and specific partition node as well.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;295c534d-5804-4be0-acc1-e860adb3595e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The backup process begins with the scheduler determining if a new backup is needed based on the last backup time and frequency, typically selecting the same partition node as before. The scheduler then instructs the node manager to initiate a backup container equipped with specific information, such as the previous manifest file if performing a differential backup. XtraBackup is used to stream the backup data directly into the backup container’s disk buffer. Metadata files are compressed into a single tarball to minimize blob store interactions and simplify restoration, while SSTable files are checked against the backup manifest to determine if they exist in the SSTFile pool location in the blob store. New files are persisted, and existing ones are noted in the manifest without re-persistence. The routine regularly updates the manifest file in the blob store to maintain backup consistency and avoid data loss.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4d7fa2ee-f375-4202-b223-edfb5b3dc162&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Once backup extraction completes, the backup manifest is marked as valid and saved, marking the end of the backup process. If issues arise, the manifest is saved with a status indicating the problem. Post-backup, manifest files are categorized by retention period, with obsolete SSTable files and expired metadata and manifest files being purged from the blob store to manage space and maintain efficiency.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;82d331bd-f079-4342-b69a-043dd79c5e94&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;ee780d70-15d4-4b48-9f9b-6d32ea6db074&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-full-vs-differential-backups&#34;&gt;Full vs. Differential Backups&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cd0b3236-abeb-48d2-8309-08771bce19b7&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Now let’s dive deep into how a full backup and differential backup differ. The Backup Scheduler plays the role of a decision-maker, determining whether to conduct a full or differential backup based on specific conditions. The default choice is a differential backup, offering efficiency by only saving changes since the last full backup.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a5819ec9-6a0a-4fea-b093-5b3e8ae22578&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Full backups&lt;/strong&gt; are like starting a new chapter. They involve a complete copy of all SSTables from a partition node, similar to traditional backups done with XtraBackup. This process is initiated in several key scenarios:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:true,&amp;quot;hash&amp;quot;:&amp;quot;31cf996d-8026-4025-8862-ad2d6aa25d57&amp;quot;,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;The first time this backup method is rolled out in production, setting the foundation for all future differential backups&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;When the previous partition node is no longer suitable due to removal or changes that impact backup integrity, necessitating a new full backup from a different node&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Following a MyRocks version upgrade, even though SSTables from different versions might be compatible, a full backup ensures no version mismatches interfere&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;When the existing pool from a full backup has reached its limit of differential backups, to prevent potential corruption that could jeopardize all backups&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;59348321-98b0-4352-83ff-84bb4d6770bd&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Differential backups&lt;/strong&gt; then take the stage, stepping in after a full backup unless conditions dictate otherwise. These backups focus on storing only the new or altered SSTable files from the partition node, significantly reducing the volume of data stored. This strategy not only saves space but also cleverly shares SSTable files across multiple backups, with a structure that organizes manifest files and metadata efficiently.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d91b0bed-b62a-4caf-aa29-337dc3576573&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Each backup type has its role, either laying a robust foundation with full backups or building efficiently on that base with differential backups, ensuring data integrity and optimization in the backup process.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;83645669-b71c-4f33-88e6-f53c2c992671&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;7fd2a191-7e15-4083-8e75-4b1a6925bb24&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXc1ze77IZP1-kJ_nUlyh0Ovfg9OrhlyHwKP5zPG-pilregTxExTGTcQTa2Zm7WeHeixdUsACkr1vw29rlbRe9_VwMr4UtjsMH6oEjfP4UHEYm-HfQnZg2nGHqi1iA9bFqMZ_KWo5f0KtYAz50g_HqV5qJs8?key=5NK_RoiIfdfw2rcH98YHAg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 5: Terrablob Structure.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d1a4a5c9-57a2-4006-b066-ac0c94a261ce&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;3ac9f57c-b411-4dbd-bc13-23c42e27693b&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-limitations&#34;&gt;Limitations&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4224a347-49e7-4a79-a5be-4f2cea497437&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Every design approach comes with its set of tradeoffs and assumptions, and backup strategies are no exception. The chosen method primarily leverages the efficiency of sharing older files while only adding new data to the backups. However, this model isn’t without its challenges.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6a1143e3-ddfa-459b-8961-346d1eada4b6&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;In environments where partitions are highly active or undergo frequent compaction—referred to as “hot partitions”—the expected benefits of differential backups might be reduced. These scenarios, which vary from one partition to another, are inevitable and affect the overall savings of the backup process.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Moreover, the strategy is designed with a preference for using the same partition node for all backups, assuming that the node remains in a suitable state for backups. Should a node be replaced or shifted into an unsuitable state, a full backup becomes necessary.&amp;nbsp;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;03573336-ee1c-4276-a385-37f927ae04cc&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;93af71a9-cc2a-4295-9108-da1c71268293&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-restoring-differential-backups&#34;&gt;Restoring Differential Backups&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e9546b53-244e-4687-8a19-51acaaae268b&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Restoring a XtraBackup MyRocks backup is pretty straightforward as it uses the same Percona XtraBackup tool to restore. The high-level steps of the recovery process go something like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;96214fb7-269d-4e2d-bcef-5cb8dcd22b63&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Locate the valid backup that needs to be restored in the blob store.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Download the backup manifest file from the blob store to a temporary restore location.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Lookup the backup manifest file and download the SSTable and the metadata file (and extract from tarball) and move the SSTable files to the .rocksdb folder that will appear after extraction. At this point, we have assembled the backup exactly like what XtraBackup took the backup during backup time.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Run XtraBackup prepare command, which will run the crash recovery process to make the backup consistent to the scanned LSN.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Stop MySQL process and run the XtraBackup copy back command to replace the MySQL data folder (e.g., /var/lib/mysql/*) with the downloaded and prepared backup data.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Restart MySQL process and start using the restored database.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c28d35e3-4fd8-4591-882a-3eb7ca1b68ef&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;a1038b89-bf80-4d2b-926a-e7350c4fa45e&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-conclusion&#34;&gt;Conclusion&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ee2197a8-4e67-41fb-9ceb-c1776322c0d7&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The differential backup strategy has proven to be a game-changer in terms of efficiency and storage savings:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8feee233-e226-4912-9aea-496cc9447a73&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;By not persisting unchanged data to the blob store, we’ve managed to cut down blob store usage by petabytes, achieving a substantial 45% reduction in data storage across most instances. In some of our larger instances, the savings are even more dramatic, with reductions of 70% or more every time a differential backup is executed.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Additionally, the redesigned backup process is also a lot faster. The full backup completion time averaged 2X faster than the speed of previous backup completion time and differential backups averaged 5X faster.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ce168bfa-ba63-42f5-bd4b-e82fe2898134&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9ef7a1c9-0e51-4b07-8b12-8921fa8dbbfc&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Thanks to Hao Xu, Vlad Dmitriev, Amit Garg, Mohammed Khatib, David Turner and Andrew Regner for the mentorship and guidance in shaping this project to what it is today. Thanks Osama Mazahir and Piyush Patel for their invaluable leadership.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;7bc03d51-d18d-431a-a587-0c6f4ac7f7b2&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Percona®, Percona Xtrabackup, Percona Server for MySql, and the star logo are either registered trademarks or trademarks of Percona in the United States and/or other countries. No endorsement by Percona is implied by the use of these marks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;9c4bee1c-f759-4e78-95a6-383c36fb4bf3&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Cover Photo Attribution: “&lt;a href=&#34;https://www.flickr.com/photos/8718930@N07/3462606643&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Server room at CERN&lt;/a&gt;” by &lt;a href=&#34;https://www.flickr.com/photos/8718930@N07&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;torkildr&lt;/a&gt; is licensed under &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;CC BY-SA 2.0&lt;/a&gt;. No changes have been made.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;957d3f33-a9e4-491f-9c3e-ea43050a8ff1&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;】&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;cb351f55-04f3-4e29-a8f1-bda473e92c48&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-introduction&#34;&gt;简介&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d1e54aba-4df5-4373-876c-540d6c7af934&#34;,&#34;dropCap&#34;:false}&#34;&gt;Uber 使用 MySQL ® 作为 &lt;a href=&#34;https://www.uber.com/blog/schemaless-part-one-mysql-datastore/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Schemaless&lt;/ 的底层存储层a&gt; 和 &lt;a href=&#34;https://www.uber.com/blog/schemaless-sql-database/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Docstore&lt;/a&gt;，Uber 的内部分布式数据库。它存储数十 PB 的运营数据，每秒处理数千万个请求，是 Uber 最大的数据库服务之一，被所有业务垂直领域的微服务所使用，并为全球关键的 Uber 使用案例提供服务。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4170752c-241e-42f3-a98a-e0c17f923e66&#34;,&#34;dropCap&#34;:false}&#34;&gt;Uber 存储平台团队&lt;a href=&#34;https://www.uber.com/blog/mysql-to-myrocks-migration-in-uber-distributed-datastores/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;已迁移&lt; /a&gt; 所有 Schemaless 和 Docstore 实例到 &lt;a href=&#34;http://myrocks.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;MyRocks&lt;/a&gt;，a &lt;a href=&#34;http:// /rocksdb.org/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;RocksDB&lt;/a&gt; 基于 MySQL® 版本的存储引擎。主要动机是 RocksDB 引擎针对写入和出色的存储空间效率进行了优化。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5609ac99-c108-4293-9cc7-5ac548c171fc&#34;,&#34;dropCap&#34;:false}&#34;&gt;在这篇文章中，我们将讨论如何解决 MyRocks 缺乏增量备份的后续问题，以及我们在此过程中遇到的有趣的技术挑战。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5bfa7005-88b7-4a0d-808d-4a1390c251bc&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;edfc2d5d-f55f-4209-b096-b4ac4301fe24&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-motivation-and-background-behind-project&#34;&gt;项目背后的动机和背景&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ee48dc21-1f00-4f36-80f9-5221c7894a04&#34;,&#34;dropCap&#34;:false}&#34;&gt;Uber 的操作，连接物理和数字领域，在很大程度上依赖于强大的分布式数据库，这对于从灾难恢复到业务连续性和合规性的一系列关键功能至关重要。这些数据库还支持整个公司的重要审计和数据分析工作。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p数​​据-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a439f569-a1c9-4145-808b-43551e7bbd6b&#34;,&#34;dropCap&#34;:false}&#34;&gt;Schemaless 和 Docstore 有分层架构和 Docstore 部署称为实例。在单个 Schemaless 或 Docstore 实例的架构中，数据被组织成分片。这些分片进一步分布在多个物理分区上，以确保可扩展性和可管理性。每个分区都包含数据的冗余副本，全部位于各个 MySQL® 服务器上的单个区域内。值得注意的是，每台服务器都配置为存储单个 RocksDB 列族，从而优化了数据处理和存储效率。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b867f543-c38a-4b0e-a7f4-d06156cca200&#34;,&#34;dropCap&#34;:false}&#34;&gt;支持数据为了保证可用性并防止区域中断，每个分区也会跨不同地理区域进行复制。这种复制策略不仅增强了 Uber 数据基础设施的弹性，还支持公司的全球运营，确保不同市场的数据完整性和可访问性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c2434482-6809-44e2-b4e5-44ef6fb7eec1&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;b2bbc28c-baad-4e8b-abb1-dc8d36e3f106&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXet4ixteU8WtBdOdSDlpa7d86iT5RDse1PD4J_47W_Hqx4fGG2CV9py7A5qDW0NVIVKRQhbxshdrT8bm91wJB6Dyr6e92 25iU2tAzh-vObD1vr9eTOTytetWvX2fDeN0QsvYc7ZdUcrru4PGW06Zb37EOo?key=5NK_RoiIfdfw2rcH98YHAg&#34; alt=&#34;&#34;referrerpolicy=&#34; no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 1：Docstore 架构。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d64fa94d-597b-4ef6-a8dd-fff80caa9fa8&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cba0615a-6621-452a-af91-bdc7bf2bfc45&#34;,&#34;dropCap&#34;:false}&#34;&gt;备份进程单个 Schemaless 和 Docstore 实例传统上涉及使用 &lt;a href=&#34;https://www.percona.com/mysql/software/percona-xtrabackup&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Percona Xtrabackup&lt;/a &gt;™ 工具用于跨所有分区进行定期备份，并将这些备份存储在 Blob 存储中预定的保留期限。然而，随着向 MyRocks 引擎的过渡，缺乏对增量备份的支持显着增加了维护这些备份的成本和复杂性。在 Uber 的 e由于规模庞大，此更改导致存储数百 PB 的完整备份，这会产生数百万美元的 Blob 存储费用。我们的动机是解决不断增长的实例的两个重要痛点：不断上升的成本和备份速度。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;28441045-989d-4acf-9aaa-4e4ee1789833&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;成本&lt;/strong&gt;：备份存储会导致 Blob 存储每月产生大量成本用法;这个问题预计会随着我们的业务而增长，从而进一步增加开支。与我们的备份策略相关的成本很大程度上是由于缺乏对 &lt;a href=&#34;https://docs.percona.com/percona-xtrabackup/8.0/create-incremental-backup.html&#34; target=&#34;_blank&#34; 的支持rel=&#34;noreferrer noopener&#34;&gt;增量备份&lt;/a&gt; 或 &lt;a href=&#34;https://docs.percona.com/percona-xtrabackup/innovation-release/create-partial-backup.html&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Percona Xtrabackup for MyRocks 引擎的部分备份&lt;/a&gt;，每次备份时都需要对每个分区进行完整备份。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;速度：&lt;/strong&gt;另一个关键问题，随着实例分区大小的增加 - 随实例分区大小的增加而变化客户数据的分片——完成每个完整备份所需的时间也会增加。这种变化给我们的备份基础设施带来了额外的压力，无法跟上不断增长的数据量，对我们高效扩展的能力提出了挑战。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1c3d3e06-8559-470d-99a6-bc4126c15be0&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fe77267a-a983-4580-95dd-e4b9520dde3f&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-challenges&#34;&gt;挑战&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;66f2a4e9-3476-4a75-9036-e8daff7a1faf&#34;,&#34;dropCap&#34;:false}&#34;&gt;在追求中为了找到解决方案，我们为自己制定了具有挑战性但可衡量的指导原则。我们的解决方案应该是：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e6475110-853b-4f0e-bead-10b5f6596e33&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;可跨实例扩展到数千个分区，并且还可以随着分区大小的增长而很好地扩展&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;每 TB 分区数据的备份完成时间具有可预测的服务级别协议 (SLA)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;兼容当前构建的生态系统为了备份而不是沸沸扬扬地采用截然不同的方法，这会拖长项目时间表&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5ec0cb51-5122-4e2e-a6d8-258e8502c1b6&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们探索了多个解决痛点的选项，例如：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1bdd8bb1-dd1a-45be-938c-3359cfcfe8ef&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;提高备份计划程序的效率，它智能地将备份推迟到以后的时间，但没有解决根本原因&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;解决了备份过程中的一些潜在错误，这有助于降低成本，但只是一种一次性药物&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;更改 Blob 存储中的分层选项，再次成为一次性解决方案&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;评估逻辑备份（带压缩），与已经高效的物理备份相比，这并没有带来太多节省（主要是因为 MyRocks 已经能够高效地存储数据）&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;逻辑备份与Binlog备份搭配使用，不仅节省不了多少，还增加了成本单独的二进制日志&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5038ca91-bf7f-40fd-ae54-2735e086f6f3&#34;,&#34;dropCap&#34;:false}&#34;&gt;所有这些选项是一次性解决方案，有助于减少大约 17-20% 的 Blob 存储使用量，但确实提供了一种可行的长期解决方案来保持较低的备份占用空间。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d29cd7dd-28c0-4c1b-8da9-c5f93e64463d&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;162d8894-9eb2-4137-87de-d00ff4bbbcfa&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h- Differential-backups-to-rescue-nbsp&#34;&gt;差异备份进行救援&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;865dee3b-9230-4274-92ec-4ae4c47d72cb&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-context&#34;&gt;上下文&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;21ba3183-4577-4326-809c-7b437c54ca32&#34;,&#34;dropCap&#34;:false}&#34;&gt;评估期间为了增强我们的备份策略，我们探索了各种选项，包括逻辑备份（使用 &lt;a href=&#34;https://github.com/mydumper/mydumper&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Mydumper&lt;/a 等工具） &gt;）并尝试了不同的物理压缩方法和逻辑备份。我们还考虑了涉及具有随机分区共享的物理备份的设计。通过这些探索，我们发现 Percona XtraBackup 工具对于捕获特定时间点（对应于特定日志序列号或 LSN）的数据库一致快照最有效。事实证明，与其他方法相比，它在内存使用和速度方面具有优越性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1d72be60-76ff-4888-9e01-f2c52ce6de5e&#34;,&#34;dropCap&#34;:false}&#34;&gt;Percona XtraBackup 运行直接在实时数据库上复制数据文件以及配置后的 MyRocks 服务器的二进制日志文件。它确保数据一致性并防止正在进行的事务损坏。此外，它还可以准备用于恢复的备份文件、应用增量备份并将其流式传输到不同的存储设备。鉴于这些强大的功能，我们决定继续使用 XtraBackup 进行备份和恢复流程。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cea2ed85-5c7f-41e2-8835-46c316037d9a&#34;,&#34;dropCap&#34;:false}&#34;&gt;但是，仍然存在重大挑战：XtraBackup 不支持 MyRocks 引擎的增量或部分备份。因此，每个备份都是完整备份，通常包含来自先前备份的高达 95% 的重复数据。为了解决这种低效率问题，我们深入研究了备份文件结构，以确定重复数据删除的机会。这次深入研究揭示了优化存储和减少备份过程中冗余的潜在策略。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a493523e-97c4-48f0-8911-6b6d22a1a978&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们发现XtraBackup 备份和 RocksDB 文件结构有很多相似之处：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;20c16f67-1f41-4429-b511-61d4fb01cd9f&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;MyRocks 数据库的所有数据都存储在多个称为 SSTable 文件 (*.sst) 的文件中。每个 SSTable 文件中的数据都是不可变的，因此每次需要将更多数据写入数据库时​​，都会创建一个新的 SSTable 文件并将其刷新到磁盘。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3ea1c9af-e1fa-426d-9448-59d1d2131d6b&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;XtraBackup 进行物理备份，其中涉及数据库目录和物理文件的备份。对于 MyRocks，备份是启动备份时磁盘上所有相关 SSTable 文件的副本，并且需要数据库元数据，例如数据库架构 (*.sdi)、ibdata1、.log、检查点文件和一些小型元数据文件编辑以恢复。 SSTable 文件包含实际的数据库数据，占备份大小的 95% 以上。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;MyRocks 存储引擎上的每个增量备份都无法确定较早的完整备份或增量备份是否包含相同的文件。 XtraBackup 每次进行备份时都会复制所有 SSTable 文件。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;192c0f7e-368d-43c7-8391-22311265fe4a&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34; :&#34;40c8792d-e87d-4852-96d1-1d81074ff884&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent. com/docsz/AD_4nXdsNC4wSooJDb71D6WbpAnC19sxXPB2P95sIn78ppwweBduT2-BSZsRhnzvwu9QVf7Q4njUNg4uYf94Nyncfm38A4gGDCLk6aQvPypu2bb9​​xqnf2qWJGW4qzQbsnrSdyVjtMxj 88NYpP4mfv1C1HXiUa9zK?key=5NK_RoiIfdfw2rcH98YHAg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 2: MyRocks 文件结构。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e9e05ce5-b4bf-4253-a289-b0422573e142&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;3af8a8c9-cc84-4578-b746-9e8bd439f57d&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-solution&#34;&gt;解决方案&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0c144131-e76e-4535-8e1e-dd5926641f30&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们的备份策略利用 SSTable 文件的不变性，这些文件通常在连续备份之间保持不变。这种洞察使我们能够采用更有效的差异备份方法，通过跨备份共享 SSTable 文件来减少不必要的数据重复。以下是这个简化流程的实施方式：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:true,&#34;hash&#34;:&#34;f880f2a1-9bde-4fdb-9ffd-c8f2ef26b29d&#34;,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;初始完整备份&lt;/strong&gt;：我们从完整备份开始，在此期间所有元数据和 SSTable 文件存储在 Blob 存储内的共享“&lt;strong&gt;SSTable 文件池&lt;/strong&gt;”中。这充当未来所有备份的基础数据存储库。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;差异备份&lt;/strong&gt;：后续备份仅将新创建的 SSTable 文件添加到池中。现有S重复使用以前备份的稳定文件，最大限度地减少数据冗余。每个备份都由一个简单的&lt;strong&gt;备份清单文件&lt;/strong&gt;定义，该文件记录该备份中包含的特定文件。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;恢复&lt;/strong&gt;：从特定备份恢复数据时，备份清单文件会指导的过程。它充当最终的参考，指示从池中检索哪些文件以准确地重建数据。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;清理&lt;/strong&gt;：最终不再被任何备份清单引用的 SSTable 文件通过专用的 TTL（生存时间）过程从系统中清除，确保存储空间的有效利用。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b430dbf7-1fce-4f6b-bff9-7def645e4a64&#34;,&#34;dropCap&#34;:false}&#34;&gt;这种方法不不仅通过减少存储的数据量来提高备份效率，而且通过消除冗余数据处理来加快备份和恢复过程。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b9fee78a-2f4b-406c-916d-6db63a8115a5&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34; :&#34;edebd41a-203a-45a7-aeb9-5da2942876ac&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent. com/docsz/AD_4nXfCb9fhrKQmz_E-om9aLszMnP8rwiDhPJ3IbLdv7snvE5I3BpmXVJpMhVz9ZvPyvKsxc-PgGbw8bTlYbtX75kJvX-6KuRW9K8AHfM8lXpgAMPizpvXZg-Uq1T85 9HvLe5JPRn4RNTmH27ZjvYT3ryXluU9R?key=5NK_RoiIfdfw2rcH98YHAg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element- title&#34;&gt;图3：算法流程图示例。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2a5ddde0-4a5e-4bd3-8a2e-089b3b5cd75b&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;0d521f99-e0c7-4210-8ef0-ee8e452c35e4&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-backup-manifest-file&#34;&gt;备份清单文件&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1dc00266-87e1-4ab9-870f-6d8fcecd5d99&#34;,&#34;dropCap&#34;:false}&#34;&gt;接下来，让我们深入研究清单文件的内部结构。差异备份的核心概念是与以前的备份共享SSTable文件。因此，对于每个差异备份来说，拥有备份清单文件非常重要，该文件可以跟踪备份内容在恢复过程中恢复该特定备份最终需要 t 文件。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;aa219709-14a0-4fd2-a956-fe0957c5e1bd&#34;,&#34;dropCap&#34;:false}&#34;&gt;清单是一个简单的 json 文件。每个备份都有自己的关联清单文件，用于快速索引备份中的各个文件。备份清单文件由不同的属性组成。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;930a4598-a11b-4a18-b1c5-72ba6012a757&#34;,&#34;dropCap&#34;:false}&#34;&gt;清单文件首先记录备份的类型及其成功状态（成功/失败）。对于每个备份操作，它记录开始和结束时间以及实际备份大小和完整数据集大小。它还存储实例名称、备份所在的物理节点以及 MySQL 版本等信息。清单文件的核心是一组清单记录（每个 SSTable 文件一个），它为给定的 SSTable 文件定义 — 包括文件名、存储路径、文件大小以及用于完整性检查的 SHA256 校验和。最后，它记录元数据 tarball 的 Blob 存储路径，其中包含数据库文件 (*.sdi)、ibdata1、.log、检查点文件、xtrabackup_info 等。此信息是触发下一次备份的起点，构建于现有的 SSTable 文件池。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;0741f261-6e09-4000-b490-d6931bc68204&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-backup-architecture&#34;&gt;备份架构&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;53656ec6-c7b2-4682-aeb9-080cce3439ef&#34;,&#34;dropCap&#34;:false}&#34;&gt;现在我们介绍完了差异备份的核心组件，下面我们来详细看看备份是如何编排的。备份是在分区级别从分区的 2 个从属节点之一上的 MyRocks 容器之一进行的，并由称为备份调度程序的无状态服务触发。它根据分区的备份状态（单独存储在状态机中）来处理备份的频率和时间。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6b94cb64-b72f-41c5-a00a-7025fed1cedd&#34;,&#34;dropCap&#34;:false}&#34;&gt;一旦触发，实际备份由分区节点中的各个备份容器进行。备份容器是一个临时容器，它会在需要备份时启动，并运行 XtraBackup 工具来提取备份，将备份保留在 Blob 存储中，保留状态并返回睡眠状态。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;524cff10-2d66-48b7-9ba7-6cdac23591dd&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator has-al”pha 通道不透明度&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34; :&#34;3df1e79b-0ec2-43a8-9dbf-c06a5c60db3b&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent. com/docsz/AD_4nXdABiGaoJnGyPBMqOAb3IA8tG4c-yDW3p7Z0y3fTwF9NGLW6FB5FJCXGOwBF5_qcymJOqeQeY6rHnOsXK5qvFbv2K6trWTimxTinZJ4vhoaKLXVSr0BbP3O9XGOJtOl02RuGF2 WSAoF50er63JOVtlIVHdg?key=5NK_RoiIfdfw2rcH98YHAg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 4:通用备份控制流程。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;54fede1e-04e4-44a5-9c22-cff3288830ec&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7b057e30-c38d-4519-964b-695d00af782e&#34;,&#34;dropCap&#34;:false}&#34;&gt;在我们得到之前进入备份容器进程级别，以下是术语定义：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ba8a98b4-12d3-4690-b59e-fdbd8f633a01&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt; SSTFile 池位置：&lt;/strong&gt;这是 Blob 存储中的位置，所有 SSTable 文件都汇集在一起​​，以便由差异备份共享/引用。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;99e60407-d915-45bd-a26f-2fde13ab876c&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;单独备份位置：这是 Blob 存储中特定于备份的位置，将包含 tarball 元数据文件和备份清单文件。这不与任何其他备份共享，并且对于特定备份和特定分区节点也是唯一的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;295c534d-5804-4be0-acc1-e860adb3595e&#34;,&#34;dropCap&#34;:false}&#34;&gt;备份过程首先，调度程序根据上次备份时间和频率确定是否需要新的备份，通常选择与之前相同的分区节点。然后，调度程序指示节点管理器启动配备有特定信息的备份容器，例如，如果执行差异备份，则为先前的清单文件。 XtraBackup 用于将备份数据直接流式传输到备份容器的磁盘缓冲区中。元数据文件被压缩到单个 tarball 中，以最大程度地减少 Blob 存储交互并简化恢复，同时根据备份清单检查 SSTable 文件，以确定它们是否存在于 Blob 存储中的 SSTFile 池位置中。新文件将被持久化，现有文件将被记录在清单中，而无需重新持久化。这例程定期更新 blob 存储中的清单文件，以保持备份一致性并避免数据丢失。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4d7fa2ee-f375-4202-b223-edfb5b3dc162&#34;,&#34;dropCap&#34;:false}&#34;&gt;一次备份提取完成后，备份清单被标记为有效并保存，标志着备份过程的结束。如果出现问题，清单将被保存并带有指示问题的状态。备份后，清单文件按保留期限进行分类，过时的 SSTable 文件和过期的元数据以及清单文件将从 Blob 存储中清除，以管理空间并保持效率。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;82d331bd-f079-4342-b69a-043dd79c5e94&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;ee780d70-15d4-4b48-9f9b-6d32ea6db074&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-full-vs- Differential-backups&#34;&gt;完整备份与差异备份&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cd0b3236-abeb-48d2-8309-08771bce19b7&#34;,&#34;dropCap&#34;:false}&#34;&gt;现在让我们开始吧深入了解完整备份和差异备份的区别。备份调度器扮演决策者的角色，根据具体情况决定进行全量备份还是差异备份。默认选择是差异备份，通过仅保存自上次完整备份以来的更改来提高效率。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a5819ec9-6a0a-4fea-b093-5b3e8ae22578&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;完整备份就像开始新的篇章一样。它们涉及分区节点中所有 SSTable 的完整副本，类似于使用 XtraBackup 完成的传统备份。此过程在几个关键场景中启动：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:true,&#34;hash&#34;:&#34;31cf996d-8026-4025-8862-ad2d6aa25d57&#34;,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;此备份方法首次在生产中推出，为未来所有差异备份奠定了基础&lt;/李&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;当由于删除或更改影响备份完整性而导致先前的分区节点不再适合时，需要新的分区节点来自不同节点的完整备份&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;MyRocks 版本升级后，即使不同版本的 SSTable 可能兼容，完整备份也可确保没有版本不匹配干扰&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;当完整备份中的现有池已达到差异限制时所有备份，以防止可能危及所有备份的潜在损坏&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;59348321-98b0-4352-83ff-84bb4d6770bd&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;然后差异备份上台，在完整备份之后介入，除非情况另有规定。这些备份侧重于仅存储分区节点中新的或更改的 SSTable 文件，从而显着减少存储的数据量。这种策略不仅节省了空间，而且还巧妙地在多个备份之间共享 SSTable 文件，其结构可以有效地组织清单文件和元数据。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d91b0bed-b62a-4caf-aa29-337dc3576573&#34;,&#34;dropCap&#34;:false}&#34;&gt;每种备份类型有其作用，要么通过完整备份奠定坚实的基础，要么通过差异备份在此基础上高效构建，确保备份过程中的数据完整性和优化。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;83645669-b71c-4f33-88e6-f53c2c992671&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34; :&#34;7fd2a191-7e15-4083-8e75-4b1a6925bb24&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent. com/docsz/AD_4nXc1ze77IZP1-kJ_nUlyh0Ovfg9OrhlyHwKP5zPG-pilregTxExTGTcQTa2Zm7WeHeixdUsACkr1vw29rlbRe9_VwMr4UtjsMH6oEjfP4UHEYm-HfQnZg2nGHqi1iA9bFqMZ_KWo5 f0KtYAz50g_HqV5qJs8?key=5NK_RoiIfdfw2rcH98YHAg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34; &gt;图 5：Terrablob 结构。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d1a4a5c9-57a2-4006-b066-ac0c94a261ce&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;3ac9f57c-b411-4dbd-bc13-23c42e27693b&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-limitations&#34;&gt;限制&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4224a347-49e7-4a79-a5be-4f2cea497437&#34;,&#34;dropCap&#34;:false}&#34;&gt;每种设计方法伴随着一系列权衡和假设，备份策略也不例外。所选择的方法主要利用共享旧文件的效率，同时仅将新数据添加到备份中。然而，这种模式并非没有挑战。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6a1143e3-ddfa-459b-8961-346d1eada4b6&#34;,&#34;有序&#34;:false,&#34;值&#34;:&#34;&#34;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;在分区高度活跃或频繁压缩的环境中（称为“热分区”），预期差异备份的好处可能会减少。这些情况因分区而异，是不可避免的，并且会影响备份过程的总体节省。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;此外，该策略的设计优先考虑为所有备份使用相同的分区节点，假设节点保持在适合备份的状态。如果节点被替换或转变为不合适的状态，则需要完整备份。 &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;03573336-ee1c-4276-a385-37f927ae04cc&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;93af71a9-cc2a-4295-9108-da1c71268293&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-restoring-differential-backups&#34;&gt;恢复差异备份&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e9546b53-244e-4687-8a19-51acaaae268b&#34;,&#34;dropCap&#34;:false}&#34;&gt;恢复 XtraBackup MyRocks 备份非常简单，因为它使用相同的 Percona XtraBackup 工具进行恢复。恢复过程的高级步骤如下所示：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;96214fb7-269d-4e2d-bcef-5cb8dcd22b63&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;找到需要在 Blob 存储区中恢复的有效备份。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;将备份清单文件从 blob 存储下载到临时还原位置。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;查找备份清单文件并下载 SSTable 和元数据文件（并从 tarball 中提取）并将将 SSTable 文件复制到提取后出现的 .rocksdb 文件夹中。至此，我们已经组装了备份，就像 XtraBackup 在备份期间进行的备份一样。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;运行 XtraBackup 准备命令，该命令将运行崩溃恢复过程以使备份与扫描的 LSN 一致。 &lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;停止 MySQL 进程并运行 XtraBackup 复制回命令以替换 MySQL 数据文件夹（例如 /var/ lib/mysql/*) 以及下载并准备好的备份数据。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;重新启动 MySQL 进程并开始使用恢复的数据库。&lt;/里&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c28d35e3-4fd8-4591-882a-3eb7ca1b68ef&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;a1038b89-bf80-4d2b-926a-e7350c4fa45e&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-conclusion&#34;&gt;结论&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ee2197a8-4e67-41fb-9ceb-c1776322c0d7&#34;,&#34;dropCap&#34;:false}&#34;&gt;差异备份事实证明，该策略在效率和节省存储空间方面可以改变游戏规则：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8feee233-e226-4912-9aea-496cc9447a73&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;通过不将未更改的数据保留到 Blob 存储中，我们成功地将 Blob 存储的使用量减少了 PB 级，在大多数实例中实现数据存储量大幅减少 45%。在我们的一些较大实例中，节省的效果甚至更为显着，每次执行差异备份时，节省量可减少 70% 或更多。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;此外，重新设计的备份过程也快了很多。完整备份完成时间平均比以前的备份完成时间快 2 倍，差异备份平均快 5 倍。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ce168bfa-ba63-42f5-bd4b-e82fe2898134&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-acknowledgements&#34;&gt;致谢&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9ef7a1c9-0e51-4b07-8b12-8921fa8dbbfc&#34;,&#34;dropCap&#34;:false}&#34;&gt;感谢郝Xu、Vlad Dmitriev、Amit Garg、Mohammed Khatib、David Turner 和 Andrew Regner 在将这个项目塑造成今天的样子时提供了指导和指导。感谢 Osama Mazahir 和 Piyush Patel 的宝贵领导。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;7bc03d51-d18d-431a-a587-0c6f4ac7f7b2&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;Percona®、Percona Xtrabackup、Percona Server for MySql 和星形徽标是 Percona 在美国和/或其他国家/地区的注册商标或商标。使用这些标记并不暗示 Percona 的认可。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;9c4bee1c-f759-4e78-95a6-383c36fb4bf3&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;封面照片归属：“&lt;a href=&#34;https://www.flickr.com/photos/8718930@N07/3462606643&#34; target=&#34;_blank&#34; rel =&#34;noreferrer noopener&#34;&gt;CERN 的服务器机房&lt;/a&gt;”由 &lt;a href=&#34;https://www.flickr.com/photos/8718930@N07&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;torkildr&lt;/a&gt; 获得许可&lt;a href=&#34;https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;CC BY-SA 2.0&lt;/a&gt;。未进行任何更改。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;957d3f33-a9e4-491f-9c3e-ea43050a8ff1&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;</description>
      <pubDate>Thu, 01 Aug 2024 07:47:22 +0000</pubDate>
    </item>
    <item>
      <title>【Navigating the LLM Landscape: Uber’s Innovation with GenAI Gateway】驾驭 LLM 格局：Uber 通过 GenAI Gateway 进行创新</title>
      <link>https://www.uber.com/blog/genai-gateway/</link>
      <description>【&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;639e6d58-1937-4773-a697-02436d75c659&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a4ced9b4-63a9-4d41-995b-c471a659eed4&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Large Language Models (LLMs) have emerged as pivotal instruments in the tech industry, unlocking new avenues for innovation and progress across various sectors. At Uber, the impact of LLMs is particularly noticeable, with over 60 distinct use cases being identified in diverse domains, ranging from process automation to customer support and content generation. As teams at Uber embark on the journey of integrating LLMs into their products, several challenges have surfaced. Notably, the disparate integration strategies adopted by different teams have led to inefficiencies and redundant efforts.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;83d057b4-73ce-417e-bd02-f91cc428a2a9&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To address these challenges and harness the growing demand for LLMs, Uber’s Michelangelo team has innovated a solution: the GenAI Gateway. The GenAI Gateway serves as a unified platform for all LLM use cases within Uber, offering seamless access to models from various vendors like OpenAI and Vertex AI, as well as Uber-hosted models, through a consistent and efficient interface. The GenAI Gateway is designed to simplify the integration process for teams looking to leverage LLMs in their projects. Its easy onboarding process reduces the effort required by teams, providing a clear and straightforward path to harness the power of LLMs. In addition, a standardized review process, managed by the Engineering Security team, reviews use cases against Uber’s data handling standard before use cases are granted access to the gateway.&amp;nbsp; If testing is successful these projects go through our standard, cross-functional software development process. The centralized nature of the gateway also streamlines the management of usage and budgeting across various teams, promoting greater control and operational efficiency in the integration of LLMs across the company.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8f5812a9-767d-43f3-915e-0de639f58afc&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;271006ea-850d-4aaa-9b12-649b7fd88696&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-design-amp-architecture&#34;&gt;&lt;strong&gt;Design &amp;amp; Architecture&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e3d75695-952c-4dcf-aa1b-c4a1f2346c22&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;A pivotal design decision was to mirror the HTTP/JSON interface of the OpenAI API. This strategic choice is rooted in OpenAI’s widespread adoption and thriving open-source ecosystem highlighted by libraries like LangChain and LlamaIndex. This alignment fosters seamless interoperability, ensuring GenAI Gateway’s compatibility with existing open-source tools and libraries while minimizing the need for adjustments. Given the rapid evolution of the open-source community, a proprietary interface would risk becoming quickly outdated. By aligning with OpenAI’s interface, GenAI Gateway stays in step with cutting-edge advancements. This approach not only streamlines the onboarding process for developers but also extends GenAI Gateway’s reach, allowing users to access LLMs from various vendors, like Vertex AI, through a familiar OpenAI API framework. See Figure 1 below for the high-level architecture diagram.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6d3618fd-55dd-44d8-b957-838a8b5dec19&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The following code snippets demonstrate how to use GenAI Gateway to access LLMs from different vendors with unified interface:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6963d647-7655-44b0-9310-a7d510ddb789&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;681fd513-bcc0-4e80-8715-d6e834a589a2&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXeSeRL_lT222k2UnrRa4Ywx8XbtT5alPHmZjCu_7-_RJvRPf-p7TZlT8Lsbm-ysathYSSxqVVCXBE_ALt1IfTeY_Z6CAuCcPuVas1vxly9HbguZ0si1KR0eZ_-9OOlfy3kAmmTSXUrU8SkD5Ct34F2hJB0?key=5egmPsUgOM1kuqOeYjTalA&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2d022d50-550a-4c3c-96f3-5e2b4a27e6d5&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote data-wp-block-name=&#34;core/quote&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;da9b9046-5685-4270-95e5-f47d4210332a&amp;quot;,&amp;quot;value&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Answer from gpt-4:&amp;nbsp;&lt;/strong&gt;The capital of the USA is Washington D.C.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Answer from chat-bison:&amp;nbsp;&lt;/strong&gt;&amp;nbsp;Washington, D.C. is the capital of the USA.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Answer from llama-2-70b-chat-hf-0:&amp;nbsp;&lt;/strong&gt;The capital of the United States of America is Washington, D.C.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d3cfdec3-78b6-45e0-a99f-228020ce7900&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;As can be seen from above, developers write code as if they’re using native OpenAI client, while being able to access LLMs from different vendors.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fac9714d-4d09-45e5-a474-501a4df1f5f0&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Architecture-wise, GenAI Gateway is a Go service that acts as an encompassing layer around the clients for third-party vendors, complemented by the in-house serving stacks tailored for Uber’s LLMs.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;35331a76-20eb-4ee4-96cb-06d88b4940d5&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Our approach to integrating with OpenAI involved developing an internal fork of the Go client implementation sourced from &lt;a href=&#34;https://github.com/sashabaranov/go-openai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;the GitHub repository&lt;/a&gt;. When it came to integrating Vertex AI, specifically for accessing PaLM2, we faced a challenge: the absence of a suitable Go implementation at that time. We took the lead in developing our version and subsequently &lt;a href=&#34;https://github.com/uber/go-vertex-ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;open-sourced&lt;/a&gt; it, contributing to the broader community. We encourage community engagement, inviting contributions ranging from bug reports to feature requests. This library mainly focused on features like text generation, chat generation, and embeddings. At the time of writing this blog, Google has also published their &lt;a href=&#34;https://pkg.go.dev/cloud.google.com/go/aiplatform@v1.58.0/apiv1#PredictionClient&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Vertex AI Prediction Client&lt;/a&gt; but we will continue to support our library because of its ease of use.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;08e977a6-95bc-4fea-9bec-77963803511a&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;For Uber-hostedLLMs, we’ve engineered a robust serving stack built upon the STOA inference libraries, to optimize performance and efficiency. This blend of external integration and internal innovation reflects our commitment to staying at the forefront of LLM technology and contributing to its evolving ecosystem.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;dd0f0e30-74ee-4cb0-88cf-849b7182a21c&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;de64298b-1d41-4f31-94dd-8efe0c3fd1f4&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdm1yQEGqJOKLbzBDtbpKf9DxKGFweuXueR8Din4Z1dKCy1nB2RZgD1OnrxROhyEQlogzt4fLUijzaLvIqkpRwZsNxK9h-tDI9LhoAeKrZDG2fo5j8ThIVw-DQ-XJ234XQIMpDH9k2G0REM6Mdqjsb-dsE?key=5egmPsUgOM1kuqOeYjTalA&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2e53726b-6bc9-4d35-86ab-73767c4c9764&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;25fc1db6-aa70-49af-8770-af946ab52ae8&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Beyond its serving component, an integral facet of GenAI Gateway is the incorporation of a Personal Identifiable Information (PII) redactor. Numerous studies have underscored the susceptibility of LLMs to potential data breaches, presenting significant security concerns for Uber. In response, GenAI Gateway incorporates a PII redactor that anonymizes sensitive information within requests before forwarding them to third-party vendors. Upon receiving responses from these external LLMs, the redacted entities are restored through an un-redaction process. The goal of this redaction/un-redaction process is to minimize the risk of exposing sensitive data.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c6a305fc-df19-4b59-855f-81e55b619b53&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Complementing its core functionalities, GenAI Gateway incorporates additional components designed for authentication and authorization, metrics emission to facilitate reporting and alerting, and the generation of audit logs for comprehensive cost attribution, security audit purposes, quality evaluation, and so on. All these components are seamlessly integrated into Uber’s in-house ecosystem, ensuring a cohesive and synergistic integration that aligns with the organization’s broader technological framework. This strategic alignment underscores GenAI Gateway’s commitment to not only meeting immediate needs, but also seamlessly integrating with Uber’s established infrastructure for enhanced efficiency and compatibility.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e465a37b-3942-438f-b401-047964f7ea17&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Today, GenAI Gateway is used by close to 30 customer teams and serves 16 million queries per month, with a peak QPS of 25.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;c39f3924-1f0f-4622-8415-aff024774735&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-comparison-to-similar-offering&#34;&gt;&lt;strong&gt;Comparison to similar offering&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;dd5d024c-69af-4884-8d0f-693d06a660f3&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Although Databricks recently introduced the &lt;a href=&#34;https://www.databricks.com/blog/announcing-mlflow-ai-gateway&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;MLflow AI Gateway&lt;/a&gt; that shares several features with our GenAI Gateway, GenAI Gateway stands apart in key ways from the MLflow AI Gateway. Our GenAI Gateway closely mirrors OpenAI’s interface, offering benefits not found in the MLflow AI Gateway, which has adopted a unique syntax for LLM access (create_route and query). In addition to aligning with OpenAI’s interface, GenAI Gateway enables a consistent approach to data security and privacy across all use cases. Furthermore, our platform extends beyond Python, providing support for Java and Go, which are the primary programming languages used in Uber’s backend infrastructure. This multi-language support, combined with our focus on security and alignment with OpenAI’s familiar interface, underscores GenAI Gateway’s unique position in the realm of LLM platforms.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7522f8ea-645f-421f-8016-c4ed75c83a03&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;802ff089-59a7-495a-9ff4-6f8b132bc26c&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-challenges&#34;&gt;&lt;strong&gt;Challenges&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7620ca6d-1028-4428-990e-21fa9481b986&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The aim is for this platform to emulate the performance and quality of the native OpenAI API so closely that the transition is imperceptible. In this section of our blog, we will delve into the challenges encountered in achieving this seamless integration, particularly through the lens of handling PII.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;c6009cac-ddc0-4bce-9bd5-312707413fe7&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-pii-redactor&#34;&gt;&lt;strong&gt;PII Redactor&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f3b7ab62-f0d5-4785-9b6b-e777a175b9ea&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The PII redactor, while essential for privacy and security, introduces challenges to both latency and result quality. To understand how PII redactor introduces these challenges, we first need to understand how PII redaction works.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5f946373-3a8b-4878-8264-377734a7ac83&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The PII redactor scans input data, identifying and replacing all instances of PII with anonymized placeholders. Its sophisticated algorithm adeptly recognizes a wide range of PII categories. Each type of PII is substituted with a unique placeholder–for example, names are converted to ANONYMIZED_NAME_, while phone numbers are changed to ANONYMIZED_PHONE_NUMBER_. To maintain distinctiveness, these placeholders are assigned sequential numbers, creating unique identifiers for each occurrence: the first name in a dataset is labeled as ANONYMIZED_NAME_0, followed by ANONYMIZED_NAME_1 for the second, and so forth. The following example illustrates this process in action:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;494420ae-568d-437b-b3c1-d52afd067ab9&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;div data-wp-block-name=&#34;core/columns&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1cac3655-b6a7-4699-9a1a-0258a84f8f74&amp;quot;,&amp;quot;isStackedOnMobile&amp;quot;:true}&#34; class=&#34;wp-block-columns is-layout-flex wp-container-core-columns-is-layout-1 wp-block-columns-is-layout-flex&#34;&gt;&#xA;&lt;div data-wp-block-name=&#34;core/column&#34; data-wp-block=&#34;[]&#34; class=&#34;wp-block-column is-layout-flow wp-block-column-is-layout-flow&#34;&gt;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Original Text&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;George Washington is the first president of the United States. Abraham Lincoln is known for his leadership during the Civil War and the Emancipation Proclamation.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div data-wp-block-name=&#34;core/column&#34; data-wp-block=&#34;[]&#34; class=&#34;wp-block-column is-layout-flow wp-block-column-is-layout-flow&#34;&gt;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Anonymized Text&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;mark class=&#34;has-inline-color has-276-ef-1-color&#34;&gt;ANONYMIZED_NAME_0&lt;/mark&gt; is the first president of the United States. &lt;mark class=&#34;has-inline-color has-276-ef-1-color&#34;&gt;ANONYMIZED_NAME_1&lt;/mark&gt; is known for his leadership during the Civil War and the Emancipation Proclamation.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a390ee88-0d87-45ea-9e39-ef9714ef0190&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1bd16342-68ba-447b-b1e9-58dc0f417e92&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The mapping of PII data to anonymized placeholders is used in the un-redaction process that restores PII data from anonymized placeholders back to its original form, before returning the result to users.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4b0a9f7f-be47-4c51-8207-4f666854ec06&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Depending on the location the PII data is in the input, the same word can be redacted to different anonymized placeholders as it will be appended with different sequential numbers:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6da4648d-9005-47d8-9c1b-457872f11f9f&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;div data-wp-block-name=&#34;core/columns&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2a7aa4dd-5804-45ff-b42d-dce68e393123&amp;quot;,&amp;quot;isStackedOnMobile&amp;quot;:true}&#34; class=&#34;wp-block-columns is-layout-flex wp-container-core-columns-is-layout-2 wp-block-columns-is-layout-flex&#34;&gt;&#xA;&lt;div data-wp-block-name=&#34;core/column&#34; data-wp-block=&#34;[]&#34; class=&#34;wp-block-column is-layout-flow wp-block-column-is-layout-flow&#34;&gt;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Original Text&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Abraham Lincoln is known for his leadership during the Civil War and the Emancipation Proclamation. George Washington is the first president of the United States.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div data-wp-block-name=&#34;core/column&#34; data-wp-block=&#34;[]&#34; class=&#34;wp-block-column is-layout-flow wp-block-column-is-layout-flow&#34;&gt;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Anonymized Text&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;mark class=&#34;has-inline-color has-276-ef-1-color&#34;&gt;ANONYMIZED_NAME_0&lt;/mark&gt; is known for his leadership during the Civil War and the Emancipation Proclamation. &lt;mark class=&#34;has-inline-color has-276-ef-1-color&#34;&gt;ANONYMIZED_NAME_1&lt;/mark&gt; is the first president of the United States.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b782d3a7-95b8-4f93-bfdc-f6fac2dea047&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:4,&amp;quot;hash&amp;quot;:&amp;quot;c80b26e7-853a-4bfd-a948-197d4519ed46&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-latency&#34;&gt;&lt;strong&gt;Latency&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1470fed1-1c5b-404c-bb90-d42067674e3b&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;While most additional components in GenAI Gateway are lightweight, the PII redactor, by nature of scanning and anonymizing entire requests, incurs added latency proportional to the length of the input request. In cases where the input request is notably large, such as a few thousand tokens, the PII redactor alone can introduce a latency of several seconds. To address this, we’ve transitioned to a CPU-optimized model, resulting in a substantial 80%+ reduction in latency, without compromising accuracy. Furthermore, we are in the process of assessing more advanced models, including one that leverages GPU technology, to further enhance the processing speed and overall efficiency of the PII redactor.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:4,&amp;quot;hash&amp;quot;:&amp;quot;087365ba-af49-4fff-916f-f0fbfa2998ee&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-quality&#34;&gt;&lt;strong&gt;Quality&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0e2e3c69-1d74-47d1-8e92-34988bac8b83&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The PII redactor can inadvertently impact the quality of results. Its process of anonymizing sensitive data, while safeguarding user information, sometimes strips away crucial context, thereby affecting the response quality of LLMs. For example, a query like “Who is George Washington?” is transformed into “Who is ANONYMIZED_NAME_0” for LLM processing. This anonymization can hinder LLMs’ ability to generate relevant responses due to the loss of specific contextual information.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9fdb0ac0-6a2a-44cb-a471-14462165dd91&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Furthermore, the PII redactor’s mechanism presents unique challenges in scenarios like LLM caching and Retrieval Augmented Generation (RAG). LLM caching, which stores responses for frequently asked questions to facilitate quick retrieval, faces a dilemma. Anonymized queries, such as “Who is George Washington?” and “Who is Abraham Lincoln?”, become indistinguishable post-redaction, leading to potential inaccuracies in cached responses. Similarly, RAG, which relies on fetching pertinent documents to aid LLMs in response generation, struggles with the inconsistencies introduced by anonymization. For instance, embedding a historical article about the American Revolutionary War might involve different anonymized placeholders for the same entity in offline and online contexts, leading to the retrieval of incorrect documents.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2f0c4eea-0d91-4bbb-80ba-cbf460e06a0b&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;These challenges highlight a fundamental issue: the difficulty in linking anonymized placeholders back to their original entities, causing errors in cached results or document retrieval. While maintaining a global table mapping original entities to anonymized placeholders is impractical, we are exploring solutions:&amp;nbsp; One approach encourages customers to use Uber-hostedLLMs, which do not require PII redaction. Simultaneously, we are evaluating the security assurances of third-party vendors to consider the possibility of forgoing the PII redactor entirely, striving to balance privacy concerns with operational effectiveness.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;e1b6de5b-fe78-4d7c-bde0-04951bf725f2&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-other-challenges&#34;&gt;&lt;strong&gt;Other Challenges&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d0f6465d-4a72-4f92-988d-4f7b68bee7ac&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Beyond the previously mentioned difficulties with PII redaction, the GenAI Gateway encounters additional, diverse challenges. The ever-evolving landscape of Large Language Models (LLMs) in recent months has prompted us to dynamically adjust our priorities. For instance, the recent introduction of GPT-4V shook things up by altering the request interface to accommodate image URLs and base64-encoded images. Fortunately, the responsive open-source community swiftly proposed solutions to seamlessly adapt to this change.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;87fc3c4d-83a0-4ebf-9418-3b40fbf0a158&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Given that GenAI Gateway’s core functionality revolves around forwarding requests to relevant LLM vendors, its availability is closely tied to the operational status of these vendors. In the event of a vendor outage, effective communication with users is crucial to prevent any misattribution of issues to the gateway itself. To bolster resilience, we are actively exploring the possibility of incorporating Uber-hostedLLMs as a fallback option when a third-party vendor encounters downtime.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e65b175f-48ce-4c6d-a292-db0227c90f3a&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4c72bb58-460c-4754-be32-ed051f246615&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-use-case-summarisation-of-chats-for-agents-resolution&#34;&gt;&lt;strong&gt;Use Case: Summarisation of Chats for Agents Resolution&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b1ed6027-0b3d-4a7e-a8ab-4e59ae013f4b&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In our continuous pursuit of enhancing customer support efficiency, Uber leverages large language models (LLMs) to streamline the process for our customer support agents. The primary focus is on swiftly summarizing user issues and suggesting potential resolution actions, significantly reducing the time it takes to address user queries. This not only expedites query resolutions but also contributes to an overall improved user experience. We leverage LLMs internally for following&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:true,&amp;quot;hash&amp;quot;:&amp;quot;0aee051c-93e7-4bbf-a71b-afb5c6d71f45&amp;quot;,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Enhance chatbot-to-agent transitions, providing agents with concise summaries of prior interactions for improved understanding, faster resolution and addressing key challenges&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Furnish agents with crucial background information and user sentiments, enabling empathetic and contextually accurate support.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Implement automatic summarization of contact interactions, reducing manual summarization time by 80% and improving operational efficiency.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;d7dfbca3-3c84-4ee3-92e3-780a9377fe18&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-impact&#34;&gt;&lt;strong&gt;Impact&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c2da11cc-0927-4036-9dd7-500c408aa19a&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The implementation of LLMs has proven highly beneficial, with 97% of generated summaries found to be useful in resolving customer issues. Agents report significant time savings in reading and documentation, thereby enhancing their overall productivity. The agents are able to revert back to users 6 seconds faster than before. We are generating ~20 million summaries per week leveraging the LLMs, which we plan to expand to more regions and contact types in the future.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d87d11d4-8c61-4223-975e-5b8ee9b78de2&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In essence, our utilization of LLMs at Uber transcends mere automation–it’s a strategic enhancement focused on empowering our customer support agents to provide faster, more accurate, and empathetic resolutions.&amp;nbsp; All with the goal of delivering an unparalleled experience to our users.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;6f3fff59-4eda-4a9e-ba4e-e4d86dd0f080&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-how-gen-ai-gateway-is-used&#34;&gt;&lt;strong&gt;How Gen AI Gateway is used&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3c251dc5-8f76-4f63-8cf6-46b83d4617df&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Within Uber’s Customer Obsession organization, the CO Inference Gateway was initially employed to expose various ML Task-based API contracts internally to other services, abstracting out different ML Model hosts. For Summarization, we expanded this service to include a new Generation ML Task for Text, Chat, and Embedding Generation. This extension enables connections to both Open AI and Google Vertex AI models, fostering flexibility and adaptability.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6e9637f3-267e-49bc-8cbf-b2dbf1ce70e8&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;3a0a4eff-508a-450e-b7b6-d30abd888d73&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXe5_r9WXoElXeydaicgO-hAtPt5YOQr4MqZ3CKPRaVxgFWIkw1EnRrd9QKeEsmG-9SsHwbhGG-epfVUKYq6AGCli5guSvFn445zPBDxDkh6PLqYFzzwu4WE7yWr7f_4E7YOqYQ1z7o_WP3iO5Qyza27D7rI?key=5egmPsUgOM1kuqOeYjTalA&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;631b8d21-ba2a-4a6d-acc3-604bff6e1da1&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7e654ae5-5120-4e71-86fd-67fa18b95853&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1mIgcSDvcFjzWO7H774u1ay20mF00sZEG/view?usp=drive_link&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4c59483c-8bd4-4b50-9c4b-5f82c528ad7d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Encountering challenges such as PII Redaction, Cost attribution, and the imperative for a centralized service at Uber to connect with any external Language Model (LLM), we made a strategic decision to leverage the Gen AI Gateway instead of directly calling external models. This decision was guided by the need for a comprehensive solution that not only addresses challenges effectively but also ensures a robust and secure integration. By doing so, we navigate complexities and optimize the utility of our AI-powered solutions, aligning with Uber’s commitment to innovation and excellence in customer support.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;85d40cc1-5690-4a43-9b85-98191edf7e4f&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;d8ba6985-0b79-4558-a5b8-078e0e381f63&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXcfFXsQPWf1S4ppo-vik3bFc867WLviUl182cHDZ-K7JZNaut0dfHXU5kKhFpVHJX9KERz4NVNwQS9un8lHVCwt74uD4MJ0sINAUrcyQwifX3COr4k3ZiJwdE338BFLKxROhBOSxwNKAqntSNFWi6-zNTxu?key=5egmPsUgOM1kuqOeYjTalA&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ee250d3c-364e-42b1-bde5-b5d81c1e3f20&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;c5f5f108-19fa-4345-b0bd-99044df1648e&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-prompt&#34;&gt;&lt;strong&gt;Prompt&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ec567b34-0b9f-453d-8864-911845c39aa5&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Following is a sample Prompt for the summarisation of contact tickets. We provide a few examples in the prompt context for few-shot learning.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote data-wp-block-name=&#34;core/quote&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;01b04911-552d-4e9d-b786-8ecb2b12562c&amp;quot;,&amp;quot;value&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The following is a conversation between an Uber Customer support Bot called BOT, possibly a customer support agent called AGENT and an Uber Eats Customer called USER. Provide a detailed breakdown of the conversation. Identify all issues or intents and associated sentiments from the user. Extract the most pertinent part of user utterances and agent responses for each identified problem.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;input:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;dropCap&amp;quot;:false}&#34;&gt;{{conversation_log}} // Conversation log is the actual message history between customers, chatbots and agents.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f23b1fbe-c359-4c05-95ae-1a952fe7008f&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;316cdf49-3163-4cda-b13f-79fc13d055b3&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-learnings&#34;&gt;&lt;strong&gt;Learnings&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;885e8612-3fc7-4e44-b412-044519b57ea9&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In reflecting on our journey with LLMs, a fundamental lesson stands out: the critical need for adaptability in the face of the rapidly evolving LLM landscape. As we delve deeper into the realm of advanced LLM applications, the importance of skillfully managing the interplay between ever-changing technologies, user privacy, and efficiency becomes increasingly evident. This landscape is constantly shifting, and our dedication to continuous improvement is more crucial than ever. Navigating these dynamics with agility and foresight is pivotal to our mission, ensuring that we not only keep pace with technological advancements but also uphold our commitment to user privacy and system efficiency.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;daca159f-1260-4b21-b095-be3639656066&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;314ead02-2022-4b7f-9bb3-a3dc6eb8e93c&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-future-works&#34;&gt;&lt;strong&gt;Future Works&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1ac48011-3d5a-40e7-8c50-8c1736fb0fbe&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Looking ahead, our vision for the GenAI Gateway is to elevate and refine its capabilities to better serve our users. A key focus is on enhancing the onboarding process for new models. Whether these are bespoke, fine-tuned models tailored to specific needs or those sourced from the vibrant open-source community, our goal is to make their integration as fluid and user-friendly as possible. Moreover, we are keen on augmenting the gateway with advanced features that address the dynamic challenges of working with Large Language Models. This includes implementing intelligent LLM caching mechanisms, developing robust fallback logic, and introducing sophisticated hallucination detection. Safety and policy guardrails are also on our agenda to ensure that our platform remains secure and compliant with evolving standards.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c896442c-5abb-420b-85b5-898882f48a1e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In our journey to expand the gateway’s capabilities, we also recognize the importance of tapping into the vast potential of the open-source ecosystem. To this end, we are actively working towards integrating with libraries. This will not only enhance the functional breadth of our system, but also make it more versatile, enabling our users to explore and leverage a broader range of solutions. These future endeavors underscore our commitment to continually evolve the GenAI Gateway, ensuring it remains a cutting-edge, versatile, and secure platform for harnessing the power of LLMs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;62803538-f24d-48ec-971e-28ba28c745cc&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;c312f523-db66-494c-9407-311cee6c8b60&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ad1f31f3-01c4-475c-b839-51e7dd25b3cf&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;LLMs have become a transformative force. The integration of LLMs at Uber, however, has not been without challenges. Inconsistent integration strategies and the absence of a standardized approach have led to inefficiencies and difficulties in monitoring costs and vendor usage.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8def3e9e-3285-449e-9694-e7fcfe114573&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Addressing these issues, Uber’s Michelangelo team developed the GenAI Gateway, a unified platform facilitating access to LLMs from multiple providers, including OpenAI and Vertex AI, as well as Uber’s in-house models. This platform streamlines the integration process, ensuring compliance with privacy and security standards and efficient usage management. GenAI Gateway’s design, mirroring the OpenAI API, ensures compatibility with existing tools and maintains pace with the evolving open-source community. It also features a PII redactor, enhancing security. This strategic development of the GenAI Gateway, coupled with its focus on operational efficiency and alignment with Uber’s broader technological framework, exemplifies Uber’s commitment to innovation while addressing the dynamic challenges of working with LLMs.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d24118d6-acbf-4d50-959b-5337150cda8c&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-acknowledgments&#34;&gt;&lt;strong&gt;Acknowledgments&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ae51ea31-0c43-4ce0-9617-47822269b6e2&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We could not have accomplished the technical work outlined in this article without the help of various engineering teams, Uber AI, and the Uber Customer Obsession Team. We would also like to thank the various product teams working with us in adopting Gen AI Gateway.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;afc6017c-684e-44ac-abcc-678a2031b044&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Cover Photo Attribution: The “&lt;strong&gt;Artificial Intelligence, AI&lt;/strong&gt;” image is covered by a &lt;a href=&#34;https://creativecommons.org/licenses/by/2.0/&#34;&gt;CC BY 2.0&lt;/a&gt; license and is credited to &lt;a href=&#34;https://www.flickr.com/photos/152824664@N07&#34;&gt;mikemacmarketing&lt;/a&gt;. No changes have been made to the image (&lt;a href=&#34;https://openverse.org/image/49410795-b3ba-421b-918b-7f2cd9178f19?q=artificial%20intelligence&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;f526d83b-eb4f-49c3-9e7f-661ae12bcd4b&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Vertex AI™, PaLM™, Google Dialogflow™&amp;nbsp; and Go™ are trademarks of Google LLC and this blog post is not endorsed by or affiliated with Google in any way.&lt;/p&gt;】&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;639e6d58-1937-4773-a697-02436d75c659&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-introduction&#34;&gt;&lt;strong&gt;简介&lt;/strong&gt;&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a4ced9b4-63a9-4d41-995b-c471a659eed4&#34;,&#34;dropCap&#34;:false}&#34;&gt;大型语言模型（法学硕士）已成为科技行业的关键工具，为各个行业的创新和进步开辟了新途径。在 Uber，法学硕士的影响尤其明显，在不同领域确定了 60 多个不同的用例，从流程自动化到客户支持和内容生成。当 Uber 的团队开始将法学硕士融入到他们的产品中时，一些挑战已经浮现出来。值得注意的是，不同团队采用不同的集成策略导致效率低下和重复工作。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;83d057b4-73ce-417e-bd02-f91cc428a2a9&#34;,&#34;dropCap&#34;:false}&#34;&gt;解决这些问题为了应对挑战并利用对法学硕士不断增长的需求，Uber 的 Michelangelo 团队创新了一种解决方案：GenAI Gateway。 GenAI Gateway 作为 Uber 内所有 LLM 使用案例的统一平台，通过一致且高效的界面提供对 OpenAI 和 Vertex AI 等不同供应商的模型以及 Uber 托管模型的无缝访问。 GenAI Gateway 旨在简化希望在项目中利用 LLM 的团队的集成流程。其简单的入职流程减少了团队所需的工作量，提供了一条清晰、直接的途径来利用法学硕士的力量。此外，由工程安全团队管理的标准化审查流程会在用例被授予网关访问权限之前根据 Uber 的数据处理标准审查用例。  如果测试成功，这些项目将通过我们的标准跨功能软件开发流程。网关的集中化特性还简化了各个团队的使用和预算管理，提高了整个公司法学硕士集成的控制力和运营效率。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8f5812a9-767d-43f3-915e-0de639f58afc&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;271006ea-850d-4aaa-9b12-649b7fd88696&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-design-amp-architecture&#34;&gt;&lt;strong&gt;设计与架构&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e3d75695-952c-4dcf-aa1b-c4a1f2346c22&#34;,&#34;dropCap&#34;:false}&#34;&gt;关键设计决定是镜像 t 的 HTTP/JSON 接口OpenAI API。这一战略选择植根于 OpenAI 的广泛采用和蓬勃发展的开源生态系统，特别是 LangChain 和 LlamaIndex 等库。这种一致性促进了无缝的互操作性，确保 GenAI Gateway 与现有开源工具和库的兼容性，同时最大限度地减少调整的需要。鉴于开源社区的快速发展，专有接口可能会面临快速过时的风险。通过与 OpenAI 的界面保持一致，GenAI Gateway 与前沿技术保持同步。这种方法不仅简化了开发人员的入职流程，还扩展了 GenAI Gateway 的覆盖范围，允许用户通过熟悉的 OpenAI API 框架访问 Vertex AI 等不同供应商的 LLM。请参阅下面的图 1 了解高级架构图。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6d3618fd-55dd-44d8-b957-838a8b5dec19&#34;,&#34;dropCap&#34;:false}&#34;&gt;以下代码片段演示了如何使用 GenAI Gateway 通过统一的接口访问来自不同供应商的 LLM：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6963d647-7655-44b0-9310-a7d510ddb789&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;681fd513-bcc0-4e80-8715-d6e834a589a2&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXeSeRL_lT222k2UnrRa4Ywx8XbtT5alPHmZjCu_7-_RJvRPf-p7TZlT8Lsbm-ysathYSSxqVVCXBE_ALt1IfTeY_Z6CAuC cpuVas1vxly9HbguZ0si1KR0eZ_-9OOlfy3kAmmTSXUrU8SkD5Ct34F2hJB0?key=5egmPsUgOM1kuqOeYjTalA&#34; alt =&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2d022d50-550a-4c3c-96f3-5e2b4a27e6d5&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote data-wp-block-name=&#34;core/quote&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;da9b9046-5685-4270-95e5-f47d4210332a&#34;,&#34;value&#34;:&#34;&#34;}&#34; class=&#34; wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;gpt-4 的回答：&lt;/strong&gt;美国的首都是华盛顿特区&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;chat-bison 的回答：&lt;/strong&gt; 华盛顿特区是首都美国的。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;llama-2-70b-chat-hf-0 的回答：&lt;/ strong&gt;美利坚合众国的首都是华盛顿特区&lt;/p&gt;&#xA;&lt;/块引用&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;核心/段落ph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d3cfdec3-78b6-45e0-a99f-228020ce7900&#34;,&#34;dropCap&#34;:false}&#34;&gt;从上面可以看出，开发者编写代码就像使用原生 OpenAI 客户端，同时能够访问来自不同供应商的 LLM。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fac9714d-4d09-45e5-a474-501a4df1f5f0&#34;,&#34;dropCap&#34;:false}&#34;&gt;架构方面GenAI Gateway 是一项 Go 服务，充当第三方供应商客户周围的包围层，并由为 Uber 法学硕士量身定制的内部服务堆栈进行补充。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;35331a76-20eb-4ee4-96cb-06d88b4940d5&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们的方法与 OpenAI 集成涉及开发来自 &lt;a href=&#34;https://github.com/sashabaranov/go-openai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;GitHub 存储库&lt;的 Go 客户端实现的内部分支&lt; /a&gt;.当谈到集成 Vertex AI，特别是访问 PaLM2 时，我们面临着一个挑战：当时缺乏合适的 Go 实现。我们率先开发了我们的版本，并随后&lt;a href=&#34;https://github.com/uber/go-vertex-ai&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;开源&lt;/a&gt;它为更广泛的社区做出贡献。我们鼓励社区参与，邀请从错误报告到功能请求等各种贡献。该库主要关注文本生成、聊天生成和嵌入等功能。在撰写本博客时，Google 还发布了他们的 &lt;a href=&#34;https://pkg.go.dev/cloud.google.com/go/aiplatform@v1.58.0/apiv1#PredictionClient&#34; target=&#34;_blank “ rel=&#34;noreferrer noopener&#34;&gt;Vertex AI 预测客户端&lt;/a&gt;，但我们将继续支持我们的库，因为它易于使用。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;08e977a6-95bc-4fea-9bec-77963803511a&#34;,&#34;dropCap&#34;:false}&#34;&gt;对于 Uber-托管LLM，我们设计了一个基于 STOA 推理库的强大服务堆栈，以优化性能和效率。这种外部整合和内部创新的结合反映了我们致力于保持法学硕士技术的前沿并为其不断发展的生态系统做出贡献。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;dd0f0e30-74ee-4cb0-88cf-849b7182a21c&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;de64298b-1d41-4f31-94dd-8efe0c3fd1f4&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXdm1yQEGqJOKLbzBDtbpKf9DxKGFweuXueR8Din4Z1dKCy1nB2RZgD1OnrxROhyEQlogzt4fLUijzaLvIqkpRwZsNxK9h-tDI9LhoAeKrZDG2fo5j8ThIVw-DQ-XJ234XQIMpDH9k2G0REM6Mdqjsb-dsE?key=5egmPsUgOM1kuqOeYjTalA&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2e53726b-6bc9-4d35-86ab-73767c4c9764&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;25fc1db6-aa70-49af-8770-af946ab52ae8&#34;,&#34;dropCap&#34;:false}&#34;&gt;超出其服务范围GenAI 网关的一个组成部分是个人身份信息 (PII) 编辑器的合并。大量研究强调了法学硕士对潜在数据泄露的敏感性，这给 Uber 带来了重大的安全问题。作为回应，GenAI Gateway 集成了一个 PII 编辑器，可以在将请求转发给第三方供应商之前对请求中的敏感信息进行匿名处理。在收到这些外部法学硕士的响应后，已编辑的实体将通过取消编辑过程进行恢复。此编辑/取消编辑流程的目标是最大限度地降低敏感数据泄露的风险。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c6a305fc-df19-4b59-855f-81e55b619b53&#34;,&#34;dropCap&#34;:false}&#34;&gt;补充其核心除了功能之外，GenAI Gateway 还集成了额外的组件，这些组件旨在用于身份验证和授权、指标发射以促进报告和警报，以及生成审核日志以进行综合成本归因、安全审核目的、质量评估等。所有这些组件都无缝集成到 Uber 的内部生态系统中，确保与组织更广泛的技术框架保持一致和协同的集成。这一战略联盟强调了 GenAI Gateway 不仅致力于满足即时需求，而且还与 Uber 现有的基础设施无缝集成，以提高效率和兼容性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e465a37b-3942-438f-b401-047964f7ea17&#34;,&#34;dropCap&#34;:false}&#34;&gt;今天，GenAI Gateway 已被近 30 个客户团队使用，每月服务 1600 万次查询，峰值 QPS 为 25。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;c39f3924-1f0f-4622-8415-aff024774735&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-comparison-to-similar-offering&#34;&gt;&lt;strong&gt;与类似产品比较&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;dd5d024c-69af-4884-8d0f-693d06a660f3&#34;,&#34;dropCap&#34;:false}&#34;&gt;虽然Databricks最近推出了 &lt;a href=&#34;https://www.databricks.com/blog/announcing-mlflow-ai-gateway&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;MLflow AI 网关&lt;/a&gt;，该网关具有多个共享功能与我们的 GenAI 网关 GenAI Gateway 在关键方面与 MLflow AI Gateway 不同。我们的 GenAI 网关与 OpenAI 的界面密切相关，提供了 MLflow AI 网关所没有的优势，MLflow AI 网关采用了独特的 LLM 访问语法（create_route 和查询）。除了与 OpenAI 的界面保持一致外，GenAI Gateway 还可以在所有用例中采用一致的方法来确保数据安全和隐私。此外，我们的平台超越了 Python，提供了对 Java 和 Go 的支持，这是 Uber 后端基础设施中使用的主要编程语言。这种多语言支持，加上我们对安全性的关注以及与 OpenAI 熟悉的界面的一致性，强调了 GenAI Gateway 在 LLM 平台领域的独特地位。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7522f8ea-645f-421f-8016-c4ed75c83a03&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;802ff089-59a7-495a-9ff4-6f8b132bc26c&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-challenges&#34;&gt;&lt;strong&gt;挑战&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7620ca6d-1028-4428-990e-21fa9481b986&#34;,&#34;dropCap&#34;:false}&#34;&gt;目标是该平台能够如此紧密地模拟原生 OpenAI API 的性能和质量，以至于这种转变是难以察觉的。在我们博客的这一部分中，我们将深入探讨实现这种无缝集成所遇到的挑战，特别是通过处理 PII 的角度。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;c6009cac-ddc0-4bce-9bd5-312707413fe7&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-pii-redactor&#34;&gt;&lt;strong&gt;PII 编辑器&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f3b7ab62-f0d5-4785-9b6b-e777a175b9ea&#34;,&#34;dropCap&#34;:false}&#34;&gt;PII 编辑器虽然对于隐私和安全至关重要，但它给延迟和结果质量带来了挑战。要了解 PII 编辑器如何引入这些挑战，我们首先需要了解 PII 编辑器的工作原理。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5f946373-3a8b-4878-8264-377734a7ac83&#34;,&#34;dropCap&#34;:false}&#34;&gt;PII 编辑器扫描输入数据，识别并用匿名占位符替换所有 PII 实例。其复杂的算法能够熟练地识别各种 PII 类别。每种类型的 PII 都替换为唯一的占位符 - 例如，姓名会转换为 ANONYMIZED_NAME_，而电话号码会更改为 ANONYMIZED_PHONE_NUMBER_。为了保持独特性，这些占位符被分配了连续的数字，为每次出现创建唯一的标识符：数据集中的第一个名称被标记为 ANONYMIZED_NAME_0，后面是ANONYMIZED_NAME_1 代表第二个，依此类推。以下示例说明了此过程的实际应用：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;494420ae-568d-437b-b3c1-d52afd067ab9&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;div data-wp-block-name=&#34;core/columns&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1cac3655-b6a7-4699-9a1a-0258a84f8f74&#34;,&#34;isStackedOnMobile&#34;:true}&#34; class=&#34;wp -block-columns is-layout-flex wp-container-core-columns-is-layout-1 wp-block-columns-is-layout-flex&#34;&gt;&#xA;&lt;div data-wp-block-name=&#34;core/column&#34; data-wp-block=&#34;[]&#34; class=&#34;wp-block-column is-layout-flow wp-block-column-is-layout-flow&#34; &gt;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;原始文本&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;乔治·华盛顿是美国第一任总统。亚伯拉罕·林肯因其在内战和《解放黑奴宣言》期间的领导才能而闻名。&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div data-wp-block-name=&#34;core/column&#34; data-wp-block=&#34;[]&#34; class=&#34;wp-block-column is-layout-flow wp-block-column-is-layout-flow&#34; &gt;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;匿名文本&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;&lt;mark class=&#34;has-inline-color has-276-ef-1-color &#34;&gt;ANONYMIZED_NAME_0&lt;/mark&gt; 是美国第一任总统。 &lt;mark class=&#34;has-inline-color has-276-ef-1-color&#34;&gt;ANONYMIZED_NAME_1&lt;/mark&gt; 因其在内战和《解放黑奴宣言》期间的领导才能而闻名。&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a390ee88-0d87-45ea-9e39-ef9714ef0190&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1bd16342-68ba-447b-b1e9-58dc0f417e92&#34;,&#34;dropCap&#34;:false}&#34;&gt;映射匿名占位符的 PII 数据用于取消编辑过程，该过程将 PII 数据从匿名占位符恢复为其原始形式，然后将结果返回给用户。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4b0a9f7f-be47-4c51-8207-4f666854ec06&#34;,&#34;dropCap&#34;:false}&#34;&gt;取决于PII 数据在输入中的位置，相同的单词可以编辑为不同的匿名占位符，因为它将附加不同的序列号：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6da4648d-9005-47d8-9c1b-457872f11f9f&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;div data-wp-block-name=&#34;核心/列&#34; data-wp-block=&#34;{&#34;哈希&#34;:&#34;2a7aa4dd-5804-45ff-b42d-dce68e393123&#34;,&#34;isStackedOnMobile&#34;:true}&#34; class=&#34;wp-block-columns is-layout-flex wp-container-core-columns-is-layout-2 wp-block-columns-is-layout-flex&#34; &gt;&#xA;&lt;div data-wp-block-name=&#34;core/column&#34; data-wp-block=&#34;[]&#34; class=&#34;wp-block-column is-layout-flow wp-block-column-is-layout-flow&#34; &gt;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;原始文本&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;亚伯拉罕·林肯因其在内战和《解放黑奴宣言》期间的领导才能而闻名。乔治·华盛顿是美国第一任总统。&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div data-wp-block-name=&#34;core/column&#34; data-wp-block=&#34;[]&#34; class=&#34;wp-block-column is-layout-flow wp-block-column-is-layout-flow&#34; &gt;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;匿名文本&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;&lt;mark class=&#34;has-inline-color has-276-ef-1-color &#34;&gt;ANONYMIZED_NAME_0&lt;/mark&gt; 因其在内战和《解放黑奴宣言》期间的领导才能而闻名。 &lt;mark class=&#34;has-inline-color has-276-ef-1-color&#34;&gt;ANONYMIZED_NAME_1&lt;/mark&gt; 是美国第一任总统。&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b782d3a7-95b8-4f93-bfdc-f6fac2dea047&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:4,&#34;hash&#34;:&#34;c80b26e7-853a-4bfd-a948-197d4519ed46&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-latency&#34;&gt;&lt;strong&gt;延迟&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1470fed1-1c5b-404c-bb90-d42067674e3b&#34;,&#34;dropCap&#34;:false}&#34;&gt;虽然大多数附加GenAI Gateway 中的组件是轻量级的，PII 编辑器由于扫描和匿名化整个请求的性质，会产生与输入请求的长度成比例的延迟。在输入请求特别大的情况下，例如几千个令牌，单独的 PII 编辑器可能会导致几秒钟的延迟。为了解决这个问题，我们转向了 CPU 优化模型，在不影响准确性的情况下，延迟大幅减少了 80% 以上。此外，我们正在评估更先进的模型，包括利用 GPU 技术的模型，以进一步提高 PII 编辑器的处理速度和整体效率。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:4,&#34;hash&#34;:&#34;087365ba-af49-4fff-916f-f0fbfa2998ee&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-quality&#34;&gt;&lt;strong&gt;质量&lt;/strong&gt;&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0e2e3c69-1d74-47d1-8e92-34988bac8b83&#34;,&#34;dropCap&#34;:false}&#34;&gt;PII 编辑器可能会无意中影响结果的质量。其在保护用户信息的同时对敏感数据进行匿名化的过程有时会剥夺关键的上下文，从而影响法学硕士的响应质量。例如，像“谁是乔治·华盛顿？”这样的查询转换为“Who is ANONYMIZED_NAME_0”以进行 LLM 处理。由于特定上下文信息的丢失，这种匿名化可能会阻碍法学硕士生成相关回复的能力。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9fdb0ac0-6a2a-44cb-a471-14462165dd91&#34;,&#34;dropCap&#34;:false}&#34;&gt;此外， PII 编辑器的机制在 LLM 缓存和检索增强生成 (RAG) 等场景中提出了独特的挑战。 LLM 缓存存储常见问题的答案以方便快速检索，但它面临着两难境地。匿名查询，例如“乔治·华盛顿是谁？”和“谁是亚伯拉罕·林肯？”在编辑后变得难以区分，导致缓存的响应可能不准确。同样，RAG 依靠获取相关文档来帮助法学硕士生成回复，它也面临着匿名化带来的不一致问题。例如，嵌入一篇有关美国独立战争的历史文章可能会在离线和在线上下文中涉及同一实体的不同匿名占位符，从而导致检索到不正确的文档。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2f0c4eea-0d91-4bbb-80ba-cbf460e06a0b&#34;,&#34;dropCap&#34;:false}&#34;&gt;这些挑战突显了一个根本问题：难以将匿名占位符链接回其原始实体，从而导致缓存结果或文档检索出现错误。虽然维护将原始实体映射到匿名占位符的全局表是不切实际的，但我们正在探索解决方案：一种方法鼓励客户使用 Uber 托管的LLM，这不需要 PII 编辑。同时，我们正在评估第三方供应商的安全保证，以考虑完全放弃 PII 编辑器的可能性，努力平衡隐私问题和运营效率。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;e1b6de5b-fe78-4d7c-bde0-04951bf725f2&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-other-challenges&#34;&gt;&lt;strong&gt;其他挑战&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d0f6465d-4a72-4f92-988d-4f7b68bee7ac&#34;,&#34;dropCap&#34;:false}&#34;&gt;超越之前除了 PII 编辑方面的困难之外，GenAI 网关还遇到了更多不同的挑战。近几个月来，大型语言模型 (LLM) 不断发展的格局促使我们动态调整我们的优先事项。例如，最近推出的 GPT-4V 通过改变请求集成而改变了一切rface 来容纳图像 URL 和 base64 编码的图像。幸运的是，响应迅速的开源社区迅速提出了解决方案来无缝适应这一变化。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;87fc3c4d-83a0-4ebf-9418-3b40fbf0a158&#34;,&#34;dropCap&#34;:false}&#34;&gt;鉴于 GenAI Gateway的核心功能围绕着向相关LLM供应商转发请求，其可用性与这些供应商的运营状态密切相关。如果发生供应商中断，与用户的有效沟通对于防止将问题错误地归因于网关本身至关重要。为了增强弹性，我们正在积极探索将 Uber 托管的LLM 纳入第三方供应商遇到停机时作为后备选项的可能性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e65b175f-48ce-4c6d-a292-db0227c90f3a&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4c72bb58-460c-4754-be32-ed051f246615&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-use-case-summarization-of-chats-for-agents-resolution&#34;&gt;&lt;strong&gt;用例：用于代理解析的聊天摘要&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b1ed6027-0b3d-4a7e-a​​8ab-4e59ae013f4b&#34;,&#34;dropCap&#34;:false}&#34;&gt;在我们的连续为了提高客户支持效率，Uber 利用大型语言模型 (LLM) 来简化客户支持代理的流程。主要重点是快速总结用户问题并建议潜在的解决措施，从而显着减少解决用户查询所需的时间。这不仅可以加快查询解决速度，还有助于整体改善用户体验。我们在内部利用法学硕士来进行以下工作&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:true,&#34;hash&#34;:&#34;0aee051c-93e7-4bbf-a71b-afb5c6d71f45&#34;,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;增强聊天机器人到客服人员的转换，为客服人员提供先前交互的简明摘要，以便更好地理解、更快地解决问题并解决关键挑战&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;为客服人员提供重要的背景信息和用户情绪，从而提供富有同理心且上下文准确的支持。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;实现联系人交互自动汇总，减少 80% 的手动汇总时间，提高运营效率。&lt;/li &gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;d7dfbca3-3c84-4ee3-92e3-780a9377fe18&#34;}&#34; class=&#34;wp -块标题“id=”h-影响&#34;&gt;&lt;strong&gt;影响&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c2da11cc-0927-4036-9dd7-500c408aa19a&#34;,&#34;dropCap&#34;:false}&#34;&gt;执行事实证明，法学硕士非常有益，97% 的生成摘要对于解决客户问题很有用。代理商表示，在阅读和记录文档方面节省了大量时间，从而提高了他们的整体生产力。代理能够比以前更快 6 秒恢复用户。我们每周利用法学硕士生成约 2000 万份摘要，并计划在未来将其扩展到更多地区和联系人类型。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d87d11d4-8c61-4223-975e-5b8ee9b78de2&#34;,&#34;dropCap&#34;:false}&#34;&gt;本质上，我们在 Uber 对法学硕士的利用超越了单纯的自动化——这是一项战略增强，重点是让我们的客户支持代理能够提供更快、更准确和更有同理心的解决方案。  所有这些都是为了向我们的用户提供无与伦比的体验。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;6f3fff59-4eda-4a9e-ba4e-e4d86dd0f080&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-how-gen-ai-gateway-is-used&#34;&gt;&lt;strong&gt;Gen AI网关如何使用&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3c251dc5-8f76-4f63-8cf6-46b83d4617df&#34;,&#34;dropCap&#34;:false}&#34;&gt;在 Uber 客户内Obsession 组织中，CO 推理网关最初用于将各种基于 ML 任务的 API 合约在内部公开给其他服务，从而抽象出不同的 ML 模型主机。对于摘要，我们扩展了此服务，包括用于文本、聊天和嵌入生成的新生成 ML 任务。此扩展支持与 Open AI 和 Google Vertex AI 模型的连接，从而提高灵活性和适应性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6e9637f3-267e-49bc-8cbf-b2dbf1ce70e8&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;3a0a4eff-508a-450e-b7b6-d30abd888d73&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXe5_r9WXoElXeydaicgO-hAtPt5YOQr4MqZ3CKPRaVxgFWIkw1EnRrd9QKeEsmG-9SsHwbhGG-epfVUKYq6AGCli5guSvFn4 45zPBDxDkh6PLqYFzzwu4WE7yWr7f_4E7YOqYQ1z7o_WP3iO5Qyza27D7rI?key=5egmPsUgOM1kuqOeYjTalA&#34; alt=&#34; “referrerpolicy =“no-referrer”&gt; &lt;/figure&gt; &lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;631b8d21-ba2a-4a6d-acc3-604bff6e1da1&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p 数据-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7e654ae5-5120-4e71-86fd-67fa18b95853&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;a href=&#34;https:// /drive.google.com/file/d/1mIgcSDvcFjzWO7H774u1ay20mF00sZEG/view?usp=drive_link&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4c59483c-8bd4-4b50-9c4b-5f82c528ad7d&#34;,&#34;dropCap&#34;:false}&#34;&gt;遇到诸如此类的挑战由于 PII 编辑、成本归因以及 Uber 集中服务与任何外部语言模型 (LLM) 连接的必要性，我们做出了利用 Gen AI 网关而不是直接调用外部模型的战略决策。这一决定是基于对全面解决方案的需求，该解决方案不仅能有效应对挑战，还能确保稳健和安全的集成。通过这样做，我们可以应对复杂性并优化人工智能解决方案的效用，这与 Uber 对创新和卓越客户支持的承诺保持一致。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;85d40cc1-5690-4a43-9b85-98191edf7e4f&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;d8ba6985-0b79-4558-a5b8-078e0e381f63&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-us.googleusercontent.com/docsz/AD_4nXcfFXsQPWf1S4ppo-vik3bFc867WLviUl182cHDZ-K7JZNaut0dfHXU5kKhFpVHJX9KERz4NVNwQS9un8lHVCwt74u D4MJ0sINAUrcyQwifX3COr4k3ZiJwdE338BFLKxROhBOSxwNKAqntSNFWi6-zNTxu?key=5egmPsUgOM1kuqOeYjTalA&#34; alt=&#34; “referrerpolicy =“no-referrer”&gt; &lt;/figure&gt; &lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ee250d3c-364e-42b1-bde5-b5d81c1e3f20&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;c5f5f108-19fa-4345-b0bd-99044df1648e&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-prompt&#34;&gt;&lt;strong&gt;提示&lt;/strong&gt;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ec567b34-0b9f-453d-8864-911845c39aa5&#34;,&#34;dropCap&#34;:false}&#34;&gt;以下是示例 提示汇总联系票据。我们在提示上下文中提供了一些示例，以进行小样本学习。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;blockquote data-wp-block-name=&#34;core/quote&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;01b04911-552d-4e9d-b786-8ecb2b12562c&#34;,&#34;value&#34;:&#34;&#34;}&#34; class=&#34; wp-block-quote is-layout-flow wp-block-quote-is-layout-flow&#34;&gt;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;以下是名为 BOT 的 Uber 客户支持机器人（可能是客户支持代理）之间的对话称为 AGENT 和 Uber Eats 优食定制呃叫USER。提供对话的详细分类。识别用户的所有问题或意图以及相关情绪。针对每个已识别的问题，提取用户话语和代理响应中最相关的部分。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;输入：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;dropCap&#34;:false}&#34;&gt;{{conversation_log}} // 对话日志是客户、聊天机器人和聊天机器人之间的实际消息历史记录代理。&lt;/p&gt;&#xA;&lt;/块引用&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f23b1fbe-c359-4c05-95ae-1a952fe7008f&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;316cdf49-3163-4cda-b13f-79fc13d055b3&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-learnings&#34;&gt;&lt;strong&gt;学习内容&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;885e8612-3fc7-4e44-b412-044519b57ea9&#34;,&#34;dropCap&#34;:false}&#34;&gt;反映在我们的法学硕士之旅中，我们学到了一个重要的教训：面对快速发展的法学硕士环境，迫切需要适应能力。随着我们深入研究高级法学硕士应用领域，巧妙管理不断变化的技术、用户隐私和效率之间相互作用的重要性变得越来越明显。形势在不断变化，我们致力于持续改进比以往任何时候都更加重要。灵活而富有远见地驾驭这些动态对于我们的使命至关重要，确保我们不仅跟上技术进步的步伐，而且恪守我们对用户隐私和系统效率的承诺。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;daca159f-1260-4b21-b095-be3639656066&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;314ead02-2022-4b7f-9bb3-a3dc6eb8e93c&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-future-works&#34;&gt;&lt;strong&gt;未来作品&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1ac48011-3d5a-40e7-8c50-8c1736fb0fbe&#34;,&#34;dropCap&#34;:false}&#34;&gt;展望未来，我们对 GenAI 网关的愿景是提升和完善其功能，以更好地为我们的用户服务。重点是加强新模型的入职流程。无论这些模型是根据特定需求定制的、经过微调的模型，还是来自充满活力的开源社区的模型，我们的目标都是使它们的集成尽可能流畅且用户友好。此外，我们热衷于通过高级功能来增强网关，以解决使用大型语言模型的动态挑战。这包括实施智能 LLM 缓存机制、开发强大的后备逻辑以及引入复杂的幻觉检测。安全和政策护栏也已列入我们的议程，以确保我们的平台保持安全并符合不断发展的标准。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c896442c-5abb-420b-85b5-898882f48a1e&#34;,&#34;dropCap&#34;:false}&#34;&gt;在我们的旅程中为了扩展网关的功能，我们还认识到挖掘开源生态系统巨大潜力的重要性。为此，我们正在积极努力与图书馆整合。这不仅会增强我们系统的功能广度，而且会使其更加通用，使我们的用户能够探索和利用更广泛的解决方案。这些未来的努力强调了我们对不断发展 GenAI Gateway 的承诺，确保它仍然是一个利用法学硕士力量的尖端、多功能和安全的平台。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;62803538-f24d-48ec-971e-28ba28c745cc&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;c312f523-db66-494c-9407-311cee6c8b60&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-conclusion&#34;&gt;&lt;strong&gt;结论&lt;/strong&gt;&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ad1f31f3-01c4-475c-b839-51e7dd25b3cf&#34;,&#34;dropCap&#34;:false}&#34;&gt;LLM 已成为一种变革的力量。然而，法学硕士在 Uber 的整合并非没有挑战。不一致的集成策略和缺乏标准化方法导致监控成本和供应商使用情况效率低下和困难。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8def3e9e-3285-449e-9694-e7fcfe114573&#34;,&#34;dropCap&#34;:false}&#34;&gt;解决这些问题Uber 的 Michelangelo 团队开发了 GenAI Gateway，这是一个统一平台，方便从多个提供商（包括 OpenAI 和 Vertex AI）以及 Uber 的内部模型访问 LLM。该平台简化了集成流程，确保遵守隐私和安全标准以及高效的使用管理。 GenAI Gateway 的设计反映了 OpenAI API，确保与现有工具的兼容性，并与不断发展的开源社区保持同步。它还具有 PII 编辑器，增强了安全性。 GenAI Gateway 的这一战略发展，加上其对运营效率的关注以及与 Uber 更广泛的技术框架的一致性，体现了 Uber 对创新的承诺，同时解决与法学硕士合作的动态挑战。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d24118d6-acbf-4d50-959b-5337150cda8c&#34;,&#34;level&#34;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-acknowledgments&#34;&gt;&lt;strong&gt;致谢&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ae51ea31-0c43-4ce0-9617-47822269b6e2&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们不能在没有各个工程团队、Uber AI 和 Uber Customer Obsession 团队的帮助下，我们完成了本文中概述的技术工作。我们还要感谢与我们合作采用 Gen AI Gateway 的各个产品团队。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;afc6017c-684e-44ac-abcc-678a2031b044&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;封面照片归属：“&lt;strong&gt;人工智能，AI&lt;/strong&gt;”图像被&lt;a href=&#34;https://creativecommons. org/licenses/by/2.0/&#34;&gt;CC BY 2.0&lt;/a&gt; 许可证，并记入 &lt;a href=&#34;https://www.flickr.com/photos/152824664@N07&#34;&gt;mikemacmarketing&lt;/a&gt;。未对图像进行任何更改（&lt;a href=&#34;https://openverse.org/image/49410795-b3ba-421b-918b-7f2cd9178f19?q=artificial%20intelligence&#34;&gt;来源&lt;/a&gt;）。&lt;/a&gt; p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;f526d83b-eb4f-49c3-9e7f-661ae12bcd4b&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;Vertex AI™、PaLM™、Google Dialogflow™ 和 Go™ 是 Google LLC 的商标，本博文未获得 Google 的认可或以任何方式与 Google 存在关联。 &lt;/p&gt;</description>
      <pubDate>Thu, 11 Jul 2024 06:58:53 +0000</pubDate>
    </item>
    <item>
      <title>【Shifting E2E Testing Left at Uber】Uber 的 E2E 测试左移</title>
      <link>https://www.uber.com/blog/shifting-e2e-testing-left/</link>
      <description>【&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6085e52b-6b27-403c-9ed7-1351d6b7a62b&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;A few years ago, Uber primarily relied on incremental rollout and production probing/alerts to catch regressions. While this approach is sound, it became very operationally expensive and we also experienced many leakages. Detecting issues late requires developers to bisect the exact bad change and then go back through the whole process again.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9168d5ea-ebc8-414d-8cc2-86971c8e5a4a&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;31898c14-1051-4014-8dd3-62376fe4f619&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeTv9oVp0FRXBYsN2h_eWOk892eC4uTeBBIjL4V1G6pKG0cT1NuAR866VuKBy1sG8cKa19zp7K18yp1FTUrWHt8Wp7LIbEd_XB5YWrbJdRIwTtUG03cLXcfVARtGveGAhpkYyfhwu-j2_VRBCrADHWuJsrv?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 1: Catching issues as early as possible (i.e. shifting detection left)&amp;nbsp; reduces the above operational burden.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1955f830-2171-4e24-b02a-6c6f18790ec1&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;01abcd0b-a489-4cf1-a9db-fbcfb868bed3&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Many of our outage postmortems indicated little to no testing beyond a few basic unit tests, which were often so dependent on mocks that it was difficult to understand how much protection they actually offered.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;88c8df19-850c-4dcc-9e8b-d1e5112653f7&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Uber is well-known for having fully embraced microservice-based architecture. Our core business logic is encapsulated across large groups of microservices such that it is difficult to validate functionality within a single service without over-mocking. Because of our architecture, the only sane way to test has been to perform E2E testing. At the same time, our internal NPS surveys consistently highlighted that E2E testing was the hardest part of the job for developers – it’s no surprise it was often skipped.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;62f880a5-433b-424e-8669-633d0264676e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;A well-known testing blog from 2015, &lt;a href=&#34;https://testing.googleblog.com/2015/04/just-say-no-to-more-end-to-end-tests.html&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Just Say No To More End-To-End Tests&lt;/a&gt;, calls out E2E tests as difficult to maintain, expensive to write, flaky, slow, and difficult to debug.&amp;nbsp; Uber’s journey into testing has run into all of the above problems and we’ve had to come up with creative ways to solve them.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;40045377-241c-446d-90b1-3d1eb720e570&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In this blog, we describe how we built a system that gates every code and configuration change to our core backend systems (1,000+ services). We have several thousand E2E tests that have an average pass rate of 90%+ per attempt. Imagine each of these tests going through a real E2E user flow, like going through an Uber Eats group order. We do all this fast enough to run on every diff before it gets landed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;eed67a41-b10b-49c6-bb6f-c6d46f69dbbe&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c919c3a3-1716-4781-bd04-de8f7cd766d2&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-prior-approaches&#34;&gt;Prior Approaches&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;dbf51c90-1961-4ee2-b9e1-b126c0821ebf&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-docker-compose&#34;&gt;Docker Compose&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;45728472-5529-407d-8b2e-c964387b79b8&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In the past, some teams at Uber have tried to start up all relevant services and datastores locally to run a suite of integration tests against the stack. At a certain point of microservice complexity, this becomes prohibitively expensive with the number of containers that need to be started.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d753f29b-5a9a-4f61-af4d-c3a635a21223&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;7f36bb85-8107-4187-ba3d-9881c08be1ab&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXf9KSVI-vAb7Gf0KLqdb_7B3DOTMhE89WWH9o7VglNMENwOFTeI0q3X50RL5fJDbtuZpuCRf2GYs_ym2lUOOBvloQUZo1-9nhMpUegU6_8Cnpj6i4AKe4ZmKeLrQdri48LnGDYd62qPOliQQIjvMvAjhsc?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 2: How docker compose works early, but fails to scale as microservices proliferate.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;095de1ea-8523-42c8-bd69-17a43cc62c20&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;257d72e1-b4b4-43ac-bdad-2fadc1ad56cd&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-deploy-to-staging&#34;&gt;Deploy to staging&lt;/h3&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;91e2a781-2094-41b5-b7b7-344932e8c832&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXey0P730aPMXRawiFFbszQw5jL7kZ9myEItY4SRJimiNv3WY8DL3_urIYM1dEZnRFBLuK5r9VzY30i4T51tWmsId3ABiIU8monKCrRw8MbUEGn1M3lI1NasnuDEgT4Gdu3xKPr8qNFvEZF9cdTxw6wdryZR?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 3: Once the limits of running docker-compose on a single machine was reached, some folks shifted towards deploying their code to a shared staging or testing environment to run their tests.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;61e10f68-9789-4954-8d02-911eb404ca47&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3d66f0ef-cbe8-4966-ab6a-b5a187e49c24&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;At Uber, shared staging became too difficult to keep reliably usable for testing, because a single developer deploying a bad change could break the whole thing for everyone else. This approach only ever worked in small domains where developers could manually coordinate rollouts between a few teams.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7e9ff16d-e70a-41b1-921c-b3ad9689e72e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Could E2E tests really work well at Uber scale? This is the story of how we made it happen.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5f76510b-cd87-4aca-bd95-134a7ebc8e56&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b5336975-0b78-4197-a93c-ba032da6c5f9&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-shift-left-with-bits-uber-s-current-testing-strategy&#34;&gt;Shift Left with BITS – Uber’s Current Testing Strategy&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;74f1aaac-3b00-414b-8a24-dc94f94a4a68&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-we-use-bits-to-slice-production&#34;&gt;We use BITS to slice production&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f85e50c0-add8-4eec-91ac-026f1b11f2a3&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In order to shift testing left, we need to enable the ability to test changes without first deploying to production. To solve this, we launched a company-wide initiative called BITS (Backend Integration Testing Strategy) that enables on-demand deployment and routing to test sandboxes. Practically, this means individual commits can be tested in parallel before landing, without interfering with anyone else!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;38e2cac0-fec1-411c-972e-524fe8cfa26e&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-data-isolation&#34;&gt;Data Isolation&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;636b9758-f5f7-4c0a-9855-cfc3daa13b96&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To shift testing left &lt;em&gt;safely&lt;/em&gt;, we need to provide isolation between production and testing traffic. At Uber, production services are expected to receive both testing and production traffic.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:4,&amp;quot;hash&amp;quot;:&amp;quot;d8af4eca-2aa0-4e3c-8371-48b346b688d1&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-user-context&#34;&gt;User Context&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1b667c80-6e13-4639-8743-f7901d13fe13&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Most APIs are entity-scoped, meaning that access is scoped to a particular entity (i.e. user account). Using test users automatically limits the scope of side effects to that account. A small handful of cross-entity interactions (think algorithmic matching or Eats shopping) use the below strategies for isolation.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;730c828e-8874-4896-9217-c1e8b655e08c&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2f32e9a4-8b9f-4684-83d2-de2fb031312e&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXc4_IYf6vuKtd4HJwwIxr8CAF6ellhcG0l31y_U9LKAdae4L4n2w5hPQfYhkqElK14Ju0vFtJntOKaanMbJB_XjfdnWVz4HLprYHpJrl6eYpg7rsFhzrTl-zA4MO3DcudDw040vMpjxguYMFuhDABH7QSPD?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 4: At Uber, requests are tagged with “tenancy” identifiers and services typically follow one of the above approaches:&lt;br&gt;&lt;br&gt;1. Storage clients route testing traffic to a logically separated datastore&lt;br&gt;2. Data is saved to the prod DB with a tenancy column; range-based queries also pass a tenancy identifier for filtering&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;85240543-a5fe-487e-ad75-d1baeb28c51f&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;947eb176-c516-4588-bf21-016143034d96&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-architecture&#34;&gt;Architecture&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1742e7ef-1881-4ef4-85ff-def75f742b3a&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;BITS’ architecture can be expressed as a series of workflows that orchestrate between different infrastructure components to define a meaningful experience for the developer. We use &lt;a href=&#34;https://cadenceworkflow.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Cadence&lt;/a&gt;, an open-source workflow engine developed at Uber that gives us a programming model for expressing retries, workflow state, and timers for teardown of resources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e3f21a3a-6e24-4807-ba8b-2f6e898246b5&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Our CI workflows run test/service selection algorithms, build and deploy test sandboxes, schedule tests, analyze results, and report results. Our provisioning workflows control container resource utilization and scheduling of workloads across different CI queues optimized for particular workload types (i.e., high network usage vs. CPU usage).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;70b048e8-710b-4dd1-9262-ed822314be77&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;id&amp;quot;:1094419,&amp;quot;sizeSlug&amp;quot;:&amp;quot;large&amp;quot;,&amp;quot;linkDestination&amp;quot;:&amp;quot;none&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;7cb274ca-6bb7-45e5-9924-a194a593796d&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter size-large&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1024&#34; height=&#34;720&#34; src=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.40.34pm-17240661668495-1024x720.jpeg&#34; alt=&#34;&#34; class=&#34;wp-image-1094419&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.40.34pm-17240661668495.jpeg 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.40.34pm-17240661668495.jpeg 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.40.34pm-17240661668495.jpeg 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1342,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.40.34pm-17240661668495.jpeg 1342w&#34; sizes=&#34;(max-width: 1024px) 100vw, 1024px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 5: BITS Architectural components.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a9cc7ac2-079d-4de3-86ec-5f85fdadddb9&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a241fd68-2882-48b9-80ed-b168a49f1469&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;em&gt;*We’ve previously discussed &lt;/em&gt;&lt;a href=&#34;https://www.uber.com/blog/simplifying-developer-testing-through-slate/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;&lt;em&gt;SLATE&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, a sister project built on top of BITS infrastructure primitives for performing manual sanity testing. Check out SLATE if you haven’t already seen it!&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1e472560-e7b7-4a93-b573-14ebe06ca05f&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;45a20d03-2b91-4660-89f6-8a006a9565ea&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-infrastructure-isolation&#34;&gt;Infrastructure Isolation&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a52d2c8f-2218-4c65-8134-5cb8bb142efb&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We’ve done a lot of work to properly isolate BITS’ development containers against production side effects.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4db52f9b-4cdd-4fcd-bb6a-a68ec292852b&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;BITS test sandboxes do not receive any production traffic.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;The native Apache Kafka™ protocol at Uber is abstracted by a &lt;a href=&#34;https://www.uber.com/blog/kafka-async-queuing-with-consumer-proxy/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;consumer proxy&lt;/a&gt; that remaps messages into a push-based forwarding model over gRPC. This design allows for intelligent routing of messages to BITS test sandboxes and also prevents test sandboxes from inadvertently polling production topics.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;a href=&#34;https://cadenceworkflow.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Cadence&lt;/a&gt; workflows created from testing are isolated to the test sandbox.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Metrics and logs are tagged with tenancy, so that they can be filtered out.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Test sandboxes do not participate in production leader election protocols.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;a href=&#34;https://spiffe.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;SPIFFE&lt;/a&gt;™ and SPIRE™ provide strongly attested identities to BITS sandboxes and test traffic.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Uber’s forwarding proxy inspects requests and performs a conditional P2P redirect if routing overrides are detected.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d75482da-6daa-4a78-b3b9-27b6514f7dc2&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;id&amp;quot;:1094418,&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;sizeSlug&amp;quot;:&amp;quot;large&amp;quot;,&amp;quot;linkDestination&amp;quot;:&amp;quot;none&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;a3cc17a4-9ea8-42d4-a520-5034134cb87d&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;wp-block-image size-large is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1024&#34; height=&#34;729&#34; src=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.47.57pm-17240659852082-1024x729.jpeg&#34; alt=&#34;&#34; class=&#34;wp-image-1094418&#34; style=&#34;width:700px;height:auto&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.47.57pm-17240659852082.jpeg 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.47.57pm-17240659852082.jpeg 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.47.57pm-17240659852082.jpeg 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1258,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.47.57pm-17240659852082.jpeg 1258w&#34; sizes=&#34;(max-width: 1024px) 100vw, 1024px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 6: We encode routing override headers as &lt;a href=&#34;https://opentelemetry.io/docs/specs/otel/context/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;context baggage&lt;/a&gt; and implement the &lt;a href=&#34;https://opentelemetry.io/docs/concepts/context-propagation/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;OpenTelemetry&lt;/a&gt;™ protocol in our RPC and client middlewares to propagate them across microservices.&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b8ca4c29-d6dd-4f93-8ef6-8f3f48f26d24&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;eac7d5e9-9cc4-48ef-a524-92fe689b635a&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-testing-configuration-changes&#34;&gt;Testing Configuration Changes&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5607b51e-ab18-4579-8164-061998c8405e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;At Uber, configuration changes cause up to 30% of incidents. BITS also provides test coverage for configuration rollouts made in our largest backend config management system, Flipr.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;71fcbd28-a8df-4632-864d-0b98f29cd335&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;04202126-8262-4131-bf35-869d370d2990&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXc8eADv7yAOroZe27lp8FL7lRsrYpGcufvPWiNY1Ngd0Z97e-k3AcIKnW99DV1WNYHjoH6lOuLpOIpHavY_2K1KvGIwdmWTd_jN26rnpg61rMmnMr2sOEjpVOAPCqS1NqlUiQOgc93i_2yPfGCDMckxwNXU?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 7: How config changes are tested with BITS.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6f9ef36b-cb20-4e21-b504-1bc941ae06b2&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;ac229b21-9810-43c0-b1ea-93d0630bb4d8&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-test-management-nbsp&#34;&gt;Test Management&amp;nbsp;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;54f488b1-9782-46f5-8c10-a90ce66eea56&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Developers are able to manage their test suites in real time from our UI to perform actions like downtiming tests, tracking their health, and checking endpoint coverage of their services.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c1cf1be1-e5dd-46c3-a18b-1ff369368079&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;589cc3da-291f-4f2f-9659-f064d23222fe&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXep59SQBQHIFwY8l7DaCzku06cppMCajPYDYvk67_J3JZ0Y2sHzGPW4wJiN0ZvqrwzgwA3URuLZBslaRkIUYgzGFXldx6Jp14DYqWShlyczF8IYSP0ByzfGHQlSjUA7SdWqYusv3Qcr1_i010_v5ClkV4I?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 8: How tests are managed in the BITS UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;22f4de0f-2a1e-4f24-a60d-2722096687a0&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;c21ee5a3-8ad1-4ca6-8c08-993f987b6d96&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-trace-indexing&#34;&gt;Trace Indexing&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ea3daa04-b072-4daa-80e9-774b5286cc53&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Every test execution is force-sampled with &lt;a href=&#34;https://www.jaegertracing.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Jaeger&lt;/a&gt;™. After each test execution, we capture the “trace” and use it to build several indexes:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;eb5dc6e6-f544-401f-9ab2-e89f124fc535&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;What services and endpoints were covered by the test flow&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;What tests cover each service and endpoint&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d939dc9a-a9e3-4326-a6bb-577448242f0c&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;0ec9e7c5-7c50-40fd-bb6c-0a2e0d809bcc&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfVKWaAL76HFxPz5oprlOyP1T5-rXCFJERDvjFQDqPfjhVYYoPsYOXVrxSJVZrxaDyEqwWjPsTUkMGb-lWQuyFHo0cigzNJloYc-dwQKFgu1RlztuhJA4F2iIqSV31vVMN15fxwtIFl9qHo5QNeqDbQ_Vjv?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 9: How a trace index is used to configure tests.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;28cfd1e2-8b3d-45ff-ac37-b733ec07b5d2&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;09a92871-e759-4070-b9ff-37a8ef03c088&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;These indexes are used to surface endpoint coverage metrics for teams, but more importantly, they allow us to intelligently determine which tests need to run when a given service is changed.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;11a58c65-8200-4caf-9ff8-366dee828e8a&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;a8f558cc-a774-48cd-b970-4c152ed9d619&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeqHr2XAFFuQ3xtGERXkDdsNaVjowZnO9BjafnAGDO7Ye53dp11xj4ok6Ni1WHv1XHnvifXYk-bhp8qcOWhI8rR7Cq5sPsQeajhAZqWsb_sHt6smqNT9EXsyOZCDXQqZ113GbCc_N9GWbz2iSstivSK0AA?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 10: How a trace index is used to show endpoint coverage.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2df04fc0-e46c-44ee-a69c-4a50d04543a9&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;bb910a2d-13ae-4401-af1e-3632f3a431a4&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Getting the infrastructure right has always been the easier part of the problem of testing. Machine problems can be solved, but developers at Uber are very sensitive to degradations in productivity and experience. The below captures some of our strategies&amp;nbsp; around mitigating problems with test maintenance, reliability, and signal.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f9d0c752-a97c-4afd-b8a3-1a6eb64a81df&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;28f47c91-671c-4474-9db4-d13353856079&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-challenges&#34;&gt;Challenges&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;07af9e2a-039a-4063-966e-f757426b67c5&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-challenge-1-maintenance-state-and-fixture-management-nbsp&#34;&gt;Challenge #1: Maintenance, state, and fixture management&amp;nbsp;&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;97f85784-03c9-4791-b3da-45c0dffdd8f9&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Let’s say that you want to test forward dispatch, where a driver gets a new pickup request before they complete their current dropoff. Or shared rides, where you may or may not pick up another rider on the way to your destination. How do you represent the state of the trip in each situation?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;294918d7-25bd-4072-bec4-29028d20b190&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Composable Testing Framework (CTF)&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5922cc80-300a-47b2-94d6-2b2df5350fb0&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;A foundational building block we built is CTF, a code-level DSL that provides a programming model for composing individual actions and flows and modeling their side effects on centralized trip state. CTF was originally developed to validate &lt;a href=&#34;https://www.uber.com/blog/fulfillment-platform-rearchitecture/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Uber’s Fulfillment Re-architecture&lt;/a&gt; and has since been adopted across the company.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;253174db-efda-4e7d-a349-13bb64756127&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;be6ab631-f10e-4daf-94ce-e0d37ee8a91f&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdCxyV6q-AlgPouJGGa5grzGxOJR2TBx_MUczQLU6RoG5lr9AweuxwMImv_by25m1s6fXM6pqSD4Xxm_U4E9-vhB0a8T6FjyTwfXqNya99x_hdHUVxTu56hXKg3gHUF3VtGtbORv2GvVhQ7jeLEHGamX4FH?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 11: How CTF is used to define a tripflow test.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;807e2e99-88c1-48a6-a522-a8ea0d719fcf&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;05e520af-cf3b-4a1e-82cc-a13cac421023&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Test Cataloging&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;bd0ad6dd-1dae-453c-ab86-3748e3d9de34&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Every test case is automatically registered in our datastore on land. Analytics on historical pass rate, common failure reasons, and ownership information are all available to developers who can track the stability of their tests over time. Common failures are aggregated so that they can be investigated more easily.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d8a8caa3-c661-4fe1-9483-05ab3dfe5824&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;fc021a1b-6414-40ab-9e1d-23b249a85eb2&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdqzV5DoH2UNdp0z7yolyM_H3NIVjDEXFzGpGISKpctoomNlvM6rwNgVmrbjEMZhYn-1pI-0jvjHWl_9IWrbL-TnfHrI3TkEFWynUiSRhwNkhrfZRl5HQxAATxwDmwP6M89jEJJYwBqiLVs9JG9qK9KmA7l?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 12: How we aggregate common test failures.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;973e9745-df2e-492a-bd2a-cc5826da1649&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;abd938a3-7f3a-4912-aa1a-f04f1ca65219&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-challenge-2-reliability-and-speed&#34;&gt;Challenge #2: Reliability and Speed&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2f47d5dc-1907-447d-b9ce-8b18bf48cbba&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Test authors get their tests to pass at ≥90% per attempt (against main). Our platform then boosts this to 99.9% with retries.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c2b59d3a-8ea9-4428-a89a-a5307c4fb1b1&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Our largest service has 300 tests. If tests individually pass at 90% per attempt, there is a 26% chance of one test failing due to non-determinism. In this case, we got the 300 tests to pass at 95% and with retries boost the signal up to ≥99%.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e616a807-9876-48a3-9406-37d5fc02f8c2&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The above is repeated thousands of times a day, debunking the myth that E2E tests can’t be stable.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cbf9a770-2935-463b-8a03-ade15c0e87b7&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Latency&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1761cb82-9bee-4206-a626-50dcbd5f185e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Our tests directly hit Uber APIs, so the majority of them run in sub-minute time.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5a0e0707-5909-44df-95fe-83a2b3f9ef2e&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8db1266d-4803-487f-8662-8f557e4438ee&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-challenge-3-debuggability-and-actionability&#34;&gt;Challenge #3: Debuggability and Actionability&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9f8bb2b4-779f-4fe5-8cf6-04952746528d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Placebo Executions&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5bb7e5c6-50d0-48eb-9e75-3d8b5ed51085&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;On every test run, we kick off a “placebo” parallel execution against a sandbox running the latest main branch version of code. The combination of these two executions allows us to move beyond a binary signal into a truth table:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1c4c613d-f308-4902-b0f4-e6b58d8da93c&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;cd7aa9f9-ee22-4ed2-adb7-887a08a6c83f&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeI1SvhkXKn8DHNvc_rNVSsat2ns5BX04A9H5Ggz06VtLoguKHpHbWqEtbTBJ5Qvyu3GPi6qkVxUNtJJDCukLvd3X6UDUTpydkj0kLNAmkEdmqXWxrSDy1zwbzWSeYMKrqgUMSJYDN3Jlahk7zA9NUktpxd?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 13: Truth table computed by BITS’ placebo runs.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;eb40926b-3afb-414a-a312-5ab5e1ec3427&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fb23a01e-85c2-47b2-b4e1-bd36a09639a2&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;With retries, non-determinism is rare. Either production or your test is broken, or you’ve introduced a real defect. No matter what, the developer has a clear action.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;abe7fe82-a720-437e-a194-62cab37175d5&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Automated quarantining of tests&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;21050909-c1ce-49f1-bb0f-b83b0607165d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The pass rate of our tests follows a bimodal distribution–the vast majority of tests are very reliable. A small subset of tests are broken (consistently fail). An even smaller subset of tests pass non-deterministically.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;05df214a-6ac4-4fc9-b3d5-c52b41786239&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3c0eeb53-6a36-448d-b07e-54ceb4ad2916&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;wp-block-image&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXf0s6_csZZtSnrdiWAq3aO9wIU_Q1ecMJPvmHY-5yH_xfQq678xs1p8-nqizbfBvjS6rosj3Ruww6baqBgyufCxstEJq5QX9CMkauWRkjAnlk2M4Jjv5e5gblMWvNml38kVCE-WTpxLwjoDw5yoL-BpqF90?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 14: High level distribution of E2E test states at Uber.&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b8dc3458-5898-4b79-b7ee-16706cd3e1fd&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;21cbd40a-ae48-488c-8e48-f6da0bb33475&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;A single broken test can block CI/CD, so we take special care to guard against this. Once a test drops below 90% placebo pass rate, it is automatically marked non-blocking and a ticket and alert are automatically filed against the owning team. Our core flow and compliance tests are explicitly opted out of this behavior, given their importance to the business.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;53676568-687c-4151-8175-e21105882995&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;730d0b1f-fb34-440b-a6b3-e0bda27156c1&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-conclusion&#34;&gt;Conclusion&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d82d918f-84d0-4974-b37a-12ed34d15fb7&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;At the end of the day, testing requires a balance between quality and velocity. When tuned well, developers spend less time rolling back deployments and mitigating incidents. Both velocity and quality go up. As part of our testing efforts, we tracked incidents per 1,000 diffs and we reduced this by &lt;strong&gt;71%&lt;/strong&gt; in 2023.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b487908f-9f60-4d9c-b8ac-57cc55c5b190&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Testing is a people problem as much as it is a technical problem&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;67896684-e419-4b34-b9be-15b11c4f7782&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Microservice architecture was born out of a desire to empower independent teams in a growing engineering organization. In a perfect world, we would have clean API abstractions everywhere to reinforce this model. Teams would be able to build AND test without impacting other teams. In reality, shipping most features at Uber requires heavy collaboration–E2E testing is a contract that enforces communication across independent teams, which inevitably breaks down because we’re human!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;06122135-5bd5-4889-9833-5f76325926e2&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Testing and Architecture are intrinsically linked&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;eb800d6c-a043-4017-8129-d26e6bd185e3&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;No one at Uber intentionally planned for heavy reliance on E2E testing, but our microservice-heavy architecture and historical architectural investments in multi-tenancy paved the way to make this the best possible solution.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2c5cf973-f770-4964-b538-4ab87fb8c16a&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Trying to fit our testing strategy to the classic “Testing Pyramid” didn’t work for us at Uber. Good testing requires you to think critically about what you’re actually trying to validate. You should try to test the smallest set of modules that serve some meaningful user functionality. At Uber, that just happens to often involve dozens of services.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1441c58b-cbb1-425b-ba33-38dfd85cfb95&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;604ebfbc-efed-4e1c-91e0-66412184cd29&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7fd539c4-6412-4b71-93d0-b6cad6753921&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We are thankful to a dozen+ teams who have collaborated with us to make testing a first-class citizen in Uber Infrastructure, as well as all the various leaders who believed in the BITS initiative.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;e144b5d0-eecf-4341-80c2-2015a1ab651c&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Apache®, Apache Kafka, Kafka, and the star logo are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries. No endorsement by The Apache Software Foundation is implied by the use of these marks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;4a0d50a5-12c9-4803-9801-7ec2d5f6325d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;SPIFFE, OpenTelemetry and its logo are registered trademarks of The Cloud Native Computing Foundation® in the United States and other countries. No endorsement by The Cloud Native Computing Foundation is implied by the use of these marks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;b160df72-8817-4354-a1d8-037b2e5cceb9&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Jaeger and its logo are registered trademarks of The Linux Foundation® in the United States and other countries. No endorsement by The Linux Foundation is implied by the use of these marks.&lt;/p&gt;】&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6085e52b-6b27-403c-9ed7-1351d6b7a62b&#34;,&#34;dropCap&#34;:false}&#34;&gt;几年以前，Uber 主要依靠增量部署和生产探测/警报来捕获回归。虽然这种方法是合理的，但它在运营上变得非常昂贵，而且我们也经历了许多泄漏。较晚检测到问题需要开发人员将确切的错误更改一分为二，然后再次返回整个过程。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9168d5ea-ebc8-414d-8cc2-86971c8e5a4a&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34; :&#34;31898c14-1051-4014-8dd3-62376fe4f619&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent. com/docsz/AD_4nXeTv9oVp0FRXBYsN2h_eWOk892eC4uTeBBIjL4V1G6pKG0cT1NuAR866VuKBy1sG8cka19zp7K18yp1FTURWHt8Wp7LIbEd_XB5YWrbJdRIwTtUG03clXcfVARtGveGAhpky yfhwu-j2_VRBCrADHWuJsrv?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 1:尽早发现问题（即向左移动检测）可减轻上述运营负担。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1955f830-2171-4e24-b02a-6c6f18790ec1&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;01abcd0b-a489-4cf1-a9db-fbcfb868bed3&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们的许多停机事后分析表明，除了一些基本的单元测试之外，几乎没有进行任何测试，这些单元测试通常非常依赖于模拟，以至于很难理解它们实际上提供了多少保护。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;88c8df19-850c-4dcc-9e8b-d1e5112653f7&#34;,&#34;dropCap&#34;:false}&#34;&gt;Uber 很好- 以完全接受基于微服务的架构而闻名。我们的核心业务逻辑被封装在大量的微服务中，因此很难在不过度模拟的情况下验证单个服务中的功能。由于我们的架构，唯一明智的测试方法是执行端到端测试。与此同时，我们的内部 NPS 调查始终强调，端到端测试是开发人员工作中最困难的部分 - 毫不奇怪，它经常被跳过。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;62f880a5-433b-424e-8669-633d0264676e&#34;,&#34;dropCap&#34;:false}&#34;&gt;一个很好的-知道2015 年的测试博客，&lt;a href=&#34;https://testing.googleblog.com/2015/04/just-say-no-to-more-end-to-end-tests.html&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Just Say No To More End-To-End Tests&lt;/a&gt;，指出 E2E 测试难以维护、编写成本高昂、不稳定、缓慢且难以调试。  Uber 的测试之旅遇到了上述所有问题，我们必须想出创造性的方法来解决这些问题。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;40045377-241c-446d-90b1-3d1eb720e570&#34;,&#34;dropCap&#34;:false}&#34;&gt;在此博客中中，我们描述了如何构建一个系统来控制我们核心后端系统（1,000 多个服务）的每个代码和配置更改。我们进行了数千次 E2E 测试，每次尝试的平均通过率为 90% 以上。想象一下每个测试都经过真实的 E2E 用户流程，就像经过 Uber Eats 优食团体订单一样。我们做这一切的速度足够快，可以在每个差异落地之前运行它。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;eed67a41-b10b-49c6-bb6f-c6d46f69dbbe&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c919c3a3-1716-4781-bd04-de8f7cd766d2&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-prior-approaches&#34;&gt;先前的方法&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;dbf51c90-1961-4ee2-b9e1-b126c0821ebf&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-docker-compose&#34;&gt;Docker 撰写&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;45728472-5529-407d-8b2e-c964387b79b8&#34;,&#34;dropCap&#34;:false}&#34;&gt;过去，Uber 的一些团队尝试在本地启动所有相关服务和数据存储，以针对堆栈运行一套集成测试。在微服务复杂性达到一定程度时，随着需要启动的容器数量的增加，这会变得异常昂贵。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d753f29b-5a9a-4f61-af4d-c3a635a21223&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;7f36bb85-8107-4187-ba3d-9881c08be1ab&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXf9KSVI-vAb7Gf0KLqdb_7B3DOTMhE89WWH9o7VglNMENwOFTeI0q3X50RL5fJDbtuZpuCRf2GYs_ym2lUOOBvloQUZo1- 9nhMpUegU6_8Cnpj6i4AKe4ZmKeLrQdri48LnGDYd62qPOliQQIjvMvAjhsc?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; 推荐策略=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 2：docker compose 如何早期工作，但无法扩展为微服务增殖。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;095de1ea-8523-42c8-bd69-17a43cc62c20&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;257d72e1-b4b4-43ac-bdad-2fadc1ad56cd&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-deploy-to-staging&#34;&gt;部署到暂存&lt;/h3&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;91e2a781-2094-41b5-b7b7-344932e8c832&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXey0P730aPMXRawiFFbszQw5jL7kZ9myEItY4SRJimiNv3WY8DL3_urIYM1dEZnRFBLuK5r9VzY30i4T51tWmsId3ABiIU8monK CrRw8MbUEGn1M3lI1NasnuDEgT4Gdu3xKPr8qNFvEZF9cdTxw6wdryZR?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34;referrerpolicy=&#34;no- referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 3：一旦达到在单台计算机上运行 docker-compose 的限制，一些人就会转向将代码部署到共享的暂存或测试环境来运行他们的代码。测试。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;61e10f68-9789-4954-8d02-911eb404ca47&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3d66f0ef-cbe8-4966-ab6a-b5a187e49c24&#34;,&#34;dropCap&#34;:false}&#34;&gt;在 Uber，共享暂存变得太难，无法可靠地用于测试，因为部署错误更改的单个开发人员可能会破坏其他所有人的整个事情。这种方法只适用于小领域，开发人员可以手动协调几个团队之间的部署。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7e9ff16d-e70a-41b1-921c-b3ad9689e72e&#34;,&#34;dropCap&#34;:false}&#34;&gt;是否可以进行E2E测试在 Uber 规模上真的运作良好吗？这是我们如何实现这一目标的故事。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5f76510b-cd87-4aca-bd95-134a7ebc8e56&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b5336975-0b78-4197-a93c-ba032da6c5f9&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-shift-left-with-bits-uber-s-current-testing-strategy&#34;&gt;用 BITS 左移 – Uber 当前的测试策略&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;74f1aaac-3b00-414b-8a24-dc94f94a4a68&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-we-use-bits-to-slice-product&#34;&gt;我们使用 BITS 来切片生产&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p 数据可湿性粉剂块-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f85e50c0-add8-4eec-91ac-026f1b11f2a3&#34;,&#34;dropCap&#34;:false}&#34;&gt;为了将测试左移，我们需要无需先部署到生产环境即可测试更改。为了解决这个问题，我们启动了一项名为 BITS（后端集成测试策略）的全公司计划，该计划支持按需部署和路由来测试沙箱。实际上，这意味着可以在落地之前并行测试各个提交，而不会干扰其他任何人！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;38e2cac0-fec1-411c-972e-524fe8cfa26e&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-data-isolation&#34;&gt;数据隔离&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;636b9758-f5f7-4c0a-9855-cfc3daa13b96&#34;,&#34;dropCap&#34;:false}&#34;&gt;切换测试安全地离开，我们需要在生产和测试流量之间提供隔离。在 Uber，生产服务预计会同时接收测试流量和生产流量。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:4,&#34;hash&#34;:&#34;d8af4eca-2aa0-4e3c-8371-48b346b688d1&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-user-context&#34;&gt;用户上下文&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1b667c80-6e13-4639-8743-f7901d13fe13&#34;,&#34;dropCap&#34;:false}&#34;&gt;大多数 API 都是实体范围，意味着访问范围仅限于特定实体（即用户帐户）。使用测试用户会自动限制该帐户的副作用范围。少数跨实体交互（例如算法匹配或 Eats 购物）使用以下策略进行隔离。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;730c828e-8874-4896-9217-c1e8b655e08c&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2f32e9a4-8b9f-4684-83d2-de2fb031312e&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34; wp-block-image&#34;&gt;&lt;img 解码=&#34;异步&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXc4_IYf6vuKtd4HJwwIxr8CAF6ellhcG0l31y_U9LKAdae4L4n2w5hPQfYhkqElK14Ju0vFtJntOKaanMbJB_XjfdnWVz 4HLprYHpJrl6eYpg7rsFhzrTl-zA4MO3DcudDw040vMpjxguYMFuhDABH7QSPD?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34;referrerpolicy=&#34;无引用者&#34; &gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 4：在 Uber，请求被标记为“租赁”标识符，服务通常遵循上述方法之一：&lt;br&gt;&lt;br&gt;1.存储客户端将测试流量路由到逻辑上独立的数据存储&lt;br&gt;2。数据通过租户列保存到产品数据库中；基于范围的查询还传递用于过滤的租赁标识符&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;85240543-a5fe-487e-ad75-d1baeb28c51f&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;947eb176-c516-4588-bf21-016143034d96&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-architecture&#34;&gt;架构&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1742e7ef-1881-4ef4-85ff-def75f742b3a&#34;,&#34;dropCap&#34;:false}&#34;&gt;BITS 的架构可以表示为一系列工作流程，这些工作流程在不同的基础架构组件之间进行编排，为开发人员定义有意义的体验。我们使用 &lt;a href=&#34;https://cadenceworkflow.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Cadence&lt;/a&gt;，这是 Uber 开发的开源工作流引擎，为我们提供了一个编程模型表达重试、工作流状态和资源拆卸计时器。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e3f21a3a-6e24-4807-ba8b-2f6e898246b5&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们的 CI 工作流程运行测试/服务选择算法、构建和部署测试沙箱、安排测试、分析结果并报告结果。我们的配置工作流程控制容器资源利用率以及针对特定工作负载类型（即高网络使用率与 CPU 使用率）优化的不同 CI 队列之间的工作负载调度。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;70b048e8-710b-4dd1-9262-ed822314be77&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;id&#34;:1094419,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;align&#34;:&#34;中心”，“散列”：“7cb274ca-6bb7-45e5-9924-a194a593796d”，“alt”：“”}“类=“aligncenter size-large”&gt;&lt;img加载=“惰性”解码=“异步”宽度= “1024”高度=“720”src=“https://blog.uber-cdn.com/cdn-cgi/image/width=2160，质量= 80，onerror=重定向，format=auto/wp-content/uploads /2024/08/screenshot-2024-02-26-at-4.40.34pm-17240661668495-1024x720.jpeg&#34; alt=&#34;&#34; class=&#34;wp-image-1094419&#34; srcset=&#34;https://blog.uber-cdn .com/cdn-cgi/image/width=1024，质量=80，onerror=重定向，格式=自动/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.40.34pm-17240661668495 .jpeg 1024w，https://blog.uber-cdn.com/cdn-cgi/image/width=300，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/08/screenshot- 2024-02-26-at-4.40.34pm-17240661668495.jpeg 300w，https://blog.uber-cdn.com/cdn-cgi/image/width=768，quality=80，onerror=redirect，format=auto /wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.40.34pm-17240661668495.jpeg 768w，https://blog.uber-cdn.com/cdn-cgi/image/width= 1342，质量= 80，onerror =重定向，格式=自动/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.40.34pm-17240661668495.jpeg 1342w&#34;sizes=&#34;(max-width: 1024px) 100vw, 1024px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 5：BITS 架构组件。&lt;/figcaption&gt;&lt;/图&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a9cc7ac2-079d-4de3-86ec-5f85fdadddb9&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a241fd68-2882-48b9-80ed-b168a49f1469&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;em&gt; *我们之前讨论过&lt;a href=&#34;https://www.uber.com/blog/simplifying-developer-testing-through-slate/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt; &lt;em&gt;SLATE&lt;/em&gt;&lt;/a&gt;&lt;em&gt;，一个建立在 BITS 基础设施原语之上的姐妹项目，用于执行手动健全性测试。如果您还没有看过 SLATE，请查看它！&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1e472560-e7b7-4a93-b573-14ebe06ca05f&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;45a20d03-2b91-4660-89f6-8a006a9565ea&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-infrastruct-isolation&#34;&gt;基础设施隔离&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a52d2c8f-2218-4c65-8134-5cb8bb142efb&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们已经做了很多工作来正确隔离 BITS 的开发容器以防止生产副作用。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4db52f9b-4cdd-4fcd-bb6a-a68ec292852b&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;BITS 测试沙箱不接收任何生产流量。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Uber 的原生 Apache Kafka™ 协议由 &lt;a href=&#34;https://www .uber.com/blog/kafka-async-queuing-with-consumer-proxy/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;消费者代理&lt;/a&gt;，通过 gRPC 将消息重新映射到基于推送的转发模型。这种设计允许将消息智能路由到 BITS 测试沙箱，还可以防止测试沙箱无意中轮询生产主题。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;a href=&#34;https://cadenceworkflow.io/&#34; target=&#34;_blank&#34; rel=&#34; noreferrer noopener&#34;&gt;Cadence&lt;/a&gt; 通过测试创建的工作流程被隔离到测试沙箱。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;指标和日志带有租户标记，以便可以将其过滤掉。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;测试沙箱不参与生产领导者选举协议。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;a href=&#34;https://spiffe.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;SPIFFE&lt;/a&gt;™ 和 SPIRE™ 为 BITS 沙箱和测试流量。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Uber 的转发代理会检查请求，并在检测到路由覆盖时执行有条件的 P2P 重定向。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d75482da-6daa-4a78-b3b9-27b6514f7dc2&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;id&#34;:1094418,&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;sizeSlug&#34;:&#34;大&#34;,&#34;linkDestination&#34;:&#34;无&#34;,&#34;哈希&#34;:&#34;a3cc17a4-9ea8-42d4-a520-5034134cb87d&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;wp-block-image size-large is-resized&#34; &gt;&lt;img 加载=“惰性”解码=“异步”宽度=“1024”高度=“729”src=“https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality= 80,onerror=重定向，格式=自动/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.47.57pm-17240659852082-1​​024x729.jpeg&#34; alt=&#34;&#34; class=&#34;wp-image -1094418&#34; style=&#34;width:700px;height:auto&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto /wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.47.57pm-17240659852082.jpeg 1024w，https://blog.uber-cdn.com/cdn-cgi/image/width= 300，质量= 80，onerror =重定向，格式=自动/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.47.57pm-17240659852082.jpeg 300w，https://blog.uber -cdn.com/cdn-cgi/image/width=768，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/08/screenshot-2024-02-26-at-4.47.57pm -17240659852082.jpeg 768w，https://blog.uber-cdn.com/cdn-cgi/image/width=1258，质量=80，onerror=重定向，格式=auto/wp-content/uploads/2024/08/截图-2024-02-26-at-4.47.57pm-17240659852082.jpeg 1258w“尺寸=”（最大宽度：1024px）100vw，1024px“referrerpolicy=“no-referrer”&gt; &lt;figcaption class =“wp-element- title&#34;&gt;图 6：我们将路由覆盖标头编码为 &lt;a href=&#34;https://opentelemetry.io/docs/specs/otel/context/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;上下文行李&lt;/ a&gt; 并在我们的 RPC 中实现 &lt;a href=&#34;https://opentelemetry.io/docs/concepts/context-propagation/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;OpenTelemetry&lt;/a&gt;™ 协议客户端中间件将它们传播到微服务。&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b8ca4c29-d6dd-4f93-8ef6-8f3f48f26d24&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;eac7d5e9-9cc4-48ef-a524-92fe689b635a&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-testing-configuration-changes&#34;&gt;测试配置更改&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5607b51e-ab18-4579-8164-061998c8405e&#34;,&#34;dropCap&#34;:false}&#34;&gt;在 Uber，配置更改导致高达 30% 的事件。 BITS 还为我们最大的后端配置管理系统 Flipr 中进行的配置部署提供测试覆盖率。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;71fcbd28-a8df-4632-864d-0b98f29cd335&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;04202126-8262-4131-bf35-869d370d2990&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXc8eADv7yAOroZe27lp8FL7lRsrYpGcufvPWiNY1Ngd0Z97e-k3AcIKnW99DV1WNYHjoH6lOuLpOIpHavY_2K1KvGIwdm WTd_jN26rnpg61rMmnMr2sOEjpVOAPCqS1NqlUiQOgc93i_2yPfGCDMckxwNXU?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34;referrerpolicy=&#34; no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 7：如何使用 BITS 测试配置更改。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6f9ef36b-cb20-4e21-b504-1bc941ae06b2&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;ac229b21-9810-43c0-b1ea-93d0630bb4d8&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-test-management-nbsp&#34;&gt;测试管理&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;54f488b1-9782-46f5-8c10-a90ce66eea56&#34;,&#34;dropCap&#34;:false}&#34;&gt;开发者能够从我们的 UI 实时管理他们的测试套件，以执行停机测试、跟踪其运行状况以及检查其服务的端点覆盖率等操作。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c1cf1be1-e5dd-46c3-a18b-1ff369368079&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;589cc3da-291f-4f2f-9659-f064d23222fe&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXep59SQBQHIFwY8l7DaCzku06cppMCajPYDYvk67_J3JZ0Y2sHzGPW4wJiN0ZvqrwzgwA3URuLZBslaRkIUYgzGFXldx6 Jp14DYqWShlyczF8IYSP0ByzfGHQlSjUA7SdWqYusv3Qcr1_i010_v5ClkV4I?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34;referrerpolicy=&#34;no- referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 8：如何在 BITS UI 中管理测试。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;22f4de0f-2a1e-4f24-a60d-2722096687a0&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;c21ee5a3-8ad1-4ca6-8c08-993f987b6d96&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-trace-indexing&#34;&gt;跟踪索引&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ea3daa04-b072-4daa-80e9-774b5286cc53&#34;,&#34;dropCap&#34;:false}&#34;&gt;每次测试执行使用 &lt;a href=&#34;https://www.jaegertracing.io/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Jaeger&lt;/a&gt;™ 进行强制采样。每次测试执行后，我们都会捕获“跟踪”并使用它来构建多个索引：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;eb5dc6e6-f544-401f-9ab2-e89f124fc535&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;测试流程涵盖了哪些服务和端点&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;哪些测试涵盖每个服务和端点&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d939dc9a-a9e3-4326-a6bb-577448242f0c&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34; :&#34;0ec9e7c5-7c50-40fd-bb6c-0a2e0d809bcc&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent. com/docsz/AD_4nXfVKWaAL76HFxPz5oprlOyP1T5-rXCFJERDvjFQDqPfjhVYYoPsYOXVrxSJVZrxaDyEqwWjPsTUkMGb-lWQuyFHo0cigzNJloYc-dwQKFgu1RlztuhJA4F2iIqSV31vVM N15fxwtIFl9qHo5QNeqDbQ_Vjv?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34; &gt;图 9：如何使用跟踪索引来配置测试。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;28cfd1e2-8b3d-45ff-ac37-b733ec07b5d2&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;09a92871-e759-4070-b9ff-37a8ef03c088&#34;,&#34;dropCap&#34;:false}&#34;&gt;这些索引是用于为团队提供端点覆盖率指标，但更重要的是，它们使我们能够智能地确定在给定服务发生更改时需要运行哪些测试。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;11a58c65-8200-4caf-9ff8-366dee828e8a&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;图e data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;a8f558cc-a774-48cd-b970-4c152ed9d619&#34;,&#34;alt&#34;: &#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeqHr2XAFFuQ3xtGERXkDdsNaVjowZnO9BjafnAGDO7Ye53dp11xj4ok6Ni1WHv1XHnvifXYk-bhp8qcOWhI8rR7Cq5sPsQeajh AZqWsb_sHt6smqNT9EXsyOZCDXQqZ113GbCc_N9GWbz2iSstivSK0AA?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34;referrerpolicy=&#34;否-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 10：如何使用跟踪索引来显示端点覆盖范围。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2df04fc0-e46c-44ee-a69c-4a50d04543a9&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;bb910a2d-13ae-4401-af1e-3632f3a431a4&#34;,&#34;dropCap&#34;:false}&#34;&gt;获取基础架构正确一直是测试问题中比较容易的部分。机器问题是可以解决的，但 Uber 的开发人员对生产力和体验的下降非常敏感。下面介绍了我们关于缓解测试维护、可靠性和信号问题的一些策略。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f9d0c752-a97c-4afd-b8a3-1a6eb64a81df&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;28f47c91-671c-4474-9db4-d13353856079&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-challenges&#34;&gt;挑战&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;07af9e2a-039a-4063-966e-f757426b67c​​5&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-challenge-1-maintenance-state-and-fixture-management-nbsp&#34;&gt;挑战 1：维护、状态和装置管理&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;97f85784-03c9-4791-b3da-45c0dffdd8f9&#34;,&#34;dropCap&#34;:false}&#34;&gt;比方说您想要测试前向调度，即司机在完成当前的下车之前收到新的接载请求。或者拼车，您可能会也可能不会在前往目的地的途中接载其他乘客。您如何描述每种情况下的旅行状态？&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;294918d7-25bd-4072-bec4-29028d20b190&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;可组合测试框架（CTF）&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5922cc80-300a-47b2-94d6-2b2df5350fb0&#34;,&#34;dropCap&#34;:false}&#34;&gt;基础建筑我们构建的块是 CTF，它是一种代码级 DSL，为组件提供了编程模型计算单独的动作和流程，并对其对集中式行程状态的副作用进行建模。 CTF 最初开发是为了验证&lt;a href=&#34;https://www.uber.com/blog/fulfillment-platform-rearchitecture/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Uber 的履行重新架构&lt;/a &gt; 此后已在整个公司采用。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;253174db-efda-4e7d-a349-13bb64756127&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34; :&#34;be6ab631-f10e-4daf-94ce-e0d37ee8a91f&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent. com/docsz/AD_4nXdCxyV6q-AlgPouJGGa5grzGxOJR2TBx_MUczQLU6RoG5lr9AweuxwMImv_by25m1s6fXM6pqSD4Xxm_U4E9-vhB0a8T6FjyTwfXqNya99x_hdHUVxTu56hXKg3gHUF3VtG tbORv2GvVhQ7jeLEHGamX4FH?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图11：如何使用 CTF 定义 Tripflow 测试。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;807e2e99-88c1-48a6-a522-a8ea0d719fcf&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;05e520af-cf3b-4a1e-82cc-a13cac421023&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;测试编目&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;bd0ad6dd-1dae-453c-ab86-3748e3d9de34&#34;,&#34;dropCap&#34;:false}&#34;&gt;每个测试用例自动注册在我们的陆地数据存储中。对历史通过率、常见失败原因和所有权信息的分析都可供开发人员使用，他们可以跟踪测试随着时间的推移的稳定性。常见的故障被汇总起来，以便更容易地进行调查。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d8a8caa3-c661-4fe1-9483-05ab3dfe5824&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;fc021a1b-6414-40ab-9e1d-23b249a85eb2&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdqzV5DoH2UNdp0z7yolyM_H3NIVjDEXFzGpGISKpctoomNlvM6rwNgVmrbjEMZhYn-1pI-0jvjHWl_9IWrbL-TnfHr I3TkEFWynUiSRhwNkhrfZRl5HQxAATxwDmwP6M89jEJJYwBqiLVs9JG9qK9KmA7l?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34; “referrerpolicy =“no-referrer”&gt; &lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 12：我们如何汇总常见的测试失败。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;973e9745-df2e-492a-bd2a-cc5826da1649&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;abd938a3-7f3a-4912-aa1a-f04f1ca65219&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-challenge-2-reliability-and-speed&#34;&gt;挑战#2：可靠性和速度&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2f47d5dc-1907-447d-b9ce-8b18bf48cbba&#34;,&#34;dropCap&#34;:false}&#34;&gt;测试作者得到他们的测试每次尝试的通过率≥90%（相对于主要测试）。然后，我们的平台通过重试将其提高到 99.9%。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c2b59d3a-8ea9-4428-a89a-a5307c4fb1b1&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们最大的服务有300个测试。如果每次尝试单独测试的通过率为 90%，则一次测试由于不确定性而失败的可能性为 26%。在本例中，我们的 300 次测试通过率达到 95%，并且通过重试将信号提升至 ≥99%。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e616a807-9876-48a3-9406-37d5fc02f8c2&#34;,&#34;dropCap&#34;:false}&#34;&gt;以上是每天重复数千次，揭穿端到端测试不稳定的神话。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cbf9a770-2935-463b-8a03-ade15c0e87b7&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;延迟&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1761cb82-9bee-4206-a626-50dcbd5f185e&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们直接测试命中 Uber API，因此大多数都在不到一分钟的时间内运行。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5a0e0707-5909-44df-95fe-83a2b3f9ef2e&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8db1266d-4803-487f-8662-8f557e4438ee&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-challenge-3-debuggability-and-actionability&#34;&gt;挑战#3：可调试性和可操作性&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9f8bb2b4-779f-4fe5-8cf6-04952746528d&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;安慰剂执行&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5bb7e5c6-50d0-48eb-9e75-3d8b5ed51085&#34;,&#34;dropCap&#34;:false}&#34;&gt;每次测试运行时，我们针对运行最新主分支版本代码的沙箱启动“安慰剂”并行执行。这两种执行的组合使我们能够超越二进制 si分析成真值表：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1c4c613d-f308-4902-b0f4-e6b58d8da93c&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;cd7aa9f9-ee22-4ed2-adb7-887a08a6c83f&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeI1SvhkXKn8DHNvc_rNVSsat2ns5BX04A9H5Ggz06VtLoguKHpHbWqEtbTBJ5Qvyu3GPi6qkVxUNtJJDCukLvd3X6UDUTpyd kj0kLNAmkEdmqXWxrSDy1zwbzWSeYMKrqgUMSJYDN3Jlahk7zA9NUktpxd?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34;referrerpolicy=&#34;no- referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 13：由 BITS 安慰剂运行计算得出的真值表。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;eb40926b-3afb-414a-a312-5ab5e1ec3427&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fb23a01e-85c2-47b2-b4e1-bd36a09639a2&#34;,&#34;dropCap&#34;:false}&#34;&gt;重试后，非决定论很少见。要么生产或你的测试被破坏，要么你引入了真正的缺陷。不管怎样，开发者有明确的行动。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;abe7fe82-a720-437e-a194-62cab37175d5&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;自动隔离测试&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;21050909-c1ce-49f1-bb0f-b83b0607165d&#34;,&#34;dropCap&#34;:false}&#34;&gt;通过率我们的测试遵循双峰分布——绝大多数测试都非常可靠。一小部分测试被破坏（始终失败）。甚至更小的测试子集不确定地通过。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;05df214a-6ac4-4fc9-b3d5-c52b41786239&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3c0eeb53-6a36-448d-b07e-54ceb4ad2916&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34; wp-block-image&#34;&gt;&lt;img 解码=&#34;异步&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXf0s6_csZZtSnrdiWAq3aO9wIU_Q1ecMJPvmHY-5yH_xfQq678xs1p8-nqizbfBvjS6rosj3Ruww6baqBgyufCxstEJq5QX9 CMkauWRkjAnlk2M4Jjv5e5gblMWvNml38kVCE-WTpxLwjoDw5yoL-BpqF90?key=eqKHcO_p5nGzUiHHU0afvg&#34; alt=&#34;&#34;referrerpolicy =&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 14：Uber E2E 测试状态的高级分布。&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b8dc3458-5898-4b79-b7ee-16706cd3e1fd&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;21cbd40a-ae48-488c-8e48-f6da0bb33475&#34;,&#34;dropCap&#34;:false}&#34;&gt;单破测试可以阻止 CI/CD，因此我们要特别注意防范这一点。一旦测试的安慰剂通过率低于 90%，它就会自动标记为非阻塞，并且会自动向所属团队提交罚单和警报。鉴于其对业务的重要性，我们的核心流程和合规性测试明确选择不参与这种行为。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;53676568-687c-4151-8175-e21105882995&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;730d0b1f-fb34-440b-a6b3-e0bda27156c1&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-conclusion&#34;&gt;结论&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d82d918f-84d0-4974-b37a-12ed34d15fb7&#34;,&#34;dropCap&#34;:false}&#34;&gt;最后如今，测试需要在质量和速度之间取得平衡。如果调整得当，开发人员可以花更少的时间回滚部署和缓解事件。速度和质量都提高了。作为测试工作的一部分，我们跟踪了每 1,000 个差异的事件，并在 2023 年将这一数字减少了&lt;strong&gt;71%&lt;/strong&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b487908f-9f60-4d9c-b8ac-57cc55c5b190&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;测试既是技术问题，也是人员问题&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;67896684-e419-4b34-b9be-15b11c4f7782&#34;,&#34;dropCap&#34;:false}&#34;&gt;微服务架构诞生于在不断发展的工程组织中赋予独立团队权力的愿望。在完美的世界中，我们将在各处拥有干净的 API 抽象来强化这个模型。团队将能够在不影响其他团队的情况下构建和测试。事实上，Uber 的大部分功能都需要大量协作——E2E 测试是一份强制独立团队之间进行沟通的合同，但由于我们是人，所以不可避免地会出现问题！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;06122135-5bd5-4889-9833-5f76325926e2&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;测试和架构有着内在的联系&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;eb800d6c-a043-4017-8129-d26e6bd185e3&#34;,&#34;dropCap&#34;:false}&#34;&gt;没有人Uber 有意计划严重依赖 E2E 测试，但我们的微服务架构和历史上对多租户架构的投资为使其成为最佳解决方案铺平了道路。可能的解决方案。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2c5cf973-f770-4964-b538-4ab87fb8c16a&#34;,&#34;dropCap&#34;:false}&#34;&gt;尝试适应我们的经典“测试金字塔”测试策略对 Uber 来说并不适用。好的测试需要您批判性地思考您实际想要验证的内容。您应该尝试测试提供一些有意义的用户功能的最小模块集。在 Uber，这往往涉及数十项服务。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1441c58b-cbb1-425b-ba33-38dfd85cfb95&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;604ebfbc-efed-4e1c-91e0-66412184cd29&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-acknowledgments&#34;&gt;致谢&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7fd539c4-6412-4b71-93d0-b6cad6753921&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们很感激感谢与我们合作使测试成为 Uber 基础设施中的一等公民的十几个团队，以及所有相信 BITS 计划的领导者。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;e144b5d0-eecf-4341-80c2-2015a1ab651c&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;Apache®、Apache Kafka、Kafka 和星形徽标是 Apache Software Foundation 在美国和/或其他国家/地区的注册商标或商标。使用这些标记并不暗示 Apache 软件基金会的认可。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;4a0d50a5-12c9-4803-9801-7ec2d5f6325d&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;SPIFFE、OpenTelemetry 及其徽标是云原生计算基金会® 在美国和其他国家/地区的注册商标。使用这些标记并不暗示云原生计算基金会的认可。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;b160df72-8817-4354-a1d8-037b2e5cceb9&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;Jaeger 及其徽标是 Linux Foundation® 在美国和其他国家/地区的注册商标。使用这些标记并不暗示 Linux 基金会的认可。&lt;/p&gt;</description>
      <pubDate>Thu, 22 Aug 2024 07:08:44 +0000</pubDate>
    </item>
    <item>
      <title>【Pinot for Low-Latency Offline Table Analytics】用于低延迟离线表分析的 Pinot</title>
      <link>https://www.uber.com/blog/pinot-for-low-latency/</link>
      <description>【&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;d0b0567b-b1c7-47fc-9253-50253f445151&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;67baaccd-d1f4-4f09-aad9-56aa29519724&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Apache Pinot&lt;sup&gt;™&lt;/sup&gt; is a real-time OLAP database capable of ingesting data from streams like Apache Kafka® and offline data sources like Apache Hive&lt;sup&gt;™&lt;/sup&gt;. At Uber, Pinot has proven to be really versatile in handling a wide spectrum of use cases: from real-time use cases with over one million writes per second, 100+ QPS, and &amp;lt;500 ms latency, to use cases which require low-latency analytics on offline data.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;408d1e5b-a20f-461c-9c24-f91b1490fa78&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Pinot tables fall in three broad categories: &lt;em&gt;real-time&lt;/em&gt;, &lt;em&gt;offline&lt;/em&gt; and &lt;em&gt;hybrid&lt;/em&gt;. Real-time tables support ingesting data from streams like Kafka, offline tables allow uploading pre-built “segments” via Pinot Controller’s HTTP APIs, and hybrid tables have both real-time and offline parts. Hybrid tables allow a single logical table (same name and schema) to ingest data from real-time streams as well as batch sources.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e0349cce-7cf0-466a-aa95-e4c31261fd6c&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;This article shares how Uber uses Pinot’s offline tables to serve 100+ low-latency analytics use cases spanning all lines of businesses.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8ff09083-e289-4bf5-aded-418f0f076d16&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ff679ca4-a030-4ad3-999c-71a7f30385c9&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-background&#34;&gt;Background&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;41b8c7d7-16ab-4a83-8f34-b260d4c9c096&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Uber has a huge data lake with more than 100 PB of data, and we have been using Presto®, Apache Spark&lt;sup&gt;™&lt;/sup&gt; and Apache Hive&lt;sup&gt;™&lt;/sup&gt; since almost a decade to serve many of our internal analytics use-cases. Presto® in particular is quite good at handling use-cases with low QPS (in the low 10s) and latencies on the order of a few seconds. However, there are a lot of use-cases where our users need sub-second p99 latency at a higher QPS. Most users also want dedicated resources for their use-cases to avoid noisy neighbors.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;549f949b-724a-4b2b-9005-d240f0921229&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Apache Pinot’s ability to run low-latency queries at high qps, with built-in multi-tenancy and offline data ingestion support, made it a natural fit for serving these use-cases. We have seen this play out over the last few years and Pinot now supports 100+ offline table analytics use-cases at Uber, with 500+ offline tables running in production.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;808bf1b3-157c-4475-bcf1-82a3ce3958aa&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;bc3284e3-c73c-4d24-aaa8-ed942906b141&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-pinot-offline-table-ingestion-primer&#34;&gt;Pinot Offline Table Ingestion Primer&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;31051218-84dd-40c8-9fcf-b6d16d136fe0&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Pinot stores data for its tables in “segments” for both real-time and offline tables. For real-time tables, the data from Kafka is consumed in memory in a “mutable segment” until a threshold is reached, after which the segment is committed to build an “immutable segment.” After the commit is done, a new mutable segment is created to continue consumption from the last read offset.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1fd27012-e249-4f11-8c60-03b7b214c4cf&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;For offline tables, Pinot allows uploading pre-built immutable segments via a POST /segments API in the Pinot controller. Building segments outside Pinot can be achieved using the “SegmentIndexCreationDriver” and “RecordReader” interfaces. The RecordReader interface allows feeding data to the segment driver from any arbitrary source. The segment driver also allows configuring the indexes and the encodings that should be used for each column.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f7c9317f-676d-4528-86e3-39ea565a079e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Once a segment for an Offline table is built outside Pinot, you can create a tarball out of it, upload it to a “deep-store” location, which Pinot servers can access, and hit the POST /segments API in the controller. The controller will create a new segment or update an existing segment’s metadata if one with the same name exists already. It will then send a message to the servers to download and load the segments, making them queryable. The controller also has an API to delete an existing segment, which means that for offline tables you can manage your Pinot data at segment level by adding a new segment, replacing an existing segment, or removing an existing segment.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;87ad7115-f832-4479-a5c6-3605d913ba8c&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;886e379e-0c35-4f0f-9bef-289977654dc9&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-self-serve-platform&#34;&gt;Self-Serve Platform&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b531ea11-81be-46d5-a151-608313cf9cef&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Given how segments are built and added/updated/removed, the question is how do we build a self-serve platform to allow 100+ users to create and manage new offline Pinot tables seamlessly?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0a94e7fd-05e8-4034-923d-e7e68f731254&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;For this, we leveraged our internal no-code workflow orchestration framework called &lt;a href=&#34;https://www.uber.com/blog/no-code-workflow-orchestrator/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;uWorc&lt;/a&gt;. We have a dedicated workflow type for ingesting data from Hive to Pinot. uWorc provides users a UI where they can set their table name, their Apache Spark&lt;sup&gt;TM&lt;/sup&gt; or Presto® query, the indexes they want, etc. It also allows users to configure when a job should run, and has rich support for tracking execution history, which can be used to see the status and logs for each run.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b804b111-dd1e-499c-8efd-ad1558eaedc2&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In each pipeline run, we run the SparkSQL or PrestoSQL specified by the users, and the output set of records is added/updated in the table.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;567db32f-15e6-4aec-bf2d-481aa0b690ab&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;cabba826-d3ac-4555-a839-2998c161834e&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXekLZSA8Z6kP2lVH54G9FoQtMaw7H_VzSCo3FKw93r1AEFaF6D1ar_hiQbz_4yPgEFC6domG92vEpj2jTTs_ACKQ9XE021IstDD2jVR4bxRCct0d34ENGFhAgvrISKlDmPdrfJcUp3pMXwMeN52_QS4Gac?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 1: Shows a sample Hive to RTA Pipeline which runs daily in uWorc.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;25cd8ec9-b58c-4010-8367-74012b392223&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3d7a2a35-7033-47ac-ae98-ab929ed9f368&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-data-modeling&#34;&gt;Data Modeling&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9f8f2e4f-0763-45e3-bb74-332eb6486a5e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We support two types of offline tables via uWorc:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f167793a-94e7-48e9-b13e-d45dccb667a8&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Overwrite Tables&lt;/strong&gt;: When a table is marked as Overwrite, each run of the pipeline will rewrite all of the data for the table completely.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Append Tables&lt;/strong&gt;: These tables have “day-based partitions” reminiscent of the datestr-based partitions found in Hive tables. A pipeline run for a given day will either add data for that day, or if it already exists, update data for that day.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4acee207-7936-4740-bbc9-8af117862eff&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Both of these table-types are required to have a “secondsSinceEpoch” column generated in the SparkSQL query, which is used by Pinot for retention purposes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cf5903da-7fba-4b4c-b486-611d00ea66d5&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d484e0cd-4502-46f9-bc11-6c0b34b20967&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-ingestion-job&#34;&gt;Ingestion Job&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;32853506-73ea-4ffc-b591-10820ebd2bf2&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Pinot only allows managing data for offline tables at segment level and the overwrite/append table semantics described above do not natively exist in Pinot. We use our ingestion job instead to implement them.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;44679f2d-87b4-48f8-864c-d4b71e1992dc&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Our ingestion job is implemented using &lt;a href=&#34;https://www.uber.com/blog/marmaray-hadoop-ingestion-open-source/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Marmaray&lt;/a&gt;: Uber’s open-source data dispersal framework built on top of Spark. Marmaray supports ingesting data from “any source to any sink.” It does this by providing 3 key SPIs: Source, Sink, and an intermediate data format based on Apache Avro™. We have implemented a PinotSink which can technically ingest from all internally available sources. However, at present we only use it to ingest data from a Hive source.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0827e05d-2ab1-4aac-ab89-49b92ef0a15f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;PinotSink receives a JavaRDD&amp;lt;AvroPayload&amp;gt; from the Source and performs the following steps at a high level:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e6cddac0-d0b6-4988-bf9e-ef6afbfd2564&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Repartition the RDD to meet the number of segments, table-type, and column partitioning requirements. Each Spark partition ultimately corresponds to exactly 1 segment.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Build and upload the segments&amp;nbsp; to deep-store.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Make a HTTP POST /segments call to the Pinot Controller to add/update the built segments.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Remove any segments from Pinot to meet the overwrite/append table-type semantics.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5b41c620-65f6-43c6-95e0-437333ea76be&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Below we explain how each of the table-types are supported.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;351910f9-6a42-44e1-813a-de99ac91b393&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;0231c7be-fcdf-4c08-a34d-013724d6a435&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-overwrite-tables&#34;&gt;Overwrite Tables&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;945803bb-3d00-4c88-a4a7-8e3e3f670398&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Overwrite tables may involve a RDD repartition to meet either the configured “Number of Segments” requirement and/or the “column partitioning” requirement. If we don’t have any column partitioning requirement, then we use a custom Spark &lt;a href=&#34;https://spark.apache.org/docs/latest/api/java/org/apache/spark/Partitioner.html&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Partitioner&lt;/a&gt; to randomly distribute records across each Spark partition.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;835fbf25-0a2e-4e70-a69e-81034ed882f7&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;When column partitioning is enabled, we use Murmur2 hash on the configured column to partition the tuples into the required “Number of Segments” partitions. The logic for that is quite simple:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a1c73a7c-335f-423a-bb63-49af48ba0921&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;3f8d49e6-3193-4020-b7aa-9ddfdcc2dfef&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdJkT4aUHGjWn2573GZ__05sG4XsthSrmCufulnqCuxOQGTNFbA5EdocKYACFE3LR1LMZjKD1-bHGwKC-NvSY8VdOXYKtsBMGDnclOjkYkZ3s85HfHXfOhYVI5IgTC0mq-5bI0wjBT7pYUcQUcrTtxbDps?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 2: Spark Partitioner for Overwrite Tables that have Column Partitioning Enabled.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f07a97f2-b36b-475d-bf00-a78997982d01&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a474987a-8470-48a7-8c71-ce9d925a815e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;For overwrite tables, we always generate segment names with the format “&amp;lt;table-name&amp;gt;_&amp;lt;spark-partition-number&amp;gt;”. If users decrease their “Number of Segments”, then the subsequent job will produce fewer segments than the one before it. In that case, the subsequent job will delete the extra segments in Pinot to prevent data-duplication.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;15eabb5c-72fa-4d0d-a1f6-85c3bf66ad22&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;370c7015-5283-49f3-8c96-bb8b1d82133d&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-append-tables&#34;&gt;Append Tables&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e3faaac1-d7a7-4ce1-808d-cc9ba2f335de&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;For append tables, the “Number of Segments” property is used to control the number of segments that should be created for each unique day in the data. For example, if you configure the number of segments as 4, and your ingestion job generates data for the dates “2022-01-01 to 2022-01-05”, then the total number of segments generated by the job would be 20, 4 for each day. This means that we always have to repartition the RDD for append tables since the Source RDD may be arbitrarily partitioned. Moreover, for each date we have to ensure that we generate a preconfigured number of segments.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;27dafb25-e1bf-443c-98f8-b1af1ee9fc8c&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The Spark Partitioner API allows us to map a tuple to an integer partition ID. So the question is how do we map a record to an integer partition ID to meet the append table semantics?&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a1cc01a1-3805-4a56-8dd6-2189d0fd9e85&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To achieve this, we use a Spark partitioner that looks like the following:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;483675f0-a89a-4338-9ccf-30cc3fc9c733&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;35550e03-3645-4cd1-8ed5-f9c1e88a1df8&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXf9mI7DccXdRz5dHm2Gs80y_sa-22Wf-9Xw28M0iOGbnbqQ1jGeiD8JW32PViZRJiUEv4OXaYp4IvGKaiKJ_zq4P_6vS9gri7XeyhE2sqOqz6unZQCZBVHOm3zw7zCQ-vyUIYFYhqqpD4kgz3yP_nsXmF3L?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 3: Spark Partitioner for append tables. Segments per day constraint is met by assigning records to each segment in a round-robin fashion.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;84cfb1b3-77a2-4f36-923a-26ea64310b1a&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d62eca43-a825-4053-b709-16a77fd2dd3d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The partitioning is done on the time-column configured by the user. Before calling the partitioner we figure out the “lookbackDays” value, which is the number of days between the smallest and the largest Unix Day in the RDD.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;64d1c65a-d0b5-4d25-a467-35aed683b623&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The algorithm can best be understood visually. Say we have lookbackDays = 5 and segmentsPerDay = 4. Saying that a row belongs to a given day and a given segment is equivalent to assigning it to a cell of a lookbackDays x segmentsPerDay grid. This is depicted below. The rows represent segment number, and the columns represent “unixDays % lookbackDays” (i.e., number of days since 1970-01-01).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cf49d824-1377-45b9-b7d9-1249e3edd76e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;As can be seen, we end up with a matrix with row and column numbers from [0, 5) and [0, 4) respectively. The example below assumes that 2022-01-03 has a unixDay that is divisible by 5.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2ae99075-06b6-4f4a-b862-2b9ada495d2c&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;1f2a883c-6dcd-4f23-9097-c36c13469b23&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeVPWie8G8MD2gy3Uw1lPrleKqFBs_O-AYEMBj3H1xnYs90_l8ku4K9i83tQ5-sAg4YD5ZawTk1SF4bfN0uthfj87xOXHVs3X4wpwL8MOjrTYTz8ytyYI366P5aokpCrVpz-BI9zCVgFdlw3X8MnALS4JVL?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 4:&lt;strong&gt; &lt;/strong&gt;Shows how you can map a record to a Spark partition. The record’s column is assigned based on the date the record represents. Each date has 4 segments, and the row can be assigned either based on the hash of a partitioning column or randomly.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;dd4c4df6-c2c5-498d-9a3b-2bd8b88b7028&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;22dfa589-62a5-4b98-9c42-ccdf68ad870d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Our pipeline has validations to prevent multiple time-column values for the same Unix day. This is because we want Pinot to apply retention for all segments of a day at the same time, so we can provide the append table semantics consistently. We use the segment name format: “&amp;lt;table-name&amp;gt;_&amp;lt;time-column-value&amp;gt;_&amp;lt;time-column-value&amp;gt;_&amp;lt;sequence-id&amp;gt;”. Here the sequence-ID is an integer between [0, Number of Segments). So for the example above, we will have segments with names as follows:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;16f35265-70f7-4879-b258-45e7a1c0a15e&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;0f3d82ca-8440-415a-b195-6fc67b92233f&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeb5jhwPRYmE9IX3Qoxf7v11vJEmBNHw6KMu3UkOzFY_Ar_HUZaw20QAa_OX1pu2fBeEMTlJinlZGCgWRUW5zklp52MWNgmSlHOKKmxCc1Z_h550TtBYOPY9r6fVTwoC78K_TKmYkfvgD6eeyOjNrY8DG8?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 5:&lt;strong&gt; &lt;/strong&gt;Sample Segment Names.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c011dcb2-4c66-42b7-b4a4-3b77038c1a7c&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d9661fb1-8ae5-420e-821c-28bb59254db6&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Finally, we allow users to increase or decrease their number of segments, and the ingestion job can handle deleting extra segments in the Pinot controller as required.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9d53f3e7-fb69-4dd6-9bc1-f57a10212c5a&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;9331640a-044b-4778-b41f-c8f0a1630b80&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-column-partitioning-for-append-tables&#34;&gt;Column Partitioning for Append Tables&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7520154d-658c-4e68-a614-332b1e9a0b14&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We support column partitioning for append tables as well. Everything remains the same as described for append tables above, except for the partitioner, which is augmented as follows:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;58b40468-5237-45ca-aab0-f9b073e2c7b6&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;4e2b5398-0bc1-4f19-9996-9fb024fbbbd7&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfIMz4IDuy_ln8XdVNtBGTKVWDTYXogUla5o1MgiI356p4A_3qrytlcqIZ0sUHaM76d2N8AuBidk558iUCfYn1gm1x8fWU4aIrxzEWcvc_xkOwEVWJ_JtAHb7s2C9RehCj9o0co160VYfzfBP-Zoas27B0e?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 6:&lt;strong&gt; &lt;/strong&gt;Spark Partitioner for Append Tables that also have Column Partitioning enabled on a column. Before calling the partitioner, we concatenate the time column and the partitioning column values with “\0” as the delimiter.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;618e0c7c-a8c2-4d19-8556-4aeaa5fa6c81&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4bf3d955-ac82-42fe-a3ac-24942688c09e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We again reduce it to a problem of assigning a tuple to a Grid. The column number is determined using the Unix Day value and the row number is determined using the hash value of the partitioning column. Additionally, we allow users to keep the number of segments parameter a multiple of the number of logical partitions of the partitioning column, to allow them to tune the segment size without changing the number of partitions.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4d28c455-e93f-44a9-a104-74ff0cec7467&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:4,&amp;quot;hash&amp;quot;:&amp;quot;5510831b-ab0b-4d7e-8fe2-d5c62bf13bc8&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-advantages-of-column-partitioning&#34;&gt;Advantages of Column Partitioning&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;92ccd092-991e-4d35-ab60-cca0eb4781bb&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In some cases, we have seen column partitioning reduce data volume by as much as 4x. This is because certain datasets really benefit from dictionary encoding when you partition by a key that colocates similar data in the same Spark partition. Column partitioning can also help with segment pruning, which can often really improve query performance.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;347a33d6-f256-4255-ba76-e0058de18c34&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;32cc0eb5-d186-4e84-917c-83937f4d3f49&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-spark-data-exports&#34;&gt;Spark Data Exports&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2d38f181-81eb-4bda-a914-f710d4c94eb2&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;So far we talked about ingesting data into Pinot from offline sources, but Pinot can also be used to export data from its real-time upsert tables to offline sinks like Hive/Object Stores.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c91612d9-73b9-4187-9b15-0917d90768d9&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;PInot is primarily designed for low-latency analytical queries, which fetch relatively small amounts of data. Uber’s internal usage of Pinot is not different from this paradigm, so we have a 10K limit on the maximum number of rows that can be read from Pinot in a single query. We also keep a close eye on query latency, since our query gateway and query clients are designed around the assumption of short execution time.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ff15141d-a42a-4484-9e6f-cb7e549d59fb&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;That being said, there are scenarios where our users need to extract large amounts of data from their Pinot tables. Even though Pinot tables are not generally considered to be the source of truth, there are setups with advanced ingestion functionality such as “upsert” or “dedup” that make the Pinot table “cleanest” snapshot of the source data. Also, Pinot retention is typically longer than the real-time source (Kafka topic), which makes it handy if you want to extract data for longer time slices.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3027bb17-f56a-48b6-9038-ee4de2625a08&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;For such use cases, we utilize Spark and the Spark-Pinot connector to do batch/bulk reads from Pinot.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;08b10bb4-f1aa-4ee9-b9d7-1ba4678b8f9e&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;edf672fd-72c1-4a5a-b538-c1c7f5327b3a&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-spark-pinot-connector&#34;&gt;Spark Pinot Connector&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4d3259f3-aed1-453e-a162-876ebc4f5574&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Apache Spark exposes a standard datasource interface for external service providers to implement. Services can implement this interface and gain interoperability with other built-in or extended formats.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ed3bd45b-d697-4e37-9f6f-eb78eeb9b19f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Pinot’s open-source codebase contains packages that implement the “read interface” for Spark 2 and Spark 3. Pinot&amp;nbsp; builds also contain two ready-to-use jars, pinot-spark-connector.jar and pinot-spark-3-connector.jar for corresponding Spark versions. Those artifacts can be included into your Spark application as “external-jars” and will provide the ability to use Pinot as a data source. A simple PySpark application that dumps the last days data of a Pinot table to Hive would look like this:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;909a7925-7d57-4d40-b0b9-91cafd7c1625&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;5ed6f100-59e3-4c18-8fb8-2245d2c33f86&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdrT91FpbRHeFhNp2GJlSrzrLatYNf5IZQ2FJI17vNTZPcRoB09VoA-DnutTWVx-zF8KlfcAD9Z_4TgrjqCEqSc7vYFvWtYEZhGGcHhqdm61OxTla6cT_vrsL17eLlT6tjK__5z5i1uPvEF5ZZWEFT1gPmK?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 7:&lt;strong&gt; &lt;/strong&gt;Example PySpark app for exporting Pinot data into Hive.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3af8aaa5-23bd-4cca-a9cc-ab4978007986&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;40456453-3214-4be3-8532-3e9d17596940&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The connector accepts a variety of &lt;a href=&#34;https://github.com/apache/pinot/blob/master/pinot-connectors/pinot-spark-3-connector/documentation/read_model.md&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;options&lt;/a&gt; that control things like controller address and table name, or advanced configs such as read parallelism and timeout.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;28ec613a-6602-438f-a170-fc055a35df44&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;162c53ed-c549-4d09-8c7d-3921e393c399&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-read-model-of-the-spark-pinot-connector&#34;&gt;Read Model of the Spark Pinot Connector&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;dff82436-bee2-4355-aed4-affb58b6af25&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Spark reader uses Pinot brokers in order to fetch a server-to-segment mapping, however the queries are executed directly on Pinot servers to avoid resource contention. The connector pushes down filters to Pinot in order to minimize unnecessary data movement, however it doesn’t support aggregation push down. This means if an aggregation query is issued, all needed data needs to be downloaded to Spark executors before the aggregation is done on Spark side. This makes the reader suitable for data exports, but not ideal for general analytics, which should be noted.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;88a33ae1-baf2-452c-800d-5f1b18acefb6&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;40f8f426-9b98-4af4-97d9-77b12674560f&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-grpc-streaming&#34;&gt;gRPC Streaming&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;02f7270d-41bd-47a9-9634-89af8b25f07e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;One noteworthy feature of the connector which we make use of at Uber is the gRPC streaming support.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;64f2ffca-4553-4e92-a003-fccf14aac5e1&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Pinot servers have two interfaces for fetching data. One is a plain HTTP server and the other one is a gRPC endpoint. The plain HTTP endpoint is the more feature-complete and commonly used one, however the gRPC streaming endpoint has a distinct advantage. When executing a query, the plain HTTP endpoint loads the full resultset into heap before returning it to the caller. However the gRPC endpoint supports “chunking” and only needs to load the next chunk into heap while waiting for the caller to consume the available chunk. This alleviates the memory pressure on Pinot servers, especially for large reads.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;073cb6eb-689c-4641-9782-ece26981adb9&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Spark reader provides an option to use the gRPC endpoint when fetching data from Pinot. Moreover, when this option is enabled, the connector also uses streaming on Spark side in order to achieve true end-to-end streaming. This means when exporting a table from Pinot to Hive, the pipeline may have already written some chunks to Hive even before the Pinot server has fully processed the local segment.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cbae6370-c79d-41b7-b22c-f408b13bebd9&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;635f00a9-1776-4e55-8769-7dd261297a23&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfIs6_TNTUprRBC9k1fuV07RLXp-44Ii5D5YoCRi13lIeWmbKoVaPnln_2oLNzcubH7qwfnYGN6ss8rAhSvY1RPWsh2CZuRFCxpznFBtlkVK-QrAV5kbBWoCrW__o0aFa6MdrkwoEXb9z4F2epaNxk4YU2C?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 8:&lt;strong&gt; &lt;/strong&gt;Shows the data flow from Pinot Table to Spark Executor and Hive Sink. Resultset is loaded and transferred in chunks in both Pinot Server and Spark Executor which enables extracting datasets larger than heap.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;df64d9c3-8589-47a3-bf6b-cc55291e7b6d&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;10e5e2a5-4630-4d42-9012-c7b38471be18&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Streaming is critical for export operations that transfer large amounts of data. Non-streaming mode limits per segment export sizes to available memory and either the Pinot server or the Spark executors can quickly run out of resources, failing the pipeline.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b5e88728-4654-4509-9b91-f2cdc7b56234&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6e89f2f2-f137-46a6-ac82-e07deb94eb05&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-usage-at-uber&#34;&gt;Usage at Uber&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;dc69ea48-a106-42ff-9ec0-5288dc7bb45e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;At Uber, we build a customized version of the connector jar that automatically discovers controller address based on tenant information and injects recommended configurations such as gRPC streaming. As with any Spark application, developers can choose to write their pipelines in Java, Scala, or Python and include the provided jar to enable Pinot read support with little effort. These applications typically cover scenarios like backup or exports.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d2d8fc2f-8741-4b79-81b0-e5ca682aa067&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Expectedly, batch read workflows bring additional load on Pinot servers, which can be detrimental to system performance if not configured correctly. For this reason, we avoid enabling such workloads on shared environments and have some guidelines for users to limit the read parallelism and monitor overall load when onboarding new such pipelines.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cdd8ea35-e369-442c-a151-575378e28bc9&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In summary, the Spark connector provides a powerful tool for extracting large volumes of data out of Pinot. However it should be noted that it’s not a general analytics solution. It needs to be configured correctly and its impact on cluster performance should be evaluated carefully before productionisation.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4b26f514-d356-426f-9626-cbc4a70d439f&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;49643c83-05a4-4f7a-a2fe-2ddaae4c90e4&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-next-steps&#34;&gt;Next Steps&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fb54fcb6-b47c-4ec3-b5bc-e4a28dd5d3c3&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Our current ingestion system allows users to write data at a pre-configured cadence, and some of our users have shown interest in on-demand ingestion support. We also have some power users who want to be able to write to Pinot from their own Spark pipelines.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;bc2ae31f-21bb-4a03-903c-f23f8a5136b1&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We will continue to explore and build out more integrations to enable even more use cases.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;46c0b78b-ef9e-45e4-8b21-ebf3e53e9ff0&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;91dc13ea-1c94-4f9c-9a84-3187786a10a6&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-conclusion&#34;&gt;Conclusion&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7c4e2950-c420-4209-a7d7-7c5c9b412f00&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Building an easy-to-use, self-serve platform for ingesting data from batch data sources to Pinot has unlocked a whole new set of use cases at Uber that couldn’t be supported before. Uber has a mature data lake ecosystem with Presto and Spark as its core query engines. Pinot augments this ecosystem, providing users a way to run low-latency analytics use cases on data from their offline sources with ease.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0467a3a2-3476-42cd-b414-30fc8e5a85f9&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We feel that Pinot’s ability to seamlessly integrate with batch sources, along with its out-of-the-box support for multi-tenancy are severely underrated.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;3b6afd19-01c6-4ee3-b763-04cd0777a5ba&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Apache®, Apache Pinot™, Apache Kafka®, Apache Spark&lt;sup&gt;TM&lt;/sup&gt;, Apache Avro&lt;sup&gt;TM&lt;/sup&gt;, Apache Hive™, Pinot™, Kafka®, Spark&lt;sup&gt;TM&lt;/sup&gt;, Avro&lt;sup&gt;TM&lt;/sup&gt;, and Hive™ are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries. No endorsement by The Apache Software Foundation is implied by the use of these marks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;f31ce4cc-73ca-4f66-8538-fca0c87b7a83&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Oracle, Java, MySQL, and NetSuite are registered trademarks of Oracle and/or its affiliates. Other names may be trademarks of their respective owners.&lt;br&gt;Cover Photo Attribution: “&lt;a href=&#34;https://www.flickr.com/photos/32824724@N02/3145971395&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Vineyard 002&lt;/a&gt;” by &lt;a href=&#34;https://www.flickr.com/photos/32824724@N02&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Caliterra&lt;/a&gt; is licensed under &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;CC BY-SA 2.0&lt;/a&gt;.&lt;/p&gt;】&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;d0b0567b-b1c7-47fc-9253-50253f445151&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-introduction&#34;&gt;简介&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;67baaccd-d1f4-4f09-aad9-56aa29519724&#34;,&#34;dropCap&#34;:false}&#34;&gt;Apache Pinot&lt; support&gt;™&lt;/sup&gt; 是一个实时 OLAP 数据库，能够从 Apache Kafka® 等流和 Apache Hive&lt;sup&gt;™&lt;/sup&gt; 等离线数据源获取数据。在 Uber，Pinot 已被证明在处理各种用例方面具有真正的多功能性：从每秒超过 100 万次写入、100+ QPS 和 &lt;500 毫秒延迟的实时用例，到需要低延迟的用例。离线数据的延迟分析。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;408d1e5b-a20f-461c-9c24-f91b1490fa78&#34;,&#34;dropCap&#34;:false}&#34;&gt;黑皮诺表下降分为三大类：&lt;em&gt;实时&lt;/em&gt;、&lt;em&gt;离线&lt;/em&gt;和&lt;em&gt;混合&lt;/em&gt;。实时表支持从 Kafka 等流中提取数据，离线表允许通过 Pinot Controller 的 HTTP API 上传预先构建的“段”，混合表同时具有实时和离线部分。混合表允许单个逻辑表（相同的名称和架构）从实时流和批处理源中提取数据。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e0349cce-7cf0-466a-aa95-e4c31261fd6c&#34;,&#34;dropCap&#34;:false}&#34;&gt;本文分享Uber 如何使用 Pinot 的离线表来服务涵盖所有业务线的 100 多个低延迟分析用例。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8ff09083-e289-4bf5-aded-418f0f076d16&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ff679ca4-a030-4ad3-999c-71a7f30385c9&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-background&#34;&gt;​​背景&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;41b8c7d7-16ab-4a83-8f34-b260d4c9c096&#34;,&#34;dropCap&#34;:false}&#34;&gt;Uber 有拥有超过 100 PB 数据的庞大数据湖，近十年来我们一直在使用 Presto®、Apache Spark&lt;sup&gt;™&lt;/sup&gt; 和 Apache Hive&lt;sup&gt;™&lt;/sup&gt; 为我们的许多内部客户提供服务分析用例。 Presto® 尤其擅长处理 QPS 较低（低于 10 秒）和延迟约为几秒的用例。然而，在很多用例中，我们的用户需要在更高 QPS 下实现亚秒级 p99 延迟。大多数用户还希望为其用例提供专用资源，以避免吵闹的邻居。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;549f949b-724a-4b2b-9005-d240f0921229&#34;,&#34;dropCap&#34;:false}&#34;&gt;Apache Pinot 的能力运行 l高 qps 下的低延迟查询，以及内置的多租户和离线数据摄取支持，使其非常适合服务这些用例。我们在过去几年中看到了这种情况的发生，Pinot 现在支持 Uber 的 100 多个离线表分析用例，其中有 500 多个离线表在生产中运行。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;808bf1b3-157c-4475-bcf1-82a3ce3958aa&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;bc3284e3-c73c-4d24-aaa8-ed942906b141&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-pinot-offline-table-ingestion-primer&#34;&gt;Pinot 离线表摄取入门&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;31051218-84dd-40c8-9fcf-b6d16d136fe0&#34;,&#34;dropCap&#34;:false}&#34;&gt;Pinot 存储数据实时表和离线表的“分段”中的表。对于实时表，来自 Kafka 的数据在内存中的“可变段”中被消耗，直到达到阈值，之后该段被提交以构建“不可变段”。提交完成后，会创建一个新的可变段，从上次读取的偏移量开始继续消费。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1fd27012-e249-4f11-8c60-03b7b214c4cf&#34;,&#34;dropCap&#34;:false}&#34;&gt;对于离线表, Pinot 允许通过 Pinot 控制器中的 POST /segments API 上传预构建的不可变段。可以使用“SegmentIndexCreationDriver”和“RecordReader”接口在 Pinot 之外构建段。 RecordReader 接口允许将数据从任意源馈送到段驱动程序。段驱动程序还允许配置每列应使用的索引和编码。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f7c9317f-676d-4528-86e3-39ea565a079e&#34;,&#34;dropCap&#34;:false}&#34;&gt;一次分段对于在 Pinot 外部构建的离线表，您可以从中创建一个 tarball，将其上传到 Pinot 服务器可以访问的“深层存储”位置，然后点击控制器中的 POST /segments API。控制器将创建一个新段或更新现有段的元数据（如果已存在同名段）。然后，它会向服务器发送消息以下载并加载段，使它们可查询。该控制器还有一个用于删除现有段的 API，这意味着对于离线表，您可以通过添加新段、替换现有段或删除现有段来管理段级别的 Pinot 数据。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;87ad7115-f832-4479-a5c6-3605d913ba8c&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-块-分离”r 有-alpha-通道-不透明度&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;886e379e-0c35-4f0f-9bef-289977654dc9&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-self-serve-platform&#34;&gt;自助服务平台&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b531ea11-81be-46d5-a151-608313cf9cef&#34;,&#34;dropCap&#34;:false}&#34;&gt;给出如何分段建立和添加/更新/删除，问题是我们如何构建一个自助平台，允许100+用户无缝地创建和管理新的离线Pinot表？&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0a94e7fd-05e8-4034-923d-e7e68f731254&#34;,&#34;dropCap&#34;:false}&#34;&gt;为此，我们利用了名为 &lt;a href=&#34;https://www.uber.com/blog/no-code-workflow-orchestrator/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;uWorc 的内部无代码工作流程编排框架&lt;/a&gt;。我们有一个专用的工作流程类型，用于将数据从 Hive 摄取到 Pinot。 uWorc 为用户提供了一个 UI，他们可以在其中设置表名称、Apache Spark&lt;sup&gt;TM&lt;/sup&gt; 或 Presto® 查询、所需的索引等。它还允许用户配置作业何时运行，并且具有对跟踪执行历史记录的丰富支持，可用于查看每次运行的状态和日志。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b804b111-dd1e-499c-8efd-ad1558eaedc2&#34;,&#34;dropCap&#34;:false}&#34;&gt;在每个管道中run，我们运行用户指定的SparkSQL或PrestoSQL，并在表中添加/更新输出记录集。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;567db32f-15e6-4aec-bf2d-481aa0b690ab&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;cabba826-d3ac-4555-a839-2998c161834e&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXekLZSA8Z6kP2lVH54G9FoQtMaw7H_VzSCo3FKw93r1AEFaF6D1ar_hiQbz_4yPgEFC6domG92vEpj2jTTs_ACKQ9XE0 21IstDD2jVR4bxRCct0d34ENGFhAgvrISKlDmPdrfJcUp3pMXwMeN52_QS4Gac?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34;referrerpolicy=&#34;no- referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 1：显示了 uWorc 中每天运行的 Hive 到 RTA 管道示例。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;25cd8ec9-b58c-4010-8367-74012b392223&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3d7a2a35-7033-47ac-ae98-ab929ed9f368&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-data-modeling&#34;&gt;数据建模&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p 数据可湿性粉剂块名称=core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9f8f2e4f-0763-45e3-bb74-332eb6486a5e&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们通过 uWorc 支持两种类型的离线表：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f167793a-94e7-48e9-b13e-d45dccb667a8&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;覆盖表&lt;/strong&gt;：当表标记为覆盖时，每次运行pipeline 将完全重写表中的所有数据。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;附加表&lt;/strong&gt;：这些表具有“基于天的分区”，让人想起Hive 表中基于 datestr 的分区。给定日期运行的管道将添加该天的数据，或者如果数据已存在，则更新该天的数据。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4acee207-7936-4740-bbc9-8af117862eff&#34;,&#34;dropCap&#34;:false}&#34;&gt;这两个表类型需要在 SparkSQL 查询中生成“secondsSinceEpoch”列，Pinot 使用该列进行保留。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cf5903da-7fba-4b4c-b486-611d00ea66d5&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d484e0cd-4502-46f9-bc11-6c0b34b20967&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-ingestion-job&#34;&gt;摄取作业&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;32853506-73ea-4ffc-b591-10820ebd2bf2&#34;,&#34;dropCap&#34;:false}&#34;&gt;Pinot 只允许在段级别管理脱机表的数据以及上述覆盖/追加表语义在 Pinot 中本身并不存在。我们使用摄取作业来实现它们。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;44679f2d-87b4-48f8-864c-d4b71e1992dc&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们的摄取工作使用 &lt;a href=&#34;https://www.uber.com/blog/marmaray-hadoop-ingestion-open-source/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Marmaray&lt;/a&gt; 实现：Uber 的构建在 Spark 之上的开源数据分散框架。 Marmaray 支持从“任何源到任何接收器”提取数据。它通过提供 3 个关键 SPI 来实现此目的：源、接收器和基于 Apache Avro™ 的中间数据格式。我们已经实现了一个 PinotSink，从技术上讲，它可以从所有内部可用的来源中摄取。但是，目前我们仅使用它从 Hive 源获取数据。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0827e05d-2ab1-4aac-ab89-49b92ef0a15f&#34;,&#34;dropCap&#34;:false}&#34;&gt;PinotSink 收到一个JavaRDD&lt;AvroPayload&gt; 来自源并执行 t他遵循高层次的步骤：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e6cddac0-d0b6-4988-bf9e-ef6afbfd2564&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;重新分区 RDD 以满足段数、表类型和列分区要求。每个 Spark 分区最终对应 1 个段。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;构建细分并将其上传到深层存储。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;对 Pinot 控制器进行 HTTP POST /segments 调用以添加/更新构建的段。&lt;/li &gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;从 Pinot 中删除任何段以满足覆盖/追加表类型语义。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5b41c620-65f6-43c6-95e0-437333ea76be&#34;,&#34;dropCap&#34;:false}&#34;&gt;下面我们解释一下如何支持每种表类型。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;351910f9-6a42-44e1-813a-de99ac91b393&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;0231c7be-fcdf-4c08-a34d-013724d6a435&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-overwrite-tables&#34;&gt;覆盖表&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;945803bb-3d00-4c88-a4a7-8e3e3f670398&#34;,&#34;dropCap&#34;:false}&#34;&gt;可能会覆盖表涉及 RDD 重新分区以满足配置的“段数”要求和/或“列分区”要求。如果我们没有任何列分区要求，那么我们使用自定义 Spark &lt;a href=&#34;https://spark.apache.org/docs/latest/api/java/org/apache/spark/Partitioner.html&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;分区器&lt;/a&gt;，在每个 Spark 分区上随机分布记录。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;835fbf25-0a2e-4e70-a69e-81034ed882f7&#34;,&#34;dropCap&#34;:false}&#34;&gt;当列分区时启用后，我们在配置的列上使用 Murmur2 哈希将元组分区为所需的“段数”分区。其逻辑非常简单：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a1c73a7c-335f-423a-bb63-49af48ba0921&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;3f8d49e6-3193-4020-b7aa-9ddfdcc2dfef&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=“https://lh7-rt.googleusercontent.com/docsz/AD_4nXdJkT4aUHGjWn2573GZ__05sG4XsthSrmCufulnqCuxOQGTNFbA5EdocKYACFE3LR1LMZjKD1-bHGwKC-NvSY8VdOXYKtsBMGDnclOjkYkZ3s85HfHXfOhYVI5IgTC 0mq-5bI0wjBT7pYUcQUcrTtxbDps?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34; &gt;图 2：用于覆盖启用了列分区的表的 Spark 分区程序。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f07a97f2-b36b-475d-bf00-a78997982d01&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a474987a-8470-48a7-8c71-ce9d925a815e&#34;,&#34;dropCap&#34;:false}&#34;&gt;用于覆盖表，我们总是生成格式为“&lt;table-name&gt;_&lt;spark-partition-number&gt;”的段名称。如果用户减少“段数”，则后续作业将产生比前一个作业更少的段。在这种情况下，后续作业将删除 Pinot 中多余的段以防止数据重复。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;15eabb5c-72fa-4d0d-a1f6-85c3bf66ad22&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;370c7015-5283-49f3-8c96-bb8b1d82133d&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-append-tables&#34;&gt;追加表格&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e3faaac1-d7a7-4ce1-808d-cc9ba2f335de&#34;,&#34;dropCap&#34;:false}&#34;&gt;用于追加表，“分段数”属性用于控制应为数据中的每个唯一日期创建的分段数。例如，如果您将分段数配置为 4，并且您的提取作业生成日期“2022-01-01 至 2022-01-05”的数据，则该作业生成的分段总数将为 20，每天4个。这意味着我们总是必须为追加表重新分区 RDD，因为源 RDD 可能是任意分区的。此外，对于每个日期，我们必须确保生成预先配置的分段数量。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;27dafb25-e1bf-443c-98f8-b1af1ee9fc8c&#34;,&#34;dropCap&#34;:false}&#34;&gt;Spark 分区器API 允许我们将元组映射到整数分区 ID。那么问题是我们如何将记录映射到整数分区ID以满足追加表语义？&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a1cc01a1-3805-4a56-8dd6-2189d0fd9e85&#34;,&#34;dropCap&#34;:false}&#34;&gt;实现此目的，我们使用如下所示的 Spark 分区器：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;哈希&amp;quot;:&#34;483675f0-a89a-4338-9ccf-30cc3fc9c733&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;35550e03-3645-4cd1-8ed5-f9c1e88a1df8&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXf9mI7DccXdRz5dHm2Gs80y_sa-22Wf-9Xw28M0iOGbnbqQ1jGeiD8JW32PViZRJiUEv4OXaYp4IvGKaiKJ_zq4 P_6vS9gri7XeyhE2sqOqz6unZQCZBVHOm3zw7zCQ-vyUIYFYhqqpD4kgz3yP_nsXmF3L?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34; &#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 3：用于追加表的 Spark 分区器。通过以循环方式将记录分配给每个分段来满足每天的分段限制。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;84cfb1b3-77a2-4f36-923a-26ea64310b1a&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d62eca43-a825-4053-b709-16a77fd2dd3d&#34;,&#34;dropCap&#34;:false}&#34;&gt;分区是在用户配置的时间列上完成。在调用分区器之前，我们计算出“lookbackDays”值，它是 RDD 中最小和最大 Unix Day 之间的天数。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;64d1c65a-d0b5-4d25-a467-35aed683b623&#34;,&#34;dropCap&#34;:false}&#34;&gt;该算法可以最好从视觉上理解。假设我们有lookbackDays = 5和segmentsPerDay = 4。说一行属于给定的一天和给定的段相当于将其分配给lookbackDays xsegmentsPerDay网格的单元格。如下图所示。行代表段编号，列代表“unixDays %lookbackDays”（即自 1970-01-01 以来的天数）。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cf49d824-1377-45b9-b7d9-1249e3edd76e&#34;,&#34;dropCap&#34;:false}&#34;&gt;可以是可以看到，我们最终得到一个矩阵，行数和列数分别来自 [0, 5) 和 [0, 4)。下面的示例假设 2022-01-03 的 unixDay 可以被 5 整除。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2ae99075-06b6-4f4a-b862-2b9ada495d2c&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;1f2a883c-6dcd-4f23-9097-c36c13469b23&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeVPWie8G8MD2gy3Uw1lPrleKqFBs_O-AYEMBj3H1xnYs90_l8ku4K9i83tQ5-sAg4YD5ZawTk1SF4bfN0uthfj87xOXHVs3X4wpwL8MOjrTYTz8ytyYI366P5aokpCrVpz-BI9zCVgFdlw3X8MnALS4JVL?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34; =&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 4：&lt;strong&gt; &lt;/strong&gt;显示如何映射记录到 Spark 分区。记录的列根据记录代表的日期进行分配，每个日期有 4 个段，并且可以根据分区列的哈希或随机分配行。&lt;/figcaption&gt;&lt;/figure &gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;dd4c4df6-c2c5-498d-9a3b-2bd8b88b7028&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;22dfa589-62a5-4b98-9c42-ccdf68ad870d&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们的管道有验证以防止同一 Unix 天出现多个时间列值。这是因为我们希望 Pinot 同时对一天中的所有时段应用保留，因此我们可以一致地提供追加表语义。我们使用段名称格式：“&lt;table-name&gt;_&lt;time-column-value&gt;_&lt;time-column-value&gt;_&lt;sequence-id&gt;”。这里的序列 ID 是 [0, 段数) 之间的整数。因此，对于上面的示例，我们将拥有名称如下的段：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;16f35265-70f7-4879-b258-45e7a1c0a15e&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;0f3d82ca-8440-415a-b195-6fc67b92233f&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeb5jhwPRYmE9IX3Qoxf7v11vJEmBNHw6KMu3UkOzFY_Ar_HUZaw20QAa_OX1pu2fBeEMTlJinlZGCgWRUW5zklp52MWNgmSl HOKKmxCc1Z_h550TtBYOPY9r6fVTwoC78K_TKmYkfvgD6eeyOjNrY8DG8?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34;referrerpolicy=&#34;no- referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 5：&lt;strong&gt; &lt;/strong&gt;示例段名称。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c011dcb2-4c66-42b7-b4a4-3b77038c1a7c&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d9661fb1-8ae5-420e-821c-28bb59254db6&#34;,&#34;dropCap&#34;:false}&#34;&gt;最后，我们允许用户增加或减少分段数量，并且摄取作业可以根据需要删除 Pinot 控制器中的额外分段。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9d53f3e7-fb69-4dd6-9bc1-f57a10212c5a&#34;,&#34;opacity&#34;:&#34;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;9331640a-044b-4778-b41f-c8f0a1630b80&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-column-partitioning-for-append-tables&#34;&gt;追加表的列分区&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7520154d-658c-4e68-a614-332b1e9a0b14&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们支持列附加表的分区也是如此。除了分区器之外，所有内容都与上面的附加表所述相同，分区器按如下方式增强：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;58b40468-5237-45ca-aab0-f9b073e2c7b6&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;4e2b5398-0bc1-4f19-9996-9fb024fbbbd7&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfIMz4IDuy_ln8XdVNtBGTKVWDTYXogUla5o1MgiI356p4A_3qrytlcqIZ0sUHaM76d2N8AuBidk558iUCfYn1gm1x8f WU4aIrxzEWcvc_xkOwEVWJ_JtAHb7s2C9RehCj9o0co160VYfzfBP-Zoas27B0e?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34;referrerpolicy=&#34; no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 6：&lt;strong&gt; &lt;/strong&gt;用于在列上启用列分区的追加表的 Spark 分区程序。在调用分区器之前，我们使用“\0”作为分隔符连接时间列和分区列值。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;618e0c7c-a8c2-4d19-8556-4aeaa5fa6c81&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4bf3d955-ac82-42fe-a3ac-24942688c09e&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们再次减少它涉及将元组分配给网格的问题。列号是使用 Unix Day 值确定的，行号是使用分区列的哈希值确定的。此外，我们允许用户将段数参数保留为分区列的逻辑分区数的倍数，以允许他们在不更改分区数的情况下调整段大小。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4d28c455-e93f-44a9-a104-74ff0cec7467&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h4 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:4,&#34;hash&#34;:&#34;5510831b-ab0b-4d7e-8fe2-d5c62bf13bc8&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-advantages-of-column-partitioning&#34;&gt;C 的优点列分区&lt;/h4&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;92ccd092-991e-4d35-ab60-cca0eb4781bb&#34;,&#34;dropCap&#34;:false}&#34;&gt;在某些情况下，我们发现列分区将数据量减少了 4 倍之多。这是因为当您按将相似数据并置在同一 Spark 分区中的键进行分区时，某些数据集确实受益于字典编码。列分区还可以帮助进行段修剪，这通常可以真正提高查询性能。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;347a33d6-f256-4255-ba76-e0058de18c34&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;32cc0eb5-d186-4e84-917c-83937f4d3f49&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-spark-data-exports&#34;&gt;Spark 数据导出&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2d38f181-81eb-4bda-a914-f710d4c94eb2&#34;,&#34;dropCap&#34;:false}&#34;&gt;到目前为止，我们讨论了从离线源将数据提取到 Pinot 中，但 Pinot 也可用于将数据从其实时更新插入表导出到离线接收器（如 Hive/对象存储）。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c91612d9-73b9-4187-9b15-0917d90768d9&#34;,&#34;dropCap&#34;:false}&#34;&gt;PInot 主要是专为低延迟分析查询而设计，可获取相对少量的数据。 Uber 对 Pinot 的内部使用与这种范例没有什么不同，因此我们对单个查询中可以从 Pinot 读取的最大行数有 10K 的限制。我们还密切关注查询延迟，因为我们的查询网关和查询客户端是围绕短执行时间的假设而设计的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ff15141d-a42a-4484-9e6f-cb7e549d59fb&#34;,&#34;dropCap&#34;:false}&#34;&gt;话虽这么说，在某些情况下，我们的用户需要从 Pinot 表中提取大量数据。尽管 Pinot 表通常不被认为是事实来源，但有一些具有高级摄取功能的设置，例如“upsert”或“dedup”，可以使 Pinot 表成为源数据的“最干净”快照。此外，Pinot 保留时间通常比实时源（Kafka 主题）更长，如果您想提取更长时间片的数据，这会很方便。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3027bb17-f56a-48b6-9038-ee4de2625a08&#34;,&#34;dropCap&#34;:false}&#34;&gt;对于此类用途在这种情况下，我们利用 Spark 和 Spark-Pinot 连接器从 Pinot 进行批量/批量读取。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;08b10bb4-f1aa-4ee9-b9d7-1ba4678b8f9e&#34;,&#34;opacity&#34;:&#34;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;edf672fd-72c1-4a5a-b538-c1c7f5327b3a&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-spark-pinot-connector&#34;&gt;Spark Pinot 连接器&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4d3259f3-aed1-453e-a162-876ebc4f5574&#34;,&#34;dropCap&#34;:false}&#34;&gt;Apache Spark 公开供外部服务提供商实施的标准数据源接口。服务可以实现此接口并获得与其他内置或扩展格式的互操作性。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ed3bd45b-d697-4e37-9f6f-eb78eeb9b19f&#34;,&#34;dropCap&#34;:false}&#34;&gt;Pinot 的开放-源代码库包含实现 Spark 2 和 Spark 3 的“读取接口”的包。Pinot 版本还包含两个即用型 jar，即用于相应 Spark 的 pinot-spark-connector.jar 和 pinot-spark-3-connector.jar版本。这些工件可以作为“external-jars”包含到您的 Spark 应用程序中，并将提供使用 Pinot 作为数据源的能力。将 Pinot 表的最后几天数据转储到 Hive 的简单 PySpark 应用程序如下所示：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;909a7925-7d57-4d40-b0b9-91cafd7c1625&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;5ed6f100-59e3-4c18-8fb8-2245d2c33f86&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdrT91FpbRHeFhNp2GJlSrzrLatYNf5IZQ2FJI17vNTZPcRoB09VoA-DnutTWVx-zF8KlfcAD9Z_4TgrjqCEqSc7vYF vWtYEZhGGcHhqdm61OxTla6cT_vrsL17eLlT6tjK__5z5i1uPvEF5ZZWEFT1gPmK?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34; 推荐策略=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 7：&lt;strong&gt; &lt;/strong&gt;用于将 Pinot 数据导出到 Hive 的示例 PySpark 应用程序。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div &gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3af8aaa5-23bd-4cca-a9cc-ab4978007986&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;40456453-3214-4be3-8532-3e9d17596940&#34;,&#34;dropCap&#34;:false}&#34;&gt;连接器接受各种 &lt;a href=&#34;https://github.com/apache/pinot/blob/master/pinot-connectors/pinot-spark-3-connector/documentation/read_model.md&#34; target=&#34;_blank&#34; rel=&#34; noreferrer noopener&#34;&gt;选项&lt;/a&gt;，用于控制控制器地址和表名称等内容，或高级配置（例如读取并行性和超时）。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;核心/分隔符&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;28ec613a-6602-438f-a170-fc055a35df44&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;162c53ed-c549-4d09-8c7d-3921e393c399&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-read-model-of-the-spark-pinot-connector&#34;&gt;读取 Spark Pinot 连接器的模型&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;dff82436-bee2-4355-aed4-affb58b6af25&#34;,&#34;dropCap&#34;:false}&#34;&gt;Spark 阅读器使用Pinot 代理以获取服务器到段的映射，但是查询直接在 Pinot 服务器上执行以避免资源争用。连接器将过滤器下推到 Pinot，以最大限度地减少不必要的数据移动，但它不支持聚合下推。这意味着如果发出聚合查询，则在 Spark 端完成聚合之前，需要将所有需要的数据下载到 Spark 执行器。这使得阅读器适合数据导出，但不适合一般分析，这一点应该注意。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;88a33ae1-baf2-452c-800d-5f1b18acefb6&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;40f8f426-9b98-4af4-97d9-77b12674560f&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-grpc-streaming&#34;&gt;gRPC 流&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;02f7270d-41bd-47a9-9634-89af8b25f07e&#34;,&#34;dropCap&#34;:false}&#34;&gt;一项值得注意的功能我们在 Uber 使用的连接器之一是 gRPC 流支持。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;64f2ffca-4553-4e92-a003-fccf14aac5e1&#34;,&#34;dropCap&#34;:false}&#34;&gt;Pinot 服务器有两个接口用于获取数据。一种是普通 HTTP 服务器，另一种是 gRPC 端点。普通 HTTP 端点是功能更完整且常用的端点，但 gRPC 流端点具有明显的优势。执行查询时，普通 HTTP 端点将完整结果集加载到堆中，然后将其返回给调用者。然而，gRPC端点支持“分块”，并且只需要在等待调用者消耗可用块的同时将下一个块加载到堆中。这减轻了 Pinot 服务器的内存压力，特别是对于大量读取。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;073cb6eb-689c-4641-9782-ece26981adb9&#34;,&#34;dropCap&#34;:false}&#34;&gt;Spark 阅读器提供从 Pinot 获取数据时使用 gRPC 端点的选项。此外，启用此选项后，连接器还会在 Spark 端使用流式传输，以实现真正的端到端流式传输。时间这意味着当将表从 ​​Pinot 导出到 Hive 时，甚至在 Pinot 服务器完全处理本地段之前，管道可能已经将一些块写入 Hive。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cbae6370-c79d-41b7-b22c-f408b13bebd9&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;635f00a9-1776-4e55-8769-7dd261297a23&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfIs6_TNTUprRBC9k1fuV07RLXp-44Ii5D5YoCRi13lIeWmbKoVaPnln_2oLNzcubH7qwfnYGN6ss8rAhSvY1RPWsh2CZu RFCxpznFBtlkVK-QrAV5kbBWoCrW__o0aFa6MdrkwoEXb9z4F2epaNxk4YU2C?key=XXCdwYg2DoEiA4rnJhNrkg&#34; alt=&#34;&#34;referrerpolicy =&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 8：&lt;strong&gt; &lt;/strong&gt;显示从 Pinot Table 到 Spark Executor 和 Hive Sink 的数据流。结果集在 Pinot Server 和 Spark Executor 中以块的形式加载和传输，从而能够提取大于堆的数据集。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;df64d9c3-8589-47a3-bf6b-cc55291e7b6d&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;10e5e2a5-4630-4d42-9012-c7b38471be18&#34;,&#34;dropCap&#34;:false}&#34;&gt;流媒体至关重要用于传输大量数据的导出操作。非流模式将每个段导出大小限制为可用内存，Pinot 服务器或 Spark 执行器可能会很快耗尽资源，从而导致管道失败。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b5e88728-4654-4509-9b91-f2cdc7b56234&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6e89f2f2-f137-46a6-ac82-e07deb94eb05&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-usage-at-uber&#34;&gt;Uber 的使用情况&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;dc69ea48-a106-42ff-9ec0-5288dc7bb45e&#34;,&#34;dropCap&#34;:false}&#34;&gt;在 Uber，我们构建了一个定制版本的连接器 jar，它根据租户信息自动发现控制器地址并注入推荐的配置，例如 gRPC 流。与任何 Spark 应用程序一样，开发人员可以选择使用 Java、Scala 或 Python 编写管道，并包含提供的 jar 以轻松启用 Pinot 读取支持。这些应用程序通常涵盖备份或导出等场景。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;哈希&#34;:&#34;d2d8fc2f-8741-4b79-81b0-e5ca682aa067&#34;,&#34;dropCap&#34;:false}&#34;&gt;预计，批量读取工作流程会给 Pinot 服务器带来额外的负载，如果配置不正确，可能会损害系统性能。因此，我们避免在共享环境中启用此类工作负载，并为用户提供了一些指导原则，以在加入新的此类管道时限制读取并行性并监控总体负载。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cdd8ea35-e369-442c-a151-575378e28bc9&#34;,&#34;dropCap&#34;:false}&#34;&gt;总而言之， Spark 连接器提供了一个强大的工具，用于从 Pinot 中提取大量数据。但应该注意的是，它不是通用的分析解决方案。它需要正确配置，并且在生产之前应仔细评估其对集群性能的影响。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4b26f514-d356-426f-9626-cbc4a70d439f&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;49643c83-05a4-4f7a-a2fe-2ddaae4c90e4&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-next-steps&#34;&gt;后续步骤&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fb54fcb6-b​​47c-4ec3-b5bc-e4a28dd5d3c3&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们当前的摄取系统允许用户以预先配置的节奏写入数据，并且我们的一些用户对按需摄取支持表现出了兴趣。我们还有一些高级用户希望能够从自己的 Spark 管道写入 Pinot。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;bc2ae31f-21bb-4a03-903c-f23f8a5136b1&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们将继续探索和构建更多集成以实现更多用例。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;46c0b78b-ef9e-45e4-8b21-ebf3e53e9ff0&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;91dc13ea-1c94-4f9c-9a84-3187786a10a6&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-conclusion&#34;&gt;结论&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7c4e2950-c420-4209-a7d7-7c5c9b412f00&#34;,&#34;dropCap&#34;:false}&#34;&gt;构建一个简单的用于将数据从批量数据源提取到 Pinot 的自助服务平台为 Uber 解锁了以前无法支持的一组全新用例。 Uber拥有成熟的数据湖生态系统，以Presto和Spark为核心查询引擎。 Pinot 增强了这个生态系统，为用户提供了一种轻松地对离线来源的数据运行低延迟分析用例的方法。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p达ta-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0467a3a2-3476-42cd-b414-30fc8e5a85f9&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们感觉Pinot的能力与批处理源无缝集成以及其对多租户的开箱即用支持被严重低估。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;3b6afd19-01c6-4ee3-b763-04cd0777a5ba&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;Apache®、Apache Pinot™、Apache Kafka®、Apache Spark&lt;sup&gt;TM&lt;/sup&gt;、Apache Avro&lt;sup&gt;TM&lt;/sup&gt;、Apache Hive™、Pinot™、Kafka®、Spark&lt;sup&gt;TM&lt;/sup&gt;、Avro&lt;sup&gt;TM&lt;/sup&gt; 和 Hive™ 是 Apache Software Foundation 在美国的注册商标或商标和/或其他国家。使用这些标记并不暗示 Apache 软件基金会的认可。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;f31ce4cc-73ca-4f66-8538-fca0c87b7a83&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;Oracle、Java、MySQL 和 NetSuite 是 Oracle 和/或其附属公司的注册商标。其他名称可能是其各自所有者的商标。&lt;br&gt;封面照片归属：“&lt;a href=&#34;https://www.flickr.com/photos/32824724@N02/3145971395&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;葡萄园 002&lt;/a&gt;” 作者：&lt;a href=&#34;https://www.flickr.com/photos/32824724@N02&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Caliterra&lt;/a&gt;已获得 &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;CC BY-SA 2.0&lt;/a&gt; 许可。 &lt;/p&gt;</description>
      <pubDate>Thu, 29 Aug 2024 05:00:00 +0000</pubDate>
    </item>
    <item>
      <title>【Continuous deployment for large monorepos】大型单一仓库的持续部署</title>
      <link>https://www.uber.com/blog/continuous-deployment/</link>
      <description>【&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;74a29a78-98aa-4e44-9a05-28b950154f6c&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0b2bf6ee-9262-4a04-abe9-0ffbf6e2f079&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Uber’s business runs on a myriad of microservices. Ensuring that changes to all of these services are deployed safely and in a timely manner is critical. By utilizing continuous deployment to automate this process, we ensure that new features, library updates, and security patches are all delivered to production without unnecessary delays, improving the overall quality of code serving our business.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9a72ee63-87be-4b92-8ed5-fd056464f5f4&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In this article, we share how we reimagined continuous deployment of microservices at Uber to improve our deployment automation and the user experience of managing microservices, while tackling some of the peculiar challenges of working with large monorepos with increasing commit volumes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;179ec782-a13f-45ed-a40f-82918bfbcdc8&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ad37c6de-6055-467d-a212-e621224018d1&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-background&#34;&gt;Background&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5c743339-aca6-4bf1-b8d5-4b519cd96799&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Over the last few years we have invested heavily in maturing our tools to accommodate the continued growth of our business, while reducing production incidents. With a steady growth in code output, and more than 50% of all incidents being caused directly by code changes, our ability to enable continuous, safe deployments without impeding productivity as the business scales, is crucial for Uber’s success.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ab4556a5-7469-4d64-aaa7-9104c6581825&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;It is commonly known and evident in the industry &lt;a href=&#34;https://continuousdelivery.com/evidence-case-studies/#research&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;[1]&lt;/a&gt; &lt;a href=&#34;https://services.google.com/fh/files/misc/state-of-devops-2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;[2]&lt;/a&gt; that continuous deployment (CD) of code to production intrinsically reduces the risk of introducing bugs or defects. This is not only due to the use of CD itself ensuring that bugs and vulnerabilities are patched in a timely manner, but more so the best practices, culture, and discipline that must be in place before engineers gain enough confidence to let a machine automatically deploy their code. Before enabling CD, engineers tend to ensure they have adopted good engineering practices, such as:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3e3968ae-2199-4a89-9378-a9813e8f529a&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Code reviews&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Continuous integration (unit- integration- and load tests)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Detection (continuous monitoring and alerting, automated rollback mechanisms)&lt;br&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8e948041-261b-4bf9-a00e-1d515497224d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;em&gt;What constitutes good code reviews, sufficient unit/integration test coverage, etc, is a heavily debated topic, which is beyond the scope of this article&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;99708628-96a9-4c8b-a1ac-7109320dfdf5&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Uber has a vast platform of engineering and dev tools (e.g., &lt;a href=&#34;https://www.uber.com/en-DK/blog/introducing-ballast-an-adaptive-load-test-framework/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Ballast&lt;/a&gt;, &lt;a href=&#34;https://www.uber.com/en-DK/blog/simplifying-developer-testing-through-slate/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;SLATE&lt;/a&gt;), supporting good practices for engineers to adopt. However, historically, there have been a multitude of deployment processes in place, with limited company-wide standards or best practices for individuals and teams to adhere to. With our recent migration of all microservices to our internal cloud platform, &lt;a href=&#34;https://www.uber.com/en-DK/blog/up-portable-microservices-ready-for-the-cloud/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Up&lt;/a&gt;, we identified the opportunity to change this for the better.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;13daeda6-556e-46da-bf56-2bcbd56949ba&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;When we kicked off this project in 2022, we had approximately:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;edab0eb4-6852-4c8f-82e8-3344149606b5&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;4,500 microservices distributed across 3 monorepos (Go, Java, and Web)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;5,600 commits per week, with &lt;a href=&#34;https://www.uber.com/en-DK/blog/ubuild-fast-and-safe-building-of-thousands-of-container-images/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;many commits affecting &amp;gt;1 service&lt;/a&gt;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;7,000 production deployments per week&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;34% manually triggered (not using any kind of CD at all)&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;7% of services deploying automatically to production using CD&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2a70429e-fa9c-4b12-9e84-9733a55efeb7&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;740f9a13-cf0e-4432-92bd-f052ee457992&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-the-prior-state-of-cd-at-uber&#34;&gt;The prior state of CD at Uber&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;072b7498-dae4-4f22-81b0-3165c4cca8ad&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;CD is not a new discipline at Uber. Historically, Uber’s CD system ran as a standalone and separate CD system, making it opt-in and left to each individual team to configure.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c6f383ba-621a-40c1-ae91-b8e0a898e8a5&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;It was highly flexible and offered the ability to build completely customized CD pipelines in a YAML-based DSL. With this flexibility, we inevitably ended up with more than 100 unique pipeline templates for deploying microservices, with no enforcement of testing, monitoring, or what else it might (or might not) be doing, outside of running a sequence of actions as illustrated below:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c8a22d41-af42-41cf-992a-997e3d0b6942&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;09b72964-a937-4b4a-9435-5d7dbcef0414&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdD4Z6y2otyHDiJ4tqKnKm4HhYEMUSPWJtfc7D4sXQKxLmhXZoZsszvrCe-34cnCYHMkIKCKTmnMATEV91hyn02o25Le_5CNX91CS4PNEZu5DVlEr5rkntHysEzJD6xb8Q3ges-2fCl8d34z87YNYwN-_QR?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 1: Pipeline actions in Uber’s legacy CD system.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;321aa72f-cec6-45a9-b4d2-39e3b4ab5c31&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7acaf26d-e2b0-46c4-a87e-f9a858e7e98f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;As a consequence, the lack of standardization in CD pipelines impeded our ability to improve deployment safety and reliability company-wide, which carries significant risk when managing a microservice fleet at Uber’s scale, with a large number of changes going into production every day.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d04fbe3b-21c0-4cf9-b3d1-5d5f816f4fc5&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Besides these shortcomings, having two separate deployment systems was, in itself, confusing and undesirable. Therefore, with the recent migration to &lt;a href=&#34;https://www.uber.com/en-DK/blog/up-portable-microservices-ready-for-the-cloud/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Up&lt;/a&gt;, and given its maturity and adoption, we decided to sunset the existing CD system in favor of a new, integrated CD experience: Up CD.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;95af3e63-42d5-4cba-a7a3-e4abc0107030&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;311ae835-1c03-4fbf-9339-a44e2494ec53&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-goals&#34;&gt;Goals&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;109d6a75-8ad1-4ea1-b1d2-71b1e3c2700d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Built from the ground up to continuously apply changes in a repeatable and safe way, we sought to automate deployments to prevent human errors, converge on existing tooling for testing, and ensure monitoring of regressions as changes are being applied.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0d1b06aa-f23d-4c6a-a82f-a23d36cb2b33&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To this end, Up CD provides:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8991de37-24bb-45d6-8315-f3c99e1c92a4&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Standardized and automated deployments to production&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Safety at its core, integrating closely with Uber’s observability and testing stacks&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;A CD experience integrated tightly with the Up platform, and enabled by default&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;UI/UX tailored to the needs of Uber engineers, with built-in support for monorepo-based development&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;18ade1bf-d4a0-4426-90f5-00f11758b5d0&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;By building a CD system with these features, our expectation was that we could increase the adoption of automation and allow more services to be automatically deployed to production. Additionally, it was crucial that we could do so while decreasing (or at least not increasing) the rate of incidents in production.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7b33593b-c6dc-4f3f-8289-18b6f7afaa6d&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;655f53ec-0385-4c26-8bed-a8fe239f4079&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-designing-for-automation&#34;&gt;Designing for automation&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cc940733-f7af-4737-ad45-b2b3f6130209&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In order to realize our vision, we set out with the goal of designing the most streamlined deployment experience possible. The system should safely advance every service’s production environments to run a build with all relevant changes on the Git repository’s main branch.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;292f0635-d138-40cf-913c-6326ce41eb10&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;68ddb4fe-82e4-44c0-accd-108239bbcdb1&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXcAxerVOf7ZNN0YsuWRrWCyh4n5Xyg6VNdmeZZORhkm7EcZPeTD1Bd1g5EfT4liKpBYzP0-eZ1GgqQT9F_xpFHQKBH-2kEmhLuq6Ox3wj4hpedWs6JzgtNGS2pAUboGJCKCfNKBsbco7fLiq4DHkemGWk4N?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 2: The architecture of the new CD system.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2073610f-be98-4704-ab78-2f02948edb37&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;63af962a-5d26-4d1a-b515-1ae185051c31&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In the following section, we highlight some of the most important principles for our revamped CD system.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8a497b72-84a6-4eff-b8e8-ef7025c30ef3&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;01cad03e-7102-449f-99cd-72a90a210e0b&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-simple-core-data-structures&#34;&gt;Simple core data structures&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b0abc03c-5934-44a1-84e6-e8bf1aa862ba&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;As is evident in many of Uber’s blog posts, one challenge was the scale of our monorepos. For example, at time of writing in 2024 our Go monorepo sees more than 1,000 commits per day, and is the source for almost 3,000 microservices, which could all be affected by a single commit. It is clear that building and deploying every service in the repository for every commit would be extremely inefficient. More importantly, there’s also little advantage to doing so, as most commits only impact a small subset of services. The set of services that actually have their code binaries impacted by a commit could be &lt;a href=&#34;https://www.uber.com/blog/research/keeping-master-green-at-scale/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;computed via the repository’s Bazel graph&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fb7d3da8-087a-4a40-a46e-01c7cad986fb&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;With this in mind, we determined that while our CD system had to understand the entire history of a Git repository to ensure that commits were deployed in the correct order, for individual services we could–and should–reduce the scope dramatically. By scoping each service to the subset of commits that actually changed the code binary, it is also much easier for service owners to identify what exact changes their service is picking up with each deployment, compared to diving into the voluminous Git log for the monorepo.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a789c16a-9d55-4ade-9a31-1c158ed34d35&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;This allowed us to settle on a relatively simple data structure, where each service would be linked to all the commits in the history that are actually associated with it. This is illustrated in the following figure:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e274b7c5-ec31-481e-a95f-8303774c0cb7&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;26c21ac8-ab5b-4ba8-b9ba-dc5442b682d6&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXe66g5dv80xbUgmTQ7XpZDBaciUiwAJdsUZZG1-uNtkuIwaDZIEfnYnWoNh4_LkweZdR2dv25eJNFCRlvjD7uVHPM_q6QH4b4poLmbUGe4AlorZc0wDzL5CX22p7OgV-i9i73-q3Y13UE8zqkq0VeBEyhY_?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 3: Mapping of service to commit history.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f3a49982-1125-4d2f-9c06-bc8ff3e78eed&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;c63a6217-288b-4212-b642-b92643107211&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-a-unified-commit-flow-model-for-all-services&#34;&gt;A unified commit flow model for all services&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7f6beb9a-8a05-4a9c-b634-ccdbbf7a617f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To get to this point, we decided to utilize Uber’s &lt;a href=&#34;https://www.uber.com/en-DK/blog/kafka-async-queuing-with-consumer-proxy/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Kafka Consumer Proxy&lt;/a&gt; to consume a Kafka topic emitting an event when a commit is pushed to a Git repository. Whenever this occurs, an analysis phase is conducted to organize the commit into the appropriate structure, and determine the set of services that were impacted (changed) by the commit:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ebf133e1-5c1d-4241-8060-43d4fb3e6e81&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;e13aeb1e-2e5b-40c0-ad61-8d6a3aaea2c9&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeksuoy11nym0hEEzrzP6B2YE1KZmdOk2N8Kk9Oj2ayu2t_lIJ50VJiYFY9w2wVO92pc_wMw5e24qYDoUuwBZ0amfxMi3LgQAUxkyka6h17rWhvf2n1Ln-8BydFjv5q0IgbEQ3hExX8n6BplfVMgKuauVQ?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 4: Unified commit flow, from push to service processing.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;468de68c-452a-4843-9fe4-e2b812f3951d&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6fe9d016-5171-4066-9306-290d54458da9&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Obviously, the first step for each service impacted by a commit, would have to be to build it into a deployable container image. Afterwards, we allow engineers to customize a series of deployment stages relevant for their service. For an arbitrary service, the flow of a single commit might then be represented as the following figure shows:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e57e30d7-f698-4351-ab44-379dce5cc5f9&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;bd6f6c7c-9fb5-4557-9d38-127f908f4389&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfYZIzjtCQdFL-q8uEY82wJK8CGxACpXlnU2xZiNOb19orkZ5_pOXbWlgjJUzN7MY6zaFbhabspJaXLhHS2WUBtXpo7Ry6D_BGCIbAGFQHS2DHNqbtIlgTdLHn3beb0BeKY_INDJKFtv3Q_KPS9fnW3cG6d?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 5: Unified commit flow, from build to deployment.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ea94aa9c-abb1-4bc3-98fe-a494f7d779ce&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;adb7f53c-1078-4f42-a01d-9d8ab87579b7&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Given our experience with the prior, highly customizable, CD system, we knew that the flow had to be opinionated. Thus, we decided that the stages themselves had to be fairly simple. The configurability was primarily limited to the gating conditions, where users could combine a variety of predefined options to express what must be satisfied before a deployment stage is started. These conditions could include:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;00a04be6-3ecb-44b1-962d-3c9a161701ec&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Has the commit soaked in the previous stage for the desired time?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Are we within a user-defined deployment window (e.g., during the team’s business hours)?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Are there any other operations running for the service (e.g., a manually triggered deployment, or an automatic horizontal scaling)?&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Are there any firing service alerts that would cause a deployment to roll back?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0124bfcb-d65b-42a9-bb80-099f8ae8fdba&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Each stage is run independently of any other. For each stage, the newest commit that has successfully completed the prior stage, and is satisfying all gating conditions (if any), is thus immediately advanced to the next stage.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;423ecec7-6aa0-495b-b5fe-8e827fb9affe&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To implement these mechanisms, we utilized &lt;a href=&#34;https://www.uber.com/en-DK/blog/announcing-cadence/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Cadence&lt;/a&gt;, an open-source workflow orchestration engine built at Uber. It was straightforward to implement build and deployment workflows to be started on demand. Moreover, we implemented the gating mechanisms as workflows. Each deployment stage has its own gate workflow, which runs periodically to check whether any commit has passed the prior stage. If that is the case, it then considers its gate conditions to determine if a deployment should be triggered now.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;14a8171d-c1a6-46ec-b751-3ee6b0e4dd98&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;16b18378-0f64-47f0-899c-20c9d077f56d&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-intuitive-user-experience&#34;&gt;Intuitive user experience&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;82fcad29-1fe4-483a-9cd5-8291c7c0384b&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To ensure we got the product design right, we conducted user research and outreach in order to understand what our engineers would actually need from a CD system, given our existing tooling and monorepo scale. Based on this, we designed a user experience around a service’s commit history, where engineers can easily see the complete list of commits that were determined to impact their service, along with the current state of the service.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3354fa77-e54a-4fe2-b5c2-b6b35d8ac694&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Given the commit volume of the monorepos, it was also clear that even with automatic deployments, not every commit impacting the service could–nor should–be deployed to production stages. To make it simple to reason about what exactly had been serving production traffic or other commits deemed “interesting,” we collapsed in-between commits to provide a clearer view, addressing a key pain point of our engineers. This means that if an issue is discovered for a given production deployment, it would be very easy to expand the section down to the prior deployment to see the exact set of changes that were made to the service with that deployment. It should be immediately apparent how this view is backed by the service-level data structure of the commit history.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;041b2098-3e51-4002-bdc4-a13c1937bf9a&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;09393440-55cc-472d-bf23-866a1902b9ad&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdD1QRulIpC2fgI_ZG85PEsxek53zsqjPrMGGhswon5lrSjRwzZg5D0qAiQQG7SovXyVVHD9uHry2LMKKVHhBC-4OlnDnnoJtUIlHUFK2z2asjzOs-Q_rt5QTPNptzykQwxjAoBK0OWo_jhb7yAf6ZplNk?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 6: Collapsed view of commit history, showing just the relevant commits to the current state of the service.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1c9478d2-04c0-48bd-a0be-22b81029f543&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;51298020-115e-4bba-ad09-35f7b52ca941&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The current state of the world is concisely represented via the so-called “swimlanes” on the left of the commit history. Each of these lanes displays exactly what code is and has been deployed to the different service environments. By hovering over the lanes, details about the deployment status for that environment can be obtained. See the figure below:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;216fe511-d6a0-41c7-8af2-5f15b36966f6&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;d8d5c293-64c1-4143-b388-fcb2832926cf&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfsa9pQ6mpsc9zmHyfj5LzDJ5mHYliAQzxETDeNAiyrfv3Xtl0RUfCz-54_DII0it2LpQYagH4utLkCKxH9Dx2N3gULUH1Ko5AA_YAXq0O_y2pNjslZ-FN7d02xUBBU9_sNcVwtCmQNiqhJloyxvdQ8C7A?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 7: Environments represented by swimlanes, visualizing how commits progress through the CD pipeline.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;368c1105-2b17-4ced-86e8-a3be466444d9&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fda68b1c-618c-4260-9ffb-59841709afd1&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Furthermore, we associate a series of events with every commit. The top-level view shows the most relevant event for the commit, though every commit can be expanded to understand not only exactly what has happened to it already, but also what is going to happen for it in the future, given the deployment stages and gating events configured for the service.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e5dd7e3c-bcca-4865-8996-145099767b34&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;c60b65e0-4403-452f-9ff0-bb64e46f26bd&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfboVkIIQJhX4QuRkXGPgAuiQt_LyiMJPlaAnMRaLYOFZWlPYfOe5CP2C7-9F2yqMbVuIikduGWoG14VXJz7w9MFMdBKxlVWhBsYnF-inTAdNxCCOk6tN20w7G94zvzlwun_om5lbMYzECWKKki1c9WVZda?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 8:The history of a specific commit, followed by current state and scheduled deployment actions.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;178fc3f7-5f4b-4ee1-8bfd-6744faa2b2c6&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;332df2aa-341c-4cc5-850e-184dc7ee91e5&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-tight-coupling&#34;&gt;Tight coupling&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8ddfb47a-1b48-4000-bfa3-c2472aa7fa30&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In order to provide trust in and increase adoption of automation, it was clear that we had to provide a unified, streamlined deployment experience; it would not be satisfactory to simply bolt a separate CD orchestration layer on top of the deployment system.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f3bc92ce-2b97-4b91-9e2a-709de3917c20&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We built the new CD system to not only do its own thing, but to be tightly coupled to Up and mindful of other operations, guaranteeing that its actions would not catch users by surprise. For example, this means that if an engineer manually starts a deployment to a production environment outside the CD pipeline, then the CD UI will incorporate this deployment in the service’s commit history. Moreover, the system’s internal state is mutated to incorporate the fact that the commit (and anything prior to it in the Git history) was already deployed to the targeted environments.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;82df7ce1-fa21-4e14-a789-5b4057dc838d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;This is also the case if an engineer builds a commit not strictly related to the service (e.g., by building from HEAD of main) and deploys that to their service, then that commit is added to the internal state, so the CD view is able to always represent the correct state of affairs and isn’t misleading to users who use this as the primary place to understand their service state.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1a2db057-80c0-4678-8661-b9678293b122&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;This was an important strategic decision, as it allowed an incremental shift from manual deployments towards CD, instead of being an all-or-nothing approach. This also means that whenever an engineer takes some manual action to, for example, mitigate an incident, the CD system will be able to automatically do the right thing (which frequently is to not do anything, or pause), given the situation.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4d96e1c1-01b2-439d-8e12-f7be9f0edb17&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2d323077-9b21-48b3-88b4-90b166775f64&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-post-release-observations&#34;&gt;Post-release observations&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;58729e94-cf9c-45aa-b9df-1c32b825c158&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In this section, we highlight some of the effects that we saw from the release of Up CD.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;999058e8-8874-4d16-8f7a-a83453c710fc&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;0db3f75b-6b43-4ee0-a633-2107164328ba&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-increase-in-adoption-of-automation&#34;&gt;Increase in adoption of automation&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;81c03cf4-a88d-456b-956d-345ed1207d77&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;After releasing our CD experience internally, we started seeing significant shifts in behavior. As we had hoped for, engineers started embracing it and we saw immediate adoption, which kept rising: Concretely, we saw the number of services being deployed automatically increase from less than 10% to almost 70% over a 12 month period.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;da865d9b-8cd1-4b7f-89b4-7697ba25a6a1&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;As services are deployed more frequently, it has also become easier to attribute fault to specific commits, as fewer commits are going out with each deployment.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;81a68168-44ab-49a0-81e8-8eed3bb3faa6&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;32a3771a-c7b7-44c9-8bcc-df7ff865b0fa&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-no-increase-in-production-incidents&#34;&gt;No increase in production incidents&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9622779d-a4ce-4b5c-b46b-5ccd98fd9bfc&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;While increasing the rate of deployments, we were happy to find that the overall rate of production incidents did not increase proportionally. In fact, during the same 12 month period where CD adoption climbed, we saw a decrease in reported incidents per 1,000 code changes of more than 50%.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fa7585e0-fd4a-45aa-b0ce-ce005dab8a03&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;As there were other efforts ongoing while this was carried out, which will be detailed in a separate blog post, we cannot claim causation for this feat. However, it is clear that we could in fact succeed in our vision to get our engineers to deploy their services to production automatically, without increasing the frequency or severity of incidents.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6803f4b7-a11d-4350-ae69-073a1d2bf9b0&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;4ba89534-ea4d-4885-ad2e-9455570acc02&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-new-challenges&#34;&gt;New challenges&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;23ca1a99-2324-4829-a083-9f3c0a91c73e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;However, we also started seeing new challenges. In particular, we found that the risk of making changes to monorepo code shared by many services (e.g., if someone changed a common RPC library shared by all services), had suddenly increased, as the changes would more rapidly get deployed to all the impacted services (and many of those deployments might happen in parallel). This meant that if such a change introduced a significant bug that wasn’t caught during CI, automation would be able to quickly break a lot of services simultaneously.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;767e8f7a-966c-4ef0-9a7c-5e9104c7e51d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Generally, some services would have mechanisms in place to detect the issue and automatically roll the deployment back, but it is unlikely that the problem would be automatically detected for every service.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;71c6fb58-2ac5-4d37-9e7e-6cb8598f3633&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;For this reason, we introduced the ability to utilize signals about a commit across services, so if some significant fraction of the services did not deploy successfully, the commit was considered to be problematic.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;00b23978-4571-460a-92f4-a782269c5b35&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To get as clear a signal as possible, we stagger the deployment of risky, cross-cutting commits according to our &lt;a href=&#34;https://www.uber.com/en-DK/blog/cinnamon-using-century-old-tech-to-build-a-mean-load-shedder/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;internal service tiering&lt;/a&gt;. Initially, Up CD deploys it to our least important cohort of services, and, when a sufficient percentage of those are successfully deployed, it advances to the following tier. If a significant fraction of the services start experiencing problems, the deployment is halted and the commit author notified about a potential issue.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;348ac0ab-613e-4876-b91e-5bc645892513&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;With this deployment strategy, we mitigated the risk of customer impact from such risky changes to acceptable levels and, just as importantly, increased trust in automated production deployments by providing additional guardrails for particularly risky changes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;dd3ed3f4-335f-478f-87f8-5584b5f2cfa7&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;f5fc3556-386e-4041-88f0-ea1d2d18ac1b&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-key-metrics&#34;&gt;Key metrics&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1c08aa92-93ac-4ba8-9c2f-3673f1ed22c3&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To quantify the results of this project, we summarized how some of our key metrics changed during this project in the following table:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/table&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;a39c5ffa-469b-42b7-b2c3-7893d8732790&amp;quot;,&amp;quot;hasFixedLayout&amp;quot;:false,&amp;quot;head&amp;quot;:[],&amp;quot;body&amp;quot;:[],&amp;quot;foot&amp;quot;:[]}&#34; class=&#34;wp-block-table aligncenter&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Metric&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Before Up CD (primo 2022)&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Post Up CD (March 2024)&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;# services&lt;/td&gt;&lt;td&gt;4,500&lt;/td&gt;&lt;td&gt;5,000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Monorepo commits / week&lt;/td&gt;&lt;td&gt;5,600&lt;/td&gt;&lt;td&gt;11,000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Production deployments / week&lt;/td&gt;&lt;td&gt;7,000&lt;/td&gt;&lt;td&gt;50,000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;% of deployments CD orchestrated (partially or fully to production)&lt;/td&gt;&lt;td&gt;66%&lt;/td&gt;&lt;td&gt;95%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;% of services fully automated to production&lt;/td&gt;&lt;td&gt;7%&lt;/td&gt;&lt;td&gt;65%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fb9afa3f-4c1e-43a7-98b8-0f43ab66e12e&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;f9591e21-ed1d-4577-904a-2070fe685878&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-conclusion&#34;&gt;Conclusion&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;66d53876-9133-4bd9-abd1-6c347d60b215&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;As we have detailed in the previous sections, our revamped CD system, Up CD, embodies a strategic shift towards automation as a core principle in our deployment methodology. We deem that this shift has been crucial for managing the complexity and scale of our operations with heightened safety and efficiency, shifting the burden of delivering code to production from our engineers to automation.&lt;/p&gt;】&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;74a29a78-98aa-4e44-9a05-28b950154f6c&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-introduction&#34;&gt;简介&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0b2bf6ee-9262-4a04-abe9-0ffbf6e2f079&#34;,&#34;dropCap&#34;:false}&#34;&gt;Uber 的业务运行在无数的微服务上。确保安全、及时地部署所有这些服务的更改至关重要。通过利用持续部署来自动化此过程，我们确保新功能、库更新和安全补丁全部交付到生产中，而不会出现不必要的延迟，从而提高为我们的业务服务的代码的整体质量。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9a72ee63-87be-4b92-8ed5-fd056464f5f4&#34;,&#34;dropCap&#34;:false}&#34;&gt;在本文中，我们分享了如何在 Uber 重新构想微服务的持续部署，以提高我们的部署自动化和管理微服务的用户体验，同时解决与不断增加的提交量的大型单一存储库合作的一些特殊挑战。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;179ec782-a13f-45ed-a40f-82918bfbcdc8&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ad37c6de-6055-467d-a212-e621224018d1&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-background&#34;&gt;​​背景&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5c743339-aca6-4bf1-b8d5-4b519cd96799&#34;,&#34;dropCap&#34;:false}&#34;&gt;过去几年来，我们投入了大量资金来完善我们的工具，以适应业务的持续增长，同时减少生产事故。随着代码输出的稳定增长，并且超过 50% 的事件是由代码更改直接引起的，我们能够实现持续、安全的部署，并且不会随着业务规模的扩大而影响生产力，这对于 Uber 的成功至关重要。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ab4556a5-7469-4d64-aaa7-9104c6581825&#34;,&#34;dropCap&#34;:false}&#34;&gt;通常是在业界众所周知且明显&lt;a href=&#34;https://continuousdelivery.com/evidence-case-studies/#research&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;[1]&lt;/a&gt; &lt;a href =&#34;https://services.google.com/fh/files/misc/state-of-devops-2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;[2]&lt;/a&gt; 持续部署代码到生产的（CD）本质上降低了引入错误或缺陷的风险。这不仅是由于 CD 本身的使用确保了错误和漏洞得到及时修补，更重要的是在工程师获得足够的知识之前必须落实的最佳实践、文化和纪律。有信心让机器自动部署其代码。在启用 CD 之前，工程师往往会确保他们采用了良好的工程实践，例如：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3e3968ae-2199-4a89-9378-a9813e8f529a&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;代码审查&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;持续集成（单元集成和负载测试）&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;检测（持续监控和警报、自动回滚机制）&lt;br&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8e948041-261b-4bf9-a00e-1d515497224d&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;em&gt;什么构成了良好的代码审查、足够的单元/集成测试覆盖率等，这是一个备受争议的话题，这超出了本文的范围&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;99708628-96a9-4c8b-a1ac-7109320dfdf5&#34;,&#34;dropCap&#34;:false}&#34;&gt;Uber 有一个庞大的工程和开发工具平台（例如，&lt;a href=&#34;https://www.uber.com/en-DK/blog/introducing-ballast-an-adaptive-load-test-framework/&#34; target=&#34;_blank “ rel=&#34;noreferrer noopener&#34;&gt;镇流器&lt;/a&gt;，&lt;a href=&#34;https://www.uber.com/en-DK/blog/simplifying-developer-testing-through-slate/&#34; target=&#34;_blank “ rel=&#34;noreferrer noopener&#34;&gt;SLATE&lt;/a&gt;），支持工程师采用的良好实践。然而，从历史上看，已经存在多种部署流程，而个人和团队需要遵守的公司范围标准或最佳实践有限。随着我们最近将所有微服务迁移到内部云平台，&lt;a href=&#34;https://www.uber.com/en-DK/blog/up-portable-microservices-ready-for-the-cloud/&#34; 目标=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;向上&lt;/a&gt;，我们发现了改善这一情况的机会。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;13daeda6-556e-46da-bf56-2bcbd56949ba&#34;,&#34;dropCap&#34;:false}&#34;&gt;当我们踢出时到 2022 年，完成这个项目后，我们大约有：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;edab0eb4-6852-4c8f-82e8-3344149606b5&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;分布在 3 个单一存储库（Go、Java 和 Web）的 4,500 个微服务&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;每周提交 5,600 次，&lt;a href=&#34;https://www.uber.com/en -DK/blog/ubuild-fast-and-safe-building-of-thousands-of-container-images/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;许多提交影响 &gt;1 项服务&lt;/a&gt;&lt;/李&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;每周 7,000 次生产部署&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:false,&#34;values&#34;:&#34;&#34;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;34% 手动触发（根本不使用任何类型的 CD）&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;7% 的服务使用 CD 自动部署到生产&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2a70429e-fa9c-4b12-9e84-9733a55efeb7&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;740f9a13-cf0e-4432-92bd-f052ee457992&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-the-prior-state-of-cd-at-uber&#34;&gt;Uber CD 的先前状态&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;072b7498-dae4-4f22-81b0-3165c4cca8ad&#34;,&#34;dropCap&#34;:false}&#34;&gt;CD 不是Uber 的一项新纪律。从历史上看，Uber 的 CD 系统作为独立且单独的 CD 系统运行，使其可以选择加入并留给每个单独的团队进行配置。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c6f383ba-621a-40c1-ae91-b8e0a898e8a5&#34;,&#34;dropCap&#34;:false}&#34;&gt;这是高度灵活，并提供在基于 YAML 的 DSL 中构建完全定制的 CD 管道的能力。有了这种灵活性，我们不可避免地会得到 100 多个用于部署微服务的独特管道模板，除了运行如下所示的一系列操作之外，无需强制执行测试、监控或其他可能（或可能不）执行的操作：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c8a22d41-af42-41cf-992a-997e3d0b6942&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;09b72964-a937-4b4a-9435-5d7dbcef0414&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdD4Z6y2otyHDiJ4tqKnKm4HhYEMUSPWJtfc7D4sXQKxLmhXZoZsszvrCe-34cnCYHMkIKCKTmnMATEV91hyn02o2 5Le_5CNX91CS4PNEZu5DVlEr5rkntHysEzJD6xb8Q3ges-2fCl8d34z87YNYwN-_QR?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34; &#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 1：Uber 旧 CD 系统中的管道操作。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;321aa72f-cec6-45a9-b4d2-39e3b4ab5c31&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7acaf26d-e2b0-46c4-a87e-f9a858e7e98f&#34;,&#34;dropCap&#34;:false}&#34;&gt;因此，CD p 缺乏标准化ipelines 阻碍了我们在全公司范围内提高部署安全性和可靠性的能力，这在管理 Uber 规模的微服务队列时带来了巨大的风险，每天都会有大量的变更投入生产。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d04fbe3b-21c0-4cf9-b3d1-5d5f816f4fc5&#34;,&#34;dropCap&#34;:false}&#34;&gt;除了这些缺点，拥有两个独立的部署系统本身就是令人困惑且不可取的。因此，最近迁移到 &lt;a href=&#34;https://www.uber.com/en-DK/blog/up-portable-microservices-ready-for-the-cloud/&#34; target=&#34;_blank&#34; rel= “noreferrer noopener&#34;&gt;Up&lt;/a&gt;，鉴于其成熟度和采用率，我们决定废弃现有的 CD 系统，转而采用新的集成 CD 体验：Up CD。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;95af3e63-42d5-4cba-a7a3-e4abc0107030&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;311ae835-1c03-4fbf-9339-a44e2494ec53&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-goals&#34;&gt;目标&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;109d6a75-8ad1-4ea1-b1d2-71b1e3c2700d&#34;,&#34;dropCap&#34;:false}&#34;&gt;从为了以可重复且安全的方式持续应用变更，我们寻求自动化部署以防止人为错误，集中于现有的测试工具，并确保在应用变更时监控回归。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0d1b06aa-f23d-4c6a-a82f-a23d36cb2b33&#34;,&#34;dropCap&#34;:false}&#34;&gt;为此，Up CD提供：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8991de37-24bb-45d6-8315-f3c99e1c92a4&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;标准化、自动化的生产部署&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;以安全为核心，与 Uber 的可观察性和测试堆栈紧密集成&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;与 Up 平台紧密集成且默认启用的 CD 体验&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;根据 Uber 工程师的需求量身定制的 UI/UX，内置对基于 monorepo 的开发的支持&lt; /里&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;18ade1bf-d4a0-4426-90f5-00f11758b5d0&#34;,&#34;dropCap&#34;:false}&#34;&gt;通过构建具有这些功能的CD系统，我们的期望是我们可以增加自动化的采用，并允许更多的服务自动部署到生产中。此外，至关重要的是，我们能够在e 降低（或至少不增加）生产中的事故发生率。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7b33593b-c6dc-4f3f-8289-18b6f7afaa6d&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;655f53ec-0385-4c26-8bed-a8fe239f4079&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-designing-for-automation&#34;&gt;自动化设计&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cc940733-f7af-4737-ad45-b2b3f6130209&#34;,&#34;dropCap&#34;:false}&#34;&gt;为了为了实现我们的愿景，我们的目标是设计尽可能简化的部署体验。系统应该安全地推进每个服务的生产环境，以便在 Git 存储库的主分支上运行包含所有相关更改的构建。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;292f0635-d138-40cf-913c-6326ce41eb10&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34; :&#34;68ddb4fe-82e4-44c0-accd-108239bbcdb1&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent. com/docsz/AD_4nXcAxerVOf7ZNN0YsuWRrWCyh4n5Xyg6VNdmeZZORhkm7EcZPeTD1Bd1g5EfT4liKpBYzP0-eZ1GgqQT9F_xpFHQKBH-2kEmhLuq6Ox3wj4hpedWs6JzgtNGS2pAUboGJCKCfNK Bsbco7fLiq4DHkemGWk4N?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图2：新CD系统的架构。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2073610f-be98-4704-ab78-2f02948edb37&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;63af962a-5d26-4d1a-b515-1ae185051c31&#34;,&#34;dropCap&#34;:false}&#34;&gt;在下面部分，我们重点介绍了我们改进后的 CD 系统的一些最重要的原则。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8a497b72-84a6-4eff-b8e8-ef7025c30ef3&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;01cad03e-7102-449f-99cd-72a90a210e0b&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-simple-core-data-structs&#34;&gt;简单核心数据结构&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&amp;quot;b0abc03c-5934-44a1-84e6-e8bf1aa862ba&#34;,&#34;dropCap&#34;:false}&#34;&gt;正如许多 Uber 博客文章中所表明的那样，我们面临的挑战之一是单一仓库的规模。例如，在 2024 年撰写本文时，我们的 Go monorepo 每天会看到超过 1,000 次提交，并且是近 3,000 个微服务的来源，这些微服务都可能受到一次提交的影响。显然，为每次提交构建和部署存储库中的每项服务效率极低。更重要的是，这样做也没有什么好处，因为大多数提交只影响一小部分服务。实际上其代码二进制文件受提交影响的服务集可能是 &lt;a href=&#34;https://www.uber.com/blog/research/keeping-master-green-at-scale/&#34; target=&#34;_blank “ rel=&#34;noreferrer noopener&#34;&gt;通过存储库的 Bazel 图计算&lt;/a&gt;。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fb7d3da8-087a-4a40-a46e-01c7cad986fb&#34;,&#34;dropCap&#34;:false}&#34;&gt;有了这个请注意，我们确定，虽然我们的 CD 系统必须了解 Git 存储库的整个历史记录，以确保按正确的顺序部署提交，但对于各个服务，我们可以而且应该大幅缩小范围。与深入研究 monorepo 的大量 Git 日志相比，通过将每个服务的范围限定为实际更改了代码二进制文件的提交子集，服务所有者也可以更轻松地确定他们的服务在每次部署中获取的确切更改。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a789c16a-9d55-4ade-9a31-1c158ed34d35&#34;,&#34;dropCap&#34;:false}&#34;&gt;这允许我们选择一个相对简单的数据结构，其中每个服务都将链接到历史记录中实际与其关联的所有提交。如下图所示：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e274b7c5-ec31-481e-a95f-8303774c0cb7&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34; :&#34;26c21ac8-ab5b-4ba8-b9ba-dc5442b682d6&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent. com/docsz/AD_4nXe66g5dv80xbUgmTQ7XpZDBaciUiwAJdsUZZG1-unTkuIwaDZIEfnYnWoNh4_LkweZdR2dv25eJNFCRlvjD7uVHPM_q6QH4b4poLmbUGe4AlorZc0wDzL5CX22p7OgV-i9 i73-q3Y13UE8zqkq0VeBEyhY_?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34; &gt;图 3：服务到提交历史记录的映射。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&amp;quot;f3a49982-1125-4d2f-9c06-bc8ff3e78eed&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;c63a6217-288b-4212-b642-b92643107211&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-a-unified-commit-flow-model-for-all-services&#34;&gt;所有服务的统一提交流程模型&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7f6beb9a-8a05-4a9c-b634-ccdbbf7a617f&#34;,&#34;dropCap&#34;:false}&#34;&gt;要到达此时，我们决定利用 Uber 的 &lt;a href=&#34;https://www.uber.com/en-DK/blog/kafka-async-queuing-with-consumer-proxy/&#34; target=&#34;_blank&#34; rel=&#34; noreferrer noopener&#34;&gt;Kafka Consumer Proxy&lt;/a&gt; 用于使用 Kafka 主题，在提交推送到 Git 存储库时发出事件。每当发生这种情况时，就会进行分析阶段，将提交组织成适当的结构，并确定受提交影响（更改）的服务集：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ebf133e1-5c1d-4241-8060-43d4fb3e6e81&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;e13aeb1e-2e5b-40c0-ad61-8d6a3aaea2c9&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXeksuoy11nym0hEEzrzP6B2YE1KZmdOk2N8Kk9Oj2ayu2t_lIJ50VJiYFY9w2wVO92pc_wMw5e24qYDoUuwBZ0 amfxMi3LgQAUxkyka6h17rWhvf2n1Ln-8BydFjv5q0IgbEQ3hExX8n6BplfVMgKuauVQ?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34;referrerpolicy=&#34; no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 4：统一提交流程，从推送到服务处理。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;468de68c-452a-4843-9fe4-e2b812f3951d&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6fe9d016-5171-4066-9306-290d54458da9&#34;,&#34;dropCap&#34;:false}&#34;&gt;显然，对于受提交影响的每个服务，第一步必须是将其构建到可部署的容器映像中。之后，我们允许工程师定制一系列与其服务相关的部署阶段。对于任意服务，单个提交的流程可能如下图所示：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e57e30d7-f698-4351-ab44-379dce5cc5f9&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;：“center”，“哈希”：“bd6f6c7c-9fb5-4557-9d38-127f908f4389”，“alt”：“”}“class =“aligncenter”&gt;&lt;img解码=“异步”src =“https://lh7” -rt.googleusercontent.com/docsz/AD_4nXfYZIzjtCQdFL-q8uEY82wJK8CGxACpXlnU2xZiNOb19orkZ5_pOXbWlgjJUzN7MY6zaFbhabspJaXLhHS2WUBtXpo7Ry6D_BGCIbAGFQHS2DHNqbtIlgTdLHn3beb0BeKY_INDJ KFtv3Q_KPS9fnW3cG6d?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 5：来自构建的统一提交流程部署。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ea94aa9c-abb1-4bc3-98fe-a494f7d779ce&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;adb7f53c-1078-4f42-a01d-9d8ab87579b7&#34;,&#34;dropCap&#34;:false}&#34;&gt;根据我们的经验对于之前的高度可定制的 CD 系统，我们知道流程必须是固定的。因此，我们决定阶段本身必须相当简单。可配置性主要限于门控条件，用户可以组合各种预定义选项来表达部署阶段开始之前必须满足的条件。这些条件可能包括：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;00a04be6-3ecb-44b1-962d-3c9a161701ec&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;提交是否在前一阶段中浸泡了所需的时间？&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;我们是否处于用户定义的部署窗口内（例如，在团队的工作时间内）？&lt;/li &gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;该服务是否正在运行任何其他操作（例如，手动触发的部署或自动水平扩展） ）？&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;是否触发任何会导致部署回滚的服务警报？&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0124bfcb-d65b-42a9-bb80-099f8ae8fdba&#34;,&#34;dropCap&#34;:false}&#34;&gt;每个阶段都是独立于任何其他运行。对于每个阶段，已成功完成前一阶段并满足所有门控条件（如果有）的最新提交将立即进入下一阶段。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;423ecec7-6aa0-495b-b5fe-8e827fb9affe&#34;,&#34;dropCap&#34;:false}&#34;&gt;实现这些机制，我们利用了 &lt;a href=&#34;https://www.uber.com/en-DK/blog/announcing-cadence/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Cadence&lt;/a&gt;，一个开放的- Uber 构建的源工作流编排引擎。实施构建和部署工作流程非常简单按需开始。此外，我们将门控机制实现为工作流程。每个部署阶段都有自己的门控工作流程，该工作流程定期运行以检查是否有任何提交已通过前一阶段。如果是这种情况，它会考虑其门条件来确定是否现在应该触发部署。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;14a8171d-c1a6-46ec-b751-3ee6b0e4dd98&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;16b18378-0f64-47f0-899c-20c9d077f56d&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-intuitive-user-experience&#34;&gt;直观的用户体验&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;82fcad29-1fe4-483a-9cd5-8291c7c0384b&#34;,&#34;dropCap&#34;:false}&#34;&gt;为了确保我们在产品设计正确后，我们进行了用户研究和推广，以了解我们的工程师在考虑到我们现有的工具和 monorepo 规模的情况下，实际上需要从 CD 系统中获得什么。基于此，我们围绕服务的提交历史记录设计了用户体验，工程师可以轻松查看确定影响其服务的提交的完整列表以及服务的当前状态。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3354fa77-e54a-4fe2-b5c2-b6b35d8ac694&#34;,&#34;dropCap&#34;:false}&#34;&gt;鉴于提交考虑到 monorepos 的数量，很明显，即使采用自动部署，也不是每个影响服务的提交都可以（也不应该）部署到生产阶段。为了更容易地推断出到底是什么服务于生产流量或其他被认为“有趣”的提交，我们在提交之间折叠以提供更清晰的视图，解决了工程师的关键痛点。这意味着，如果在给定的生产部署中发现问题，则可以很容易地将该部分扩展到之前的部署，以查看对该部署对服务所做的确切更改集。应该立即显而易见的是该视图是如何由提交历史记录的服务级别数据结构支持的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;041b2098-3e51-4002-bdc4-a13c1937bf9a&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;09393440-55cc-472d-bf23-866a1902b9ad&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdD1QRulIpC2fgI_ZG85PEsxek53zsqjPrMGGhswon5lrSjRwzZg5D0qAiQQG7SovXyVVHD9uHry2LMKKVHhBC-4Oln DnnoJtUIlHUFK2z2asjzOs-Q_rt5QTPNptzykQwxjAoBK0OWo_jhb7yAf6ZplNk?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 6：提交历史记录的折叠视图，仅显示与当前状态相关的提交服务。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1c9478d2-04c0-48bd-a0be-22b81029f543&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;51298020-115e-4bba-ad09-35f7b52ca941&#34;,&#34;dropCap&#34;:false}&#34;&gt;当前状态世界的情况通过提交历史左侧所谓的“泳道”简洁地表示。每个通道都准确显示代码是什么以及已部署到不同的服务环境。通过将鼠标悬停在通道上，可以获得有关该环境的部署状态的详细信息。见下图：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;216fe511-d6a0-41c7-8af2-5f15b36966f6&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;d8d5c293-64c1-4143-b388-fcb2832926cf&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfsa9pQ6mpsc9zmHyfj5LzDJ5mHYliAQzxETDeNAiyrfv3Xtl0RUfCz-54_DII0it2LpQYagH4utLkCKxH9Dx2N3gULUH1 Ko5AA_YAXq0O_y2pNjslZ-FN7d02xUBBU9_sNcVwtCmQNiqhJloyxvdQ8C7A?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34;referrerpolicy =&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 7：由泳道表示的环境，可视化如何通过 CD 管道提交进度。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;368c1105-2b17-4ced-86e8-a3be466444d9&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fda68b1c-618c-4260-9ffb-59841709afd1&#34;,&#34;dropCap&#34;:false}&#34;&gt;此外，我们将一系列事件与每次提交相关联。顶级视图显示了与提交最相关的事件，尽管每个提交都可以扩展，不仅可以准确了解它已经发生的情况，还可以了解它未来将发生的情况，考虑到部署阶段和为服务配置的门控事件。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e5dd7e3c-bcca-4865-8996-145099767b34&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;c60b65e0-4403-452f-9ff0-bb64e46f26bd&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https ://lh7-rt.googleusercontent.com/docsz/AD_4nXfboVkIIQJhX4QuRkXGPgAuiQt_LyiMJPlaAnMRaLYOFZWlPYfOe5CP2C7-9F2yqMbVuIikduGWoG14VXJz7w9MFMdBKxlVWhBsYnF-inTAdNxCCOk6tN20w7 G94zvzlwun_om5lbMYzECWKKki1c9WVZda?key=Oxu4WZYqSolAv1oG8zB5fg&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 8:特定提交的历史记录，后跟当前状态和计划的部署操作。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;178fc3f7-5f4b-4ee1-8bfd-6744faa2b2c6&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;332df2aa-341c-4cc5-850e-184dc7ee91e5&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-tight-coupling&#34;&gt;紧密耦合&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8ddfb47a-1b48-4000-bfa3-c2472aa7fa30&#34;,&#34;dropCap&#34;:false}&#34;&gt;为了提供对自动化的信任并增加自动化的采用，很明显，我们必须提供统一、简化的部署体验；简单地在部署系统之上附加一个单独的 CD 编排层是不够的。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f3bc92ce-2b97-4b91-9e2a-709de3917c20&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们构建了新的CD系统不仅要做自己的事情，而且要与Up紧密耦合并关注其他操作，保证其行为不会让用户措手不及。例如，这意味着如果工程师在 CD 管道之外手动启动到生产环境的部署，则 CD UI 会将此部署合并到服务的提交历史记录中。此外，系统的内部状态会发生变化，以纳入提交（以及 Git 历史记录中之前的任何内容）已经部署到目标环境的事实。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;82df7ce1-fa21-4e14-a789-5b4057dc838d&#34;,&#34;dropCap&#34;:false}&#34;&gt;这也是如果工程师构建与服务不严格相关的提交（例如，通过从 main 的 HEAD 构建）并将其部署到他们的服务，则该提交将添加到内部状态，因此 CD 视图始终能够表示正确的事务状态，并且不会误导使用此作为了解其服务状态的主要位置的用户。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1a2db057-80c0-4678-8661-b9678293b122&#34;,&#34;dropCap&#34;:false}&#34;&gt;这是一个重要的战略决策，因为它允许从手动操作逐步转变所有部署都转向 CD，而不是采取全有或全无的方法。这也意味着，每当工程师采取一些手动操作（例如，缓解事件）时，CD 系统将能够根据情况自动执行正确的操作（通常是不执行任何操作或暂停）。&lt; /p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4d96e1c1-01b2-439d-8e12-f7be9f0edb17&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2d323077-9b21-48b3-88b4-90b166775f64&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-post-release-observations&#34;&gt;发布后观察&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;58729e94-cf9c-45aa-b9df-1c32b825c158&#34;,&#34;dropCap&#34;:false}&#34;&gt;在本节中，我们重点介绍了我们在 Up CD 发行中看到的一些效果。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;999058e8-8874-4d16-8f7a-a83453c710fc&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;0db3f75b-6b43-4ee0-a633-2107164328ba&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-increase-in-adoption-of-automation&#34;&gt;自动化采用率提高&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;81c03cf4-a88d-456b-956d-345ed1207d77&#34;,&#34;dropCap&#34;:false}&#34;&gt;发布我们的在内部 CD 体验中，我们开始看到行为的重大转变。正如我们所希望的那样，工程师开始接受它，我们立即看到采用率不断上升：具体来说，我们看到在 12 个月内，部署的服务数量自动从不到 10% 增加到近 70%。&lt;/p &gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;da865d9b-8cd1-4b7f-89b4-7697ba25a6a1&#34;,&#34;dropCap&#34;:false}&#34;&gt;因为服务是部署更频繁，将错误归因于特定提交也变得更容易，因为每次部署的提交更少。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;81a68168-44ab-49a0-81e8-8eed3bb3faa6&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;32a3771a-c7b7-44c9-8bcc-df7ff865b0fa&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-no-increase-in-product-incidents&#34;&gt;生产事件没有增加&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9622779d-a4ce-4b5c-b46b-5ccd98fd9bfc&#34;,&#34;dropCap&#34;:false}&#34;&gt;同时增加部署率，我们很高兴发现生产事故的总体发生率并没有按比例增加。事实上，在 CD 采用率攀升的同一 12 个月期间，我们发现每 1,000 次代码更改报告的事件减少了 50% 以上。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fa7585e0-fd4a-45aa-b0ce-ce005dab8a03&#34;,&#34;dropCap&#34;:false}&#34;&gt;因为有在进行此操作的同时，正在进行其他工作（将在单独的博客文章中详细说明），但我们不能声称这一壮举的原因。然而，很明显，我们实际上可以成功实现我们的愿景，让我们的工程师自动将他们的服务部署到生产中，而不会增加事件的频率或严重性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6803f4b7-a11d-4350-ae69-073a1d2bf9b0&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;4ba89534-ea4d-4885-ad2e-9455570acc02&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-new-challenges&#34;&gt;新挑战&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;23ca1a99-2324-4829-a083-9f3c0a91c73e&#34;,&#34;dropCap&#34;:false}&#34;&gt;但是，我们也开始看到新的挑战。特别是，我们发现对许多服务共享的 monorepo 代码进行更改的风险（例如，如果有人更改了所有服务共享的公共 RPC 库）突然增加，因为更改会更快地部署到所有受影响的服务服务（其中许多部署可能会并行发生）。这意味着，如果此类更改引入了 CI 期间未发现的重大错误，自动化将能够同时快速破坏大量服务。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;767e8f7a-966c-4ef0-9a7c-5e9104c7e51d&#34;,&#34;dropCap&#34;:false}&#34;&gt;一般来说，一些服务将有适当的机制来检测问题并自动回滚部署，但不太可能为每个服务自动检测到问题。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;71c6fb58-2ac5-4d37-9e7e-6cb8598f3633&#34;,&#34;dropCap&#34;:false}&#34;&gt;因此原因，我们引入了利用有关跨服务提交的信号的功能，因此，如果服务的某些重要部分未成功部署，则该提交被认为是有问题的。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;00b23978-4571-460a-92f4-a782269c5b35&#34;,&#34;dropCap&#34;:false}&#34;&gt;获取为尽可能明确信号，我们根据我们的 &lt;a href=&#34;https://www.uber.com/en-DK/blog/cinnamon-using- century-old-tech 交错部署有风险的、跨领域的提交-构建平均负载脱落器/” target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;内部服务分层&lt;/a&gt;。最初，Up CD 将其部署到我们最不重要的服务组中，当成功部署了足够比例的服务时，它会前进到下一层。如果很大一部分服务开始遇到问题，部署就会停止，提交作者会收到有关潜在问题的通知。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;348ac0ab-613e-4876-b91e-5bc645892513&#34;,&#34;dropCap&#34;:false}&#34;&gt;使用此部署战略，我们将此类风险变更对客户影响的风险降低到可接受的水平，同样重要的是，通过为特别危险的变更提供额外的防护措施，增加了对自动化生产部署的信任。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;dd3ed3f4-335f-478f-87f8-5584b5f2cfa7&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;f5fc3556-386e-4041-88f0-ea1d2d18ac1b&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-key-metrics&#34;&gt;关键指标&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1c08aa92-93ac-4ba8-9c2f-3673f1ed22c3&#34;,&#34;dropCap&#34;:false}&#34;&gt;量化根据该项目的结果，我们在下表中总结了该项目期间一些关键指标的变化：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/table&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;a39c5ffa-469b-42b7-b2c3-7893d8732790&#34;,&#34;hasFixedLayout&#34; :false,&#34;head&#34;:[],&#34;body&#34;:[],&#34;foot&#34;:[]}&#34; class=&#34;wp-block-tablealigncenter&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong &gt;公制&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Before Up CD（2022 年首次）&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Post Up CD（2024 年 3 月）&lt;/strong&gt;&lt;/ td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;# 服务&lt;/td&gt;&lt;td&gt;4,500&lt;/td&gt;&lt;td&gt;5,000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Monorepo 提交/周&lt;/ td&gt;&lt;td&gt;5,600&lt;/td&gt;&lt;td&gt;11,000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;生产部署/周&lt;/td&gt;&lt;td&gt;7,000&lt;/td&gt;&lt;td&gt;50,000&lt;/td&gt;&lt;td&gt;50,000&lt;/td&gt;&lt;td&gt;7,000&lt;/td&gt;&lt;td&gt;50,000 td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;协调部署 CD 的百分比（部分或全部用于生产）&lt;/td&gt;&lt;td&gt;66%&lt;/td&gt;&lt;td&gt;95%&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;完全自动化生产的服务百分比&lt;/td&gt;&lt;td&gt;7%&lt;/td&gt;&lt;td&gt;65%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure &gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fb9afa3f-4c1e-43a7-98b8-0f43ab66e12e&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;f9591e21-ed1d-4577-904a-2070fe685878&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-conclusion&#34;&gt;结论&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;66d53876-9133-4bd9-abd1-6c347d60b215&#34;,&#34;dropCap&#34;:false}&#34;&gt;正如我们在前面的章节中已详细介绍，我们改进的 CD 系统 Up CD 体现了向自动化的战略转变，将其作为我们部署方法的核心原则。我们认为，这种转变对于管理我们运营的复杂性和规模、提高安全性和效率、将代码交付到生产的负担从我们的工程师转移到自动化至关重要。&lt;/p&gt;</description>
      <pubDate>Mon, 26 Aug 2024 14:02:56 +0000</pubDate>
    </item>
    <item>
      <title>【Upgrading Uber’s MySQL Fleet to version 8.0】将 Uber 的 MySQL Fleet 升级到版本 8.0</title>
      <link>https://www.uber.com/blog/upgrading-ubers-mysql-fleet/</link>
      <description>【&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;60acf9d6-828e-4f1e-872a-178b7cc750b2&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;882fb73e-b365-4b01-9fda-28f98237326f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;At Uber, our MySQL fleet is the backbone of our data infrastructure, supporting a vast array of operations critical to our platform. Starting 2023, we embarked on a significant journey to upgrade our MySQL fleet to the latest version (i.e., MySQL v8.0).&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c394f3c4-3ef7-4e57-b5d9-62b1cc40cdd4&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In this blog post, we delve into the motivations, challenges, and solutions involved in this monumental upgrade process and how we completed this upgrade without impacting our Service Level Objectives (SLO).&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;25742638-7a56-4f60-b7af-04fe6768d229&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;63c58d84-5459-421e-81fa-c7738e8177fe&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-motivation-for-the-upgrade&#34;&gt;Motivation for the Upgrade&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f9678a19-b904-4e62-99a1-a7025b702f7b&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Several compelling factors drove our decision to transition from MySQL v5.7 to v8.0:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;104b20e7-65ec-4ccb-a992-b1e24581230a&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Addressing End-of-Life Concerns:&lt;/strong&gt; As MySQL v5.7 reached its &lt;a href=&#34;https://endoflife.date/mysql&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;extended support end date&lt;/a&gt;, continuing to use it exposed us to potential security vulnerabilities and a lack of ongoing bug fixes. This posed a significant risk to the stability and integrity of our data.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2941ee6b-d043-48a9-a097-1bbcd87cfd3f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Boosting Performance and Concurrency:&lt;/strong&gt; MySQL v8.0 offered a compelling proposition with its promise of &lt;a href=&#34;https://www.mysql.com/why-mysql/benchmarks/mysql/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;substantial performance enhancements&lt;/a&gt;. Optimizations in indexing and resource utilization translated to faster query execution speeds and improved concurrency handling. This directly translates to a smoother user experience for our customers.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8af84401-ad3a-44a9-8cc4-07a4c0448b02&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Unlocking New Functionality:&lt;/strong&gt; Beyond performance improvements, v8.0 introduced valuable features like support for window functions, enhanced JSON handling, and better spatial data capabilities. These features opened new avenues for data manipulation and analysis, empowering us to unlock new functionalities within our platform.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c303ae73-b702-40ad-b3c3-0a0514c82464&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Password Rotation:&lt;/strong&gt; The introduction of &lt;a href=&#34;https://www.percona.com/blog/using-mysql-8-dual-passwords/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;“Dual passwords”&lt;/a&gt; in v8.0 allows for smoother password rotations during security incidents, minimizing service disruptions.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;df3fa9aa-e335-424f-aec2-b2941f9adff4&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Streamlining Operational Efficiency:&lt;/strong&gt; Managing schema changes is an ongoing task. The &lt;a href=&#34;https://dev.mysql.com/blog-archive/mysql-8-0-instant-add-and-drop-columns/#:~:text=This%20feature%20enables%20users%20to,28%20(before%20MySQL%208.0.&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Instant ADD Column feature&lt;/a&gt; in v8.0 significantly streamlined this process. This translates to reduced downtime during schema alterations, improving our overall operational efficiency.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;56cb723c-c1ea-40f6-bd33-67517ffac03c&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c5044528-9d8d-4443-afbc-c3a1022281af&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-a-closer-look-uber-s-massive-mysql-infrastructure&#34;&gt;A Closer Look: Uber’s Massive MySQL Infrastructure&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;23a61f19-01e0-4d93-a52e-83383b6e9de0&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Before delving into the details of our MySQL upgrade journey, it’s essential to grasp the scale and complexity of Uber’s MySQL infrastructure:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;973f1de4-5c7a-44e7-a7fc-1e3eeb6f40db&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Scale:&lt;/strong&gt; Uber’s MySQL infrastructure comprises over 2,100 clusters, distributed across 19 production zones spanning three regions. With over 16,000 nodes, our infrastructure forms the backbone of Uber’s data storage and processing capabilities.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b09daa3b-0a48-4f0a-ac54-edf085832835&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Data Volume and Query Load:&lt;/strong&gt; Supporting multiple Petabytes of data and serving approximately 3 million queries per second, our MySQL infrastructure handles a vast amount of data and traffic on a daily basis.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0d351fdc-4676-4a12-ad4d-7457cb9c8a1a&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Clustered Architecture:&lt;/strong&gt; Each MySQL cluster consolidates multiple MySQL processes running on individual nodes. While each node within a cluster contains identical data, they are strategically distributed across different data centers to ensure data availability and support failover mechanisms.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;19f0b6d8-9f83-4c55-b67e-bb971a773086&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Primary-Secondary Replication:&lt;/strong&gt; Within each cluster, a primary node manages all write traffic, while secondary nodes replicate data asynchronously. This architecture ensures redundancy and fault tolerance, allowing for seamless failover in the event of primary node failure.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;22f59725-0eec-4e40-8ae8-9af85dcdc4ff&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Upgrade Considerations:&lt;/strong&gt; Notably, while MySQL v5.7 primary to MySQL v8.0 read replica replication is compatible, the reverse scenario—MySQL v8.0 primary to MySQL v5.7 read replica replication—is not supported. This distinction played a crucial role in our upgrade planning and execution strategy.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5038b58e-aab0-46cd-b367-a41b26163f68&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f79b46c0-6999-4224-a124-6e5a625ebcd0&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-navigating-challenges-in-the-mysql-upgrade-journey&#34;&gt;Navigating Challenges in the MySQL Upgrade Journey&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6e7d0f02-b8a7-43cc-8106-b48d2a934318&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The sheer scale of Uber’s MySQL infrastructure, with over 2,100 clusters and 16,000+ nodes spread across regions and zones, presented a significant challenge. Manual upgrades were simply not an option. To address this, we devised a comprehensive, multi-step upgrade strategy that could be executed efficiently across diverse environments, requiring meticulous coordination.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8150ea73-6fd7-4f40-8b2c-f3404b16408c&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Another key concern was minimizing downtime during the upgrade process.&amp;nbsp; Maintaining Service Level Objectives (SLOs) and Service Level Agreements (SLAs) was paramount to ensure uninterrupted service for our users.&amp;nbsp; Our solution involved meticulous planning and a focus on minimizing downtime throughout the upgrade process.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b476a159-124e-4d0c-adb7-4dd44d332d8e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Compatibility with existing applications and services was another hurdle. Ensuring seamless integration with our existing ecosystem necessitated extensive testing, including thorough validation and regression checks.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;31150427-4c72-4493-a1c6-9c5d2f492335&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To further enhance system reliability and minimize service disruptions, we implemented automated rollback mechanisms. These mechanisms could automatically revert upgrades in case of failures or compatibility issues.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3a90e1bd-10ff-44b7-9375-67ab03de08a5&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Finally, minimizing manual intervention during the upgrade process was crucial.&amp;nbsp; To streamline operations and reduce the risk of human error, we developed robust automated workflows. These workflows automated repetitive tasks, enabling seamless upgrades across thousands of clusters and nodes.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2c9b7a88-cf29-4d44-9baf-e7ba77053054&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Overall, upgrading to v8.0 seemed like a huge win for everyone at Uber, as it promised a security boost, performance leap, and exciting new features. But manually tackling this across thousands of clusters? No, thanks! We needed a smarter solution–a solution that scaled. Enter our custom-built automation system, designed to guide each cluster meticulously through the multi-step upgrade process, all without a single human touch.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;16305bca-5ba5-4d5b-b64b-11e7aa74fbe9&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;32124e14-9595-4d2f-b721-b846bb08f1e4&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-path-to-mysql-upgrade-uber&#34;&gt;Path to MySQL Upgrade @ Uber&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b49d1e56-68e1-4e61-8584-43386d7f92f4&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;When we were considering an upgrade of our MySQL clusters from version 5.7 to 8.0, there were two possible approaches that we could have taken:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;7a465c71-7bb1-4fbd-8769-87c304a61cf3&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-side-by-side-upgrade&#34;&gt;Side-By-Side Upgrade&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;26736006-754c-48eb-8551-0f4cdcb194b6&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In a side-by-side upgrade, the new version of MySQL (in this case, v8.0) is installed alongside the existing version (v5.7). This approach involves setting up a separate server where the new version is deployed and configured. Once the new server is ready, traffic is gradually redirected to the new version, allowing for a smooth transition.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;e69a8dff-3738-4a81-b390-6f37428a7d73&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-in-place-upgrade&#34;&gt;In Place Upgrade&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1b3ca997-f6aa-45a6-bb16-cea051799c2b&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;An in-place upgrade involves directly upgrading the existing MySQL installation to the new version (v8.0) without setting up a separate environment. This process typically requires stopping the MySQL service, performing the upgrade, and then restarting the service. In-place upgrades are simpler in terms of setup, but may involve longer downtime compared to side-by-side upgrades. Additionally, there is less room for rollback in case of unexpected issues during the upgrade process.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;f85c83e9-cf32-4ecd-b719-892897583736&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-chosen-strategy&#34;&gt;Chosen Strategy&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0dbcb8f7-3dcc-4810-881e-ef2e6a54cebe&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;After careful consideration and thorough evaluation of the advantages and disadvantages, we made the decision to &lt;strong&gt;opt for a side-by-side upgrade approach&lt;/strong&gt; from v5.7 nodes to v8.0, rather than pursuing an in-place upgrade. This choice was made in anticipation of the following benefits:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:true,&amp;quot;hash&amp;quot;:&amp;quot;88d328f6-ea24-4773-b5d1-1d342aa371cd&amp;quot;,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Minimal downtime:&lt;/strong&gt; With a side-by-side upgrade, we can keep the old MySQL 5.7 nodes running while we set up the new MySQL 8.0 nodes. This means we can gradually migrate the applications to the new nodes without any significant downtime.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Reduced risk:&lt;/strong&gt; Since the old MySQL 5.7 nodes remain operational, we can roll back to it if there are any issues with the new MySQL 8.0 nodes. This reduces the risk of performance degradation, data loss &lt;em&gt;(only until the maintenance phase in the upgrade process)&lt;/em&gt; or other issues that may arise during the upgrade process.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Better testing:&lt;/strong&gt; By running the new MySQL 8.0 nodes alongside the old MySQL 5.7 nodes, we can test the new nodes with &lt;strong&gt;production read-only application load&lt;/strong&gt; before making the switch. This can help us identify any issues and ensure that everything works as expected before we complete the migration.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;25c373ac-ccd9-4285-bc4a-d53bae971969&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;9a536828-d504-4f47-ab73-662f46b5046d&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXd0hJcLDpWUQ-XXtX5ir-K31hrBrAV5xEGHRNh3pVrUKfXdyBvw95eOfAvdnuUGqEygD4ERT0mINBrNBnT1PQI4awgvSiH8rI64n65HCQ5lOPwHAww6raRta6ZlGLHvS844LdQWv3eVTOQH6uPYgARnScES?key=t0GcRSII3lNuPS3m8oPG7g&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 1: Side-by-side upgrade of MySQL cluster.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;59fa88ff-48a8-4e12-ade6-dc92f590749e&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c7c9e2b8-9744-49b2-9636-d5c6b50c60fe&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To address these challenges, we developed a system designed to completely automate the transition of a MySQL cluster from v5.7 to v8.0. Our automated alerts and monitoring system actively oversees the process to ensure a seamless transition and promptly alerts of any issues that may arise.&lt;br&gt;&lt;br&gt;A high-level overview of the upgrade process includes:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:true,&amp;quot;hash&amp;quot;:&amp;quot;a683c382-923d-4ae0-bb32-7e64dfbacfef&amp;quot;,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Node Replication:&lt;/strong&gt; For each MySQL v5.7 node in the cluster, a corresponding MySQL v8.0 replica node is added in the same region/zone, maintaining the distribution consistency between v5.7 and v8.0 nodes.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Soak Period:&lt;/strong&gt; A monitoring period of approximately one week allows us to observe the system’s performance and detect any degradation or SLA breaches caused by the newer version nodes.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Traffic Diversion:&lt;/strong&gt; Once the soak period concludes, MySQL v5.7 replica nodes are disabled to divert traffic away from them.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Primary Node Promotion:&lt;/strong&gt; A MySQL v8.0 node is promoted to primary status for the cluster.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Removal of Old Nodes:&lt;/strong&gt; Finally, all MySQL v5.7 nodes are removed, completing the upgrade to MySQL v8.0.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1dcf3c5e-f10d-47a0-8f63-f5dd1a80e297&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The above process will be broken into &lt;strong&gt;4&lt;/strong&gt;&lt;strong&gt; Stages&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;56821089-3eb4-4826-818c-7df0475d9cb6&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Pre-Maintenance:&lt;/strong&gt; During this stage, the cluster is prepared for upgrade by adding MySQL v8.0 nodes as replicas, which operate alongside existing v5.7 nodes to serve real production traffic.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;9aaa8666-2d5f-4356-ae9c-edb33939b890&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;eecdf69d-f52a-4be4-bc98-e8b3f732b8ea&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfXQGfGyf0Jm0cvg16o7CUl1x-mBjvtbi6AemObjFyEUuWgINVVtCjXWIIUZudZeziyB04Zkqg8U1rcLKDxWq0zJRTxj0nAyWU2YY0Dp_bcUvwX5VeLSKl-xOl8az7UHCMkwYBbxzzfD9TtBL69Dme-R6I?key=t0GcRSII3lNuPS3m8oPG7g&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 2: Pre Maintenance Stage.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;77f356d2-19db-455b-88f8-31dc28ccf0b3&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c184fb57-7330-4a22-a85b-60b435e8d1bb&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;System Monitoring:&lt;/strong&gt; Newly added MySQL v8.0 nodes serve as replicas, allowing for real production traffic to be monitored. Any deviations from expected behavior are noted and addressed.&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;53873ffa-943f-415a-ae95-8b8547225213&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Maintenance:&lt;/strong&gt; Once the system monitoring stage is successfully completed, a MySQL v8.0 node is promoted to primary status, and system stability is monitored.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cef6ebcd-8b1b-4351-a586-326244676406&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;e169bbe5-827b-4bba-81ce-1c08c26aa2ad&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfab4oapkF9tbiXzKmqg7cyQeArU7oAfiA-R0PzqDixAil7Nd9PHYEh5jvjoPMEJ8GPy0bttCqFktr7de6TyFQBVSzYwU6z-DRrrQKiusdETIhdHW34ktae59msa5BR2ojPGivYPcjLXNtOPr72eI1gtnA2?key=t0GcRSII3lNuPS3m8oPG7g&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 3: Maintenance Stage.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;995efbab-2d50-4bbc-b39d-5d730ded41ae&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;77941dbf-b5e7-4217-a63f-89e076bab187&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Post-Maintenance:&lt;/strong&gt; In the final stage, non-replicating MySQL v5.7 nodes are deleted, resulting in a pure MySQL v8.0 cluster.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;549abce5-c17d-4f0a-b4b3-2bcf3116356a&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;8651255e-e7cb-41de-ab05-90e8f9ecd71e&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfhWi7L4bW3mXRuWkdlDba-7AWqAv5RBGXMYCD6t2OcQKtypKwJ86xX0RlD_iT8Fvq-PvANhyarq7QqWtVdBrWdIVcE7jK3yKyMNGayOuWbDJEZCLGqavgd-8W2aLEpAgdT6UtyZ2Uz9Kvow90oTqUP71k?key=t0GcRSII3lNuPS3m8oPG7g&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 4: Post Maintenance Stage.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;618d71f4-2ec5-4874-a8aa-f4d8f3ad0d07&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;853be8e0-f7d0-448c-9e70-131f1309e4ed&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-rollback&#34;&gt;Rollback&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;839bf08d-1951-4a92-ba25-925c233c630e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;While there was a gradual rollout strategy, we still needed the ability to rollback at every step and we needed the observability to identify signals to indicate when a rollback was needed. We prioritized minimizing risks and ensuring data integrity throughout the upgrade process. Until the Maintenance Step, all actions are fully reversible without any risk of data loss. Should our customers encounter service degradation due to factors like high latency or CPU usage, we can seamlessly and instantly revert to MySQL v5.7 with absolutely no data loss. This means that by simply deleting or disabling the MySQL v8.0 replica nodes introduced during the pre-maintenance stage, we can swiftly return to the previous state.&lt;br&gt;&lt;br&gt;However, it’s essential to note that once a MySQL v8.0 node is promoted to primary status, replication to a MySQL v5.7 node ceases. This transition marks a point of no return in terms of compatibility with MySQL v5.7.&lt;br&gt;&lt;br&gt;Attempting to revert to a MySQL v5.7 primary after this stage would entail potential data loss, as any changes made on the MySQL v8.0 primary would not be replicated back to the MySQL v5.7 nodes. Therefore, careful consideration and thorough testing preceded the promotion of a MySQL v8.0 node to primary status, ensuring a smooth transition while safeguarding our data integrity.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7182bf38-f97b-4e1a-96e0-fb7c3eeed8be&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;856d0902-a4ff-4925-9543-73716f508bdc&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-stakeholder-communication&#34;&gt;Stakeholder Communication&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8e2cff11-2e1c-4302-a435-4f41c5b83698&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We systematically advanced through each tier, commencing from tier 5 and descending to tier 0. At every tier, we organized the clusters into manageable batches, ensuring a systematic and controlled transition process. Before embarking on each stage of the version upgrade, we actively involved the on-call teams responsible for each cluster, fostering collaboration and ensuring comprehensive oversight.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cb2bb9c7-2d30-4483-9e31-059b2940cfe6&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;This deliberate and structured methodology allowed us to effectively navigate the complexities inherent in upgrading our MySQL fleet. By prioritizing coordination, communication, and teamwork, we successfully traversed through each tier, seamlessly transitioning to MySQL version 8.0.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6599955d-eec4-40f3-865b-dc82f284d99c&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;03d6ffa8-077b-4b34-b117-10645493c6e6&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-issues-experienced&#34;&gt;Issues Experienced&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;9656fd45-8553-4738-ad37-d434190efeb8&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-change-of-query-execution-plan-in-v8-0-vs-5-7&#34;&gt;Change of Query Execution Plan in v8.0 vs. 5.7&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c4aff53f-ef98-4f99-85f9-582601be013d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Upgrading to MySQL 8.0 brought not only new features, but also some unexpected tweaks in query execution plans for certain clusters. This resulted in increased latencies and resource consumption, potentially impacting user experience. This happened for the cluster which powers all the dashboards running at Uber. To address this issue, we collaborated with Percona, identified a patch fix, and successfully implemented it for the affected clusters. The resolution ensured the restoration of optimized query performance and resource efficiency in alignment with the upgraded MySQL version.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;c423d04e-4518-4e0b-bd21-017ab51872cc&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-unsupported-queries-amp-configurations&#34;&gt;Unsupported Queries &amp;amp; Configurations&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;65216b29-9b54-44fc-9071-63db49774877&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The transition from MySQL version 5.7 to version 8.0 introduced syntax changes for certain keywords, disrupting some queries in production. Additionally, a notable portion of our existing clusters did not have the “STRICT_TRANS_TABLES” SQL mode enabled, which is a default setting in MySQL 8.0. This absence resulted in errors for many customers during the upgrade procedure. Similarly, challenges emerged with the “ONLY_FULL_GROUP_BY” SQL mode, underscoring the necessity for meticulous configuration modifications to ensure compatibility with the specifications of the upgraded version.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;18aa61ce-18b3-4d7d-a50e-25f393f436c4&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-previous-default-collation&#34;&gt;Previous Default Collation&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;be751173-4e45-4aa2-bcdc-b31153bb0c55&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In MySQL 8.0, the default character set is &lt;em&gt;utf8mb4&lt;/em&gt;, accompanied by the &lt;em&gt;utf8mb4_0900_ai_ci&lt;/em&gt; collation. In contrast, the preceding MySQL 5.7 version employed the &lt;em&gt;utf8mb4_unicode_520_ci&lt;/em&gt; collation, lacking support for the latest &lt;em&gt;utf8mb4_0900_ai_ci&lt;/em&gt;. This transition introduced challenges in aligning collation settings across the upgraded system.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;ad81fb37-389a-4abf-8fc8-f6eabeb7b8ee&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-client-library-incompatibility&#34;&gt;Client Library Incompatibility&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d7da950f-af91-4a49-9ec1-82f2c236aa34&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Library Upgrade Requirement:&lt;/strong&gt; Many existing client libraries were incompatible with MySQL v8.0. To address this, we had to upgrade these libraries, conduct thorough testing to ensure their proper functionality in a staging environment, and subsequently proceed with the primary upgrade. This step was crucial to guarantee a seamless transition without compromising client interactions.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ac9dbd1f-e3c9-444d-a48a-326385448b2e&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e07aeff5-6cdf-4a8c-b9fb-02ce1cd82f92&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-improvements&#34;&gt;Improvements&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d5ac48e9-50c9-46b0-bc04-61cab2fb933a&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;With the new version, we can harness the following performance improvements:&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;848a606c-defc-4ec1-b226-09ca039f7768&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-server-side-performance-improvements-for-mysql-v8-0&#34;&gt;Server-side Performance Improvements for MySQL v8.0&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0715b954-b10a-45a6-9dbf-9b0a1ddb296e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;29% improvement in p99 latency for 1 million inserts at 1024 threads.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;id&amp;quot;:1093800,&amp;quot;sizeSlug&amp;quot;:&amp;quot;full&amp;quot;,&amp;quot;linkDestination&amp;quot;:&amp;quot;none&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;bb293eff-27fc-48ad-90d3-e04fa093d2ce&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;771&#34; height=&#34;497&#34; src=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure5.png&#34; alt=&#34;&#34; class=&#34;wp-image-1093800&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=771,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure5.png 771w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure5.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure5.png 768w&#34; sizes=&#34;(max-width: 771px) 100vw, 771px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 5: MySQL v5.7 v/s 8.0 at 1M inserts 1024 Threads.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d10801c7-d372-4a65-b214-ece840b174cb&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;949e69fd-6c33-4e6c-858a-33529212d908&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;33% improvement in p99 latency for 1 million reads at 1024 threads.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;id&amp;quot;:1093801,&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;sizeSlug&amp;quot;:&amp;quot;full&amp;quot;,&amp;quot;linkDestination&amp;quot;:&amp;quot;none&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;8968bca1-2493-4f2b-8c79-f370d7577d88&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter size-full is-resized&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;805&#34; height=&#34;477&#34; src=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure6.png&#34; alt=&#34;&#34; class=&#34;wp-image-1093801&#34; style=&#34;width:700px;height:auto&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=805,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure6.png 805w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure6.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure6.png 768w&#34; sizes=&#34;(max-width: 805px) 100vw, 805px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 6: MySQL v5.7 v/s 8.0 at 1M Reads 1024 Threads.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ebb6dea1-1a43-496f-8895-3695a5cde0da&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6f841653-3f44-4276-883c-31318f03c5bd&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;47% improvement in p99 latency for 1 million updates at 1024 threads.&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;id&amp;quot;:1093802,&amp;quot;sizeSlug&amp;quot;:&amp;quot;full&amp;quot;,&amp;quot;linkDestination&amp;quot;:&amp;quot;none&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;e1445c23-edba-4e0d-bd18-4a0ab796c7d9&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter size-full&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;763&#34; height=&#34;406&#34; src=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure7.png&#34; alt=&#34;&#34; class=&#34;wp-image-1093802&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=763,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure7.png 763w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure7.png 300w&#34; sizes=&#34;(max-width: 763px) 100vw, 763px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 7: MySQL v5.7 v/s 8.0 at 1M Updates 1024 Threads.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;aeb5ce88-1615-49d8-979e-9eeb96dc40a9&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;530de3d6-c798-4f86-972f-e441dc983e1a&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-client-side-improvements-due-to-mysql-upgrade&#34;&gt;Client-side Improvements due to MySQL Upgrade&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cb168ec7-70fb-49dd-95d6-283dd7a4d387&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;~94% reduction in overall database lock time.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;id&amp;quot;:1093803,&amp;quot;sizeSlug&amp;quot;:&amp;quot;large&amp;quot;,&amp;quot;linkDestination&amp;quot;:&amp;quot;none&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;05a3a43d-e0e4-468d-a938-04b62ca7900e&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter size-large&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1024&#34; height=&#34;533&#34; src=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure8-1024x533.png&#34; alt=&#34;&#34; class=&#34;wp-image-1093803&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure8.png 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure8.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure8.png 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1536,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure8.png 1536w, https://blog.uber-cdn.com/cdn-cgi/image/width=2048,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Figure8.png 2048w&#34; sizes=&#34;(max-width: 1024px) 100vw, 1024px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 8: Reduced locktime post upgrade.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;011e51c2-188d-4d94-a269-bb67fc70dd00&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;0b53420b-b489-4d10-8fd3-2da84ab5b830&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;~78% reduction in query time for some queries.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;id&amp;quot;:1093804,&amp;quot;sizeSlug&amp;quot;:&amp;quot;large&amp;quot;,&amp;quot;linkDestination&amp;quot;:&amp;quot;none&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;21989855-e431-47a9-a9b1-703890102ce8&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter size-large&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1024&#34; height=&#34;576&#34; src=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Screenshot-2024-08-07-at-11.26.13-1024x576.png&#34; alt=&#34;&#34; class=&#34;wp-image-1093804&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Screenshot-2024-08-07-at-11.26.13.png 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Screenshot-2024-08-07-at-11.26.13.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Screenshot-2024-08-07-at-11.26.13.png 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1536,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Screenshot-2024-08-07-at-11.26.13.png 1536w, https://blog.uber-cdn.com/cdn-cgi/image/width=2048,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Screenshot-2024-08-07-at-11.26.13.png 2048w, https://blog.uber-cdn.com/cdn-cgi/image/width=1080,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Screenshot-2024-08-07-at-11.26.13.png 1080w, https://blog.uber-cdn.com/cdn-cgi/image/width=2127,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Screenshot-2024-08-07-at-11.26.13.png 2127w&#34; sizes=&#34;(max-width: 1024px) 100vw, 1024px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 9: Improved query time post upgrade.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a36bb790-c54d-43a4-8f81-557097005ec2&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7c201bd7-ac87-4cb0-b3d3-3e7d1a9a9469&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-learning-and-takeaways&#34;&gt;Learning and Takeaways&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;505d1270-4506-4ef8-98f0-4f85de5546cb&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Throughout the comprehensive upgrade journey, which spanned over a year, our dedicated team of engineers from the MySQL Team flawlessly navigated through a series of critical stages.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7896f8e7-4899-4292-ba3d-8428ff061cf4&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The monumental task of transitioning our entire fleet to MySQL 8.0 encompassed not only staging clusters, but also production clusters supporting Uber and internal tool instances. This extensive upgrade underscores the indispensable role played by our observability platform, testing regimen, and robust rollback capabilities.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2e9f47b3-6cbc-48d0-a914-f9a9e411a13c&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Our meticulous testing procedures and phased rollout strategy proved invaluable, allowing us to unearth and address potential issues early on. By adopting this approach, we significantly mitigated the risk of encountering new failure modes during the primary upgrade phase.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7a5d2346-27cb-4194-9c1e-40d6a0698725&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;73c7b088-62d9-4073-8397-32ed14e4061e&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-conclusion&#34;&gt;Conclusion&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;e1234f21-2f9f-4123-947a-07aa956d0a6c&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The journey of upgrading our MySQL fleet at Uber to version 8.0 has been challenging but rewarding. By embracing the latest technology and leveraging automation, we’ve not only ensured the security and performance of our database infrastructure, but also demonstrated our commitment to innovation and excellence.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;dfbebed4-133b-4b7e-95ed-07934d330346&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The upgrade process, meticulously planned and executed by our dedicated team of engineers, underscores our unwavering dedication to maintaining the highest standards of reliability and efficiency. Through careful consideration of the benefits and challenges, we successfully navigated the transition, mitigating risks and minimizing disruptions to our services.&lt;br&gt;&lt;br&gt;As we reflect on this milestone achievement, we extend our gratitude to all those who contributed to the success of this endeavor. Together, we remain committed to pushing boundaries, driving innovation, and shaping the future of technology at Uber.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;dfbebed4-133b-4b7e-95ed-07934d330346&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;Cover Photo Attribution: “&lt;a href=&#34;https://www.flickr.com/photos/43767277@N04/5059176446&#34;&gt;North Lake CA&lt;/a&gt;” by &lt;a href=&#34;https://www.flickr.com/photos/pachecophotography/&#34;&gt;Pacheco&lt;/a&gt; licensed as Attribution-NoDerivs.&lt;/p&gt;】&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;60acf9d6-828e-4f1e-872a-178b7cc750b2&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-introduction&#34;&gt;简介&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;882fb73e-b365-4b01-9fda-28f98237326f&#34;,&#34;dropCap&#34;:false}&#34;&gt;在优步，我们的 MySQL 集群是我们数据基础设施的支柱，支持对我们平台至关重要的大量操作。从 2023 年开始，我们踏上了将 MySQL 机群升级到最新版本（即 MySQL v8.0）的重要旅程。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c394f3c4-3ef7-4e57-b5d9-62b1cc40cdd4&#34;,&#34;dropCap&#34;:false}&#34;&gt;在此博客中在这篇文章中，我们深入探讨了这一重大升级过程中涉及的动机、挑战和解决方案，以及我们如何在不影响我们的服务级别目标 (SLO) 的情况下完成此次升级。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;25742638-7a56-4f60-b7af-04fe6768d229&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;63c58d84-5459-421e-81fa-c7738e8177fe&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-motivation-for-the-upgrade&#34;&gt;升级动机&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f9678a19-b904-4e62-99a1-a7025b702f7b&#34;,&#34;dropCap&#34;:false}&#34;&gt;几个引人注目的因素促使我们决定从 MySQL v5.7 过渡到 v8.0：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;104b20e7-65ec-4ccb-a992-b1e24581230a&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;解决生命周期终止问题：&lt;/strong&gt; MySQL v5.7 达到其&lt;a href=&#34;https://endoflife.date/mysql&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;延长支持结束日期&lt;/a&gt;，继续使用它会让我们面临潜在的安全漏洞，并且缺乏持续的错误修复。这对我们数据的稳定性和完整性构成了重大风险。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2941ee6b-d043-48a9-a097-1bbcd87cfd3f&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;提高性能和并发性：MySQL v8.0 提供了一个令人信服的主张，承诺&lt;a href=&#34;https://www.mysql.com/why-mysql/benchmarks/mysql/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;显着的性能增强&lt;/a&gt;。索引和资源利用率的优化转化为更快的查询执行速度和改进的并发处理。这直接为我们的客户带来了更流畅的用户体验。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8af84401-ad3a-44a9-8cc4-07a4c0448b02&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;解锁新功能：除了性能改进之外，v8.0 还引入了有价值的功能，例如对窗口函数的支持、增强的 JSON 处理和更好的空间数据功能。这些功能为数据操作和分析开辟了新途径，使我们能够在平台内解锁新功能。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c303ae73-b702-40ad-b3c3-0a0514c82464&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;密码轮换：&lt;a href=&#34;https://www.percona.com/blog/using-mysql-8-dual-passwords/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;介绍v8.0 中的“双密码”&lt;/a&gt;可以在安全事件期间实现更顺畅的密码轮换，从而最大程度地减少服务中断。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;df3fa9aa-e335-424f-aec2-b2941f9adff4&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;简化运营效率：管理架构更改是一项持续的任务。 &lt;a href=&#34;https://dev.mysql.com/blog-archive/mysql-8-0-instant-add-and-drop-columns/#:~:text=This%20feature%20enables%20users% 20to,28%20(%20MySQL%208.0.&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;v8.0 中的即时 ADD 列功能&lt;/a&gt; 显着简化了此过程。这意味着架构更改期间的停机时间减少，提高我们的整体运营效率。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;56cb723c-c1ea-40f6-bd33-67517ffac03c&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c5044528-9d8d-4443-afbc-c3a1022281af&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-a-closer-look-uber-s-massive-mysql-infrastruct&#34;&gt;近距离观察：Uber 的大规模 MySQL 基础设施&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;23a61f19-01e0-4d93-a52e-83383b6e9de0&#34;,&#34;dropCap&#34;:false}&#34;&gt;在深入研究之前了解我们的 MySQL 升级过程的详细信息，了解 Uber MySQL 基础设施的规模和复杂性至关重要：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;973f1de4-5c7a-44e7-a7fc-1e3eeb6f40db&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;规模：Uber 的 MySQL 基础设施由 2,100 多个集群组成，分布在跨越三个地区的 19 个生产区。我们的基础设施拥有超过 16,000 个节点，构成了 Uber 数据存储和处理能力的支柱。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b09daa3b-0a48-4f0a-ac54-edf085832835&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;数据量和查询负载：我们的 MySQL 基础设施支持多个 PB 数据并每秒处理约 300 万次查询，可处理大量数据每天的数据量和流量。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0d351fdc-4676-4a12-ad4d-7457cb9c8a1a&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;集群架构：每个MySQL集群都整合了在各个节点上运行的多个MySQL进程。虽然集群中的每个节点都包含相同的数据，但它们战略性地分布在不同的数据中心，以确保数据可用性并支持故障转移机制。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;19f0b6d8-9f83-4c55-b67e-bb971a773086&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;主从复制：在每个集群中，主节点管理所有写入流量，而辅助节点异步复制数据。这种架构确保了冗余和容错，允许在主节点发生故障时实现无缝故障转移。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;22f59725-0eec-4e40-8ae8-9af85dcdc4ff&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;升级注意事项：&lt;/strong&gt;值得注意的是，虽然 MySQL v5.7 主数据库到 MySQL v8.0 只读副本复制是兼容的，但不支持相反的场景（MySQL v8.0 主数据库到 MySQL v5.7 只读副本复制）。这种区别在我们的升级规划和执行策略中发挥了至关重要的作用。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5038b58e-aab0-46cd-b367-a41b26163f68&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f79b46c0-6999-4224-a124-6e5a625ebcd0&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-navigating-challenges-in-the-mysql-upgrade-journey&#34;&gt;应对 MySQL 升级过程中的挑战&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6e7d0f02-b8a7-43cc-8106-b48d2a934318&#34;,&#34;dropCap&#34;:false}&#34;&gt;规模庞大Uber 的 MySQL 基础设施拥有超过 2,100 个集群和 16,000 多个节点，分布在各个地区和专区，这带来了巨大的挑战。手动升级根本不是一个选择。为了解决这个问题，我们设计了一个全面的、多步骤的升级策略，可以在不同的环境中高效执行，需要细致的协调。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8150ea73-6fd7-4f40-8b2c-f3404b16408c&#34;,&#34;dropCap&#34;:false}&#34;&gt;另一个关键问题最大限度地减少升级过程中的停机时间。  维护服务级别目标 (SLO) 和服务级别协议 (SLA) 对于确保为用户提供不间断服务至关重要。  我们的解决方案涉及精心规划并注重最大限度地减少整个升级过程中的停机时间。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;核心/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b476a159-124e-4d0c-adb7-4dd44d332d8e&#34;,&#34;dropCap&#34;:false}&#34;&gt;与现有应用程序和服务的兼容性是另一个障碍。确保与我们的无缝集成现有的生态系统需要进行广泛的测试，包括彻底的验证和回归检查。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;31150427-4c72-4493-a1c6-9c5d2f492335&#34;,&#34;dropCap&#34;:false}&#34;&gt;进一步增强系统可靠性并最大限度地减少服务中断，我们实施了自动回滚机制。这些机制可以在出现故障或兼容性问题时自动恢复升级。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3a90e1bd-10ff-44b7-9375-67ab03de08a5&#34;,&#34;dropCap&#34;:false}&#34;&gt;最后，最小化升级过程中的手动干预至关重要。  为了简化操作并降低人为错误的风险，我们开发了强大的自动化工作流程。这些工作流程自动执行重复任务，从而实现跨数千个集群和节点的无缝升级。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2c9b7a88-cf29-4d44-9baf-e7ba77053054&#34;,&#34;dropCap&#34;:false}&#34;&gt;总体而言，正在升级v8.0 对 Uber 的每个人来说似乎都是一个巨大的胜利，因为它承诺了安全性提升、性能飞跃和令人兴奋的新功能。但是要在数千个集群中手动解决这个问题吗？不，谢谢！我们需要一个更智能的解决方案——一个可扩展的解决方案。输入我们定制的自动化系统，该系统旨在精心指导每个集群完成多步骤升级过程，所有这些都无需人工干预。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;16305bca-5ba5-4d5b-b64b-11e7aa74fbe9&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;32124e14-9595-4d2f-b721-b846bb08f1e4&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-path-to-mysql-upgrade-uber&#34;&gt;MySQL 升级@ Uber 的路径&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b49d1e56-68e1-4e61-8584-43386d7f92f4&#34;,&#34;dropCap&#34;:false}&#34;&gt;当我们考虑将我们的 MySQL 集群从版本 5.7 升级到 8.0，我们可以采取两种可能的方法：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;7a465c71-7bb1-4fbd-8769-87c304a61cf3&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-side-by-side-upgrade&#34;&gt;并行升级&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;26736006-754c-48eb-8551-0f4cdcb194b6&#34;,&#34;dropCap&#34;:false}&#34;&gt;在一侧-by-side 升级，新版本的 MySQL（在本例中为 v8.0）与现有版本（v5.7）。此方法涉及设置一个单独的服务器，在其中部署和配置新版本。一旦新服务器准备就绪，流量将逐渐重定向到新版本，从而实现平稳过渡。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;e69a8dff-3738-4a81-b390-6f37428a7d73&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-in-place-upgrade&#34;&gt;就地升级&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1b3ca997-f6aa-45a6-bb16-cea051799c2b&#34;,&#34;dropCap&#34;:false}&#34;&gt;一个 in- place升级涉及直接将现有MySQL安装升级到新版本（v8.0），无需设置单独的环境。此过程通常需要停止 MySQL 服务，执行升级，然后重新启动服务。就地升级在设置方面更简单，但与并行升级相比可能会涉及更长的停机时间。此外，如果升级过程中出现意外问题，回滚的空间也较小。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;f85c83e9-cf32-4ecd-b719-892897583736&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-chosen-strategy&#34;&gt;选择的策略&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0dbc​​b8f7-3dcc-4810-881e-ef2e6a54cebe&#34;,&#34;dropCap&#34;:false}&#34;&gt;经过慎重考虑经过对优缺点的全面评估，我们决定从 v5.7 节点到 v8.0 &lt;strong&gt;选择并行升级方法&lt;/strong&gt;，而不是进行就地升级。做出这一选择是因为预期有以下好处：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:true,&#34;hash&#34;:&#34;88d328f6-ea24-4773-b5d1-1d342aa371cd&#34;,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;最短停机时间&lt;/strong&gt;：通过并行升级，我们可以保持当我们设置新的 MySQL 8.0 节点时，旧的 MySQL 5.7 节点正在运行。这意味着我们可以逐步将应用程序迁移到新节点，而不会造成任何重大停机。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;降低风险：&lt;/strong&gt;由于旧的 MySQL 5.7 节点仍然可以运行，我们可以滚动如果新的 MySQL 8.0 节点有任何问题，请返回此。这可以降低性能下降、数据丢失（仅在升级过程中的维护阶段之前）或升级过程中可能出现的其他问题的风险。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;更好的测试：&lt;/strong&gt;通过与旧的 MySQL 5.7 一起运行新的 MySQL 8.0 节点节点，我们可以在进行切换之前使用&lt;strong&gt;生产只读应用程序负载&lt;/strong&gt;测试新节点。这可以帮助我们识别任何问题d 在完成迁移之前确保一切按预期运行。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;25c373ac-ccd9-4285-bc4a-d53bae971969&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;9a536828-d504-4f47-ab73-662f46b5046d&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXd0hJcLDpWUQ-XXtX5ir-K31hrBrAV5xEGHRNh3pVrUKfXdyBvw95eOfAvdnuUGqEygD4ERT0mINBrNBnT1PQI4awgvSiH8rI 64n65HCQ5lOPwHAww6raRta6ZlGLHvS844LdQWv3eVTOQH6uPYgARnScES?key=t0GcRSII3lNuPS3m8oPG7g&#34; alt=&#34;&#34;referrerpolicy =&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图1：MySQL集群的并行升级。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;59fa88ff-48a8-4e12-ade6-dc92f590749e&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c7c9e2b8-9744-49b2-9636-d5c6b50c60fe&#34;,&#34;dropCap&#34;:false}&#34;&gt;解决这些问题面对挑战，我们开发了一个系统，旨在完全自动化 MySQL 集群从 v5.7 到 v8.0 的过渡。我们的自动警报和监控系统会积极监督该过程，以确保无缝过渡并及时对可能出现的任何问题发出警报。&lt;br&gt;&lt;br&gt;升级过程的高级概述包括：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:true,&#34;hash&#34;:&#34;a683c382-923d-4ae0-bb32-7e64dfbacfef&#34;,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;节点复制：&lt;/strong&gt;对于集群中的每个 MySQL v5.7 节点，在同一region/zone中添加相应的MySQL v8.0副本节点，保持v5.7和v8.0节点之间的分布一致性。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;浸泡期&lt;/strong&gt;：大约一周的监测期使我们能够观察到系统性能并检测由较新版本节点引起的任何降级或 SLA 违规。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;流量分流：&lt;/strong&gt;浸泡期结束后，MySQL v5.7 副本节点被禁用以转移流量。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;主节点升级&lt;/strong&gt;：MySQL v8.0 节点升级为主节点状态对于集群。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;删除旧节点&lt;/strong&gt;：最后，删除所有 MySQL v5.7 节点，完成MySQL v8.0的升级。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1dcf3c5e-f10d-47a0-8f63-f5dd1a80e297&#34;,&#34;dropCap&#34;:false}&#34;&gt;以上流程将分为&lt;strong&gt;4&lt;/strong&gt;&lt;strong&gt;阶段&lt;/strong&gt;：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;56821089-3eb4-4826-818c-7df0475d9cb6&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;预维护：&lt;/strong&gt;在此阶段，集群通过添加 MySQL v8.0 节点作为副本来准备升级，这些节点与现有 v5.7 节点一起运行以服务实际生产流量。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;9aaa8666-2d5f-4356-ae9c-edb33939b890&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;eecdf69d-f52a-4be4-bc98-e8b3f732b8ea&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfXQGfGyf0Jm0cvg16o7CUl1x-mBjvtbi6AemObjFyEUuWgINVVtCjXWIIUZudZeziyB04Zkqg8U1rcLKDxWq0z JRTxj0nAyWU2YY0Dp_bcUvwX5VeLSKl-xOl8az7UHCMkwYBbxzzfD9TtBL69Dme-R6I?key=t0GcRSII3lNuPS3m8oPG7g&#34; alt=&#34; &#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 2：预维护阶段。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;77f356d2-19db-455b-88f8-31dc28ccf0b3&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c184fb57-7330-4a22-a85b-60b435e8d1bb&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;系统监控：新添加的MySQL v8.0节点作为副本，可以监控真实的生产流量。任何与预期行为的偏差都会被记录并解决。&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;53873ffa-943f-415a-ae95-8b8547225213&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;维护：&lt;/strong&gt;系统监控阶段成功完成后，将MySQL v8.0节点提升为主状态，并监控系统稳定性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cef6ebcd-8b1b-4351-a586-326244676406&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;e169bbe5-827b-4bba-81ce-1c08c26aa2ad&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfab4oapkF9tbiXzKmqg7cyQeArU7oAfiA-R0PzqDixAil7Nd9PHYEh5jvjoPMEJ8GPy0bttCqFktr7de6TyFQBVSzYwU6z-DRrrQKiusdETIhdHW34ktae59msa5BR2ojPGivYPcjLXNtOPr72eI1gtnA2?key=t0GcRSII3lNuPS3m8oPG7g&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图3：维护阶段。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;995efbab-2d50-4bbc-b39d-5d730ded41ae&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;77941dbf-b5e7-4217-a63f-89e076bab187&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;后期维护：&lt;/strong&gt;最后阶段，删除非复制的MySQL v5.7节点，形成纯MySQL v8.0集群。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;549abce5-c17d-4f0a-b4b3-2bcf3116356a&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34; :&#34;8651255e-e7cb-41de-ab05-90e8f9ecd71e&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent. com/docsz/AD_4nXfhWi7L4bW3mXRuWkdlDba-7AWqAv5RBGXMYCD6t2OcQKtypKwJ86xX0RlD_iT8Fvq-PvANhyarq7QqWtVdBrWdIVcE7jK3yKyMNGayOuWbDJEZCLGqavgd-8W2aLEpAgdT6 UtyZ2Uz9Kvow90oTqUP71k?key=t0GcRSII3lNuPS3m8oPG7g&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34; &gt;图4：后期维护阶段。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;618d71f4-2ec5-4874-a8aa-f4d8f3ad0d07&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;853be8e0-f7d0-448c-9e70-131f1309e4ed&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-rollback&#34;&gt;回滚&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;839bf08d-1951-4a92-ba25-925c233c630e&#34;,&#34;dropCap&#34;:false}&#34;&gt;虽然有在逐步推出策略中，我们仍然需要能够在每一步进行回滚，并且需要可观察性来识别信号以指示何时需要回滚。我们优先考虑在整个升级过程中最大限度地降低风险并确保数据完整性。在维护步骤之前，所有操作都是完全可逆的，没有任何数据丢失的风险。如果我们的客户由于高延迟或 CPU 使用率等因素而遇到服务降级，我们可以立即无缝恢复到 MySQL v5.7，绝对不会丢失数据。这意味着只需删除或禁用预维护阶段引入的 MySQL v8.0 副本节点即可e，我们可以快速返回到之前的状态。&lt;br&gt;&lt;br&gt;但是，需要注意的是，一旦 MySQL v8.0 节点升级为主状态，到 MySQL v5.7 节点的复制就会停止。此转换标志着与 MySQL v5.7 的兼容性方面的不归路。&lt;br&gt;&lt;br&gt;在此阶段之后尝试恢复到 MySQL v5.7 主数据库可能会导致数据丢失，因为对 MySQL 所做的任何更改v8.0 主节点不会复制回 MySQL v5.7 节点。因此，在将 MySQL v8.0 节点升级为主节点之前，我们进行了仔细的考虑和彻底的测试，以确保平稳过渡，同时保护我们的数据完整性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7182bf38-f97b-4e1a-96e0-fb7c3eeed8be&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;856d0902-a4ff-4925-9543-73716f508bdc&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-stakeholder-communication&#34;&gt;利益相关者沟通&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8e2cff11-2e1c-4302-a435-4f41c5b83698&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们系统地推进从第 5 层开始，一直到第 0 层。在每一层，我们都将集群组织成可管理的批次，确保系统化且受控的过渡过程。在开始版本升级的每个阶段之前，我们积极让负责每个集群的待命团队参与其中，促进协作并确保全面监督。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cb2bb9c7-2d30-4483-9e31-059b2940cfe6&#34;,&#34;dropCap&#34;:false}&#34;&gt;这是故意和结构化方法使我们能够有效地应对升级 MySQL 队列所固有的复杂性。通过优先考虑协调、沟通和团队合作，我们成功地穿越了每一层，无缝过渡到 MySQL 8.0 版本。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6599955d-eec4-40f3-865b-dc82f284d99c&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;03d6ffa8-077b-4b34-b117-10645493c6e6&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-issues-experienced&#34;&gt;遇到的问题&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;9656fd45-8553-4738-ad37-d434190efeb8&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-change-of-query-execution-plan-in-v8-0-vs-5-7&#34;&gt;v8.0 与 5.7 中查询执行计划的更改&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c4aff53f-ef98-4f99-85f9-582601be013d&#34;,&#34;dropCap&#34;:false}&#34;&gt;升级到 MySQL 8.0 不仅带来了新功能，还对某些集群的查询执行计划进行了一些意想不到的调整。这导致延迟和资源消耗增加，可能会影响用户体验。为所有仪表板提供支持的集群就发生了这种情况为了解决这个问题，我们与 Percona 合作，确定了一个补丁修复，并成功地为受影响的集群实施了该解决方案，确保恢复优化的查询性能和资源效率，与升级的 MySQL 版本保持一致。&lt;/ p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;c423d04e-4518-4e0b-bd21-017ab51872cc&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-unsupported-queries-amp-configurations&#34;&gt;不支持的查询和配置&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;65216b29-9b54-44fc-9071-63db49774877&#34;,&#34;dropCap&#34;:false}&#34;&gt;从MySQL 5.7 版到 8.0 版引入了某些关键字的语法更改，从而扰乱了生产中的某些查询。此外，我们现有集群的一个显着部分没有启用“STRICT_TRANS_TABLES”SQL 模式，这是 MySQL 8.0 中的默认设置。这种缺失导致许多客户在升级过程中出现错误。同样，“ONLY_FULL_GROUP_BY”SQL 模式也出现了挑战，凸显了细致的配置修改的必要性，以确保与升级版本的规范兼容。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;18aa61ce-18b3-4d7d-a50e-25f393f436c4&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-previous-default-collat​​ion&#34;&gt;上一个默认排序规则&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;be751173-4e45-4aa2-bcdc-b31153bb0c55&#34;,&#34;dropCap&#34;:false}&#34;&gt;在 MySQL 8.0 中，默认字符集为 &lt;em&gt;utf8mb4&lt;/em&gt;，并伴有 &lt;em&gt;utf8mb4_0900_ai_ci&lt;/em&gt; 排序规则。相比之下，之前的 MySQL 5.7 版本采用 &lt;em&gt;utf8mb4_unicode_520_ci&lt;/em&gt; 排序规则，缺乏对最新 &lt;em&gt;utf8mb4_0900_ai_ci&lt;/em&gt; 的支持。这一转变在协调升级后的系统中的排序规则设置方面带来了挑战。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;ad81fb37-389a-4abf-8fc8-f6eabeb7b8ee&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-client-library-incompatibility&#34;&gt;客户端库不兼容&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d7da950f-af91-4a49-9ec1-82f2c236aa34&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;库升级要求：许多现有的客户端库与 MySQL v8.0 不兼容。为了解决这个问题，我们必须升级这些库，进行彻底的测试以确保它们的正常功能临时环境中的离子性，然后继续进行主要升级。此步骤对于保证无缝过渡而不影响客户交互至关重要。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ac9dbd1f-e3c9-444d-a48a-326385448b2e&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e07aeff5-6cdf-4a8c-b9fb-02ce1cd82f92&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-improvements&#34;&gt;改进&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d5ac48e9-50c9-46b0-bc04-61cab2fb933a&#34;,&#34;dropCap&#34;:false}&#34;&gt;使用新的版本中，我们可以利用以下性能改进：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;848a606c-defc-4ec1-b226-09ca039f7768&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-server-side-performance-improvements-for-mysql-v8-0&#34;&gt;MySQL v8.0 的服务器端性能改进&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0715b954-b10a-45a6-9dbf-9b0a1ddb296e&#34;,&#34;dropCap&#34;:false}&#34;&gt;提升 29%在 1024 个线程上进行 100 万次插入的 p99 延迟。&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;id&#34;:1093800,&#34;sizeSlug&#34;:&#34;full&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;align&#34;:&#34;中心”，“散列”：“bb293eff-27fc-48ad-90d3-e04fa093d2ce”，“alt”：“”}“类=“aligncenter size-full”&gt;&lt;img加载=“惰性”解码=“异步”宽度= “771”高度=“497”src=“https://blog.uber-cdn.com/cdn-cgi/image/width=2160，质量= 80，onerror=重定向，format=auto/wp-content/uploads /2024/08/Figure5.png&#34; alt=&#34;&#34; class=&#34;wp-image-1093800&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=771,quality=80 ,onerror=重定向,format=auto/wp-content/uploads/2024/08/Figure5.png 771w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80, onerror=重定向，format=auto/wp-content/uploads/2024/08/Figure5.png 300w，https://blog.uber-cdn.com/cdn-cgi/image/width=768，quality=80，onerror =重定向，格式=自动/wp-content/uploads/2024/08/Figure5.png 768w”尺寸=“（最大宽度：771px）100vw，771px”referrerpolicy=“no-referrer”&gt;&lt;figcaption class=“wp” -element-caption&#34;&gt;图 5：MySQL v5.7 vs/s 8.0 在 1M 插入 1024 个线程。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d10801c7-d372-4a65-b214-ece840b174cb&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;949e69fd-6c33-4e6c-858a-33529212d908&#34;,&#34;dropCap&#34;:false}&#34;&gt;改进 33% 100 万次读取的 p99 延迟1024 个线程。&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;id&#34;:1093801,&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;sizeSlug&#34;:&#34; full&#34;,&#34;linkDestination&#34;:&#34;无&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;8968bca1-2493-4f2b-8c79-f370d7577d88&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter size-full is-resized&#34;&gt;&lt;img 加载=“惰性”解码=“异步”宽度=“805”高度=“477”src=“https://blog.uber-cdn.com/cdn-cgi/image/width= 2160，质量= 80，onerror =重定向，格式=自动/wp-content/uploads/2024/08/Figure6.png“ alt =“”类=“ wp-image-1093801”样式=“宽度：700px;高度：自动” srcset =“https://blog.uber-cdn.com/cdn-cgi/image/width=805，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/08/Figure6 .png 805w，https://blog.uber-cdn.com/cdn-cgi/image/width=300，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/08/Figure6。 png 300w，https://blog.uber-cdn.com/cdn-cgi/image/width=768，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/08/Figure6.png 768w&#34;sizes=&#34;(max-width: 805px) 100vw, 805px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 6：1M 读取时的 MySQL v5.7 与 8.0 1024 个线程。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ebb6dea1-1a43-496f-8895-3695a5cde0da&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6f841653-3f44-4276-883c-31318f03c5bd&#34;,&#34;dropCap&#34;:false}&#34;&gt;提升 47%在 1024 个线程上进行 100 万次更新的 p99 延迟。&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;id&#34;:1093802,&#34;sizeSlug&#34;:&#34;full&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;align&#34;:&#34;中心”，“散列”：“e1445c23-edba-4e0d-bd18-4a0ab796c7d9”，“alt”：“”}“类=“aligncenter size-full”&gt;&lt;img加载=“惰性”解码=“异步”宽度= “763”height =“406”src =“https://blog.uber-cdn.com/cdn-cgi/image/width=2160，质量= 80，onerror =重定向，format = auto/wp-content/uploads /2024/08/Figure7.png&#34; alt=&#34;&#34; class=&#34;wp-image-1093802&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=763,quality=80 ,onerror=重定向,format=auto/wp-content/uploads/2024/08/Figure7.png 763w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80, onerror=重定向，format=auto/wp-content/uploads/2024/08/Figure7.png 300w&#34; 尺寸=&#34;(最大宽度：763px) 100vw，763px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34; wp-element-caption&#34;&gt;图 7：MySQL v5.7 与 8.0 相比，1M 更新 1024 个线程。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;aeb5ce88-1615-49d8-979e-9eeb96dc40a9&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;530de3d6-c798-4f86-972f-e441dc983e1a&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-client-side-improvements-due-to-mysql-upgrade&#34;&gt;MySQL 升级带来的客户端改进&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cb168ec7-70fb-49dd-95d6-283dd7a4d387&#34;,&#34;dropCap&#34;:false}&#34;&gt;~94%减少总体数据库锁定时间。&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;id&#34;:1093803,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;align&#34;:&#34;中心”，“散列”：“05a3a43d-e0e4-468d-a938-04b62ca7900e”，“alt”：“”}“类=“aligncenter size-large”&gt; &lt;img加载=“惰性”解码=“异步”宽度= “1024”高度=“533”src=“https://blog.uber-cdn.com/cdn-cgi/image/width=2160，质量= 80，onerror=重定向，format=auto/wp-content/uploads /2024/08/Figure8-1024x533.png&#34; alt=&#34;&#34; class=&#34;wp-image-1093803&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=1024,质量=80，onerror=重定向，format=auto/wp-content/uploads/2024/08/Figure8.png 1024w，https://blog.uber-cdn.com/cdn-cgi/image/width=300，quality= 80，onerror=重定向，format=auto/wp-content/uploads/2024/08/Figure8.png 300w，https://blog.uber-cdn.com/cdn-cgi/image/width=768，quality=80 ,onerror=重定向,format=auto/wp-content/uploads/2024/08/Figure8.png 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1536,quality=80, onerror=重定向，format=auto/wp-content/uploads/2024/08/Figure8.png 1536w，https://blog.uber-cdn.com/cdn-cgi/image/width=2048，quality=80，onerror =重定向，格式=自动/wp-content/uploads/2024/08/Figure8.png 2048w“尺寸=”（最大宽度：1024px）100vw，1024px“referrerpolicy=”no-referrer&#34;&gt;&lt;figcaption class=“wp” -element-caption&#34;&gt;图 8：升级后锁定时间缩短。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;011e51c2-188d-4d94-a269-bb67fc70dd00&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;0b53420b-b489-4d10-8fd3-2da84ab5b830&#34;,&#34;dropCap&#34;:false}&#34;&gt;~78%减少某些查询的查询时间。&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;id&#34;:1093804,&#34;sizeSlug&#34;:&#34;large&#34;,&#34;linkDestination&#34;:&#34;none&#34;,&#34;align&#34;:&#34;中心”，“哈希”：“21989855-e431-47a9-a9b1-703890102ce8”，“alt”：“”}“类=“aligncenter size-large”&gt;&lt;img加载=“惰性”解码=“异步”宽度= “1024”高度=“576”src=“https://blog.uber-cdn.com/cdn-cgi/image/width=2160，质量= 80，onerror=重定向，format=auto/wp-content/uploads /2024/08/截图-2024-08-07-at-11.26.13-1024x576.png&#34; alt=&#34;&#34; class=&#34;wp-image-1093804&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=1024,质量=80，onerror=重定向，格式=auto/wp-content/uploads/2024/08/Screenshot-2024-08-07-at-11.26.13.png 1024w，https://blog.uber-cdn.com /cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/08/Screenshot-2024-08-07-at-11.26.13.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=768，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/08/Screenshot-2024-08- 07-at-11.26.13.png 768w，https://blog.uber-cdn.com/cdn-cgi/image/width=1536，quality=80，onerror=redirect，format=auto/wp-content/uploads /2024/08/Screenshot-2024-08-07-at-11.26.13.png 1536w，https://blog.uber-cdn.com/cdn-cgi/image/width=2048，quality=80，onerror=重定向，format=auto/wp-content/uploads/2024/08/Screenshot-2024-08-07-at-11.26.13.png 2048w，https://blog.uber-cdn.com/cdn-cgi/image /width=1080，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/08/Screenshot-2024-08-07-at-11.26.13.png 1080w，https://blog. uber-cdn.com/cdn-cgi/image/width=2127，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/08/Screenshot-2024-08-07-at-11.26。 13.png 2127w&#34;sizes=&#34;(max-width: 1024px) 100vw, 1024px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 9：升级后查询时间得到改善。&lt;/图标题&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a36bb790-c54d-43a4-8f81-557097005ec2&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7c201bd7-ac87-4cb0-b3d3-3e7d1a9a9469&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-learning-and-takeaways&#34;&gt;学习与收获&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;505d1270-4506-4ef8-98f0-4f85de5546cb&#34;,&#34;dropCap&#34;:false}&#34;&gt;贯穿全面在长达一年多的升级过程中，我们来自 MySQL 团队的专业工程师团队完美地度过了一系列关键阶段。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7896f8e7-4899-4292-ba3d-8428ff061cf4&#34;,&#34;dropCap&#34;:false}&#34;&gt;艰巨的任务将我们整个集群过渡到 MySQL 8.0 的过程不仅包括临时集群，还包括支持 Uber 和内部工具实例的生产集群。此次大规模升级凸显了我们的可观测平台、测试方案和强大的回滚能力所发挥的不可或缺的作用。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2e9f47b3-6cbc-48d0-a914-f9a9e411a13c&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们细致的测试事实证明，程序和分阶段推出策略非常宝贵，使我们能够及早发现并解决潜在问题。通过采用这种方法，我们显着大大降低了主要升级阶段遇到新故障模式的风险。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7a5d2346-27cb-4194-9c1e-40d6a0698725&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;73c7b088-62d9-4073-8397-32ed14e4061e&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-conclusion&#34;&gt;结论&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;e1234f21-2f9f-4123-947a-07aa956d0a6c&#34;,&#34;dropCap&#34;:false}&#34;&gt;的旅程将 Uber 的 MySQL 机群升级到版本 8.0 充满挑战，但也很有回报。通过采用最新技术和利用自动化，我们不仅确保了数据库基础设施的安全性和性能，而且还体现了我们对创新和卓越的承诺。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;dfbebed4-133b-4b7e-95ed-07934d330346&#34;,&#34;dropCap&#34;:false}&#34;&gt;升级过程由我们专业的工程师团队精心策划和执行，突显了我们坚定不移地致力于维持最高标准的可靠性和效率。通过仔细考虑收益和挑战，我们成功地完成了过渡，降低了风险并最大程度地减少了对我们服务的干扰。&lt;br&gt;&lt;br&gt;当我们反思这一里程碑式的成就时，我们向所有为成功做出贡献的人表示感谢的这一努力。我们将共同致力于突破界限、推动创新并塑造 Uber 的技术未来。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;dfbebed4-133b-4b7e-95ed-07934d330346&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;封面照片归属：“&lt;a href=&#34;https://www.flickr.com/photos/43767277@N04/5059176446&#34;&gt;加利福尼亚州北湖&lt;/ a&gt;”，作者：&lt;a href=&#34;https://www.flickr.com/photos/pachecophotography/&#34;&gt;Pacheco&lt;/a&gt;，获得 Attribution-NoDerivs 许可。&lt;/p&gt;</description>
      <pubDate>Thu, 08 Aug 2024 07:02:52 +0000</pubDate>
    </item>
    <item>
      <title>【Enabling Security for Hadoop Data Lake on Google Cloud Storage】为 Google Cloud Storage 上的 Hadoop Data Lake 启用安全性</title>
      <link>https://www.uber.com/blog/securing-hadoop-on-gcp/</link>
      <description>【&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;83f76446-3dcf-4f7d-b5a7-154eb25d1edb&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c7bfda9f-ff82-42fb-8a6e-bc82e5f5f75c&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;As part of Uber’s cloud journey, we are migrating the on-prem &lt;a href=&#34;https://www.uber.com/blog/uber-big-data-platform/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Apache Hadoop® based data lake&lt;/a&gt; along with analytical and machine learning workloads to GCP™ infrastructure platform. The &lt;a href=&#34;https://www.uber.com/blog/modernizing-ubers-data-infrastructure-with-gcp/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;strategy&lt;/a&gt; involves replacing the storage layer, HDFS, with GCS Object Storage (PaaS) and running the rest of the tech stack (YARN, Apache Hive™, Apache Spark™, Presto®, etc.) on GCP Compute Engine (IaaS).&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3d6de626-2a7c-473c-afef-21090761263e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;A typical cloud adoption strategy involves using cloud-native components and integrating existing IAM with cloud IAM (e.g., federation, identity sync, etc.) (Figure 1.ii). Our strategy is somewhat unique: we continue to leverage part of the existing stack as is (except HDFS) and integrate with GCS (Figure 1.iii). This introduces technical challenges in the following two areas from security perspective:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:true,&amp;quot;hash&amp;quot;:&amp;quot;19a292c2-fb5a-4551-8463-bb35157ee818&amp;quot;,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Moving to the public cloud requires a different approach to security than on-premise deployments. Hence, we have to develop new IAM controls around storage of data in GCS during the integration.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;The tech stack to be migrated onto GCP IaaS continues to use the Hadoop security model (Kerberos-based auth, Delegation Tokens and ACLs). We would need to make this work with GCS Object Storage by bridging the differences between HDFS and GCS (GCP IAM) security models.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;b3789f83-33e6-4bb8-a013-14a4c26231d2&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;d8a2f8bb-4d35-4df5-baf7-73263bf0eb24&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdiB4wgUnBdG6Pk3BXzkiaxOQDkhc9LLZEgEmtnzEI6zyKquK1fKaIa5UsyG96bHZAa3_JgWOwMk893WUyppsWcqVAHgMHQ081-Ryk89PvNkdhhW3Pi2s8cCSM8vs4xVt8s8JtigbYbwZ9c2To3e6QWxa3z?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 1: Cloud Data Lake IAM Systems compared to Existing and GCP Native Stack.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1577a7e2-19d9-4ade-8111-da1c27766834&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;428fa1b6-ce1e-47de-a3ab-0da0cf21202c&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We have built several systems and integrations to support Uber’s data lake migration to GCP. Fast forward to today, we run over 19% of analytical workloads on GCP. In this article, we will explore details of how the above two technical challenges were solved at scale to support the cloud migration.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;bc484c46-a46b-4749-8945-3be248bee415&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;562f3dbc-2b00-4a66-8b36-42bda397734c&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-existing-architecture&#34;&gt;Existing Architecture&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;787742af-6389-4b7d-9af9-b5f127d2abca&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;In the existing on-premise tech stack, more than an exabyte of data is stored across &lt;a href=&#34;https://www.uber.com/blog/scaling-hdfs/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;several HDFS clusters&lt;/a&gt;. A comprehensive suite of automation and ecosystem tools uniformly manages these clusters, encompassing &lt;a href=&#34;https://www.uber.com/blog/hadoop-container-blog/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;cluster operations&lt;/a&gt;, resource management, and security. For security, we rely heavily on Hadoop’s authentication and authorization (&lt;a href=&#34;https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html#ACLs_.28Access_Control_Lists.29&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;HDFS ACLs&lt;/a&gt;) features among others. On a high level we have the following components:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6ef21cb7-0d47-484e-991d-a857ca573391&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;width&amp;quot;:&amp;quot;700px&amp;quot;,&amp;quot;height&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;162bd9a0-904b-43bf-afe2-27db5ebb5255&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXedHmmt_--HqRzqQE-llYud_pnRy0Cer3pAMOvJJNfU0gGNGOAIUyH3MIDIhUmeNCjaAchDvh4Yj6WaVsLU_HLdOL_VvLkzPwfW8OtiV2CAvpi5RvwpcKc7P0W0DeBZSvmq5DjoVqZQoU5vAHJyzLqRgYPg?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 2: Existing Architecture with HDFS.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f29435d2-4541-4753-94c5-e1a8d7d7d385&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d9a327f2-560c-4872-a1c4-9d5d6f84def9&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;HDFS NameNode and DataNodes —&lt;/strong&gt; open source HDFS setup. Thousands of DataNodes store the actual data. NameNodes are responsible for all FileSystem operations, including authentication and authorization logic. HDFS NameNode is a Policy Decision Point (&lt;a href=&#34;https://csrc.nist.gov/glossary/term/policy_decision_point&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;PDP&lt;/a&gt;) and a Policy Enforcement Point (&lt;a href=&#34;https://csrc.nist.gov/glossary/term/policy_enforcement_point&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;PEP&lt;/a&gt;) (in NIST terminology).&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Data Compute —&lt;/strong&gt; collectively refers to workloads leveraging Presto, Spark, MapReduce, etc., that access data stored in HDFS. These workloads utilize a standard HDFS client that is maintained and approved for use internally within Uber.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;User Groups —&lt;/strong&gt; an internal system that makes Unix users and groups available on every production host – these are used for HDFS ACLs. UserGroups is a NIST Policy Information Point (&lt;a href=&#34;https://csrc.nist.gov/glossary/term/policy_information_point&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;PIP&lt;/a&gt;).&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Policy Management System —&lt;/strong&gt; manages access control policies for datasets. This system translates dataset policies (configured by Uber’s data platform users) into native HDFS ACLs, ensuring they are synchronized periodically.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;a href=&#34;https://web.mit.edu/kerberos/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;&lt;strong&gt;Kerberos&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; —&lt;/strong&gt; handles authentication among Hadoop components and its clients. You can read how we scaled Kerberos infrastructure in a &lt;a href=&#34;https://www.uber.com/blog/scaling-adoption-of-kerberos-at-uber/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;previous blog&lt;/a&gt; we published.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;dd9d3876-c9ea-4bb0-89fe-628e331734ab&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Before we go into details of how we implemented authentication and authorization to seamlessly replace HDFS with Google Cloud Storage, it is important to understand how authentication works in HDFS.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;08f5a1d7-154c-4d6c-8edc-3902ec82496c&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;67c39421-786b-4c1a-a862-f089999c6ca0&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXcaR9wil0Rw_W6FUj9njt5SGogKxT0swALrqq_Gry-jSKjL2Lzuxb8_-t1bCxtYxtgbqTQ3k4ywXh8OPueEmIuxK3tXDQ91GEfQvly0gWybJmoemfSbu65Z47YBTb-SBE_M_Weq1bUmAGvRL8b1mHu2yBs?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 3: HDFS Authentication Flow.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;085cf49f-f36c-483f-85af-029b5ec4ca3a&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;65fd1203-1c5e-4163-bcc6-5723bcf448d6&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Let’s consider a case where a User wants to submit a Spark job that reads files in an HDFS directory:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:true,&amp;quot;hash&amp;quot;:&amp;quot;6d1c9f26-e51a-4c2b-9adb-7c933e3d32ee&amp;quot;,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;User runs `&lt;em&gt;kinit&lt;/em&gt;` to authenticate with Kerberos. &lt;em&gt;Hdfs-cli&lt;/em&gt; obtains a Service Ticket for HDFS (&lt;em&gt;HDFS_ST&lt;/em&gt;) using Kerberos authentication.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;HDFS client connects to HDFS NameNode to generate an &lt;a href=&#34;https://blog.cloudera.com/hadoop-delegation-tokens-explained/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;HDFS Delegation Token&lt;/a&gt; (&lt;em&gt;User_DT&lt;/em&gt;), authenticating with &lt;em&gt;HDFS_ST&lt;/em&gt;. HDFS NameNode creates a new token and stores it in an internal store, so it can verify it as FileSystem calls are made.&amp;nbsp;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;a href=&#34;https://www.uber.com/blog/uscs-apache-spark/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Drogon&lt;/a&gt;/uSCS is Uber’s Spark as a Service solution. &lt;em&gt;User_DT&lt;/em&gt; is passed down to YARN by &lt;em&gt;drogon-cli&lt;/em&gt; as part of Spark job submission. YARN saves the &lt;em&gt;User_DT&lt;/em&gt; internally and keeps track of the token’s expiration with respect to the job lifetime to renew or cancel the token as needed.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;YARN launches the User’s job as a distributed Spark application with access to &lt;em&gt;User_DT&lt;/em&gt;. The job makes HDFS FileSystem calls (using HDFS Standard Client) against HDFS NameNode using &lt;em&gt;User_DT&lt;/em&gt;. The access control check to see if the user is authorized to access the file is performed by the NameNode.&amp;nbsp; The FileSystem operation involves asking for Block Access Token (BAT) to read blocks that constitute the file.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;The Job accesses file blocks in HDFS DataNodes, authenticating with BAT.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;375049c3-fb9b-4800-840b-0a2807b77617&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8dd73737-1405-4b4d-8f88-d4bd5f6c4fa3&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-cloud-architecture&#34;&gt;Cloud Architecture&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;809d7f0f-0a5a-469d-92cf-7c53ccbef343&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-layered-access-model&#34;&gt;Layered Access Model&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c8c761b5-1423-413a-bf4e-0ffea68e41ec&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Public cloud attack vectors are quite different from the on-premise deployments. We designed the access model to include security controls at the following&amp;nbsp; layers while setting up the new tech stack in the cloud.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ad1d8eb5-bb0e-4a96-9a00-79437ed60c66&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Layer 1 – Foundations: &lt;/strong&gt;At the lowest layer we managed the following foundational primitives through our declarative &lt;a href=&#34;https://en.wikipedia.org/wiki/Infrastructure_as_code&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;infrastructure-as-code&lt;/a&gt; (IaC) codebase:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;025fd7f3-0737-4995-bd9e-795000b35511&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;em&gt;Network policies&lt;/em&gt;: allowing Uber’s existing on-premise systems to connect with our systems in GCP and vice versa&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;em&gt;Service accounts and Project setup&lt;/em&gt;: providing access to automated systems in the layers&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;em&gt;Personnel Access&lt;/em&gt;: facilitating monitoring and debugging purposes as needed&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;59ec8bef-fe84-4e54-b74b-3468ae7b66d0&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Layer 2 – Data Mesh: &lt;/strong&gt;Once layer 1 controls have been set up, the Data Mesh system takes responsibility for automatically setting up and managing several hundreds of GCP projects and GCS buckets. The Data Mesh system compartmentalizes all the Hive tables in the data lake into different departments and types of datasets (raw data, modeled, temporary tables, etc.) across these GCS buckets and projects. This layer provides the flexibility to lockdown access to data within different departments, for example: restricting access to modeled data owned by the Finance department. Access control at this layer is also used for operational automation beyond security requirements, such as locking buckets during data migrations.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4102e048-8fc3-4b4d-b224-e12878fd6edc&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Layer 3 – Dataset Access: &lt;/strong&gt;Workloads themselves manifest in the form of &lt;a href=&#34;https://www.uber.com/blog/presto/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Presto queries&lt;/a&gt;, Spark jobs, and &lt;a href=&#34;https://www.uber.com/blog/managing-data-workflows-at-scale/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Piper pipelines&lt;/a&gt;. These workloads access specific data residing in GCS buckets. This layer provides finer-grained access beyond GCS buckets, for Hive databases, tables, and partitions. This layer is aware of users or headless service accounts that submit workloads and access policies that provide time-bounded access to Hive table data stored in GCS.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;4f0bab79-9cae-4c47-a177-c511f49160eb&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;a922ce60-022f-433e-8171-2c8a9a427479&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXf1EEwExHWMLGF2UbOS_xz6p5PVsfOQQOKfOfOvBOd1Dm6ojEHuleDncXIOJxVFsjZ8-N1jUD1f4nBH9VyMFdxRcKafuR-pgEepOB1gxZ52edAslXKZwy5p9HIhXSJ-ZC6krIevqz385niKVaqX7ksUkrWY?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 4: Layered Access Model&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;204ff6e0-678f-4e73-9a08-13c98a54aff2&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3c67f023-c077-4e42-aa36-45d816e227f2&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;This layered access model enables us to solve challenge A mentioned in the Introduction. Employees who use the Data platform for their various analytics, reporting, and machine learning use cases on a daily basis typically do not need access to resources at layers 1 and 2. Resources at these layers are managed through automation and service accounts instead of humans. Limiting access to human identities&amp;nbsp; reduces the probability of unauthorized access through human identity based attacks.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ff3a4d20-8bd9-41de-a67f-e1f970af4fbc&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We continue to use Apache Hudi™ and Apache Parquet™ for the data lake hosted in GCP. This enables us to continue using the existing &lt;a href=&#34;https://www.uber.com/en-US/blog/one-stone-three-birds-finer-grained-encryption-apache-parquet/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Parquet-based solution&lt;/a&gt; for finer-grained (column-level) access control and encryption features, as is for cloud, beyond the 3 layers described above. To establish layer 3, we built a new system called Storage Access Service, which we’ll cover in the next section.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;79509367-1daa-4172-a988-d293f1025472&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;bcfab320-6f9e-4e09-8900-913e6fe8f599&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-storage-access-service&#34;&gt;Storage Access Service&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;18af0255-6828-47cf-ab23-0783bda3cb02&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To enable secure access to data residing in GCS, we explored different options that fall into two categories: (a) rely on native GCP IAM constructs or (b) build an intermediary system to handle both authentication and authorization.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;72542200-d3f3-4bab-803f-8a298ca45d15&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Even though option (a) to rely on GCP&amp;nbsp;IAM looks like an obvious choice, we came across the following challenges and drawbacks:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:true,&amp;quot;hash&amp;quot;:&amp;quot;6c3ce8ab-9436-4f6a-832a-e0be283c7efa&amp;quot;,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;Our on-premise access control policies are modeled based on group memberships, so it would require us to build a new system to synchronize on-premise users and groups to GCP (in the absence of workload/workforce identity federation for GCS when we started the project). That system should be highly reliable with low latency sync, to avoid breaking existing features we have developed internally, such as just-in-time (JIT) access.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;A single GCS bucket has a &lt;a href=&#34;https://cloud.google.com/iam/quotas#limits&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;hard limit of 250&lt;/a&gt; on the number of IAM role bindings within a policy. Whereas on premise, we have more than 150,000 access control policies (translatable to GCP IAM role bindings) configured for data in HDFS. This could easily result in a management nightmare for the Data Mesh system to move dataset across buckets as they are approaching IAM policy limits. Keeping aside the cost incurred for moving terabytes or petabytes of data, it would have been difficult to make that system operational in a reliable way.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;As mentioned in the Layered Access Model section, we wanted to reduce the possibility of identity based attacks in the cloud. Synchronizing the entire footprint of tens of thousands of users and groups to GCP IAM and authoring new IAM role bindings for GCS bucket policies could increase the possible attack paths due to human identity based attacks and policy misconfigurations. The fewer moving parts we have, the easier it is to secure the system.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;160d3610-1d51-4076-b210-93604251211e&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Taking all tradeoffs into consideration we realized that we would get better flexibility and control if we developed an intermediary system that handles authentication and authorization (option b). In the cloud data lake stack, the key architectural change comes from replacing HDFS with GCS object storage. HDFS authentication works based on Kerberos and/or Delegation tokens. GCP Cloud Storage works based on cloud tokens (OAuth 2.0 AccessTokens for GCS). To facilitate exchanging Kerberos and Delegation Tokens for Access Tokens, we built an intermediary system called Storage&amp;nbsp;Access&amp;nbsp;Service&amp;nbsp;(SAS).&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ead454c3-e1d3-4e37-b645-c493e6d68284&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;SAS began as an internal fork of &lt;a href=&#34;https://github.com/GoogleCloudPlatform/gcp-token-broker&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;gcp-token-broker&lt;/a&gt; (an open source project that was initiated by GCP), that has since been heavily customized to meet Uber’s requirements. Today, SAS intercepts FileSystem calls initiated by HDFS Standard Client against GCS (using &lt;a href=&#34;https://github.com/GoogleCloudDataproc/hadoop-connectors/tree/v3.0.1/gcs&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;gcs hadoop-connector&lt;/a&gt;), authenticates it and injects GCP Access Tokens that provide time-bound access to perform specific operations on specific GCS prefixes or objects.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;be0a58bb-de55-45d8-b59a-b482f73201fa&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;id&amp;quot;:1093328,&amp;quot;sizeSlug&amp;quot;:&amp;quot;large&amp;quot;,&amp;quot;linkDestination&amp;quot;:&amp;quot;none&amp;quot;,&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;72e15d8c-fca0-4a38-9d73-10b07e91688b&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter size-large&#34;&gt;&lt;img loading=&#34;lazy&#34; decoding=&#34;async&#34; width=&#34;1024&#34; height=&#34;913&#34; src=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure-5-1024x913.png&#34; alt=&#34;&#34; class=&#34;wp-image-1093328&#34; srcset=&#34;https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure-5.png 1024w, https://blog.uber-cdn.com/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure-5.png 300w, https://blog.uber-cdn.com/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure-5.png 768w, https://blog.uber-cdn.com/cdn-cgi/image/width=1536,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure-5.png 1536w, https://blog.uber-cdn.com/cdn-cgi/image/width=2048,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/07/Figure-5.png 2048w&#34; sizes=&#34;(max-width: 1024px) 100vw, 1024px&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 5: Cloud Architecture.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c267725e-c716-4950-8de3-890d084edd2a&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ae251fdf-3f78-46f9-841a-5e0ebbbe40a0&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;On-premise users and groups are not synchronized to GCP IAM. All humans and workloads need to go through authentication with SAS to access data residing in GCS. SAS uses its own GCP service accounts to generate GCP Access Tokens. We leveraged Hadoop’s existing Delegation Token framework to facilitate token exchange. A Delegation Token (&lt;em&gt;User_DT&lt;/em&gt;) is generated before the launch of the distributed application and localized within each of the application containers. When the container (FileSystem client) needs to operate on data residing in GCS, the &lt;em&gt;User_DT&lt;/em&gt; is exchanged with SAS to get an Access Token (AT) that can be presented to GCS for authentication. See Figure 6 below for the updated authentication flow for GCS.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;6c70dff5-c4c5-4a1d-a99c-3d6717a07d6c&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;f6e715aa-959a-4644-b0ce-b9e6f723d3e9&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXcdP7kQS_VZqQe-qm3fZDaK7KBA4MjSgZu8Mrf9SAvYEfIh2PztHuHLuMxzmA_M9u7nlosoMXEFPZxV7qe6tjoHBg37suFQQgX-KbR4lOGFNpRyckD-ynU6RSLEneS6RJO_QPrwKd50dmAiRdi4u_-NTTOr?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 6: Cloud Authentication Flow.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;caea4011-29ce-4cbe-a3ce-bdc33f2c10c6&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2520d3e3-9c82-4b3a-8de6-76a15cbfad78&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;This new diagram is very similar to Figure 3 that presents the HDFS authentication flow, except for a few key changes (highlighted in red in figure 6):&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;76a589e7-a6fc-4d28-ad46-882486784fbf&amp;quot;,&amp;quot;ordered&amp;quot;:false,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;HDFS NameNode is replaced with SAS. In the new model SAS handles both authentication and authorization like NameNode.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;HDFS DataNodes are replaced with GCS, which stores objects instead of file blocks.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;SAS is now a Delegation Token provider (implements DelegationToken related interfaces such as &lt;a href=&#34;https://github.com/apache/hadoop/blob/branch-2.8.2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;AbstractDelegationTokenIdentifier&lt;/a&gt;). FileSystem clients need to request Delegation Tokens (&lt;em&gt;User_DT&lt;/em&gt;) from SAS.&amp;nbsp;&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;And finally, HDFS Block Access Token is replaced with GCP OAuth 2.0 Access Token for reading objects&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;47af226a-f082-4460-9475-6f7721798969&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To satisfy access control requirements, the existing Policy Management System (figure 5) was extended such that it maintains a translated version of dataset access policies that can be mapped to GCS prefixes. This enabled a consistent user experience using existing UI for managing access control for datasets regardless of data storage location (GCS or HDFS). Access control policies and dataset &amp;lt;&amp;gt; GCS prefix data are retrieved from Policy Management System and cached within SAS instances.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c3815696-b934-417c-8731-1581a8c22aab&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We designed an authorization plugin that leverages &lt;a href=&#34;https://www.uber.com/blog/attribute-based-access-control-at-uber/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;AuthFx library&lt;/a&gt; in SAS to evaluate access policies by checking whether a given identity can access the requested GCS resource. If the authorization decision evaluates to ALLOW, a GCP Access Token is returned. For DENY decision, an &lt;a href=&#34;https://github.com/apache/hadoop/blob/branch-2.8.2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/AuthorizationException.java&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;AuthorizationException&lt;/a&gt; is thrown by SAS. GCP supports “downscoping” an Access Token to specific GCS prefixes or objects with its &lt;a href=&#34;https://cloud.google.com/iam/docs/reference/sts/rest&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Security Token Service (STS) API&lt;/a&gt;. We leveraged this feature to mint time-bound Access Tokens for GCS prefixes by configuring &lt;a href=&#34;https://cloud.google.com/iam/docs/downscoping-short-lived-credentials#example-object-startswith&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;appropriate access boundaries and permissions&lt;/a&gt; using IAM conditions. The workload uses the Access Token returned by SAS to perform the specific operation on a certain GCS prefix for a predetermined period of time.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ae175ccc-712e-4464-b3dc-25c4aeb7ab81&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We developed a SAS client library and packaged it into the Uber supported standard HDFS client. The SAS client library provides higher level APIs that generate requests against SAS to request ATs and User_DTs (figure 6). Access Tokens are non-renewable–the SAS client library is intelligent enough to request a new Access Token if needed as long as User_DT is valid.&amp;nbsp;&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;63aa0684-e679-4c96-88f0-feafcf392412&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The SAS client library packaged within HDFS standard client made the security aspects of the cloud migration completely transparent to existing hundreds of thousands of distinct workloads executed daily. By building SAS, we were able to bridge the differences between GCS and HDFS security models and abstracted away the intricacies from users of the data platform (solving challenge B mentioned in the Introduction).&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;8077aa88-d312-4f77-bd61-5617761b3a76&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;20ddceb2-e7de-4741-b7b0-82fbbfb9c0d0&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-scaling-the-system&#34;&gt;Scaling the System&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f503755d-c054-45a5-be62-d01a30984f01&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;SAS intercepts all FileSystem calls to support authentication and authorization. Scaling this system to support existing use cases was critical to facilitate migration. We followed the same guiding principles that we follow for other projects: “Assess current scale requirements (X) through data-driven analysis, design for 10X scale and build MVP for 1X scale with a clear feasible path towards supporting 10X”.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;391a71d4-337d-4c48-8846-575cfde0afae&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;Based on existing metrics from several HDFS clusters that support the existing data lake, we estimated X as the ability to serve ~500,000 RPS (throughput) at p99 ~20ms latency. Note that a typical FileSystem operation in HDFS includes security auth in addition to the core FileSystem logic that involves inode changes, locking, etc depending on the operation. We budgeted 10% of 20 ms (i.e., 2 ms) as the latency requirement for auth. GCP’s Security Token Service API has a quota of &lt;a href=&#34;https://cloud.google.com/iam/quotas&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;6k rpm&lt;/a&gt; (note this is RPM – not RPS) for token exchange requests. During prototype testing, we observed the request latency was around ~50 ms.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fa000b3c-3db9-4be3-b02d-97302d0cd2ee&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/table&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7dd7e2a3-9fec-4ee0-8157-4d8c6f30abec&amp;quot;,&amp;quot;hasFixedLayout&amp;quot;:false,&amp;quot;head&amp;quot;:[],&amp;quot;body&amp;quot;:[],&amp;quot;foot&amp;quot;:[]}&#34; class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;GCP Security Token Service (STS)&amp;nbsp;&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;~500,000 RPS&amp;nbsp;&lt;/td&gt;&lt;td&gt;6,000 RPM (100 RPS Limit)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Latency (p99)&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;~2 ms&lt;/td&gt;&lt;td&gt;50 ms (Uber’s test observation)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 7: Comparison of FileSystem Requirements and GCP STS.&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;67841740-a996-4418-ad5b-bb07ef03e65b&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;12d77cff-30d2-464a-908a-df89b96b7540&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-multi-level-caching&#34;&gt;Multi-Level Caching&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7a4ada1d-c0fd-45ba-b03a-b2d45cc5b47d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;To address the impedance mismatch for throughput and latency and scale SAS to meet the requirements, we employed a multi-level caching solution. The multi-level cache involved three layers of caching, which became the core of SAS’ strategy to scale.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5816b9ae-553c-468e-87c9-703094deb3e4&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;1c32e28f-e549-4e5d-b70c-a12081474ed7&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXegQs0AipraavsF1HnmlFIpWTb1s9HAGlV7oAbtably8E1oqIT5hVTO55Hj43JOHIgFcDedza4EDrYaDcKVGAbyzWD1-jd6-1bxryKGxbximpgkPgGZ8sh-rhCpf4IAaT1yHSFXxCNFC61llrP0K_uGvFgW?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 8: Multi Level Caching.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c6555002-bacc-49f1-891d-efbae708ca44&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;93d77fcb-b31d-4562-98b6-d968ffc377c3&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;1. Server Remote Cache: &lt;/strong&gt;The most critical part was to decouple the FileSystem client request (read) for Access Token from SAS, from its generation (write) from GCP APIs.&amp;nbsp; SAS is made aware of all the possible Hive datasets (tables, databases, etc.) and their GCS path prefixes (by the Policy Management System). An Access Token Generator process (hosted on SAS instances) uses this information to proactively generate all combinations of tokens (required for different operations on Hive tables) from GCP STS APIs and store them in a secure Redis cluster. The logic of generating tokens from GCP STS APIs is controlled in such a way that it stays under GCP STS API limits. This read-write segregation (&lt;a href=&#34;https://martinfowler.com/bliki/CQRS.html&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;CQRS design pattern&lt;/a&gt;) enables us to scale SAS horizontally to serve the throughput requirements required by the FileSystem client independent of the GCP limits.&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;00a77394-2e2c-40ed-af55-d035a1a0164f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The tokens stored in Redis are keyed by the GCS prefix and operation the token provides access to. This enables SAS to easily look up and respond with Access Tokens to the FileSystem client based on the GCS path and operation requested. The Access Token Generator is intelligent enough to keep track of expiration time of each token and generate a new token as it nears its expiration time. Proactively generating and caching tokens greatly reduced the likelihood of a cache miss. In the event of a cache miss, we implemented the cache aside pattern logic into SAS (request token from GCP, save to Redis and return to FileSystem client).&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;7b853e28-ab70-4ee1-a48e-b636b514ba72&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;2. Server Local Cache: &lt;/strong&gt;From our observation with existing HDFS data, we found that some tables (such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Fact_table&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;fact tables&lt;/a&gt;) are quite popular and commonly used. Hence, tokens associated with these tables would need to be retrieved from Redis frequently. We implemented an in-memory cache within SAS servers to avoid the round trip latency to Redis for this scenario. This enabled us to improve the overall latency observed by clients.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;ecee1e2f-2455-4b73-baba-3dc12a087667&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;3. Client Local Cache: &lt;/strong&gt;Any given workload (job or query) reads a finite set of Hive tables during its lifetime, which could translate to hundreds of thousands of files within a deterministic set of GCS prefixes (and hence, tokens). Since tokens can be reused for the objects under the same GCS prefix, we introduced a client side cache that significantly reduced the number of calls from FileSystem clients to SAS servers. This not only improved the token retrieval latency observed by the FileSystem clients, but also reduced the pressure on SAS servers to scale in order to support the bursty and periodic throughput changes observed for batch and interactive workloads.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;77896755-fc9f-4d34-97e5-bd111796c8e1&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;&lt;strong&gt;Cache Eviction&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5c07cbae-6c3b-4f10-a333-18a1bb975e0b&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;GCP Access Token size (~200 bytes) was minimal and cardinality of tokens at any given time was in the order of combination of number of Hive datasets (or GCS prefixes) and possible operations. Hence, cache size (&amp;lt; 20 GB) was less of a concern for Server Remote Cache backed by Redis cluster. We set reasonable upper bounds on cache size limits on both Server Local and Client Local caches.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;52000169-1f76-4592-b141-9852e3ed4adf&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;c6665f3f-5422-4a82-b3f7-86e1047cee0e&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfCGoDmbjiQaGtEPytuAxJr__A0-ZS9tueOu317al_x5kLQiTzTmCU8KgjwPVdL6MeUCLUUBQrGYVlIAb9xyqaEpauCzhFoEdfHEXjo6Iv4LGaGe0XqkeskMgC3GGBnI-6j8uXG3rMxaeoDg7CXb3mqZTq7?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 9: Cache Time with respect to Access Token Lifetime.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;27c5d92f-5975-43a8-b97d-bef3a34ed2e1&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;19daa78e-0354-4029-b3f8-4ade9704e4e1&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The cache eviction strategy primarily centered around Access Token lifetime. Access Token has a maximum lifetime of 1 hour (configurable up to 12 hours). Not evicting tokens before expiration would lead to FileSystem call failures (Unauthorized since token has expired). Evicting tokens too soon would lead to higher cache misses resulting in higher latencies and more pressure on scalability of the system. We adjusted the cache time (as indicated by figure 9) across the different caching layers to achieve a high degree of cache hit ratio taking these factors into consideration.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d3c75971-c655-4f3a-aec1-0b1a92a57ce4&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We closely monitored caching latencies and hit ratio across the different layers. With the multi level caching strategy we observed average latencies in the order of 0.026 ms.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;886b292f-956a-46ee-928a-d5b68be1df1b&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;6ef14755-3da0-4b84-8315-5f54cf487190&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXcuQzFa_Sa_6GZZUlhJWF5JirqIOVv_E_y9EObxGFWgB1PKZJKyOEzziIFdF5U2orn6lRXKm0U3pzXiMrY_DhrKQiaewYtM28DntX5dFeyaVRh79Cq1vTbhgT1hg0PedfeWErnak-olcK-Gjoqp3HblQ17K?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 10: Average Access Time Calculation.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;aab1c93b-930b-46f4-a2e6-4e20b5bd7e5d&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:3,&amp;quot;hash&amp;quot;:&amp;quot;1a283fb4-cee6-4ab8-94ff-a8eec592150d&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-benchmarking-amp-testing&#34;&gt;Benchmarking &amp;amp; Testing&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;3ea0b487-2df7-4a9c-b55b-4f580f4bd696&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;For benchmarking, we employed the same strategy that we previously used for load testing &lt;a href=&#34;https://www.uber.com/en-US/blog/hadoop-namenode-container/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;HDFS NameNodes&lt;/a&gt;: utilizing &lt;a href=&#34;https://hadoop.apache.org/docs/stable/hadoop-dynamometer/Dynamometer.html?uclick_id=b40c4d65-b1fb-43a1-8594-9839d98836c9&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Dynamometer&lt;/a&gt;. We adapted the existing dynamometer codebase such that it can replay existing HDFS cluster audit logs against SAS and GCP APIs. We conducted over 45 iterations of benchmark tests across various parameter combinations, including authorization enabled/disabled and caching enabled/disabled. Our goal was to meet performance targets for vertical scaling (single SAS container) and determine the number of containers required for horizontal scaling. These extensive tests helped us identify gaps and meet specific performance goals.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;d04861d3-cf43-4902-b057-0f7f2b7cf079&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We discovered several optimizations by analyzing results from logs, flame graphs, and memory dumps. Key improvements included datastore/caching related enhancements, reducing redundant calls to GCP APIs, tuning gRPC timeouts, improving resources utilized for logging, and utilizing a faster algorithm for token encryption and decryption. These optimizations significantly boosted our system’s overall performance and reliability.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;5f6e48fd-5d29-4f21-9de1-ae5dee7fb8ac&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&amp;quot;align&amp;quot;:&amp;quot;center&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;bafe1987-00bb-4334-adc9-372e2cf0ea52&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img decoding=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXcy_iiV5mOXu3-SbmEnzMTSzOAYCTsbmI99kbtllROxFzhGuQ4oIGdGQ4Rv3Hop9wlYL72NsL9nQkX35_17VWNjdjS2wFy9DQ2JWNeohJ2aWKK4e3ZLfDNp1wXskq3-fhD1wmAPwlOhyOQrhHKgDAvyqMAy?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34; referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;Figure 11: RPS Served by SAS instances during Benchmarking.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;f71a2308-00b9-401f-9d0f-9ee3a19a1192&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;c37bf066-bb87-427a-b06b-f996126a7340&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;We developed continuous probing to test all possible FileSystem operations on GCS buckets used for production data lake. The tests exercised different Hadoop authentication strategies (direct, proxy, and delegated auth) and access control policies through positive and negative tests. These prober tests are also executed in the dev environment to facilitate developer velocity and are triggered with every new change, both before and after merging. Additionally, we closely monitor for regressions across different environments (dev, staging, beta, and prod) using appropriate observability tools, including metrics, logs, and alerts. This diligent monitoring helps us to promptly identify and address issues, and maintain the integrity and performance of our system across all stages of deployment.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;fffc7747-144b-4b90-a960-ce3c541ede62&amp;quot;,&amp;quot;opacity&amp;quot;:&amp;quot;alpha-channel&amp;quot;}&#34; class=&#34;wp-block-separator has-alpha-channel-opacity&#34;&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;level&amp;quot;:1,&amp;quot;hash&amp;quot;:&amp;quot;6b84f248-da6a-49bb-b62f-f7dd17bc2a6a&amp;quot;}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-conclusion&#34;&gt;Conclusion&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;a500971f-f5d6-4ed9-9a7b-ef8139b1186f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;If you have read this far into this blog post, here are the three key insights we hope you take away from our journey:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&amp;quot;ordered&amp;quot;:true,&amp;quot;hash&amp;quot;:&amp;quot;53bb71d7-e45e-41ee-bc50-e5dd0c7e51d6&amp;quot;,&amp;quot;values&amp;quot;:&amp;quot;&amp;quot;}&#34;&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Enhanced Security in the Cloud:&lt;/strong&gt; We have migrated over 160+ PB of data to GCS. With the layered access model, human identities do not have direct access to data stored in GCS. SAS serves as the authentication and authorization layer for access to the data lake on GCS. We took the cloud migration as an opportunity to strengthen our access control measures.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Scaling with Multi-Layer Caching:&lt;/strong&gt; We developed a multi-layer caching strategy and conducted rigorous benchmarking/testing over several iterations with various parameters. With 19%+ of analytical workloads running on GCP, we have observed that SAS authenticates and authorizes GCS operations that peak to over 500k rps. The request volume seen by SAS instances themselves hover well below 10,000 RPS, while the GCP IAM/STS APIs stay below 60 RPS. The multi-level caching strategy has been highly effective and helped us scale the overall system.&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;Seamless Migration Support: &lt;/strong&gt;We invested judiciously on standardizing HDFS clients across the company in the past. We used the standard HDFS client as a vehicle to get all workloads to adopt a SAS client which abstracts out the security integrations through proper design. With this approach, Uber’s data platform users are not aware of any differences between HDFS and GCS security models, which enables seamless migration to the cloud.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;1eb3e610-f4b2-4696-a722-a3b40bd33ec5&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;During the initial planning stages of our cloud migration project, the “Security Integration” workstream was identified as a high priority. Through careful design and development, we successfully overcame the technical challenges. As Uber’s cloud migration continues, we invite you to follow our journey. We’ll be sharing more technical details on other critical workstreams discussed in our previous &lt;a href=&#34;https://www.uber.com/blog/modernizing-ubers-data-infrastructure-with-gcp/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;blog&lt;/a&gt; post. Stay tuned for more insights and updates!&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;cea16887-0078-4ec4-bff8-1e490e265ea7&amp;quot;,&amp;quot;level&amp;quot;:2}&#34; class=&#34;wp-block-heading&#34; id=&#34;h-acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;hash&amp;quot;:&amp;quot;2c6715fc-8fdc-47eb-92cd-ad6fd5700a57&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34;&gt;The work portrayed in this article would not have been possible without close collaboration with our partnering GCP team members: Mandeep Singh Bawa, Julien Phalip, Shlok Karpathak, Namita Sharma, Arend Dittmer and Matthew Rahman. Their expertise and support have been instrumental in designing and implementing the solution to achieve our goals.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;136ce52a-dfe8-4f2a-90a8-dcc7de86122c&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;&lt;em&gt;Header Image Attribution&lt;/em&gt;: The “&lt;a href=&#34;https://www.flickr.com/photos/111692634@N04/15855489588&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Secure Cloud – Data Security – Cyber Security&lt;/a&gt;” image is covered by a&amp;nbsp; &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;CC BY-SA 2.0&lt;/a&gt; license and is credited to &lt;a href=&#34;https://www.flickr.com/photos/111692634@N04&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;perspec_photo88&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;a5c37c34-c4fa-48d0-b04b-a4812d828c5d&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;&lt;em&gt;Apache&lt;sup&gt;®&lt;/sup&gt;, Apache Parquet™, Apache Hudi™, Apache Spark™, Apache Hadoop YARN™ are registered trademarks or trademarks of the &lt;/em&gt;&lt;a href=&#34;http://www.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;&lt;em&gt;Apache Software Foundation&lt;/em&gt;&lt;/a&gt;&lt;em&gt; in the United States and/or other countries. No endorsement by The Apache Software Foundation is implied by the use of these marks.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;4402444d-34c6-4e3a-b141-9f7403c0fe7f&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;&lt;em&gt;Presto&lt;/em&gt;&lt;em&gt;&lt;sup&gt;®&lt;/sup&gt;&lt;/em&gt;&lt;em&gt; is a registered trademark of The Linux Foundation in the United States and/or other countries. No endorsement by The Linux Foundation is implied by the use of these marks.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&amp;quot;fontSize&amp;quot;:&amp;quot;small&amp;quot;,&amp;quot;hash&amp;quot;:&amp;quot;5e92b08c-5f44-4f27-892e-8d9f88898e10&amp;quot;,&amp;quot;dropCap&amp;quot;:false}&#34; class=&#34;has-small-font-size&#34;&gt;&lt;em&gt;GCP&lt;/em&gt;™&lt;em&gt; infrastructure platform is a registered trademark of Google LLC in the United States and/or other countries. No endorsement by Goggle LLC is implied by the use of these marks.&lt;/em&gt;&lt;em&gt;Kerberos is a trademark of the Massachusetts Institute of Technology (MIT) &lt;/em&gt;&lt;em&gt;in the United States and/or other countries. No endorsement by MIT is implied by the use of these marks.&lt;/em&gt;&lt;/p&gt;】&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;83f76446-3dcf-4f7d-b5a7-154eb25d1edb&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-introduction&#34;&gt;简介&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c7bfda9f-ff82-42fb-8a6e-bc82e5f5f75c&#34;,&#34;dropCap&#34;:false}&#34;&gt;作为一部分Uber 的云之旅，我们正在迁移本地 &lt;a href=&#34;https://www.uber.com/blog/uber-big-data-platform/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Apache基于 Hadoop® 的数据湖&lt;/a&gt;以及 GCP™ 基础设施平台的分析和机器学习工作负载。 &lt;a href=&#34;https://www.uber.com/blog/modernizing-ubers-data-infrastruct-with-gcp/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;策略&lt;/a&gt;涉及替换存储层 HDFS 具有 GCS 对象存储 (PaaS)，并在 GCP 计算引擎 (IaaS) 上运行技术堆栈的其余部分（YARN、Apache Hive™、Apache Spark™、Presto® 等）。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3d6de626-2a7c-473c-afef-21090761263e&#34;,&#34;dropCap&#34;:false}&#34;&gt;典型的云采用策略涉及使用云原生组件以及将现有 IAM 与云 IAM 集成（例如联合、身份同步等）（图 1.ii）。我们的策略有些独特：我们继续按原样利用现有堆栈的一部分（HDFS 除外）并与 GCS 集成（图 1.iii）。从安全角度来看，这带来了以下两个领域的技术挑战：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:true,&#34;hash&#34;:&#34;19a292c2-fb5a-4551-8463-bb35157ee818&#34;,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;迁移到公有云需要采用与本地部署不同的安全方法。因此，我们必须在集成过程中围绕 GCS 中的数据存储开发新的 IAM 控制。   &lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;要迁移到 GCP IaaS 的技术堆栈继续使用 Hadoop 安全模型（基于 Kerberos 的身份验证、委托令牌和 ACL）。我们需要通过弥合 HDFS 和 GCS (GCP IAM) 安全模型之间的差异来使这项工作与 GCS 对象存储一起使用。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;b3789f83-33e6-4bb8-a013-14a4c26231d2&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;d8a2f8bb-4d35-4df5-baf7-73263bf0eb24&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXdiB4wgUnBdG6Pk3BXzkiaxOQDkhc9LLZEgEmtnzEI6zyKquK1fKaIa5UsyG96bHZAa3_JgWOwMk893WUyppsWcqVAHgMHQ081-Ryk89PvNkdhhW3Pi2s8cCSM8vs4xVt8s8JtigbYbwZ9c2To3e6QWxa3z?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 1：云数据湖 IAM 系统与 Ex 的比较列表和 GCP Native Stack .&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1577a7e2-19d9-4ade-8111-da1c27766834&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;428fa1b6-ce1e-47de-a3ab-0da0cf21202c&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们已经构建了支持 Uber 数据湖迁移到 GCP 的多个系统和集成。快进到今天，我们超过 19% 的分析工作负载在 GCP 上运行。在本文中，我们将详细探讨如何大规模解决上述两个技术挑战以支持云迁移。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;bc484c46-a46b-4749-8945-3be248bee415&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;562f3dbc-2b00-4a66-8b36-42bda397734c&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-existing-architecture&#34;&gt;现有架构&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;787742af-6389-4b7d-9af9-b5f127d2abca&#34;,&#34;dropCap&#34;:false}&#34;&gt;在现有的本地技术堆栈，超过 1 EB 的数据存储在&lt;a href=&#34;https://www.uber.com/blog/scaling-hdfs/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;多个HDFS集群&lt;/a&gt;。一套全面的自动化和生态系统工具统一管理这些集群，包括 &lt;a href=&#34;https://www.uber.com/blog/hadoop-container-blog/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;集群操作&lt;/a&gt;、资源管理和安全性。为了安全起见，我们严重依赖 Hadoop 的身份验证和授权 (&lt;a href=&#34;https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html#ACLs_.28Access_Control_Lists.29 “ target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;HDFS ACL&lt;/a&gt;）等功能。在较高的层面上，我们有以下组件：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6ef21cb7-0d47-484e-991d-a857ca573391&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;width&#34;:&#34;700px&#34;,&#34;height&#34;:&#34;auto&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34; :&#34;162bd9a0-904b-43bf-afe2-27db5ebb5255&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter is-resized&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent. com/docsz/AD_4nXedHmmt_--HqRzqQE-llYud_pnRy0Cer3pAMOvJJNfU0gGNGOAIUyH3MIDIhUmeNCjaAchDvh4Yj6WaVsLU_HLdOL_VvLkzPwfW8OtiV2CAvpi5RvwpcKc7P0W0DeBZSvmq5DjoVqZQo U5vAHJyzLqRgYPg?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34; style=&#34;width:700px;height:auto&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34; &gt;图 2：HDFS 的现有架构。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f29435d2-4541-4753-94c5-e1a8d7d7d385&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d9a327f2-560c-4872-a1c4-9d5d6f84def9&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;HDFS NameNode 和 DataNode -&lt;/strong&gt; 开源 HDFS 设置。数千个DataNode存储实际数据。 NameNode 负责所有文件系统操作，包括身份验证和授权逻辑。 HDFS NameNode 是一个策略决策点 (&lt;a href=&#34;https://csrc.nist.gov/glossary/term/policy_decision_point&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;PDP&lt;/a&gt;) 和一个策略执行点 (&lt;a href=&#34;https://csrc.nist.gov/glossary/term/policy_enforcement_point&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;PEP&lt;/a&gt;)（NIST 术语）。&lt;/李&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;数据计算 - &lt;/strong&gt;统指利用 Presto、Spark、MapReduce 等的工作负载.，访问存储在 HDFS 中的数据。这些工作负载使用标准 HDFS 客户端，该客户端由 Uber 内部维护和批准使用。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;用户组 - &lt;/strong&gt;一个内部系统，使 Unix 用户和组可以在每个生产主机 – 这些用于 HDFS ACL。 UserGroups 是 NIST 政策信息点 (&lt;a href=&#34;https://csrc.nist.gov/glossary/term/policy_information_point&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;PIP&lt;/a&gt;)。&lt;/a&gt;李&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;策略管理系统 -&lt;/strong&gt; 管理数据集的访问控制策略。该系统将数据集策略（由 Uber 数据平台用户配置）转换为原生 HDFS ACL，确保它们定期同步。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;a href=&#34;https://web.mit.edu/kerberos/&#34; target=&#34;_blank &#34; rel=&#34;noreferrer noopener&#34;&gt;&lt;strong&gt;Kerberos&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; —&lt;/strong&gt; 处理 Hadoop 组件及其客户端之间的身份验证。您可以在 &lt;a href=&#34;https://www.uber.com/blog/scaling-adoption-of-kerberos-at-uber/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener 中了解我们如何扩展 Kerberos 基础设施&#34;&gt;我们发布的上一篇博客&lt;/a&gt;。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;dd9d3876-c9ea-4bb0-89fe-628e331734ab&#34;,&#34;dropCap&#34;:false}&#34;&gt;在详细介绍如何实现身份验证和授权以将 HDFS 无缝替换为 Google Cloud Storage 之前，了解身份验证在 HDFS 中的工作原理非常重要。&lt;/ p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;08f5a1d7-154c-4d6c-8edc-3902ec82496c&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;67c39421-786b-4c1a-a862-f089999c6ca0&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXcaR9wil0Rw_W6FUj9njt5SGogKxT0swALrqq_Gry-jSKjL2Lzuxb8_-t1bCxtYxtgbqTQ3k4ywXh8OPueEmIuxK3tX DQ91GEfQvly0gWybJmoemfSbu65Z47YBTb-SBE_M_Weq1bUmAGvRL8b1mHu2yBs?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 3：HDFS 身份验证流程。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;085cf49f-f36c-483f-85af-029b5ec4ca3a&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;65fd1203-1c5e-4163-bcc6-5723bcf448d6&#34;,&#34;dropCap&#34;:false}&#34;&gt;让我们考虑一个用户想要提交读取 HDFS 目录中文件的 Spark 作业的情况：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:true,&#34;hash&#34;:&#34;6d1c9f26-e51a-4c2b-9adb-7c933e3d32ee&#34;,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;用户运行 `&lt;em&gt;kinit&lt;/em&gt;` 以使用 Kerberos 进行身份验证。 &lt;em&gt;Hdfs-cli&lt;/em&gt; 使用 Kerberos 身份验证获取 HDFS 服务票据 (&lt;em&gt;HDFS_ST&lt;/em&gt;)。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;HDFS 客户端连接到 HDFS NameNode 以生成 &lt;a href=&#34;https://blog.cloudera. com/hadoop-delegation-tokens-explained/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;HDFS 委托令牌&lt;/a&gt; (&lt;em&gt;User_DT&lt;/em&gt;)，使用 &lt;em&gt;HDFS_ST&lt;/em&gt; 进行身份验证&gt;。 HDFS NameNode 创建一个新令牌并将其存储在内部存储中，以便在进行文件系统调用时可以验证它。 &lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;a href=&#34;https://www.uber.com/blog/uscs-apache-spark /&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Drogon&lt;/a&gt;/uSCS 是 Uber 的 Spark 即服务解决方案。 &lt;em&gt;User_DT&lt;/em&gt; 由 &lt;em&gt;drogon-cli&lt;/em&gt; 作为 Spark 作业提交的一部分传递到 YARN。 YARN 在内部保存&lt;em&gt;User_DT&lt;/em&gt;，并根据作业生命周期跟踪令牌的过期情况，以便根据需要更新或取消令牌。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;YARN 将用户的作业作为分布式 Spark 应用程序启动，并可以访问 &lt;em&gt;User_DT&lt;/em&gt;。该作业使用 &lt;em&gt;User_DT&lt;/em&gt; 对 HDFS NameNode 进行 HDFS 文件系统调用（使用 HDFS 标准客户端）。由 NameNode 执行访问控制检查以查看用户是否有权访问该文件。  文件系统操作涉及请求块访问令牌 (BAT) 来读取构成文件的块。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;作业访问 HDFS DataNode 中的文件块，并使用 BAT 进行身份验证。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;375049c3-fb9b-4800-840b-0a2807b77617&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8dd73737-1405-4b4d-8f88-d4bd5f6c4fa3&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-cloud-architecture&#34;&gt;云架构&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;809d7f0f-0a5a-469d-92cf-7c53ccbef343&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-layered-access-model&#34;&gt;分层访问模型&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c8c761b5-1423-413a-bf4e-0ffea68e41ec&#34;,&#34;dropCap&#34;:false}&#34;&gt;公有云攻击矢量与本地部署有很大不同。我们设计的访问模型包括以下各层的安全控制，同时在云中设置新技术堆栈。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ad1d8eb5-bb0e-4a96-9a00-79437ed60c66&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;第 1 层 - 基础：&lt;/strong&gt;在最低层，我们通过声明性 &lt;a href=&#34;https://en.wikipedia.org/wiki/Infrastruction_as_code&#34; target=&#34;_blank&#34; rel=&#34;noreferrer 管理以下基础原语noopener&#34;&gt;基础设施即代码&lt;/a&gt; (IaC) 代码库：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;025fd7f3-0737-4995-bd9e-795000b35511&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;em&gt;网络政策&lt;/em&gt;：允许 Uber 现有的本地系统与我们的系统连接在 GCP 中，反之亦然&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;em&gt;服务帐户和项目设置&lt;/em&gt;：提供对各层自动化系统的访问&lt; /里&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;em&gt;人员访问&lt;/em&gt;：根据需要促进监控和调试目的&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;59ec8bef-fe84-4e54-b74b-3468ae7b66d0&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;第 2 层 - 数据网格：设置第 1 层控件后，Data Mesh系统负责自动设置和管理数百个GCP项目和GCS存储桶。数据网格系统将数据湖中的所有 Hive 表划分为不同部门和不同类型的数据集（原始数据、建模表、临时表等），这些数据集跨这些 GCS 存储桶和项目。该层提供了锁定不同部门内数据访问的灵活性，例如：限制对财务部门拥有的建模数据的访问。这一层的访问控制还用于安全要求之外的操作自动化，例如在数据迁移期间锁定存储桶。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4102e048-8fc3-4b4d-b224-e12878fd6edc&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;第 3 层 - 数据集访问：&lt;/strong&gt;工作负载本身以 &lt;a href=&#34;https://www.uber.com/blog/presto/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;Presto 的形式体现查询&lt;/a&gt;、Spark 作业和 &lt;a href=&#34;https://www.uber.com/blog/managing-data-workflows-at-scale/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt; Piper 管道&lt;/a&gt;。这些工作负载访问驻留在 GCS 存储桶中的特定数据。该层为 Hive 数据库、表和分区提供 GCS 存储桶之外的更细粒度的访问。该层了解提交工作负载和访问策略的用户或无头服务帐户，这些工作负载和访问策略提供对存储在 GCS 中的 Hive 表数据的限时访问。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;4f0bab79-9cae-4c47-a177-c511f49160eb&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;a922ce60-022f-433e-8171-2c8a9a427479&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXf1EEwExHWMLGF2UbOS_xz6p5PVsfOQQOKfOfOvBOd1Dm6ojEHuleDncXIOJxVFsjZ8-N1jUD1f4nBH9VyMFdxRcKafuR-pgE epOB1gxZ52edAslXKZwy5p9HIhXSJ-ZC6krIevqz385niKVaqX7ksUkrWY?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 4：分层访问模型&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;204ff6e0-678f-4e73-9a08-13c98a54aff2&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;3c67f023-c077-4e42-aa36-45d816e227f2&#34;,&#34;dropCap&#34;:false}&#34;&gt;这种分层访问模型使我们能够解决引言中提到的挑战 A。每天使用数据平台进行各种分析、报告和机器学习用例的员工通常不需要访问以下资源：第 1 层和第 2 层。这些层的资源通过自动化和服务帐户而不是人工进行管理。限制对人类身份的访问可降低通过基于人类身份的攻击进行未经授权访问的可能性。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ff3a4d20-8bd9-41de-a67f-e1f970af4fbc&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们继续将 Apache Hudi™ 和 Apache Parquet™ 用于 GCP 中托管的数据湖。这使我们能够继续使用现有的 &lt;a href=&#34;https://www.uber.com/en-US/blog/one-stone-third-birds-finer-grained-encryption-apache-parquet/&#34; target= &#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;基于 Parquet 的解决方案&lt;/a&gt;，用于提供更细粒度（列级）访问控制和加密功能，就像云一样，超出了上述 3 层。为了建立第 3 层，我们构建了一个名为存储访问服务的新系统，我们将在下一节中介绍该系统。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;79509367-1daa-4172-a988-d293f1025472&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;bcfab320-6f9e-4e09-8900-913e6fe8f599&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-storage-access-service&#34;&gt;存储访问服务&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;18af0255-6828-47cf-ab23-0783bda3cb02&#34;,&#34;dropCap&#34;:false}&#34;&gt;启用安全为了访问 GCS 中的数据，我们探索了分为两类的不同选项：(a) 依赖本机 GCP IAM 构造或 (b) 构建中间系统来处理身份验证和授权。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;72542200-d3f3-4bab-803f-8a298ca45d15&#34;,&#34;dropCap&#34;:false}&#34;&gt;即使选项(a) 依赖 GCP IAM 看起来是一个显而易见的选择，但我们遇到了以下挑战和缺点：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:true,&#34;hash&#34;:&#34;6c3ce8ab-9436-4f6a-832a-e0be283c7efa&#34;,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;我们的本地访问控制策略是根据组成员身份建模的，因此需要我们构建一个用于将本地用户和组同步到 GCP 的新系统（在我们启动项目时缺乏 GCS 的工作负载/劳动力身份联合的情况下）。该系统应该具有高度可靠性和低延迟同步，以避免破坏我们内部开发的现有功能，例如即时 (JIT) 访问。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;单个 GCS 存储桶有一个&lt;a href=&#34;https://cloud.google.com/iam /quotas#limits&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;对数量的硬性限制为 250&lt;/a&gt;f 策略内的 IAM 角色绑定。而在本地，我们为 HDFS 中的数据配置了超过 150,000 个访问控制策略（可转换为 GCP IAM 角色绑定）。当 Data Mesh 系统接近 IAM 策略限制时，这很容易导致跨存储桶移动数据集的管理噩梦。撇开移动 TB 或 PB 数据所产生的成本不谈，要使该系统以可靠的方式运行是很困难的。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;正如分层访问模型部分中提到的，我们希望减少云中基于身份的攻击的可能性。将数以万计的用户和组的整个足迹同步到 GCP IAM 并为 GCS 存储桶策略编写新的 IAM 角色绑定可能会增加由于基于人类身份的攻击和策略错误配置而导致的可能攻击路径。我们拥有的移动部件越少，保护系统就越容易。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;160d3610-1d51-4076-b210-93604251211e&#34;,&#34;dropCap&#34;:false}&#34;&gt;进行所有权衡考虑到我们意识到，如果我们开发一个处理身份验证和授权的中介系统（选项b），我们将获得更好的灵活性和控制力。在云数据湖堆栈中，关键的架构变化来自于用GCS对象存储替换HDFS。 HDFS 身份验证基于 Kerberos 和/或委派令牌进行工作。 GCP 云存储基于云令牌（GCS 的 OAuth 2.0 AccessTokens）工作。为了促进用 Kerberos 和委托令牌交换访问令牌，我们构建了一个名为存储访问服务 (SAS) 的中间系统。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ead454c3-e1d3-4e37-b645-c493e6d68284&#34;,&#34;dropCap&#34;:false}&#34;&gt;SAS 开始于&lt;a href=&#34;https://github.com/GoogleCloudPlatform/gcp-token-broker&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;gcp-token-broker&lt;/a&gt; 的内部分支（开源该项目由 GCP 发起），此后经过大量定制以满足 Uber 的要求。今天，SAS 拦截 HDFS 标准客户端针对 GCS 发起的文件系统调用（使用 &lt;a href=&#34;https://github.com/GoogleCloudDataproc/hadoop-connectors/tree/v3.0.1/gcs&#34; target=&#34;_blank&#34; rel=&#34; noreferrer noopener&#34;&gt;gcs hadoop-connector&lt;/a&gt;），对其进行身份验证并注入 GCP 访问令牌，这些令牌提供有时限的访问权限以对特定 GCS 前缀或对象执行特定操作。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;be0a58bb-de55-45d8-b59a-b482f73201fa&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;id&#34;:1093328,&#34;sizeSlug&#34;:&#34;large&#34;,&amp;quot;linkDestination&#34;:&#34;无&#34;,&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;72e15d8c-fca0-4a38-9d73-10b07e91688b&#34;,&#34;alt&#34;:&#34;&#34;}&#34; class=&#34;aligncenter size-large&#34;&gt;&lt; img 加载 =“惰性”解码 =“异步”宽度 =“1024”高度 =“913”src =“https://blog.uber-cdn.com/cdn-cgi/image/width=2160，质量=80， onerror=重定向，format=auto/wp-content/uploads/2024/07/Figure-5-1024x913.png&#34; alt=&#34;&#34; class=&#34;wp-image-1093328&#34; srcset=&#34;https://blog.uber- cdn.com/cdn-cgi/image/width=1024，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/07/Figure-5.png 1024w，https://blog.uber -cdn.com/cdn-cgi/image/width=300，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/07/Figure-5.png 300w，https://blog。 uber-cdn.com/cdn-cgi/image/width=768，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/07/Figure-5.png 768w，https://blog .uber-cdn.com/cdn-cgi/image/width=1536，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/07/Figure-5.png 1536w，https:// blog.uber-cdn.com/cdn-cgi/image/width=2048，quality=80，onerror=redirect，format=auto/wp-content/uploads/2024/07/Figure-5.png 2048w&#34; 尺寸=&#34; （最大宽度：1024px）100vw，1024px&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 5：云架构。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c267725e-c716-4950-8de3-890d084edd2a&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ae251fdf-3f78-46f9-841a-5e0ebbbe40a0&#34;,&#34;dropCap&#34;:false}&#34;&gt;本地部署用户和组未同步到 GCP IAM。所有人员和工作负载都需要通过 SAS 身份验证才能访问 GCS 中的数据。 SAS 使用自己的 GCP 服务帐户来生成 GCP 访问令牌。我们利用 Hadoop 现有的委托令牌框架来促进令牌交换。委托令牌 (&lt;em&gt;User_DT&lt;/em&gt;) 在分布式应用程序启动之前生成，并在每个应用程序容器中本地化。当容器（文件系统客户端）需要对驻留在 GCS 中的数据进行操作时，&lt;em&gt;User_DT&lt;/em&gt; 与 SAS 交换以获取可提供给 GCS 进行身份验证的访问令牌 (AT)。请参阅下面的图 6，了解更新后的 GCS 身份验证流程。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;6c70dff5-c4c5-4a1d-a99c-3d6717a07d6c&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;f6e715aa-959a-4644-b0ce-b9e6f723d3e9&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXcdP7kQS_VZqQe-qm3fZDaK7KBA4MjSgZu8Mrf9SAvYEfIh2PztHuHLuMxzmA_M9u7nlosoMXEFPZxV7qe6tjoHBg37suFQQgX-KbR4lOGFNpRyckD-ynU6RSLEneS6RJO_QPrwKd50dmAiRdi4u_-NTTor?key=MImG8dcYEpzLkw6 a6--MNw&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 6：云身份验证流程。&lt;/figcaption&gt; &lt;/图&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;caea4011-29ce-4cbe-a3ce-bdc33f2c10c6&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2520d3e3-9c82-4b3a-8de6-76a15cbfad78&#34;,&#34;dropCap&#34;:false}&#34;&gt;这个新图表与展示 HDFS 身份验证流程的图 3 非常相似，除了一些关键更改（在图 6 中以红色突出显示）：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ul data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;76a589e7-a6fc-4d28-ad46-882486784fbf&#34;,&#34;ordered&#34;:false,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;HDFS NameNode 已替换为 SAS。在新模型中，SAS 可以像 NameNode 一样处理身份验证和授权。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;HDFS DataNode 被 GCS 取代，GCS 存储对象而不是文件块。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;SAS 现在是委托令牌提供商（实现与 DelegationToken 相关的接口，例如 &lt;a href=&#34;https:/ /github.com/apache/hadoop/blob/branch-2.8.2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java”目标=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;AbstractDelegationTokenIdentifier&lt;/a&gt;）。文件系统客户端需要向 SAS 请求委派令牌 (&lt;em&gt;User_DT&lt;/em&gt;)。 &lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;最后，HDFS 块访问令牌被替换为用于读取对象的 GCP OAuth 2.0 访问令牌&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;47af226a-f082-4460-9475-6f7721798969&#34;,&#34;dropCap&#34;:false}&#34;&gt;满足访问为了满足控制要求，对现有策略管理系统（图 5）进行了扩展，使其维护可映射到 GCS 前缀的数据集访问策略的翻译版本。这使得使用现有 UI 来管理数据集访问控制能够获得一致的用户体验，无论数据存储位置（GCS 或 HDFS）如何。访问控制策略和数据集 &lt;&gt; GCS 前缀数据从策略管理系统检索并缓存在 SAS 实例中。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c3815696-b934-417c-8731-1581a8c22aab&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们设计了一个利用 &lt;a href=&#34;https://www.uber.com/blog/attribute-based-access-control-at-uber/&#34; target=&#34;_blank&#34; rel=&#34; 的授权插件noreferrer noopener&#34;&gt;SAS 中的 AuthFx 库&lt;/a&gt;，通过检查给定身份是否可以访问所请求的 GCS 资源来评估访问策略。如果授权决策评估为“允许”，则返回 GCP 访问令牌。对于“拒绝”决策，则返回 &lt; a href=&#34;https://github.com/apache/hadoop/blob/branch-2.8.2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/ AuthorizationException.java&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;AuthorizationException&lt;/a&gt; 由 SAS 抛出。GCP 支持使用其 &lt;a href=&#34;https:/ 将访问令牌“缩小”到特定的 GCS 前缀或对象/cloud.google.com/iam/docs/reference/sts/rest&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;安全令牌服务 (STS) API&lt;/a&gt;。我们利用此功能来创建有时限的通过配置 &lt;a href=&#34;https://cloud.google.com/iam/docs/downscoping-short-lived-credentials#example-object-startswith&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener 来访问 GCS 前缀的令牌&#34;&gt;使用 IAM 条件设置适当的访问边界和权限&lt;/a&gt;。工作负载使用 SAS 返回的访问令牌在预定时间段内对某个 GCS 前缀执行特定操作。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ae175ccc-712e-4464-b3dc-25c4aeb7ab81&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们开发了一个SAS客户端库并将其打包成Uber支持的标准HDFS客户端。 SAS 客户端库提供了更高级别的 API，可生成针对 SAS 的请求以请求 AT 和 User_DT（图 6）。访问令牌是不可更新的 - 只要 User_DT 有效，SAS 客户端库就足够智能，可以在需要时请求新的访问令牌。  &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;63aa0684-e679-4c96-88f0-feafcf392412&#34;,&#34;dropCap&#34;:false}&#34;&gt;SAS 客户端打包在 HDFS 标准客户端中的库使云迁移的安全方面对于每天执行的现有数十万个不同工作负载完全透明。通过构建 SAS，我们能够弥合 GCS 和 HDFS 安全模型之间的差异，并从数据平台的用户中抽象出复杂的问题（解决简介中提到的挑战 B）。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;8077aa88-d312-4f77-bd61-5617761b3a76&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;20ddceb2-e7de-4741-b7b0-82fbbfb9c0d0&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-scaling-the-system&#34;&gt;扩展系统&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f503755d-c054-45a5-be62-d01a30984f01&#34;,&#34;dropCap&#34;:false}&#34;&gt;SAS 拦截所有文件系统调用支持身份验证和授权。扩展该系统以支持现有用例对于促进迁移至关重要。我们遵循与其他项目相同的指导原则：“通过数据驱动分析评估当前的规模要求 (X)，针对 10 倍规模进行设计，并为 1 倍规模构建 MVP，并提供支持 10 倍的明确可行路径”。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;391a71d4-337d-4c48-8846-575cfde0afae&#34;,&#34;dropCap&#34;:false}&#34;&gt;基于现有根据支持现有数据湖的多个 HDFS 集群的指标，我们估计 X 能够以 p99 ~20ms 延迟提供 ~500,000 RPS（吞吐量）。请注意，HDFS 中的典型文件系统操作除了涉及 inode 更改、锁定等（具体取决于操作）的核心文件系统逻辑之外，还包括安全身份验证。我们将 20 毫秒（即 2 毫秒）的 10% 预算为身份验证的延迟要求。 GCP 的安全令牌服务 API 的配额为 &lt;a href=&#34;https://cloud.google.com/iam/quotas&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;6k rpm&lt;/a&gt;（请注意，这是RPM（而非 RPS）用于令牌交换请求。在原型测试期间，我们观察到请求延迟约为 50 毫秒。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fa000b3c-3db9-4be3-b02d-97302d0cd2ee&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure data-wp-block-name=&#34;core/table&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7dd7e2a3-9fec-4ee0-8157-4d8c6f30abec&#34;,&#34;hasFixedLayout&#34;:false,&#34;head&#34;:[ ],&#34;body&#34;:[],&#34;foot&#34;:[]}&#34; class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;要求&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;GCP 安全令牌服务 (STS)&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;吞吐量&lt;/strong&gt;&lt;/td &gt;&lt;td&gt;~500,000 RPS &lt;/td&gt;&lt;td&gt;6,000 RPM（100 RPS 限制）&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;延迟 (p99)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;~2 ms&lt;/td&gt;&lt;td&gt;50 ms（Uber 的测试观察）&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 7 ：文件系统要求和 GCP STS 的比较。&lt;/figcaption&gt;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;67841740-a996-4418-ad5b-bb07ef03e65b&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;12d77cff-30d2-464a-908a-df89b96b7540&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-multi-level-caching&#34;&gt;多级缓存&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7a4ada1d-c0fd-45ba-b03a-b2d45cc5b47d&#34;,&#34;dropCap&#34;:false}&#34;&gt;解决为了满足吞吐量和延迟的阻抗不匹配以及扩展 SAS 的要求，我们采用了多级缓存解决方案。多级缓存涉及三层缓存，w这成为 SAS 规模扩张战略的核心。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5816b9ae-553c-468e-87c9-703094deb3e4&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;1c32e28f-e549-4e5d-b70c-a12081474ed7&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXegQs0AipraavsF1HnmlFIpWTb1s9HAGlV7oAbtically8E1oqIT5hVTO55Hj43JOHIgFcDedza4EDrYaDcKVGAbyzWD1-jd6-1bx ryKGxbximpgkPgGZ8sh-rhCpf4IAaT1yHSFXxCNFC61llrP0K_uGvFgW?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 8：多级缓存。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c6555002-bacc-49f1-891d-efbae708ca44&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;93d77fcb-b31d-4562-98b6-d968ffc377c3&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt; 1.服务器远程缓存：最关键的部分是将来自 SAS 的文件系统客户端请求（读取）访问令牌与 GCP API 生成（写入）的访问令牌分离。  SAS 了解所有可能的 Hive 数据集（表、数据库等）及其 GCS 路径前缀（通过策略管理系统）。访问令牌生成器进程（托管在 SAS 实例上）使用此信息主动从 GCP STS API 生成所有令牌组合（Hive 表上的不同操作所需），并将它们存储在安全的 Redis 集群中。从 GCP STS API 生成令牌的逻辑受到控制，使其保持在 GCP STS API 限制之下。这种读写隔离（&lt;a href=&#34;https://martinfowler.com/bliki/CQRS.html&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;CQRS 设计模式&lt;/a&gt;）使我们能够扩展 SAS水平地满足文件系统客户端所需的吞吐量要求，不受 GCP 限制的影响。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;00a77394-2e2c-40ed-af55-d035a1a0164f&#34;,&#34;dropCap&#34;:false}&#34;&gt;存储的令牌在 Redis 中，由 GCS 前缀和令牌提供访问的操作作为键控。这使得 SAS 能够根据 GCS 路径和​​请求的操作轻松查找文件系统客户端并使用访问令牌进行响应。访问令牌生成器足够智能，可以跟踪每个令牌的过期时间，并在接近过期时间时生成新令牌。主动生成和缓存令牌大大降低了缓存未命中的可能性。在发生缓存未命中的情况下，我们在以下位置实现了缓存旁路模式逻辑：到 SAS（从 GCP 请求令牌，保存到 Redis 并返回到文件系统客户端）。 &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;7b853e28-ab70-4ee1-a48e-b636b514ba72&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt; 2.服务器本地缓存：&lt;/strong&gt;根据我们对现有 HDFS 数据的观察，我们发现一些表（例如 &lt;a href=&#34;https://en.wikipedia.org/wiki/Fact_table&#34; target=&#34;_blank&#34; rel= “noreferrer noopener&#34;&gt;事实表&lt;/a&gt;）非常流行且常用。因此，需要经常从 Redis 检索与这些表关联的令牌。我们在 SAS 服务器内实现了内存缓存，以避免在这种情况下到 Redis 的往返延迟。这使我们能够改善客户观察到的整体延迟。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;ecee1e2f-2455-4b73-baba-3dc12a087667&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt; 3.客户端本地缓存：任何给定的工作负载（作业或查询）在其生命周期内都会读取一组有限的 Hive 表，这可能会转换为一组确定性的 GCS 前缀（以及令牌）内的数十万个文件。由于令牌可以重复用于相同 GCS 前缀下的对象，因此我们引入了客户端缓存，显着减少了从文件系统客户端到 SAS 服务器的调用次数。这不仅改善了文件系统客户端观察到的令牌检索延迟，还减轻了 SAS 服务器扩展的压力，以支持批处理和交互式工作负载观察到的突发性和周期性吞吐量变化。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;77896755-fc9f-4d34-97e5-bd111796c8e1&#34;,&#34;dropCap&#34;:false}&#34;&gt;&lt;strong&gt;缓存驱逐&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5c07cbae-6c3b-4f10-a333-18a1bb975e0b&#34;,&#34;dropCap&#34;:false}&#34;&gt;GCP 访问令牌大小（~200 字节）是最小的，并且任何给定时间的令牌基数都是按照 Hive 数据集（或 GCS 前缀）数量和可能的操作的组合的顺序排列的。因此，对于 Redis 集群支持的服务器远程缓存来说，缓存大小（&lt; 20 GB）不太重要。我们对服务器本地和客户端本地缓存的缓存大小限制设置了合理的上限。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;52000169-1f76-4592-b141-9852e3ed4adf&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;c6665f3f-5422-4a82-b3f7-86e1047cee0e&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXfCGoDmbjiQaGtEPytuAxJr__A0-ZS9tueOu317al_x5kLQiTzTmCU8KgjwPVdL6MeUCLUUBQrGYVlIAb9xyqaEpauCzhFoEdfHEXjo6Iv4LGaGe0XqkeskMgC3GGBnI-6j8uXG3rMxaeoDg7CXb3mqZTq7?key=MImG8dcYEpzLkw6a6--MNw&#34; alt=&#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 9：与访问令牌相关的缓存时间一生。&lt;/figcaption &gt;&lt;/图&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;27c5d92f-5975-43a8-b97d-bef3a34ed2e1&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;19daa78e-0354-4029-b3f8-4ade9704e4e1&#34;,&#34;dropCap&#34;:false}&#34;&gt;缓存驱逐策略主要围绕访问令牌生命周期。访问令牌的最长生命周期为 1 小时（可配置为最长 12 小时）。在过期之前不驱逐令牌将导致文件系统调用失败（由于令牌已过期而未经授权）。过早驱逐令牌会导致更高的缓存未命中率，从而导致更高的延迟和对系统可扩展性的更大压力。考虑到这些因素，我们调整了不同缓存层的缓存时间（如图 9 所示），以实现较高的缓存命中率。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d3c75971-c655-4f3a-aec1-0b1a92a57ce4&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们密切监控不同层的缓存延迟和命中率。通过多级缓存策略，我们观察到平均延迟约为 0.026 毫秒。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;886b292f-956a-46ee-928a-d5b68be1df1b&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;6ef14755-3da0-4b84-8315-5f54cf487190&#34;,&#34;alt&#34; :&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXcuQzFa_Sa_6GZZUlhJWF5JirqIOVv_E_y9EObxGFWgB1PKZJKyOEzziIFdF5U2orn6lRXKm0U3pzXiMrY_DhrKQiaewYtM 28DntX5dFeyaVRh79Cq1vTbhgT1hg0PedfeWErnak-olcK-Gjoqp3HblQ17K?key=MImG8dcYEpzLkw6a6--MNw&#34; alt= &#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 10：平均访问时间计算。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;aab1c93b-930b-46f4-a2e6-4e20b5bd7e5d&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h3 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:3,&#34;hash&#34;:&#34;1a283fb4-cee6-4ab8-94ff-a8eec592150d&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-benchmarking-amp-testing&#34;&gt;基准测试和测试&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;核心/段落&#34; data-wp-block=&#34;{&#34;哈希&#34;:&#34;3ea0b487-2df7-4a9c-b55b-4f580f4bd696&#34;,&#34;dropCap&#34;:false}&#34;&gt;对于基准测试，我们采用了之前用于负载测试的相同策略 &lt;a href=&#34;https://www.uber.com/en-US/blog/hadoop-namenode -container/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;HDFS NameNode&lt;/a&gt;：利用 &lt;a href=&#34;https://hadoop.apache.org/docs/stable/hadoop-dynamometer/Dynamometer.html ?uclick_id=b40c4d65-b1fb-43a1-8594-9839d98836c9&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;测功机&lt;/a&gt;。我们调整了现有的测功机代码库，使其可以根据 SAS 和 GCP API 重放现有的 HDFS 集群审核日志。我们对各种参数组合（包括启用/禁用授权和启用/禁用缓存）进行了超过 45 次基准测试迭代。我们的目标是满足垂直扩展（单个 SAS 容器）的性能目标，并确定水平扩展所需的容器数量。这些广泛的测试帮助我们找出差距并实现特定的性能目标。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;d04861d3-cf43-4902-b057-0f7f2b7cf079&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们发现了几个通过分析日志、火焰图和内存转储的结果进行优化。主要改进包括数据存储/缓存相关的增强、减少对 GCP API 的冗余调用、调整 gRPC 超时、改进用于日志记录的资源以及利用更快的算法进行令牌加密和解密。这些优化显着提高了我们系统的整体性能和可靠性。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;5f6e48fd-5d29-4f21-9de1-ae5dee7fb8ac&#34;,&#34;opacity&#34;:&#34;alpha 通道&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;wp-block-image&#34;&gt;&#xA;&lt;figure data-wp-block-name=&#34;core/image&#34; data-wp-block=&#34;{&#34;align&#34;:&#34;center&#34;,&#34;hash&#34;:&#34;bafe1987-00bb-4334-adc9-372e2cf0ea52&#34;,&#34;alt&#34; ：&#34;&#34;}&#34; class=&#34;aligncenter&#34;&gt;&lt;img 解码=&#34;async&#34; src=&#34;https://lh7-rt.googleusercontent.com/docsz/AD_4nXcy_iiV5mOXu3-SbmEnzMTSzOAYCTsbmI99kbtllROxFzhGuQ4oIGdGQ4Rv3Hop9wlYL72NsL9nQkX35_17VWNjdjS2 wFy9DQ2JWNeohJ2aWKK4e3ZLfDNp1wXskq3-fhD1wmAPwlOhyOQrhHKgDAvyqMAy?key=MImG8dcYEpzLkw6a6--MNw&#34; alt= &#34;&#34;referrerpolicy=&#34;no-referrer&#34;&gt;&lt;figcaption class=&#34;wp-element-caption&#34;&gt;图 11：基准测试期间 SAS 实例提供的 RPS。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;f71a2308-00b9-401f-9d0f-9ee3a19a1192&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;c37bf066-bb87-427a-b06b-f996126a7340&#34;,&#34;dropCap&#34;:false}&#34;&gt;我们开发了连续探测以测试用于生产数据湖的 GCS 存储桶上所有可能的文件系统操作。进行的测试不同通过正面和负面测试了解 Hadoop 身份验证策略（直接、代理和委托身份验证）和访问控制策略。这些探测器测试也在开发环境中执行，以提高开发人员的速度，并且在合并之前和之后的每个新更改都会触发这些测试。此外，我们使用适当的可观察性工具（包括指标、日志和警报）密切监控不同环境（开发、登台、测试版和产品）的回归。这种勤奋的监控有助于我们及时发现和解决问题，并在部署的各个阶段保持系统的完整性和性能。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;hr data-wp-block-name=&#34;core/separator&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;fffc7747-144b-4b90-a960-ce3c541ede62&#34;,&#34;opacity&#34;:&#34;alpha-channel&#34;}&#34;类=“wp-block-separator有-alpha-channel-opacity”&gt;&#xA;&#xA;&#xA;&#xA;&lt;h1 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;level&#34;:1,&#34;hash&#34;:&#34;6b84f248-da6a-49bb-b62f-f7dd17bc2a6a&#34;}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-conclusion&#34;&gt;结论&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;a500971f-f5d6-4ed9-9a7b-ef8139b1186f&#34;,&#34;dropCap&#34;:false}&#34;&gt;如果您有阅读这篇博文，以下是我们希望您从我们的旅程中获得的三个关键见解：&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;ol data-wp-block-name=&#34;core/list&#34; data-wp-block=&#34;{&#34;ordered&#34;:true,&#34;hash&#34;:&#34;53bb71d7-e45e-41ee-bc50-e5dd0c7e51d6&#34;,&#34;values&#34;:&#34; “}”&gt;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;增强云安全性&lt;/strong&gt;：我们已迁移超过 160 PB 的数据到全球气候系统。通过分层访问模型，人类身份无法直接访问 GCS 中存储的数据。 SAS 充当访问 GCS 上数据湖的身份验证和授权层。我们以云迁移为契机，加强了访问控制措施。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;通过多层缓存进行扩展&lt;/strong&gt;：我们开发了多层缓存策略并使用各种参数进行了多次迭代的严格基准测试/测试。超过 19% 的分析工作负载在 GCP 上运行，我们观察到 SAS 对峰值超过 500k rps 的 GCS 操作进行身份验证和授权。 SAS 实例本身看到的请求量远低于 10,000 RPS，而 GCP IAM/STS API 保持在 60 RPS 以下。多级缓存策略非常有效，帮助我们扩展了整个系统。&lt;/li&gt;&#xA;&#xA;&#xA;&#xA;&lt;li data-wp-block-name=&#34;core/list-item&#34; data-wp-block=&#34;[]&#34;&gt;&lt;strong&gt;无缝迁移支持：&lt;/strong&gt;我们明智地投资于标准化整个公司的 HDFS 客户端过去。我们使用标准 HDFS 客户端作为工具，让所有工作负载采用 SAS 客户端，该客户端通过适当的设计抽象出安全集成。通过这种方法，Uber 的数据平台用户不会意识到 HDFS 和 GCS 安全模型之间的任何差异，从而可以无缝迁移到云端。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;1eb3e610-f4b2-4696-a722-a3b40bd33ec5&#34;,&#34;dropCap&#34;:false}&#34;&gt;在初始期间在我们的云迁移项目的规划阶段，“安全集成”工作流被确定为高优先级。通过精心设计和开发，我们成功克服了技术挑战。随着 Uber 云迁移的继续，我们邀请您关注我们的旅程。我们将分享之前&lt;a href=&#34;https://www.uber.com/blog/modernizing-ubers-data-infrastruct-with-gcp/&#34; target=&#34;_blank 中讨论的其他关键工作流的更多技术细节“ rel=&#34;noreferrer noopener&#34;&gt;博客&lt;/a&gt;帖子。请继续关注更多见解和更新！&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;h2 data-wp-block-name=&#34;core/heading&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;cea16887-0078-4ec4-bff8-1e490e265ea7&#34;,&#34;level&#34;:2}&#34; class=&#34;wp -block-heading&#34; id=&#34;h-acknowledgements&#34;&gt;致谢&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;hash&#34;:&#34;2c6715fc-8fdc-47eb-92cd-ad6fd5700a57&#34;,&#34;dropCap&#34;:false}&#34;&gt;所描绘的作品如果没有与我们的 GCP 团队成员（Mandeep Singh Bawa、Julien Phalip、Shlok Karpathak、Namita Sharma、Arend Dittmer 和 Matthew Rahman）的密切合作，本文中的内容就不可能实现。他们的专业知识和支持对于设计和实施解决方案以实现我们的目标发挥了重要作用。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;136ce52a-dfe8-4f2a-90a8-dcc7de86122c&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;&lt;em&gt;标题图像属性&lt;/em&gt;：“&lt;a href=&#34;https://www.flickr.com/photos/111692634@N04/ 15855489588&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;安全云 – 数据安全 – 网络安全&lt;/a&gt;”图像由 &lt;a href=&#34;https://creativecommons.org/licenses/by-sa 覆盖/2.0/?ref=openverse&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;CC BY-SA 2.0&lt;/a&gt; 许可证并记入 &lt;a href=&#34;https://www.flickr.com/photos /111692634@N04&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;perspec_photo88&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;a5c37c34-c4fa-48d0-b04b-a4812d828c5d&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;&lt;em&gt;Apache&lt;sup&gt;®&lt;/sup&gt;、Apache Parquet™、Apache Hudi™、Apache Spark™、Apache Hadoop YARN™ 是注册商标或商标&lt;/em&gt;&lt;a href=&#34;http://www.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34;&gt;&lt;em&gt;Apache 软件基金会&lt;/em&gt;&lt;/a&gt;&lt;em &gt; 在美国和/或其他国家。使用这些标记并不暗示 Apache 软件基金会的认可。&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;核心/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;small&#34;,&#34;hash&#34;:&#34;4402444d-34c6-4e3a-b141-9f7403c0fe7f&#34;,&#34;dropCap&#34;:false}&#34; class=&#34;has-small-font -size&#34;&gt;&lt;em&gt;Presto&lt;/em&gt;&lt;em&gt;&lt;sup&gt;®&lt;/sup&gt;&lt;/em&gt;&lt;em&gt;是 Linux 基金会在美国和/或其他国家/地区的注册商标。无认可使用这些标记暗示了 Linux 基金会的支持。&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;p data-wp-block-name=&#34;core/paragraph&#34; data-wp-block=&#34;{&#34;fontSize&#34;:&#34;小&#34;,&#34;hash&#34;:&#34;5e92b08c-5f44-4f27-892e-8d9f88898e10&#34;,&#34;dropCap&#34; :false}&#34; class=&#34;has-small-font-size&#34;&gt;&lt;em&gt;GCP&lt;/em&gt;™&lt;em&gt;基础设施平台是 Google LLC 在美国和/或其他国家/地区的注册商标。使用这些标记并不暗示 Goggle LLC 的认可。&lt;/em&gt;&lt;em&gt;Kerberos 是麻省理工学院 (MIT) 在美国和/或其他国家/地区的商标&lt;/em&gt;&lt;em&gt; 。使用这些标记并不暗示麻省理工学院的认可。&lt;/em&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 30 Jul 2024 07:01:47 +0000</pubDate>
    </item>
  </channel>
</rss>